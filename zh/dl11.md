# 深度学习：第二部分第十一课

### 链接

[**论坛**](http://forums.fast.ai/t/part-2-lesson-11-in-class/14699/1) **/** [**视频**](https://youtu.be/tY0n9OT5_nA)

### 开始之前：

*   Sylvain Gugger的1循环[政策](https://sgugger.github.io/the-1cycle-policy.html) 。 基于Leslie Smith的新论文，该论文采用了前两篇关键论文（循环学习率和超级收敛），并在其基础上进行了大量实验，以展示如何实现超收敛。 超级收敛使您可以比以前的逐步方法快五倍地训练模型（并且比CLR快，尽管它不到五次）。 超级融合让你可以在1到3之间达到高学习率。超融合的有趣之处在于，你可以在相当大的一部分时期内以极高的学习率进行训练，在此期间，失去的不是真的非常好。 但诀窍在于它正在通过空间进行大量搜索，以找到看似真正具有普遍性的区域。 Sylvain通过刷新丢失的部分在fastai实施了它，然后确认他实际上在CIFAR10的训练上实现了超级融合。 它目前称为`use_clr_beta`但将来会重命名。 他还为fastai图书馆增添了周期性的动力。
*   [如何使用](https://towardsdatascience.com/how-to-create-data-products-that-are-magical-using-sequence-to-sequence-models-703f86a231f8) Hamel Husain的[序列到序列模型创建神奇的数据产品](https://towardsdatascience.com/how-to-create-data-products-that-are-magical-using-sequence-to-sequence-models-703f86a231f8) 。 他在博客上写了关于训练模型以总结GitHub问题的文章。 以下是基于他的博客创建的Kubeflow团队[演示](http://gh-demo.kubeflow.org/) 。

### 神经机器翻译[ [5:36](https://youtu.be/tY0n9OT5_nA%3Ft%3D5m36s) ]

让我们构建一个序列到序列的模型！ 我们将致力于机器翻译。 机器翻译已经存在了很长时间，但是我们将研究一种使用神经网络进行翻译的神经翻译方法。 神经机器翻译出现在几年前，它不如使用经典特征工程和标准NLP方法的统计机器翻译方法一样好，如词干，摆弄单词频率，n-gram等。一年后，它比其他一切都好。 它基于一个名为BLEU的度量标准 - 我们不会讨论该度量标准，因为它不是一个非常好的度量标准，并且它不是很有趣，但每个人都使用它。

![](../img/1_f0hoBLrTuevFPgAl-lFIfQ.png)

我们看到机器翻译开始沿着我们在2012年开始计算机视觉对象分类的路径开始，该路径刚刚超过了现有技术并且现在以极快的速度拉开了它。 任何观看此操作的人都不太可能真正构建机器翻译模型，因为[https://translate.google.com/](https://translate.google.com/)可以很好地运行。 那么我们为什么要学习机器翻译呢？ 我们学习机器翻译的原因在于，将法语中的某种输入作为句子并将其转换为任意长度的其他类型输出（例如英语句子）的一般想法是非常有用的。 例如，正如我们刚刚看到的那样，Hamel将GitHub问题转化为摘要。 另一个例子是拍摄视频并将其转换为描述，或者基本上是任何你正在吐出任意大小的输出的东西，这通常是一个句子。 也许进行CT扫描并吐出放射学报告 - 这是您可以使用序列来排序学习的地方。

#### 神经机器翻译的四大胜利[ [8:36](https://youtu.be/tY0n9OT5_nA%3Ft%3D8m36s) ]

![](../img/1_c2kAArVl9mF_VaeqnXafBw.png)

*   端到端训练：没有充分利用启发式和hacky功能工程。
*   我们能够构建这些分布式表示，这些表示由单个网络中的许多概念共享。
*   我们能够在RNN中使用长期状态，因此它比n-gram类型方法使用更多的上下文。
*   最后，我们生成的文本也使用RNN，因此我们可以构建更流畅的东西。

#### BiLSTMs（+ Attn）不仅适用于神经MT [ [9:20](https://youtu.be/tY0n9OT5_nA%3Ft%3D9m20s) ]

![](../img/1_LrzmE8Xi5-mwsUrqzEQA6Q.png)

我们将注意使用双向GRU（基本上与LSTM相同） - 如上所述，这些一般性想法也可以用于许多其他事情。

#### 让我们跳进代码[ [9:47](https://youtu.be/tY0n9OT5_nA%3Ft%3D9m47s) ]

[笔记本](https://github.com/fastai/fastai/blob/master/courses/dl2/translate.ipynb)

我们将尝试按照标准的神经网络方法将法语翻译成英语：

1.  数据
2.  建筑
3.  损失函数

#### 1.数据

像往常一样，我们需要`(x, y)`对。 在这种情况下，x：法语句子，y：英语句子，您将比较您的预测。 我们需要大量的这些法语句子元组及其等效的英语句子 - 被称为“平行语料库”，并且比语言模型的语料库更难找到。 对于语言模型，我们只需要某种语言的文本。 对于任何生活语言，至少有几千兆字节的文字漂浮在互联网上供你抓取。 对于翻译，有一些非常好的平行语料库可用于欧洲语言。 欧洲议会在每种欧洲语言中都有一句话。 任何流向联合国的东西都被翻译成许多语言。 对于法语到英语，我们有特别好的东西，几乎任何半官方的加拿大网站都有法语版和英文版[ [12:13](https://youtu.be/tY0n9OT5_nA%3Ft%3D12m13s) ]。

#### 翻译文件

```
 **from** **fastai.text** **import** * 
```

来自[http://www.statmt.org/wmt15/translation-task.html的](http://www.statmt.org/wmt15/translation-task.html)法语/英语并行文本。 它由Chris Callison-Burch创建，他抓取了数百万个网页，然后使用_一套简单的启发法将法语网址转换为英文网址（即将“fr”替换为“en”和其他约40个其他手写规则），并假设这些文件是彼此的翻译_ 。

```
 PATH = Path('data/translate')  TMP_PATH = PATH/'tmp'  TMP_PATH.mkdir(exist_ok= **True** )  fname='giga-fren.release2.fixed'  en_fname = PATH/f' **{fname}** .en'  fr_fname = PATH/f' **{fname}** .fr' 
```

对于边界框，所有有趣的东西都在损失函数中，但对于神经翻译，所有有趣的东西都将在他的体系结构中[ [13:01](https://youtu.be/tY0n9OT5_nA%3Ft%3D13m1s) ]。 让我们快速完成这一切，杰里米希望你特别考虑的事情之一就是我们正在做的任务以及我们如何在语言建模与神经翻译之间做到这一点的关系或相似之处。

![](../img/1_458KhA7uSET5eH3fe4EhDw.png)

第一步是完成我们在语言模型中所做的完全相同的事情，这是一个句子，并通过RNN [ [13:35](https://youtu.be/tY0n9OT5_nA%3Ft%3D13m35s) ]。

![](../img/1_ujJzFdJfk2jP0466peyyrQ.png)

现在有了分类模型，我们有了一个解码器，它接受了RNN输出并抓住了三个东西：所有时间步骤的`maxpool`和`meanpool` ，以及最后一步的RNN值，将所有这些叠加在一起并通过它线性层[ [14:24](https://youtu.be/tY0n9OT5_nA%3Ft%3D14m24s) ]。 大多数人不这样做，只使用最后一步，所以我们今天要讨论的所有事情都使用最后一步。

我们首先通过RNN清除输入句子，然后从中出现一些“隐藏状态”（即一些向量，表示编码句子的RNN的输出）。

#### 编码器≈骨干[ [15:18](https://youtu.be/tY0n9OT5_nA%3Ft%3D15m18s) ]

斯蒂芬使用“编码器”这个词，但我们倾向于使用“骨干”这个词。 就像我们谈到为现有模型添加自定义头部时一样，例如，现有的预先训练过的ImageNet模型，我们说这是我们的支柱，然后我们会在它上面坚持一些能够完成我们想要的任务的头部。 顺序学习序列，他们使用单词编码器，但它基本上是相同的东西 - 它是一个神经网络架构的一部分，它接受输入并将其转换为一些表示，然后我们可以在顶部粘贴更多层从我们为分类器中抓取一些东西，我们在分类器上堆叠一个线性层，将int变成情绪。 但这一次，我们有一些东西比创造情绪更难[ [16:12](https://youtu.be/tY0n9OT5_nA%3Ft%3D16m12s) ]。 我们不是将隐藏状态转变为正面或负面情绪，而是将其变成一系列令牌，其中令牌序列是斯蒂芬的例子中的德语句子。

这听起来更像是语言模型而不是分类器，因为语言有多个令牌（对于每个输入词，都有一个输出词）。 但语言模型也更容易，因为语言模型输出中的令牌数量与语言模型输入中的令牌数量相同。 它们不仅长度相同，而且它们完全匹配（例如，在第一个词出现第二个词之后，第二个词出现第三个词，依此类推）。 对于翻译语言，您不一定知道单词“he”将被翻译为输出中的第一个单词（不幸的是，在这种特殊情况下）。 通常情况下，主题对象顺序会有所不同，或者会插入一些额外的单词，或者我们需要添加一些性别文章等一些代词。这是我们要处理的关键问题是我们有一个任意长度的输出，其中输出中的标记不对应于输入中的相同顺序或特定标记[ [17:31](https://youtu.be/tY0n9OT5_nA%3Ft%3D17m31s) ]。 但总体思路是一样的。 这是一个对输入进行编码的RNN，将其转换为某种隐藏状态，然后我们要学习的新事物就是生成一个序列输出。

#### 序列输出[ [17:47](https://youtu.be/tY0n9OT5_nA%3Ft%3D17m47s) ]

我们已经知道了：

*   序列到类（IMDB分类器）
*   序列到等长序列（语言模型）

但是我们还不知道如何做一个通用序列来排序，所以这就是今天的新事物。 除非你真正理解第6课RNN是如何工作的，否则这一点很有意义。

#### 快速回顾[第6课](https://medium.com/%40hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c) [ [18:20](https://youtu.be/tY0n9OT5_nA%3Ft%3D18m20s) ]

我们了解到RNN的核心是标准的完全连接网络。 下面是一个有4层 - 一个输入并通过四层，但在第二层，它连接在第二个输入，第三层连接在第三个输入，但我们实际上在Python中写了这只是一个四层神经网络。 除线性图层和ReLU之外，我们没有使用任何其他内容。 每当输入进入时我们使用相同的权重矩阵，每当我们从一个隐藏状态进入下一个状态时我们使用相同的矩阵 - 这就是为什么这些箭头是相同的颜色。

![](../img/1_ZTtw8vtjy-K2CptW_xpLqw.png)

我们可以像下面[ [19:29](https://youtu.be/tY0n9OT5_nA%3Ft%3D19m29s) ]重新绘制上面的图表。

![](../img/1_QE70fqvLLDhq8fq_ctxpZQ.png)

我们不仅重新绘制它，而且我们在PyTorch中使用了四行线性线性线性线性代码，我们用for循环替换它。 记住，我们有一些与下面完全相同的东西，但它只有四行代码说`self.l_in(input)` ，我们用for循环替换它，因为这很好重构。 不改变任何数学，任何想法或任何输出的重构是RNN。 它将代码中的一堆独立行转换为Python for循环。

![](../img/1_ewV_N6jZBjStFNpSCMndxg.png)

我们可以获取输出，使其不在循环之外并将其放入循环[ [20:25](https://youtu.be/tY0n9OT5_nA%3Ft%3D20m25s) ]。 如果我们这样做，我们现在将为每个输入生成一个单独的输出。 上面的代码，隐藏状态每次都被替换，我们最终只是吐出最后的隐藏状态。 但是，如果相反，我们有一些东西说`hs.append(h)`并在最后返回`hs` ，这将是下图。

![](../img/1_CX45skUFZZO6uHsR8IndzA.png)

要记住的主要事情是，当我们说隐藏状态时，我们指的是一个向量 - 技术上是小批量中每个东西的向量，所以它是一个矩阵，但通常当杰里米谈到这些东西时，他忽略了小批量只需一件物品即可。

![](../img/1_ch4De-RThVp-fthGpqsaWw.png)

我们还了解到你可以将这些层叠在一起[ [21:41](https://youtu.be/tY0n9OT5_nA%3Ft%3D21m41s) ]。 因此，不是左边的RNN（在上图中）吐出输出，它们只能将输入吐出到第二个RNN中。 如果你正在考虑这一点“我想我理解这一点，但我不太确定”这意味着你不理解这一点。 你知道自己真正了解它的唯一方法就是在PyTorch或Numpy中从头开始编写。 如果你不能这样做，那么你知道你不理解它，你可以回去重新观看第6课，看看笔记本并复制一些想法，直到你可以。 从头开始编写代码非常重要 - 它不仅仅是代码屏幕。 因此，您需要确保可以创建2层RNN。 下面是展开它的样子。

![](../img/1_2GKBK9P_zpUieQF6JFyChw.png)

为了得到我们有（x，y）对句子的点，我们将从下载数据集[ [22:39](https://youtu.be/tY0n9OT5_nA%3Ft%3D22m39s) ]开始。 训练翻译模型需要很长时间。 谷歌的翻译模型有八层RNN叠加在一起。 八层和两层之间没有概念上的区别。 如果您是Google，并且您拥有的GPU或TPU比您知道的更多，那么您可以这样做。 在其他情况下，在我们的情况下，很可能我们正在构建的序列模型的序列类型不需要那么级别的计算。 所以为了简单[起见](https://youtu.be/tY0n9OT5_nA%3Ft%3D23m22s) [ [23:22](https://youtu.be/tY0n9OT5_nA%3Ft%3D23m22s) ]，让我们做一个简单的事情，而不是学习如何将法语翻译成英语用于任何句子，让我们学习将法语问题翻译成英语问题 - 特别是从什么/哪里开始的问题/哪个/时。 所以这是一个正则表达式，它寻找以“wh”开头并以问号结尾的内容。

```
 re_eq = re.compile('^(Wh[^?.!]+\?)')  re_fq = re.compile('^([^?.!]+\?)') 
```

```
 lines = ((re_eq.search(eq), re_fq.search(fq))  **for** eq, fq **in** zip(open(en_fname, encoding='utf-8'),  open(fr_fname, encoding='utf-8'))) 
```

```
 qs = [(e.group(), f.group()) **for** e,f **in** lines **if** e **and** f] 
```

我们通过语料库[ [23:43](https://youtu.be/tY0n9OT5_nA%3Ft%3D23m43s) ]，打开两个文件中的每一个，每行是一个平行文本，将它们压缩在一起，抓住英语问题和法语问题，并检查它们是否与正则表达式匹配。

```
 pickle.dump(qs, (PATH/'fr-en-qs.pkl').open('wb'))  qs = pickle.load((PATH/'fr-en-qs.pkl').open('rb')) 
```

把它作为一个泡菜倾倒，所以我们不必再这样做，所以现在我们有52,000个句子对，这里有一些例子：

```
 qs[:5], len(qs) 
```

```
 _([('What is light ?', 'Qu'est-ce que la lumière?'),_  _('Who are we?', 'Où sommes-nous?'),_  _('Where did we come from?', "D'où venons-nous?"),_  _('What would we do without it?', 'Que ferions-nous sans elle ?'),_  _('What is the absolute location (latitude and longitude) of Badger, Newfoundland and Labrador?',_  _'Quelle sont les coordonnées (latitude et longitude) de Badger, à Terre-Neuve-etLabrador?')],_  _52331)_ 
```

关于这一点的一个[好处](https://youtu.be/tY0n9OT5_nA%3Ft%3D24m8s)是什么/谁/哪里类型问题往往相当短[ [24:08](https://youtu.be/tY0n9OT5_nA%3Ft%3D24m8s) ]。 但我们可以从头开始学习，而不是先前对语言的概念有所了解的想法，更不用说英语或法语了，我们可以创建一些可以将任意一个问题翻译成另一个只有50k句子的话，这听起来像一个非常难以理解的事情要求这样做。 如果我们能够取得任何进展，那将是令人印象深刻的。 这是一项非常少的数据，可以进行非常复杂的练习。

`qs`包含法语和英语的元组[ [24:48](https://youtu.be/tY0n9OT5_nA%3Ft%3D24m48s) ]。 您可以使用这个方便的习惯用法将它们分成英语问题列表和法语问题列表。

```
 en_qs,fr_qs = zip(*qs) 
```

然后我们将英语问题标记出来并将法语问题标记化。 所以请记住，只是意味着将它们分成单独的单词或类似单词的东西。 默认情况下[ [25:11](https://youtu.be/tY0n9OT5_nA%3Ft%3D25m11s) ]，我们在这里使用的标记器（记住这是一个围绕spaCy标记器的包装器，这是一个很棒的标记器）假设是英文。 所以要求法语，你只需添加一个额外的参数`'fr'` 。 第一次执行此操作时，您将收到一条错误消息，指出您没有安装spaCy French模型，因此您可以运行`python -m spacy download fr`来获取法语模型。

```
 en_tok = Tokenizer.proc_all_mp(partition_by_cores(en_qs)) 
```

```
 fr_tok = Tokenizer.proc_all_mp(partition_by_cores(fr_qs), 'fr') 
```

你不可能在这里遇到RAM问题，因为这不是特别大的语料库，但有些学生在本周试图训练一种新的语言模型并且有RAM问题。 如果你这样做，那么值得知道这些函数（ `proc_all_mp` ）实际上在做什么。 `proc_all_mp`正在处理多个进程中的每个句子[ [25:59](https://youtu.be/tY0n9OT5_nA%3Ft%3D25m59s) ]：

![](../img/1_3dijGYRXl1Vf9MFLD5AUOA.png)

上面的函数找出你有多少CPU，除以2（因为通常使用超线程它们实际上并不都是并行工作），然后并行运行这个`proc_all`函数。 因此，这将为您拥有的每个CPU吐出一个完整的Python进程。 如果你有很多内核，那就是很多Python进程 - 每个人都会加载所有这些数据，这可能会耗尽你所有的内存。 所以你可以用`proc_all`替换它而不是`proc_all_mp`来使用更少的RAM。 或者你可以使用更少的核心。 目前，我们正在调用`partition_by_cores` ，它调用列表中的`partition` ，并根据您拥有的CPU数量要求将其拆分为多个相等长度的内容。 因此，您可以将其替换为拆分为较小的列表，并在较少的事情上运行它。

![](../img/1_9_D6dkXM4mR8fPf0E2eLcg.png)

将英语和法语标记化后，您可以看到它如何分裂[ [28:04](https://youtu.be/tY0n9OT5_nA%3Ft%3D28m4s) ]：

```
 en_tok[0], fr_tok[0] 
```

```
 (['what', 'is', 'light', '?'],  ['qu'', 'est', '-ce', 'que', 'la', 'lumière', '?']) 
```

你可以看到法语的标记化看起来完全不同，因为法国人喜欢他们的撇号和连字符。 因此，如果您尝试使用英语标记符来表示法语句子，那么您将获得非常糟糕的结果。 你不需要知道大量的NLP思想来使用NLP的深度学习，但只是为你的语言使用正确的标记化器这些基本的东西很重要[ [28:23](https://youtu.be/tY0n9OT5_nA%3Ft%3D28m23s) ]。 本周我们学习小组的一些学生一直在尝试为中文实例建立语言模型，当然这些模型并没有真正具有标记器的概念，所以我们一直在开始研究[句子](https://github.com/google/sentencepiece) 。将事物分成任意子字单元，所以当Jeremy说令牌化时，如果你使用的语言没有空格，你可能应该检查句子或其他类似的子词单位。 希望在接下来的一两周内，我们将能够用中文报​​告这些实验的早期结果。

```
 np.percentile([len(o) **for** o **in** en_tok], 90),  np.percentile([len(o) **for** o **in** fr_tok], 90) 
```

```
 _(23.0, 28.0)_ 
```

```
 keep = np.array([len(o)<30 **for** o **in** en_tok]) 
```

```
 en_tok = np.array(en_tok)[keep]  fr_tok = np.array(fr_tok)[keep] 
```

```
 pickle.dump(en_tok, (PATH/'en_tok.pkl').open('wb'))  pickle.dump(fr_tok, (PATH/'fr_tok.pkl').open('wb')) 
```

```
 en_tok = pickle.load((PATH/'en_tok.pkl').open('rb'))  fr_tok = pickle.load((PATH/'fr_tok.pkl').open('rb')) 
```

因此将其标记化[ [29:25](https://youtu.be/tY0n9OT5_nA%3Ft%3D29m25s) ]，我们将其保存到磁盘。 然后记住，我们创建令牌后的下一步是将它们变成数字。 要做到这一点，我们有两个步骤 - 第一步是获取所有出现的单词的列表，然后我们将每个单词转换为索引。 如果出现超过40,000个单词，那么让我们将其剪掉，这样就不会太疯狂了。 我们为流（ `_bos_` ），填充（ `_pad_` ），流尾（ `_eos_` ）和未知（ `_unk` ）的开头插入了一些额外的令牌。 因此，如果我们试图查找不在40,000最常见的东西，那么我们使用`deraultdict`返回3，这是未知的。

```
 **def** toks2ids(tok,pre):  freq = Counter(p **for** o **in** tok **for** p **in** o)  itos = [o **for** o,c **in** freq.most_common(40000)]  itos.insert(0, '_bos_')  itos.insert(1, '_pad_')  itos.insert(2, '_eos_')  itos.insert(3, '_unk')  stoi = collections.defaultdict( **lambda** : 3,  {v:k **for** k,v **in** enumerate(itos)})  ids = np.array([([stoi[o] **for** o **in** p] + [2]) **for** p **in** tok])  np.save(TMP_PATH/f' **{pre}** _ids.npy', ids)  pickle.dump(itos, open(TMP_PATH/f' **{pre}** _itos.pkl', 'wb'))  **return** ids,itos,stoi 
```

现在我们可以继续将每个标记转换为ID，方法是将它通过字符串`stoi`我们刚刚创建的整数字典（ `stoi` ）中，然后在结尾处添加数字2，这是流的结尾。 你在这里看到的代码是Jeremy在迭代和试验时写的代码[ [30:25](https://youtu.be/tY0n9OT5_nA%3Ft%3D30m25s) ]。 因为他在迭代和实验时编写的代码中有99％证明是完全错误或愚蠢或令人尴尬，而你却无法看到它。 但是，当他写这篇文章时，没有点重构并使它变得美丽，所以他希望你能看到他所拥有的所有小捷径。 而不是为`_eos_`标记使用一些常量并使用它，当他进行原型设计时，他只是做了简单的事情。 并非如此，他最终会破坏代码，但他试图在美丽的代码和有效的代码之间找到一些中间立场。

**问** ：刚听到他提到我们将CPU的数量除以2，因为使用超线程，我们无法使用所有超线程内核加速。 这是基于实际经验还是有一些潜在的原因导致我们不能获得额外的加速[ [31:18](https://youtu.be/tY0n9OT5_nA%3Ft%3D31m18s) ]？ 是的，这只是实际经验而且并非所有事情都像这样，但我确实注意到了令牌化 - 超线程似乎让事情变得缓慢。 此外，如果我使用所有内核，通常我想同时做其他事情（比如运行一些交互式笔记本），我没有任何空余的空间来做这件事。

现在我们的英语和法语，我们可以获取ID列表`en_ids` [ [32:01](https://youtu.be/tY0n9OT5_nA%3Ft%3D32m1s) ]。 当我们这样做时，当然，我们需要确保我们也存储词汇。 如果我们不知道数字5代表什么是没有任何意义，那就没有数字5.这就是我们的词汇表`en_itos`和反向映射`en_stoi` ，我们可以用来转换未来更多的语料库。

```
 en_ids,en_itos,en_stoi = toks2ids(en_tok,'en')  fr_ids,fr_itos,fr_stoi = toks2ids(fr_tok,'fr') 
```

只是为了确认它是否正常工作，我们可以遍历每个ID，将int转换为字符串，然后将其吐出来 - 现在我们已经将句子返回到末尾的流结束标记。 我们的英语词汇是17,000，而我们的法语词汇是25,000，所以这不是太大，也不是我们正在处理的太复杂的词汇。

```
 **def** load_ids(pre):  ids = np.load(TMP_PATH/f' **{pre}** _ids.npy')  itos = pickle.load(open(TMP_PATH/f' **{pre}** _itos.pkl', 'rb'))  stoi = collections.defaultdict( **lambda** : 3,  {v:k **for** k,v **in** enumerate(itos)})  **return** ids,itos,stoi 
```

```
 en_ids,en_itos,en_stoi = load_ids('en')  fr_ids,fr_itos,fr_stoi = load_ids('fr') 
```

```
 [fr_itos[o] **for** o **in** fr_ids[0]], len(en_itos), len(fr_itos) 
```

```
 _(['qu'', 'est', '-ce', 'que', 'la', 'lumière', '?', '_eos_'], 17573, 24793)_ 
```

#### 单词向量[ [32:53](https://youtu.be/tY0n9OT5_nA%3Ft%3D32m53s) ]

本周我们在论坛上花了很多时间讨论无意义的单词向量是如何以及如何停止对它们如此兴奋 - 现在我们将使用它们。 为什么？ 我们一直在学习使用语言模型和预训练的适当模型而不是预先训练的线性单层（这是单词向量）的所有内容同样适用于序列到序列。 但杰里米和塞巴斯蒂安开始关注这一点。 对于任何有兴趣创造一些真正新的高度可发表结果的人来说，有一个完整的事情，用预先训练的语言模型排序的整个序列区域还没有被触及。 杰里米认为它会和分类一样好。 如果你正在努力解决这个问题，那么你就会发现一些令人兴奋的东西并希望帮助发布它，Jeremy非常乐意帮助共同撰写论文。 因此，当您有一些有趣的结果时，请随时与我们联系。

在这个阶段，我们没有任何这个，所以我们将使用很少的fastai [ [34:14](https://youtu.be/tY0n9OT5_nA%3Ft%3D34m14s) ]。 我们所拥有的只是单词向量 - 所以让我们至少使用体面的单词向量。 Word2vec是非常古老的单词词向量。 现在有更好的单词向量和fast.text是一个非常好的单词向量源。 有数百种语言可供他们使用，您的语言可能会被表示出来。

fasttext字向量可从[https://fasttext.cc/docs/en/english-vectors.html获得](https://fasttext.cc/docs/en/english-vectors.html)

pytext中没有fasttext Python库，但这是一个方便的技巧[ [35:03](https://youtu.be/tY0n9OT5_nA%3Ft%3D35m3s) ]。 如果有一个GitHub存储库中有一个setup.py和reqirements.txt，你可以在开始时查看`git+`然后将其粘贴到你的`pip install`并且它可以工作。 几乎没有人知道这一点，如果你去快照回购，他们不会告诉你这个 - 他们会说你必须下载它并插入它并等等但你没有。 你可以运行这个：

```
 _# !_ _pip install git+https://github.com/facebookresearch/fastText.git_ 
```

```
 **import** **fastText** **as** **ft** 
```

要使用fastText库，您需要为您的语言下载[fasttext字向量](https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md) （下载'bin plus text'文件）。

```
 en_vecs = ft.load_model(str((PATH/'wiki.en.bin'))) 
```

```
 fr_vecs = ft.load_model(str((PATH/'wiki.fr.bin'))) 
```

以上是我们的英语和法语模型。 有文本版本和二进制版本。 二进制版本更快，所以我们将使用它。 文本版本也有点儿麻烦。 我们将把它转换成标准的Python字典，以便更容易使用[ [35:55](https://youtu.be/tY0n9OT5_nA%3Ft%3D35m55s) ]。 这只是通过字典理解来完成每个单词并将其保存为pickle字典：

```
 **def** get_vecs(lang, ft_vecs):  vecd = {w:ft_vecs.get_word_vector(w)  **for** w **in** ft_vecs.get_words()}  pickle.dump(vecd, open(PATH/f'wiki. **{lang}** .pkl','wb'))  **return** vecd 
```

```
 en_vecd = get_vecs('en', en_vecs)  fr_vecd = get_vecs('fr', fr_vecs) 
```

```
 en_vecd = pickle.load(open(PATH/'wiki.en.pkl','rb'))  fr_vecd = pickle.load(open(PATH/'wiki.fr.pkl','rb')) 
```

```
 ft_words = ft_vecs.get_words(include_freq= **True** )  ft_word_dict = {k:v **for** k,v **in** zip(*ft_words)}  ft_words = sorted(ft_word_dict.keys(),  key= **lambda** x: ft_word_dict[x]) 
```

现在我们有了泡菜字典，我们可以继续查找一个单词，例如逗号[ [36:07](https://youtu.be/tY0n9OT5_nA%3Ft%3D36m7s) ]。 这将返回一个向量。 向量的长度是这组单词向量的维数。 在这种情况下，我们有300维英语和法语单词向量。

```
 dim_en_vec = len(en_vecd[','])  dim_fr_vec = len(fr_vecd[','])  dim_en_vec,dim_fr_vec 
```

```
 _(300, 300)_ 
```

由于您将在稍后看到的原因，我们还想知道我们的向量的平均值和标准偏差是什么。 因此平均值约为零，标准差约为0.3。

```
 en_vecs = np.stack(list(en_vecd.values()))  en_vecs.mean(),en_vecs.std() 
```

```
 _(0.0075652334, 0.29283327)_ 
```

#### 模型数据[ [36:48](https://youtu.be/tY0n9OT5_nA%3Ft%3D36m48s) ]

通常语料库具有相当长的序列长度分布，并且它是最长的序列，往往会压倒事情需要多长时间，使用多少内存等等。因此在这种情况下，我们将获得英语的第99百分位数到第97百分位数和法语并将它们截断到那个数量。 最初Jeremy使用了90个百分位数（因此变量名称）：

```
 enlen_90 = int(np.percentile([len(o) **for** o **in** en_ids], 99))  frlen_90 = int(np.percentile([len(o) **for** o **in** fr_ids], 97))  enlen_90,frlen_90 
```

```
 _(29, 33)_ 
```

我们[快到](https://youtu.be/tY0n9OT5_nA%3Ft%3D37m24s)了[ [37:24](https://youtu.be/tY0n9OT5_nA%3Ft%3D37m24s) ]。 我们已经获得了我们的标记化，数字化的英语和法语数据集。 我们有一些单词向量。 所以现在我们需要为PyTorch做好准备。 PyTorch需要一个`Dataset`对象，希望现在可以说数据集对象需要两个东西 - 长度（ `__len__` ）和索引器（ `__getitem__` ）。 Jeremy开始编写`Seq2SeqDataset` ，结果证明它只是一个通用的`Dataset` [ [37:52](https://youtu.be/tY0n9OT5_nA%3Ft%3D37m52s) ]。

```
 en_ids_tr = np.array([o[:enlen_90] **for** o **in** en_ids])  fr_ids_tr = np.array([o[:frlen_90] **for** o **in** fr_ids]) 
```

```
 **class** **Seq2SeqDataset** (Dataset):  **def** __init__(self, x, y): self.x,self.y = x,y  **def** __getitem__(self, idx): **return** A(self.x[idx], self.y[idx])  **def** __len__(self): **return** len(self.x) 
```

*   `A` ：阵列。 它将通过你传递它的每一个东西，如果它不是一个numpy数组，它会转换成一个numpy数组，并返回一个你传递它的所有东西的元组，现在保证是numpy数组[ [38 ：32](https://youtu.be/tY0n9OT5_nA%3Ft%3D38m32s) ]。
*   `V` ：变量
*   `T` ：Tensors

#### 训练集和验证集[ [39:03](https://youtu.be/tY0n9OT5_nA%3Ft%3D39m3s) ]

现在我们需要获取我们的英语和法语ID并获得训练集和验证集。 关于互联网上很多代码令人非常失望的事情之一就是他们没有遵循一些简单的最佳实践。 例如，如果你去PyTorch网站，他们有一个序列到序列翻译的例子部分。 他们的示例没有单独的验证集。 Jeremy根据他们的设置尝试了训练，并使用验证装置对其进行了测试，结果发现它大量过度。 所以这不仅仅是一个理论问题 - 实际的PyTorch repo具有序列翻译实例的实际官方序列，它不会检查过拟合和过拟合[ [39:41](https://youtu.be/tY0n9OT5_nA%3Ft%3D39m41s) ]。 此外，它无法使用迷你批次，因此它实际上无法利用任何PyTorch的效率。 即使您在官方PyTorch回购中找到代码，也不要认为它有任何好处。 你会注意到的另一件事是，几乎所有其他序列模型Jeremy在PyTorch中在互联网上的任何地方找到的都清楚地复制了那个糟糕的PyTorch仓库，因为它们都有相同的变量名，它有相同的问题，它有同样的错误。

另一个例子是Jeremy发现的几乎每个PyTorch卷积神经网络都没有使用自适应汇集层[ [40:27](https://youtu.be/tY0n9OT5_nA%3Ft%3D40m27s) ]。 换句话说，最后一层总是平均池（7,7）。 他们假设前一层是7乘7，如果你使用任何其他大小的输入，你会得到一个例外，因此几乎所有Jeremy所说的使用PyTorch的人认为CNN的基本限制是它们与输入相关联尺寸，自VGG以来一直没有。 因此，每当Jeremy抓住一个新模型并将其粘贴在fastai repo中时，他必须去搜索“pool”并在开始时添加“adaptive”，并用1替换7，现在它适用于任何大小的对象。 所以要小心。 它还处于早期阶段并且信不信由你，尽管你们大多数人在去年开始了深度学习之旅，但你们对许多重要的实践方面的了解远远超过绝大多数人在官方回购中出版和撰写文章。 因此，在阅读其他人的代码时，您需要比自己预期的更自信。 如果你发现自己认为“看起来很奇怪”，那不一定是你。

如果您正在查看的回购没有关于它的部分说这里是我们所做的测试，我们得到了与应该实施的论文相同的结果，这几乎可以肯定意味着它们没有得到相同的结果他们正在实施的论文，但可能还没有检查[ [42:13](https://youtu.be/tY0n9OT5_nA%3Ft%3D42m13s) ]。 如果你运行它，肯定不会得到那些结果，因为第一次很难把事情弄好 - Jeremy 12需要它。 如果他们没有测试过一次，那几乎肯定是行不通的。

这是获得训练和验证集的简单方法[ [42:45](https://youtu.be/tY0n9OT5_nA%3Ft%3D42m45s) ]。 抓取一堆随机数 - 每行数据一行，看看它们是否大于0.1。 这会给你一个布尔列表。 使用该布尔列表索引到您的数组中以获取训练集，使用与布尔列表相反的索引到该数组中以获得验证集。

```
 np.random.seed(42)  trn_keep = np.random.rand(len(en_ids_tr))>0.1  en_trn,fr_trn = en_ids_tr[trn_keep],fr_ids_tr[trn_keep]  en_val,fr_val = en_ids_tr[~trn_keep],fr_ids_tr[~trn_keep]  len(en_trn),len(en_val) 
```

```
 _(45219, 5041)_ 
```

Now we can create our dataset with our X's and Y's (ie French and English)[ [43:12](https://youtu.be/tY0n9OT5_nA%3Ft%3D43m12s) ]. If you want to translate instead English to French, switch these two around and you're done.

```
 trn_ds = Seq2SeqDataset(fr_trn,en_trn)  val_ds = Seq2SeqDataset(fr_val,en_val) 
```

Now we need to create DataLoaders [ [43:22](https://youtu.be/tY0n9OT5_nA%3Ft%3D43m22s) ]. We can just grab our data loader and pass in our dataset and batch size. We actually have to transpose the arrays — we won't go into the details about why, but we can talk about it during the week if you're interested but have a think about why we might need to transpose their orientation. Since we've already done all the pre-processing, there is no point spawning off multiple workers to do augmentation, etc because there is no work to do. So `making num_workers=1` will save you some time. We have to tell it what our padding index is — that is pretty important because what's going to happen is that we've got different length sentences and fastai will automatically stick them together and pad the shorter ones so that they are all equal length. Remember a tensor has to be rectangular.

```
 bs=125 
```

```
 trn_samp = SortishSampler(en_trn, key= lambda x: len(en_trn[x]),  bs=bs)  val_samp = SortSampler(en_val, key= lambda x: len(en_val[x])) 
```

```
 trn_dl = DataLoader(trn_ds, bs, transpose= True , transpose_y= True ,  num_workers=1, pad_idx=1, pre_pad= False ,  sampler=trn_samp)  val_dl = DataLoader(val_ds, int(bs*1.6), transpose= True ,  transpose_y= True , num_workers=1, pad_idx=1,  pre_pad= False , sampler=val_samp)  md = ModelData(PATH, trn_dl, val_dl) 
```

In the decoder in particular, we want our padding to be at the end, not at the start [ [44:29](https://youtu.be/tY0n9OT5_nA%3Ft%3D44m29s) ]:

*   Classifier → padding in the beginning. Because we want that final token to represent the last word of the movie review.
*   Decoder → padding at the end. As you will see, it actually is going to work out a bit better to have the padding at the end.

**Sampler [** [**44:54**](https://youtu.be/tY0n9OT5_nA%3Ft%3D44m54s) **]** Finally, since we've got sentences of different lengths coming in and they all have to be put together in a mini-batch to be the same size by padding, we would much prefer that the sentences in a mini-batch are of similar sizes already. Otherwise it is going to be as long as the longest sentence and that is going to end up wasting time and memory. Therefore, we are going to use the sampler tricks that we learnt last time which is the validation set, we are going to ask it to sort everything by length first. Then for the training set, we are going to randomize the order of things but to roughly make it so that things of similar length are about in the same spot.

**Model Data [** [**45:40**](https://youtu.be/tY0n9OT5_nA%3Ft%3D45m40s) **]** At this point, we can create a model data object — remember a model data object really does one thing which is it says “I have a training set and a validation set, and an optional test set” and sticks them into a single object. We also has a path so that it has somewhere to store temporary files, models, stuff like that.

We are not using fastai for very much at all in this example. We used PyTorch compatible Dataset and and DataLoader — behind the scene it is actually using the fastai version because we need it to do the automatic padding for convenience, so there is a few tweaks in fastai version that are a bit faster and a bit more convenient. We are also using fastai's Samplers, but there is not too much going on here.

#### Architecture [ [46:59](https://youtu.be/tY0n9OT5_nA%3Ft%3D46m59s) ]

![](../img/1_IMBl2Aiclyt6PCrg1IQg5A.png)

*   The architecture is going to take our sequence of tokens.
*   It is going to spit them into an encoder (aka backbone).
*   That is going to spit out the final hidden state which for each sentence, it's just a single vector.

None of this is going to be new [ [47:41](https://youtu.be/tY0n9OT5_nA%3Ft%3D47m41s) ]. That is all going to be using very direct simple techniques that we've already learned.

*   Then we are going to take that, and we will spit it into a different RNN which is a decoder. That's going to have some new stuff because we need something that can go through one word at a time. And it keeps going until it thinks it's finished the sentence. It doesn't know how long the sentence is going to be ahead of time. It keeps going until it thinks it's finished the sentence and then it stops and returns a sentence.

```
 def create_emb(vecs, itos, em_sz):  emb = nn.Embedding(len(itos), em_sz, padding_idx=1)  wgts = emb.weight.data  miss = []  for i,w in enumerate(itos):  try : wgts[i] = torch.from_numpy(vecs[w]*3)  except : miss.append(w)  print(len(miss),miss[5:10])  return emb 
```

```
 nh,nl = 256,2 
```

Let's start with the encoder [ [48:15](https://youtu.be/tY0n9OT5_nA%3Ft%3D48m15s) ]. In terms of the variable naming here, there is identical attributes for encoder and decoder. The encoder version has `enc` the decoder version has `dec` .

*   `emb_enc` : Embeddings for the encoder
*   `gru` : RNN. GRU and LSTM are nearly the same thing.

We need to create an embedding layer because remember — what we are being passed is the index of the words into a vocabulary. And we want to grab their fast.text embedding. Then over time, we might want to also fine tune to train that embedding end-to-end.

`create_emb` [ [49:37](https://youtu.be/tY0n9OT5_nA%3Ft%3D49m37s) ]: It is important that you know now how to set the rows and columns for your embedding so the number of rows has to be equal to your vocabulary size — so each vocabulary has a word vector. The size of the embedding is determined by fast.text and fast.text embeddings are size 300\. So we have to use size 300 as well otherwise we can't start out by using their embeddings.

`nn.Embedding` will initially going to give us a random set of embeddings [ [50:12](https://youtu.be/tY0n9OT5_nA%3Ft%3D50m12s) ]. So we will go through each one of these and if we find it in fast.text, we will replace it with the fast.text embedding. Again, something you should already know is that ( `emb.weight.data` ):

*   A PyTorch module that is learnable has `weight` attribute
*   `weight` attribute is a `Variable` that has `data` attribute
*   The `data` attribute is a tensor

Now that we've got our weight tensor, we can just go through our vocabulary and we can look up the word in our pre-trained vectors and if we find it, we will replace the random weights with that pre-trained vector [ [52:35](https://youtu.be/tY0n9OT5_nA%3Ft%3D52m35s) ]. The random weights have a standard deviation of 1\. Our pre-trained vectors has a standard deviation of about 0.3\. So again, this is the kind of hacky thing Jeremy does when he is prototyping stuff, he just multiplied it by 3\. By the time you see the video of this, we may able to put all this sequence to sequence stuff into the fastai library, you won't find horrible hacks like that in there (sure hope). But hack away when you are prototyping. Some things won't be in fast.text in which case, we'll just keep track of it [ [53:22](https://youtu.be/tY0n9OT5_nA%3Ft%3D53m22s) ]. The print statement is there so that we can see what's going on (ie why are we missing stuff?). Remember we had about 30,000 so we are not missing too many.

```
 3097 ['l'', "d'", 't_up', 'd'', "qu'"] 
 1285 ["'s", ''s', "n't", 'n't', ':'] 
```

Jeremy has started doing some stuff around incorporating large vocabulary handling into fastai — it's not finished yet but hopefully by the time we get here, this kind of stuff will be possible [ [56:50](https://youtu.be/tY0n9OT5_nA%3Ft%3D56m50s) ].

```
 class Seq2SeqRNN (nn.Module):  def __init__(self, vecs_enc, itos_enc, em_sz_enc, vecs_dec,  itos_dec, em_sz_dec, nh, out_sl, nl=2):  super().__init__()  self.nl,self.nh,self.out_sl = nl,nh,out_sl  self.emb_enc = create_emb(vecs_enc, itos_enc, em_sz_enc)  self.emb_enc_drop = nn.Dropout(0.15)  self.gru_enc = nn.GRU(em_sz_enc, nh, num_layers=nl,  dropout=0.25)  self.out_enc = nn.Linear(nh, em_sz_dec, bias= False )  self.emb_dec = create_emb(vecs_dec, itos_dec, em_sz_dec)  self.gru_dec = nn.GRU(em_sz_dec, em_sz_dec, num_layers=nl,  dropout=0.1)  self.out_drop = nn.Dropout(0.35)  self.out = nn.Linear(em_sz_dec, len(itos_dec))  self.out.weight.data = self.emb_dec.weight.data  def forward(self, inp):  sl,bs = inp.size()  h = self.initHidden(bs)  emb = self.emb_enc_drop(self.emb_enc(inp))  enc_out, h = self.gru_enc(emb, h)  h = self.out_enc(h)  dec_inp = V(torch.zeros(bs).long())  res = []  for i in range(self.out_sl):  emb = self.emb_dec(dec_inp).unsqueeze(0)  outp, h = self.gru_dec(emb, h)  outp = self.out(self.out_drop(outp[0]))  res.append(outp)  dec_inp = V(outp.data.max(1)[1])  if (dec_inp==1).all(): break  return torch.stack(res)  def initHidden(self, bs):  return V(torch.zeros(self.nl, bs, self.nh)) 
```

The key thing to know is that encoder takes our inputs and spits out a hidden vector that hopefully will learn to contain all of the information about what that sentence says and how it sets it [ [58:49](https://youtu.be/tY0n9OT5_nA%3Ft%3D58m49s) ]. If it can't do that, we can't feed it into a decoder and hope it to spit our our sentence in a different language. So that's what we want it to learn to do. We are not going to do anything special to make it learn to do that — we are just going to do the three things (data, architecture, loss function) and cross our fingers.

**Decoder [** [**59:58**](https://youtu.be/tY0n9OT5_nA%3Ft%3D59m58s) **]** : How do we now do the new bit? The basic idea of the new bit is the same. We are going to do exactly the same thing, but we are going to write our own for loop. The for loop is going to do exactly what the for loop inside PyTorch does for encoder, but we are going to do it manually. How big is the for loop? It's an output sequence length ( `out_sl` ) which was something passed to the constructor which is equal to the length of the largest English sentence. Since we are translating into English, so it can't possibly be longer than that at least in this corpus. If we then used it on some different corpus that was longer, this is going to fail — you could always pass in a different parameter, of course. So the basic idea is the same [ [1:01:06](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h1m6s) ].

*   We are going to go through and put it through the embedding.
*   We are going to stick it through the RNN, dropout, and a linear layer.
*   We will then append the output to a list which will be stacked into a single tensor and get returned.

Normally, a recurrent neural network works on a whole sequence at a time, but we have a for loop to go through each part of the sequence separately [ [1:01:37](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h1m37s) ]. Wo we have to add a leading unit axis to the start ( `.unsqueeze(0)` ) to basicaly say this is a sequence of length one. We are not really taking advantage of the recurrent net much at all — we could easily re-write this with a linear layer.

One thing to be aware of is `dec_inp` [ [1:02:34](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h2m34s) ]: What is the input to the embedding? The answer is it is the previous word that we translated. The basic idea is if you are trying to translate the 4th word of the new sentence but you don't know what the third word you just said was, that is going to be really hard. So we are going to feed that in at each time step. What was the previous word at the start? There was none. Specifically, we are going to start out with a beginning of stream token ( `_bos_` ) which is zero.

`outp` [ [1:05:24](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h5m24s) ]: it is a tensor whose length is equal to the number of words in our English vocabulary and it contains the probability for every one of those words that it is that word.

`outp.data.max` : It looks in its tensor to find out which word has the highest probability. `max` in PyTorch returns two things: the first thing is what is that max probability and the second is what is the index into the array of that max probability. So we want that second item which is the word index with the largest thing.

`dec_inp` : It contains the word index into the vocabulary of the word. If it's one (ie padding), that means we are done — we reached the end because we finished with a bunch of padding. If it's not one, let's go back and continue.

Each time, we appended our outputs (not the word but the probabilities) to the list [ [1:06:48](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h6m48s) ] which we stack up into a tensor and we can now go ahead and feed that to a loss function.

#### Loss function [ [1:07:13](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h7m13s) ]

The loss function is categorical cross entropy loss. We have a list of probabilities for each of our classes where the classes are all the words in our English vocab and we have a target which is the correct class (ie which is the correct word at this location). There are two tweaks which is why we need to write our own loss function but you can see basically it is going to be cross entropy loss.

```
 def seq2seq_loss(input, target):  sl,bs = target.size()  sl_in,bs_in,nc = input.size()  if sl>sl_in: input = F.pad(input, (0,0,0,0,0,sl-sl_in))  input = input[:sl]  return F.cross_entropy(input.view(-1,nc), target.view(-1)) 
```

Tweaks [ [1:07:40](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h7m40s) ]:

1.  If the generated sequence length is shorter than the sequence length of the target, we need to add some padding. PyTorch padding function requires a tuple of 6 to pad a rank 3 tensor (sequence length, batch size, by number of words in the vocab). Each pair represents padding before and after that dimension.

2\. `F.cross_entropy` expects a rank 2 tensor, but we have sequence length by batch size, so let's just flatten out. That is what `view(-1, ...)` does.

```
 opt_fn = partial(optim.Adam, betas=(0.8, 0.99)) 
```

The difference between `.cuda()` and `to_gpu()` : `to_gpu` will not put to in the GPU if you do not have one. You can also set `fastai.core.USE_GPU` to `false` to force it to not use GPU that can be handy for debugging.

```
 rnn = Seq2SeqRNN(fr_vecd, fr_itos, dim_fr_vec, en_vecd, en_itos,  dim_en_vec, nh, enlen_90)  learn = RNN_Learner(md, SingleModel(to_gpu(rnn)), opt_fn=opt_fn)  learn.crit = seq2seq_loss 
```

```
 3097 ['l'', "d'", 't_up', 'd'', "qu'"] 
 1285 ["'s", ''s', "n't", 'n't', ':'] 
```

We then need something that tells it how to handle learning rate groups so there is a thing called `SingleModel` that you can pass it to which treats the whole thing as a single learning rate group [ [1:09:40](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h9m40s) ]. So this is the easiest way to turn a PyTorch module into a fastai model.

![](../img/1_NW1_lYHLm8R0ML_BWq0QRA.png)

We could just call Learner to turn that into a learner, but if we call RNN_Learner, it does add in `save_encoder` and `load_encoder` that can be handy sometimes. In this case, we really could have said `Leaner` but `RNN_Learner` also works.

```
 learn.lr_find()  learn.sched.plot() 
```

![](../img/1_Fwxxo1lXoIqdM5v24sWfIA.png)

```
 lr=3e-3  learn.fit(lr, 1, cycle_len=12, use_clr=(20,10)) 
```

```
 _epoch trn_loss val_loss_ 
 0 5.48978 5.462648 
 1 4.616437 4.770539 
 2 4.345884 4.37726 
 3 3.857125 4.136014 
 4 3.612306 3.941867 
 5 3.375064 3.839872 
 6 3.383987 3.708972 
 7 3.224772 3.664173 
 8 3.238523 3.604765 
 9 2.962041 3.587814 
 10 2.96163 3.574888 
 11 2.866477 3.581224 
```

```
 [3.5812237] 
```

```
 learn.save('initial')  learn.load('initial') 
```

#### Test [ [1:11:01](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h11m1s) ]

Remember the model attribute of a learner is a standard PyTorch model so we can pass some `x` which we can grab out of our validation set or you could `learn.predict_array` or whatever you like to get some predictions. Then we convert those predictions into words by going `.max()[1]` to grab the index of the highest probability words to get some predictions. Then we can go through a few examples and print out the French, the correct English, and the predicted English for things that are not padding.

```
 x,y = next(iter(val_dl))  probs = learn.model(V(x))  preds = to_np(probs.max(2)[1])  for i in range(180,190):  print(' '.join([fr_itos[o] for o in x[:,i] if o != 1]))  print(' '.join([en_itos[o] for o in y[:,i] if o != 1]))  print(' '.join([en_itos[o] for o in preds[:,i] if o!=1]))  print() 
```

```
 quels facteurs pourraient influer sur le choix de leur emplacement ? _eos_ 
 what factors influencetheir location ? _eos_ 
 what factors might might influence on the their ? _?_ _eos_ 

 qu' est -ce qui ne peut pas changer ? _eos_ 
 what can not change ? _eos_ 
 what not change change ? _eos_ 

 que faites - vous ? _eos_ 
 what do you do ? _eos_ 
 what do you do ? _eos_ 

 qui réglemente les pylônes d' antennes ? _eos_ 
 who regulates antenna towers ? _eos_ 
 who regulates the doors doors ? _eos_ 

 où sont - ils situés ? _eos_ 
 where are they located ? _eos_ 
 where are the located ? _eos_ 

 quelles sont leurs compétences ? _eos_ 
 what are their qualifications ? _eos_ 
 what are their skills ? _eos_ 

 qui est victime de harcèlement sexuel ? _eos_ 
 who experiences sexual harassment ? _eos_ 
 who is victim sexual sexual ? _?_ _eos_ 

 quelles sont les personnes qui visitent les communautés autochtones ? _eos_ 
 who visits indigenous communities ? _eos_ 
 who are people people aboriginal aboriginal ? _eos_ 

 pourquoi ces trois points en particulier ? _eos_ 
 why these specific three ? _eos_ 
 why are these two different ? _?_ _eos_ 

 pourquoi ou pourquoi pas ? _eos_ 
 why or why not ? _eos_ 
 why or why not _eos_ 
```

Amazingly enough, this kind of simplest possible written largely from scratch PyTorch module on only fifty thousand sentences is sometimes capable, on validation set, of giving you exactly the right answer. Sometimes the right answer is in slightly different wording, and sometimes sentences that really aren't grammatically sensible or even have too many question marks. So we are well on the right track. We think you would agree even the simplest possible seq-to-seq trained for a very small number of epochs without any pre-training other than the use of word embeddings is surprisingly good. We are going to improve this later but the message here is even sequence to sequence models you think is simpler than they could possibly work even with less data than you think you could learn from can be surprisingly effective and in certain situations this may be enough for your needs.

**Question** : Would it help to normalize punctuation (eg `'` vs. `'` )? [ [1:13:10](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h13m10s) ] The answer to this particular case is probably yes — the difference between curly quotes and straight quotes is really semantic. You do have to be very careful though because it may turn out that people using beautiful curly quotes like using more formal language and they are writing in a different way. So if you are going to do some kind of pre-processing like punctuation normalization, you should definitely check your results with and without because nearly always that kind of pre-processing make things worse even when you're sure it won't.

**Question** : What might be some ways of regularizing these seq2seq models besides dropout and weight decay? [ [1:14:17](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h14m17s) ] Let me think about that during the week. AWD-LSTM which we have been relying a lot has dropouts of many different kinds and there is also a kind of a regularization based on activations and on changes. Jeremy has not seen anybody put anything like that amount of work into regularizing sequence to sequence model and there is a huge opportunity for somebody to do like the AWD-LSTM of seq-to-seq which might be as simple as stealing all the ideas from AWD-LSTM and using them directly in seq-to-seq that would be pretty easy to try. There's been an interesting paper that Stephen Merity added in the last couple weeks where he used an idea which take all of these different AWD-LSTM hyper parameters and train a bunch of different models and then use a random forest to find out the feature importance — which ones actually matter the most and then figure out how to set them. You could totally use this approach to figure out for sequence to sequence regularization approaches which one is the best and optimize them and that would be amazing. But at the moment, we don't know if there are additional ideas to sequence to sequence regularization beyond what is in that paper for regular language model.

### Tricks [ [1:16:28](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h16m28s) ]

#### **Trick #1 : Go bi-directional**

For classification, the approach to bi-directional Jeremy suggested to use is take all of your token sequences, spin them around, train a new language model, and train a new classifier. He also mentioned that wikitext pre-trained model if you replace `fwd` with `bwd` in the name, you will get the pre-trained backward model he created for you. Get a set of predictions and then average the predictions just like a normal ensemble. That is how we do bi-dir for that kind of classification. There may be ways to do it end-to-end, but Jeremy hasn't quite figured them out yet and they are not in fastai yet. So if you figure it out, that's an interesting line of research. But because we are not doing massive documents where we have to chunk it into separate bits and then pool over them, we can do bi-dir very easily in this case. It is literally as simple as adding `bidirectional=True` to our encoder. People tend not to do bi-directional for the decoder partly because it is kind of considered cheating but maybe it can work in some situations although it might need to be more of an ensemble approach in the decoder because it's a bit less obvious. But encoder it's very simple — `bidirectional=True` and we now have a second RNN that is going the opposite direction. The second RNN is visiting each token in the opposing order so when we get to the final hidden state, it is the first (ie left most) token . But the hidden state is the same size, so the final result is that we end up with a tensor with an extra axis of length 2\. Depending on what library you use, often that will be then combined with the number of layers, so if you have 2 layers and bi-directional — that tensor dimension is now length 4\. With PyTorch it depends which bit of the process you are looking at as to whether you get a separate result for each layer and/or for each bidirectional bit. You have to look up the documentation and it will tell you input's output's tensor sizes appropriate for the number of layers and whether you have `bidirectional=True` .

In this particular case, you will see all the changes that had to be made [ [1:19:38](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h19m38s) ]. For example ,when we added `bidirectional=True` , the `Linear` layer now needs number of hidden times 2 (ie `nh*2` ) to reflect the fact that we have that second direction in our hidden state. Also in `initHidden` it's now `self.nl*2` .

```
 class Seq2SeqRNN_Bidir (nn.Module):  def __init__(self, vecs_enc, itos_enc, em_sz_enc, vecs_dec,  itos_dec, em_sz_dec, nh, out_sl, nl=2):  super().__init__()  self.emb_enc = create_emb(vecs_enc, itos_enc, em_sz_enc)  self.nl,self.nh,self.out_sl = nl,nh,out_sl  self.gru_enc = nn.GRU(em_sz_enc, nh, num_layers=nl,  dropout=0.25, bidirectional= True )  self.out_enc = nn.Linear(nh *2 , em_sz_dec, bias= False )  self.drop_enc = nn.Dropout(0.05)  self.emb_dec = create_emb(vecs_dec, itos_dec, em_sz_dec)  self.gru_dec = nn.GRU(em_sz_dec, em_sz_dec, num_layers=nl,  dropout=0.1)  self.emb_enc_drop = nn.Dropout(0.15)  self.out_drop = nn.Dropout(0.35)  self.out = nn.Linear(em_sz_dec, len(itos_dec))  self.out.weight.data = self.emb_dec.weight.data  def forward(self, inp):  sl,bs = inp.size()  h = self.initHidden(bs)  emb = self.emb_enc_drop(self.emb_enc(inp))  enc_out, h = self.gru_enc(emb, h)  h = h.view(2,2,bs,-1).permute(0,2,1,3)  .contiguous().view(2,bs,-1)  h = self.out_enc(self.drop_enc(h)) 
```

```
 dec_inp = V(torch.zeros(bs).long())  res = []  for i in range(self.out_sl):  emb = self.emb_dec(dec_inp).unsqueeze(0)  outp, h = self.gru_dec(emb, h)  outp = self.out(self.out_drop(outp[0]))  res.append(outp)  dec_inp = V(outp.data.max(1)[1])  if (dec_inp==1).all(): break  return torch.stack(res)  def initHidden(self, bs):  return V(torch.zeros(self.nl *2 , bs, self.nh)) 
```

**Question** : Why is making the decoder bi-directional considered cheating? [ [1:20:13](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h20m13s) ] It's not just cheating but we have this loop going on so it is not as simple as having two tensors. Then how do you turn those two separate loops into a final result? After talking about it during the break, Jeremy has gone from “everybody knows it doesn't work” to “maybe it could work”, but it requires more thought. It is quite possible during the week, he'll realize it's a dumb idea, but we'll think about it.

**Question** : Why do you need to set a range to the loop? [ [1:20:58](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h20m58s) ] Because when we start training, everything is random so `if (dec_inp==1).all(): break` will probably never be true. Later on, it will pretty much always break out eventually but basically we are going to go forever. It's really important to remember when you are designing an architecture that when you start, the model knows nothing about anything. So you want to make sure if it's going to do something at least it's vaguely sensible.

We got 3.58 cross entropy loss with single direction [ [1:21:46](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h21m46s) ]. With bi-direction, we got down to 3.51, so that improved a little. It shouldn't really slow things down too much. Bi-directional does mean there is a little bit more sequential processing have to happen, but it is generally a good win. In the Google translation model, of the 8 layers, only the first layer is bi-directional because it allows it to do more in parallel so if you create really deep models you may need to think about which ones are bi-directional otherwise we have performance issues.

```
 rnn = Seq2SeqRNN_Bidir(fr_vecd, fr_itos, dim_fr_vec, en_vecd,  en_itos, dim_en_vec, nh, enlen_90)  learn = RNN_Learner(md, SingleModel(to_gpu(rnn)), opt_fn=opt_fn)  learn.crit = seq2seq_loss 
```

```
 learn.fit(lr, 1, cycle_len=12, use_clr=(20,10)) 
```

```
 _epoch trn_loss val_loss_ 
 0 4.896942 4.761351 
 1 4.323335 4.260878 
 2 3.962747 4.06161 
 3 3.596254 3.940087 
 4 3.432788 3.944787 
 5 3.310895 3.686629 
 6 3.454976 3.638168 
 7 3.093827 3.588456 
 8 3.257495 3.610536 
 9 3.033345 3.540344 
 10 2.967694 3.516766 
 11 2.718945 3.513977 
```

```
 [3.5139771] 
```

#### Trick #2 Teacher Forcing [ [1:22:39](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h22m39s) ]

Now let's talk about teacher forcing. When a model starts learning, it knows nothing about nothing. So when the model starts learning, it is not going to spit out “Er” at the first step, it is going to spit out some random meaningless word because it doesn't know anything about German or about English or about the idea of language. And it is going to feed it to the next process as an input and be totally unhelpful. That means, early learning is going to be very difficult because it is feeding in an input that is stupid into a model that knows nothing and somehow it's going to get better. So it is not asking too much eventually it gets there, but it's definitely not as helpful as we can be. So what if instead of feeing in the thing I predicted just now, what if we instead we feed in the actual correct word was meant to be. We can't do that at inference time because by definition we don't know the correct word - it has to translate it. We can't require the correct translation in order to do translation.

![](../img/1_DU776SGr1rhYeU7ilIKX9w.png)

So the way it's set up is we have this thing called `pr_force` which is probability of forcing [ [1:24:01](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h24m1s) ]. If some random number is less than that probability then we are going to replace our decoder input with the actual correct thing. If we have already gone too far and if it is already longer than the target sequence, we are just going to stop because obviously we can't give it the correct thing. So you can see how beautiful PyTorch is for this. The key reasons that we switched to PyTorch at this exact point in last year's class was because Jeremy tried to implement teacher forcing in Keras and TensorFlow and went even more insane than he started. It was weeks of getting nowhere then he saw on Twitter Andrej Karpathy said something about this thing called PyTorch that just came out and it's really cool. He tried it that day, by the next day, he had teacher forcing. All this stuff of trying to debug things was suddenly so much easier and and this kind of dynamic thing is so much easier. So this is a great example of “hey, I get to use random numbers and if statements”.

```
 class Seq2SeqStepper (Stepper):  def step(self, xs, y, epoch):  self.m.pr_force = (10-epoch)*0.1 if epoch<10 else 0  xtra = []  output = self.m(*xs, y)  if isinstance(output,tuple): output,*xtra = output  self.opt.zero_grad()  loss = raw_loss = self.crit(output, y)  if self.reg_fn: loss = self.reg_fn(output, xtra, raw_loss)  loss.backward()  if self.clip: # Gradient clipping  nn.utils.clip_grad_norm(trainable_params_(self.m),  self.clip)  self.opt.step()  return raw_loss.data[0] 
```

Here is the basic idea [ [1:25:29](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h25m29s) ]. At the start of training, let's set `pr_force` really high so that nearly always it gets the actual correct previous word and so it has a useful input. Then as we trained a bit more, let's decrease `pr_force` so that by the end `pr_force` is zero and it has to learn properly which is fine because it is now actually feeding in sensible inputs most of the time anyway.

```
 class Seq2SeqRNN_TeacherForcing (nn.Module):  def __init__(self, vecs_enc, itos_enc, em_sz_enc, vecs_dec,  itos_dec, em_sz_dec, nh, out_sl, nl=2):  super().__init__()  self.emb_enc = create_emb(vecs_enc, itos_enc, em_sz_enc)  self.nl,self.nh,self.out_sl = nl,nh,out_sl  self.gru_enc = nn.GRU(em_sz_enc, nh, num_layers=nl,  dropout=0.25)  self.out_enc = nn.Linear(nh, em_sz_dec, bias= False )  self.emb_dec = create_emb(vecs_dec, itos_dec, em_sz_dec)  self.gru_dec = nn.GRU(em_sz_dec, em_sz_dec, num_layers=nl,  dropout=0.1)  self.emb_enc_drop = nn.Dropout(0.15)  self.out_drop = nn.Dropout(0.35)  self.out = nn.Linear(em_sz_dec, len(itos_dec))  self.out.weight.data = self.emb_dec.weight.data  self.pr_force = 1\.  def forward(self, inp, y= None ):  sl,bs = inp.size()  h = self.initHidden(bs)  emb = self.emb_enc_drop(self.emb_enc(inp))  enc_out, h = self.gru_enc(emb, h)  h = self.out_enc(h) 
```

```
 dec_inp = V(torch.zeros(bs).long())  res = []  for i in range(self.out_sl):  emb = self.emb_dec(dec_inp).unsqueeze(0)  outp, h = self.gru_dec(emb, h)  outp = self.out(self.out_drop(outp[0]))  res.append(outp)  dec_inp = V(outp.data.max(1)[1])  if (dec_inp==1).all(): break  if (y is not None ) and (random.random()<self.pr_force):  if i>=len(y): break  dec_inp = y[i]  return torch.stack(res)  def initHidden(self, bs):  return V(torch.zeros(self.nl, bs, self.nh)) 
```

`pr_force` : “probability of forcing”. High in the beginning zero by the end.

Let's now write something such that in the training loop, it gradually decreases `pr_force` [ [1:26:01](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h26m1s) ]. How do we do that? One approach would be to write our own training loop but let's not do that because we already have a training loop that has progress bars, uses exponential weighted averages to smooth out the losses, keeps track of metrics, and does bunch of things. They also keep track of calling the reset for RNN at the start of the epoch to make sure the hidden state is set to zeros. What we've tended to find is that as we start to write some new thing and we need to replace some part of the code, we then add some little hook so that we can all use that hook to make things easier. In this particular case, there is a hook that Jeremy has ended up using all the time which is the hook called the stepper. If you look at the source code, model.py is where our fit function lives which is the lowest level thing that does not require learner or anything much at all — just requires a standard PyTorch model and a model data object. You just need to know how many epochs, a standard PyTorch optimizer, and a standard PyTorch loss function. We hardly ever used in the class, we normally call `learn.fit` , but `learn.fit` calls this.

![](../img/1_hhksba0Jh8iyWmuC_tPtqg.png)

We have looked at the source code sometime [ [1:27:49](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h27m49s) ]. We've seen how it loos through each epoch and that loops through each thing in our batch and calls `stepper.step` . `stepper.step` is the thing that is responsible for:

*   calling the model
*   getting the loss
*   finding the loss function
*   calling the optimizer

![](../img/1_dlBOu68q6RyNuQ0opzvxMg.png)

So by default, `stepper.step` uses a particular class called `Stepper` which basically calls the model, zeros the gradient, calls the loss function, calls `backward` , does gradient clipping if necessary, then calls the optimizer. They are basic steps that back when we looked at “PyTorch from scratch” we had to do. The nice thing is, we can replace that with something else rather than replacing the training loop. If you inherit from `Stepper` , then write your own version of `step` , you can just copy and paste the contents of step and add whatever you like. Or if it's something that you're going to do before or afterwards, you could even call `super.step` . In this case, Jeremy rather suspects he has been unnecessarily complicated [ [1:29:12](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h29m12s) ] — he probably could have done something like:

```
 class Seq2SeqStepper (Stepper):  def step(self, xs, y, epoch):  self.m.pr_force = (10-epoch)*0.1 if epoch<10 else 0  return super.step(xs, y, epoch) 
```

But as he said, when he is prototyping, he doesn't think carefully about how to minimize his code — he copied and pasted the contents of the `step` and he added a single line to the top which was to replace `pr_force` in the module with something that gradually decreased linearly for the first 10 epochs, and after 10 epochs, it is zero. So total hack but good enough to try it out. The nice thing is that everything else is the same except for the addition of these three lines:

```
  if (y is not None ) and (random.random()<self.pr_force):  if i>=len(y): break  dec_inp = y[i] 
```

And the only thing we need to do differently is when we call `fit` , we pass in our customized stepper class.

```
 rnn = Seq2SeqRNN_TeacherForcing(fr_vecd, fr_itos, dim_fr_vec,  en_vecd, en_itos, dim_en_vec, nh, enlen_90)  learn = RNN_Learner(md, SingleModel(to_gpu(rnn)), opt_fn=opt_fn)  learn.crit = seq2seq_loss 
```

```
 learn.fit(lr, 1, cycle_len=12, use_clr=(20,10),  stepper=Seq2SeqStepper) 
```

```
 _epoch trn_loss val_loss_ 
 0 4.460622 12.661013 
 1 3.468132 7.138729 
 2 3.235244 6.202878 
 3 3.101616 5.454283 
 4 3.135989 4.823736 
 5 2.980696 4.933402 
 6 2.91562 4.287475 
 7 3.032661 3.975346 
 8 3.103834 3.790773 
 9 3.121457 3.578682 
 10 2.917534 3.532427 
 11 3.326946 3.490643 
```

```
 [3.490643] 
```

And now our loss is down to 3.49\. We needed to make sure at least do 10 epochs because before that, it was cheating by using the teacher forcing.

#### Trick #3 Attentional model [ [1:31:00](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h31m) ]

This next trick is a bigger and pretty cool trick. It's called “attention.” The basic idea of attention is this — expecting the entirety of the sentence to be summarized into this single hidden vector is asking a lot. It has to know what was said, how it was said, and everything necessary to create the sentence in German. The idea of attention is basically maybe we are asking too much. Particularly because we could use this form of model (below) where we output every step of the loop to not just have a hidden state at the end but to have a hidden state after every single word. Why not try and use that information? It's already there but so far we've just been throwing it away. Not only that but bi-directional, we got two vectors of state every step that we can use. How can we do this?

![](../img/1_CX45skUFZZO6uHsR8IndzA.png)

Let's say we are translating a word “liebte” right now [ [1:32:34](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h32m34s) ]. Which of previous 5 pieces of state do we want? We clearly want “love” because it is the word. How about “zu”? We probably need “eat” and “to” and loved” to make sure we have gotten the tense right and know that I actually need this part of the verb and so forth. So depending on which bit we are translating, we would need one or more bits of these various hidden states. In fact, we probably want some weighting of them. In other words, for these five pieces of hidden state, we want a weighted average [ [1:33:47](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h33m47s) ]. We want it weighted by something that can figure out which bits of the sentence is the most important right now. How do we figure out something like which bits of the sentence are important right now? We create a neural net and we train the neural net to figure it out. When do we train that neural net? End to end. So let's now train two neural nets [ [1:34:18](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h34m18s) ]. Well, we've already got a bunch — RNN encoder, RNN decoder, a couple of linear layers, what the heck, let's add another neural net into the mix. This neural net is going to spit out a weight for every one of these states and we will take the weighted average at every step, and it's just another set of parameters that we learn all at the same time. So that is called “attention”.

![](../img/1_JTCoNaf3I5LQVz2SrYCz0A.png)

The idea is that once that attention has been learned, each word is going to take a weighted average as you can see in this terrific demo from Chris Olah and Shan Carter [ [1:34:50](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h34m50s) ]. Check out this [distill.pub article](https://distill.pub/2016/augmented-rnns/) — these things are interactive diagrams that shows you how the attention works and what the actual attention looks like in a trained translation model.

[![](../img/1_fkL30nxS54fKVmyC2jtMrw.png)](https://distill.pub/2016/augmented-rnns/)

Let's try and implement attention [ [1:35:47](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h35m47s) ]:

```
 def rand_t(*sz): return torch.randn(sz)/math.sqrt(sz[0])  def rand_p(*sz): return nn.Parameter(rand_t(*sz)) 
```

```
 class Seq2SeqAttnRNN (nn.Module):  def __init__(self, vecs_enc, itos_enc, em_sz_enc, vecs_dec,  itos_dec, em_sz_dec, nh, out_sl, nl=2):  super().__init__()  self.emb_enc = create_emb(vecs_enc, itos_enc, em_sz_enc)  self.nl,self.nh,self.out_sl = nl,nh,out_sl  self.gru_enc = nn.GRU(em_sz_enc, nh, num_layers=nl,  dropout=0.25)  self.out_enc = nn.Linear(nh, em_sz_dec, bias= False )  self.emb_dec = create_emb(vecs_dec, itos_dec, em_sz_dec)  self.gru_dec = nn.GRU(em_sz_dec, em_sz_dec, num_layers=nl,  dropout=0.1)  self.emb_enc_drop = nn.Dropout(0.15)  self.out_drop = nn.Dropout(0.35)  self.out = nn.Linear(em_sz_dec*2, len(itos_dec))  self.out.weight.data = self.emb_dec.weight.data 
```

```
 self.W1 = rand_p(nh, em_sz_dec)  self.l2 = nn.Linear(em_sz_dec, em_sz_dec)  self.l3 = nn.Linear(em_sz_dec+nh, em_sz_dec)  self.V = rand_p(em_sz_dec) 
```

```
 def forward(self, inp, y= None , ret_attn= False ):  sl,bs = inp.size()  h = self.initHidden(bs)  emb = self.emb_enc_drop(self.emb_enc(inp))  enc_out, h = self.gru_enc(emb, h)  h = self.out_enc(h) 
```

```
 dec_inp = V(torch.zeros(bs).long())  res,attns = [],[]  w1e = enc_out @ self.W1  for i in range(self.out_sl):  w2h = self.l2(h[-1])  u = F.tanh(w1e + w2h)  a = F.softmax(u @ self.V, 0)  attns.append(a)  Xa = (a.unsqueeze(2) * enc_out).sum(0)  emb = self.emb_dec(dec_inp)  wgt_enc = self.l3(torch.cat([emb, Xa], 1))  outp, h = self.gru_dec(wgt_enc.unsqueeze(0), h)  outp = self.out(self.out_drop(outp[0]))  res.append(outp)  dec_inp = V(outp.data.max(1)[1])  if (dec_inp==1).all(): break  if (y is not None ) and (random.random()<self.pr_force):  if i>=len(y): break  dec_inp = y[i] 
```

```
 res = torch.stack(res)  if ret_attn: res = res,torch.stack(attns)  return res 
```

```
 def initHidden(self, bs):  return V(torch.zeros(self.nl, bs, self.nh)) 
```

With attention, most of the code is identical. The one major difference is this line: `Xa = (a.unsqueeze(2) * enc_out).sum(0)` . We are going to take a weighted average and the way we are going to do the weighted average is we create a little neural net which we are going to see here:

```
 w2h = self.l2(h[-1])  u = F.tanh(w1e + w2h)  a = F.softmax(u @ self.V, 0) 
```

We use softmax because the nice thing about softmax is that we want to ensure all of the weights that we are using add up to 1 and we also expect that one of those weights should probably be higher than the other ones [ [1:36:38](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h36m38s) ]. Softmax gives us the guarantee that they add up to 1 and because it has `e^` in it, it tends to encourage one of the weights to be higher than the other ones.

Let's see how this works [ [1:37:09](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h37m9s) ]. We are going to take the last layer's hidden state and we are going to stick it into a linear layer. Then we are going to stick it into a nonlinear activation, then we are going to do a matrix multiply. So if you think about it — a linear layer, nonlinear activation, matrix multiple — it's a neural net. It is a neural net with one hidden layer. Stick it into a softmax and then we can use that to weight our encoder outputs. Now rather than just taking the last encoder output, we have the whole tensor of all of the encoder outputs which we just weight by this neural net we created.

In Python, `A @ B` is the matrix product, `A * B` the element-wise product

#### Papers [ [1:38:18](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h38m18s) ]

*   [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) — One amazing paper that originally introduced this idea of attention as well as a couple of key things which have really changed how people work in this field. They say area of attention has been used not just for text but for things like reading text out of pictures or doing various things with computer vi sion.
*   [Grammar as a Foreign Language](https://arxiv.org/abs/1412.7449) — The second paper which Geoffrey Hinton was involved in that used this idea of RNN with attention to try to replace rules based grammar with an RNN which automatically tagged each word based on the grammar. It turned out to do it better than any rules based system which today seems obvious but at that time it was considered really surprising. They are summary of how attention works which is really nice and concise.

**Question** : Could you please explain attention again? [ [1:39:46](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h39m46s) ] Sure! Let's go back and look at our original encoder.

![](../img/1_NsX_t2WEEjsPcVyWOrcXFA.png)

The RNN spits out two things: it spits out a list of the state after every time step ( `enc_out` ), and it also tells you the state at the last time step ( `h` )and we used the state at the last time step to create the input state for our decoder which is one vector `s` below:

![](../img/1_QQiYoum-_J9Rm7DEQCElxA.png)

But we know that it's creating a vector at every time steps (orange arrows), so wouldn't it be nice to use them all? But wouldn't it be nice to use the one or ones that's most relevant to translating the word we are translating now? So wouldn't it be nice to be able to take a weighted average of the hidden state at each time step weighted by whatever is the appropriate weight right now. For example, “liebte” would definitely be time step #2 is what it's all about because that is the word I'm translating. So how do we get a list of weights that is suitable fore the word we are training right now? The answer is by training a neural net to figure out the list of weights. So anytime we want to figure out how to train a little neural net that does any task, the easiest way, normally always to do that is to include it in your module and train it in line with everything else. The minimal possible neural net is something that contains two layers and one nonlinear activation function, so `self.l2` is one linear layer.

![](../img/1_fnTtr-UiW5JtNy8M9q1mkg.png)

In fact, instead of a linear layer, we can even just grab a random matrix if we do not care about bias [ [1:42:18](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h42m18s) ]. `self.W1` is a random tensor wrapped up in a `Parameter` .

`Parameter` : Remember, a `Parameter` is identical to PyTorch `Variable` but it just tells PyTorch “I want you to learn the weights for this please.” [ [1:42:35](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h42m35s) ]

So when we start out our decoder, let's take the current hidden state of the decoder, put that into a linear layer ( `self.l2` ) because what is the information we use to decide what words we should focus on next — the only information we have to go on is what the decoder's hidden state is now. So let's grab that:

*   put it into the linear layer ( `self.l2` )
*   put it through a non-linearity ( `F.tanh` )
*   put it through one more nonlinear layer ( `u @ self.V` doesn't have a bias in it so it's just matrix multiply)
*   put that through softmax

That's it — a little neural net. It doesn't do anything. It's just a neural net and no neural nets do anything they are just linear layers with nonlinear activations with random weights. But it starts to do something if we give it a job to do. In this case, the job we give it to do is to say don't just take the final state but now let's use all of the encoder states and let's take all of them and multiply them by the output of that little neural net. So given that the things in this little neural net are learnable weights, hopefully it's going to learn to weight those encoder hidden states by something useful. That is all neural net ever does is we give it some random weights to start with and a job to do, and hope that it learns to do the job. It turns out, it does.

![](../img/1_jOVVKAFwMxGt9v6WEqgLXw.png)

Everything else in here is identical to what it was before. We have teacher forcing, it's not bi-directional, so we can see how this goes.

```
 rnn = Seq2SeqAttnRNN(fr_vecd, fr_itos, dim_fr_vec, en_vecd, en_itos, dim_en_vec, nh, enlen_90)  learn = RNN_Learner(md, SingleModel(to_gpu(rnn)), opt_fn=opt_fn)  learn.crit = seq2seq_loss  lr=2e-3 
```

```
 learn.fit(lr, 1, cycle_len=15, use_clr=(20,10),  stepper=Seq2SeqStepper) 
```

```
 _epoch trn_loss val_loss_ 
 0 3.882168 11.125291 
 1 3.599992 6.667136 
 2 3.236066 5.552943 
 3 3.050283 4.919096 
 4 2.99024 4.500383 
 5 3.07999 4.000295 
 6 2.891087 4.024115 
 7 2.854725 3.673913 
 8 2.979285 3.590668 
 9 3.109851 3.459867 
 10 2.92878 3.517598 
 11 2.778292 3.390253 
 12 2.795427 3.388423 
 13 2.809757 3.353334 
 14 2.6723 3.368584 
```

```
 [3.3685837] 
```

Teacher forcing had 3.49 and now with nearly exactly the same thing but we've got this little minimal neural net figuring out what weightings to give our inputs and we are down to 3.37\. Remember, these loss are logs, so `e^3.37` is quite a significant change.

```
 learn.save('attn') 
```

#### Test [ [1:45:37](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h45m37s) ]

```
 x,y = next(iter(val_dl))  probs,attns = learn.model(V(x),ret_attn= True )  preds = to_np(probs.max(2)[1]) 
```

```
 for i in range(180,190):  print(' '.join([fr_itos[o] for o in x[:,i] if o != 1]))  print(' '.join([en_itos[o] for o in y[:,i] if o != 1]))  print(' '.join([en_itos[o] for o in preds[:,i] if o!=1]))  print() 
```

```
 quels facteurs pourraient influer sur le choix de leur emplacement ? _eos_ 
 what factors influencetheir location ? _eos_ 
 what factors might influence the their their their ? _eos_ 
```

```
 qu' est -ce qui ne peut pas changer ? _eos_ 
 what can not change ? _eos_ 
 what can not change change ? _eos_ 
```

```
 que faites - vous ? _eos_ 
 what do you do ? _eos_ 
 what do you do ? _eos_ 
```

```
 qui réglemente les pylônes d' antennes ? _eos_ 
 who regulates antenna towers ? _eos_ 
 who regulates the lights ? _?_ _eos_ 
```

```
 où sont - ils situés ? _eos_ 
 where are they located ? _eos_ 
 where are they located ? _eos_ 
```

```
 quelles sont leurs compétences ? _eos_ 
 what are their qualifications ? _eos_ 
 what are their skills ? _eos_ 
```

```
 qui est victime de harcèlement sexuel ? _eos_ 
 who experiences sexual harassment ? _eos_ 
 who is victim sexual sexual ? _eos_ 
```

```
 quelles sont les personnes qui visitent les communautés autochtones ? _eos_ 
 who visits indigenous communities ? _eos_ 
 who is people people aboriginal people ? _eos_ 
```

```
 pourquoi ces trois points en particulier ? _eos_ 
 why these specific three ? _eos_ 
 why are these three three ? _?_ _eos_ 
```

```
 pourquoi ou pourquoi pas ? _eos_ 
 why or why not ? _eos_ 
 why or why not ? _eos_ 
```

不错。 It's still not perfect but quite a few of them are correct and again considering that we are asking it to learn about the very idea of language for two different languages and how to translate them between the two, and grammar, and vocabulary, and we only have 50,000 sentences and a lot of the words only appear once, I would say this is actually pretty amazing.

**Question:** Why do we use tanh instead of ReLU for the attention mini net? [ [1:46:23](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h46m23s) ] I don't quite remember — it's been a while since I looked at it. You should totally try using value and see how it goes. Obviously tanh the key difference is that it can go in each direction and it's limited both at the top and the bottom. I know very often for the gates inside RNNs, LSTMs, and GRUs, tanh often works out better but it's been about a year since I actually looked at that specific question so I'll look at it during the week. The short answer is you should try a different activation function and see if you can get a better result.

> From Lesson 7 [ [44:06](https://youtu.be/H3g26EVADgY%3Ft%3D44m6s) ]: As we have seen last week, tanh is forcing the value to be between -1 and 1\. Since we are multiplying by this weight matrix again and again, we would worry that relu (since it is unbounded) might have more gradient explosion problem. Having said that, you can specify RNNCell to use different nonlineality whose default is tanh and ask it to use relu if you wanted to.

#### Visualization [ [1:47:12](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h47m12s) ]

What we can do also is we can grab the attentions out of the model by adding return attention parameter to `forward` function. You can put anything you'd like in `forward` function argument. So we added a return attention parameter, false by default because obviously the training loop it doesn't know anything about it but then we just had something here says if return attention, then stick the attentions on as well ( `if ret_attn: res = res,torch.stack(attns)` ). The attentions is simply the value `a` just chuck it on a list ( `attns.append(a)` ). We can now call the model with return attention equals true and get back the probabilities and the attentions [ [1:47:53](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h47m53s) ]:

```
 probs,attns = learn.model(V(x),ret_attn= True ) 
```

We can now draw pictures, at each time step, of the attention.

```
 attn = to_np(attns[...,180]) 
```

```
 fig, axes = plt.subplots(3, 3, figsize=(15, 10))  **for** i,ax **in** enumerate(axes.flat):  ax.plot(attn[i]) 
```

![](../img/1_CSG2P8oBICyPDBnoT9z7Wg.png)

When you are Chris Olah and Shan Carter, you make things that looks like ☟when you are Jeremy Howard, the exact same information looks like ☝︎[ [1:48:24](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h48m24s) ]. You can see at each different time step, we have a different attention.

![](../img/1_zOxcT0Nib1_VtVkyQN4FQQ.png)

It's very important when you try to build something like this, you don't really know if it's not working right because if it's not working (as per usual Jeremy's first 12 attempts of this were broken) and they were broken in a sense that it wasn't really learning anything useful. Therefore, it was giving equal attention to everything and it wasn't worse — it just wasn't much better. Until you actually find ways to visualize the thing in a way that you know what it ought to look like ahead of time, you don't really know if it's working [ [1:49:16](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h49m16s) ]. So it's really important that you try to find ways to check your intermediate steps in your outputs.

**Question** : What is the loss function of the attentional neural network? [ [1:49:31](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h49m31s) ] No, there is no loss function for the attentional neural network. It is trained end-to-end. It is just sitting inside our decoder loop. The loss function for the decoder loop is the same loss function because the result contains exactly same thing as before — the probabilities of the words. How come the mini neural net learning something? Because in order to make the outputs better and better, it would be great if it made the weights of weighted-average better and better. So part of creating our output is to please do a good job of finding a good set of weights and if it doesn't do a good job of finding good set of weights, then the loss function won't improve from that bit. So end-to-end learning means you throw in everything you can into one loss function and the gradients of all the different parameters point in a direction that says “hey, you know if you had put more weight over there, it would have been better.” And thanks to the magic of the chain rule, it knows to put more weight over there, change the parameter in the matrix multiply a little, etc. That is the magic of end-to-end learning. It is a very understandable question but you have to realize there is nothing particular about this code that says this particular bits are separate mini neural network anymore than the GRU is a separate little neural network, or a linear layer is a separate little function. It's all ends up pushed into one output which is a bunch of probabilities which ends up in one loss function that returns a single number that says this either was or wasn't a good translation. So thanks to the magic of the chain rule, we then back propagate little updates to all the parameters to make them a little bit better. This is a big, weird, counterintuitive idea and it's totally okay if it's a bit mind-bending. It is the bit where even back to lesson 1 “how did we make it find dogs vs. cats?” — we didn't. All we did was we said “this is our data, this is our architecture, this is our loss function. Please back propagate into the weights to make them better and after you've made them better a while, it will start finding cats from dogs.” In this case (ie translation), we haven't used somebody else's convolutional network architecture. We said “here is a custom architecture which we hope is going to be particularly good at this problem.” Even without this custom architecture, it was still okay. But we made it in a way that made more sense or we think it ought to do worked even better. But at no point, did we do anything different other than say “here is a data, here is an architecture, here is a loss function — go and find the parameters please” And it did it because that's what neural nets do.

So that is sequence-to-sequence learning [ [1:53:19](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h53m19s) ].

*   If you want to encode an image into a CNN backbone of some kind, and then pass that into a decoder which is like RNN with attention, and you make your y-values the actual correct caption of each of those image, you will end up with an image caption generator.
*   If you do the same thing with videos and captions, you will end up with a video caption generator.
*   If you do the same thing with 3D CT scan and radiology reports, you will end up with a radiology report generator.
*   If you do the same thing with Github issues and people's chosen summaries of them, you'll get a Github issue summary generator.

> Seq-to-seq is magical but they work [ [1:54:07](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h54m7s) ]. And I don't feel like people have begun to scratch the surface of how to use seq-to-seq models in their own domains. Not being a Github person, it would never have occurred to me that “it would be kind of cool to start with some issue and automatically create a summary”. But now, of course, next time I go into Github, I want to see a summary written there for me. I don't want to write my own commit message. Why should I write my own summary of the code review when I finished adding comments to lots of lines — it should do that for me as well. Now I'm thinking Github so behind, it could be doing this stuff. So what are the thing in your industry? You could start with a sequence and generate something from it. I can't begin to imagine. Again, it is a fairly new area and the tools for it are not easy to use — they are not even built into fastai yet. Hopefully there will be soon. I don't think anybody knows what the opportunities are.

### Devise [ [1:55:23](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h55m23s) ]

[Notebook](https://github.com/fastai/fastai/blob/master/courses/dl2/devise.ipynb) / [Paper](http://papers.nips.cc/paper/5204-devise-a-deep-visual-semantic-embedding-model.pdf)

We are going to do something bringing together for the first time our two little worlds we focused on — text and images [ [1:55:49](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h55m49s) ]. This idea came up in a paper by an extraordinary deep learning practitioner and researcher named Andrea Frome. Andrea was at Google at the time and her crazy idea was words can have a distributed representation, a space, which particularly at that time was just word vectors. And images can be represented in a space. In the end, if we have a fully connected layer, they ended up as a vector representation. Could we merge the two? Could we somehow encourage the vector space that the images end up with be the same vector space that the words are in? And if we could do that, what would that mean? What could we do with that? So what could we do with that covers things like well, what if I'm wrong what if I'm predicting that this image is a beagle and I predict jumbo jet and Yannet's model predicts corgi. The normal loss function says that Yannet's and Jeremy's models are equally good (ie they are both wrong). But what if we could somehow say though you know what corgi is closer to beagle than it is to jumbo jets. So Yannet's model is better than Jeremy's. We should be able to do that because in word vector space, beagle and corgi are pretty close together but jumbo jet not so much. So it would give us a nice situation where hopefully our inferences would be wrong in saner ways if they are wrong. It would also allow us to search for things that are not in ImageNet Synset ID (ie a category in ImageNet). Why did we have to train a whole new model to find dog vs. cats when we already have something that found corgis and tabbies. Why can't we just say find me dogs? If we had trained it in word vector space, we totally could because they are word vector, we can find things with the right image vector and so forth. We will look at some cool things we can do with it in a moment but first of all let's train a model where this model is not learning a category (one hot encoded ID) where every category is equally far from every other category, let's instead train a model where we're finding a dependent variable which is a word vector. so What word vector? Obviously the word vector for the word you want. So if it's corgi, let's train it to create a word vector that's the corgi word vector, and if it's a jumbo jet, let's train it with a dependent variable that says this is the word vector for a jumbo jet.

```
 **from** **fastai.conv_learner** **import** *  torch.backends.cudnn.benchmark= True  import fastText as ft 
```

```
 PATH = Path('data/imagenet/')  TMP_PATH = PATH/'tmp'  TRANS_PATH = Path('data/translate/')  PATH_TRN = PATH/'train' 
```

It is shockingly easy [ [1:59:17](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h59m17s) ]. Let's grab the fast text word vectors again, load them in (we only need English this time).

```
 ft_vecs = ft.load_model(str((TRANS_PATH/'wiki.en.bin'))) 
```

```
 np.corrcoef(ft_vecs.get_word_vector('jeremy'),  ft_vecs.get_word_vector('Jeremy')) 
```

```
 array([[1\. , 0.60866], 
 [0.60866, 1\. ]]) 
```

So for example, “jeremy” and “Jeremy” have a correlation of .6\.

```
 np.corrcoef(ft_vecs.get_word_vector('banana'),  ft_vecs.get_word_vector('Jeremy')) 
```

```
 array([[1\. , 0.14482], 
 [0.14482, 1\. ]]) 
```

Jeremy doesn't like bananas at all, and “banana” and “Jeremy” .14\. So words that you would expect to be correlated are correlated and words that should be as far away from each other as possible, unfortunately, they are still slightly correlated but not so much [ [1:59:41](https://youtu.be/tY0n9OT5_nA%3Ft%3D1h59m41s) ].

#### Map ImageNet classes to word vectors

Let's now grab all of the ImageNet classes because we actually want to know which one is corgi and which one is jumbo jet.

```
 ft_words = ft_vecs.get_words(include_freq= True )  ft_word_dict = {k:v for k,v in zip(*ft_words)}  ft_words = sorted(ft_word_dict.keys(), key= lambda x: ft_word_dict[x]) 
```

```
 len(ft_words) 
```

```
 2519370 
```

```
 from fastai.io import get_data 
```

We have a list of all of those up on files.fast.ai that we can grab them.

```
 CLASSES_FN = 'imagenet_class_index.json'  get_data(f'http://files.fast.ai/models/{CLASSES_FN}',  TMP_PATH/CLASSES_FN) 
```

Let's also grab a list of all of the nouns in English which Jeremy made available here:

```
 WORDS_FN = 'classids.txt'  get_data(f'http://files.fast.ai/data/{WORDS_FN}', PATH/WORDS_FN) 
```

So we have the names of each of the thousand ImageNet classes and all of the nouns in English according to WordNet which is a popular thing for representing what words are and are not. We can now load that list of ImageNet classes, turn that into a dictionary, so `classids_1k` contains the class IDs for the 1000 images that are in the competition dataset.

```
 class_dict = json.load((TMP_PATH/CLASSES_FN).open())  classids_1k = dict(class_dict.values())  nclass = len(class_dict); nclass 
```

```
 1000 
```

这是一个例子。 A “tench” apparently is a kind of fish.

```
 class_dict['0'] 
```

```
 ['n01440764', 'tench'] 
```

Let's do the same thing for all those WordNet nouns [ [2:01:11](https://youtu.be/tY0n9OT5_nA%3Ft%3D2h1m11s) ]. It turns out that ImageNet is using WordNet class names so that makes it nice and easy to map between the two.

```
 classid_lines = (PATH/WORDS_FN).open().readlines()  classid_lines[:5] 
```

```
 ['n00001740 entity\n', 
 'n00001930 physical_entity\n', 
 'n00002137 abstraction\n', 
 'n00002452 thing\n', 
 'n00002684 object\n'] 
```

```
 classids = dict(l.strip().split() for l in classid_lines)  len(classids),len(classids_1k) 
```

```
 (82115, 1000) 
```

So these are our two worlds — we have the ImageNet thousand and we have the 82,000 which are in WordNet.

```
 lc_vec_d = {w.lower(): ft_vecs.get_word_vector(w) for w  in ft_words[-1000000:]} 
```

So we want to map the two together which is as simple as creating a couple of dictionaries to map them based on the Synset ID or the WordNet ID.

```
 syn_wv = [(k, lc_vec_d[v.lower()]) for k,v in classids.items()  if v.lower() in lc_vec_d]  syn_wv_1k = [(k, lc_vec_d[v.lower()]) for k,v in classids_1k.items()  if v.lower() in lc_vec_d]  syn2wv = dict(syn_wv)  len(syn2wv) 
```

```
 49469 
```

What we need to do now is grab the 82,000 nouns in WordNet and try and look them up in fast text. We've managed to look up 49,469 of them in fast text. We now have a dictionary that goes from synset ID which is what WordNet calls them to word vectors. We also have the same thing specifically for the 1k ImageNet classes.

```
 pickle.dump(syn2wv, (TMP_PATH/'syn2wv.pkl').open('wb'))  pickle.dump(syn_wv_1k, (TMP_PATH/'syn_wv_1k.pkl').open('wb')) 
```

```
 syn2wv = pickle.load((TMP_PATH/'syn2wv.pkl').open('rb'))  syn_wv_1k = pickle.load((TMP_PATH/'syn_wv_1k.pkl').open('rb')) 
```

Now we grab all of the ImageNet which you can download from Kaggle now [ [2:02:54](https://youtu.be/tY0n9OT5_nA%3Ft%3D2h2m54s) ]. If you look at the Kaggle ImageNet localization competition, that contains the entirety of the ImageNet classifications as well.

```
 images = []  img_vecs = [] 
```

```
 for d in (PATH/'train').iterdir():  if d.name not in syn2wv: continue  vec = syn2wv[d.name]  for f in d.iterdir():  images.append(str(f.relative_to(PATH)))  img_vecs.append(vec) 
```

```
 n_val=0  for d in (PATH/'valid').iterdir():  if d.name not in syn2wv: continue  vec = syn2wv[d.name]  for f in d.iterdir():  images.append(str(f.relative_to(PATH)))  img_vecs.append(vec)  n_val += 1 
```

```
 n_val 
```

```
 28650 
```

It has a validation set of 28,650 items in it. For every image in ImageNet, we can grab its fast text word vector using the synset to word vector ( `syn2wv` ) and we can stick that into the image vectors array ( `img_vecs` ), stack that all up into a single matrix and save that away.

```
 img_vecs = np.stack(img_vecs)  img_vecs.shape 
```

Now what we have is something for every ImageNet image, we also have the fast text word vector that it is associated with [ [2:03:43](https://youtu.be/tY0n9OT5_nA%3Ft%3D2h3m43s) ] by looking up the synset ID → WordNet → Fast text → word vector.

```
 pickle.dump(images, (TMP_PATH/'images.pkl').open('wb'))  pickle.dump(img_vecs, (TMP_PATH/'img_vecs.pkl').open('wb')) 
```

```
 images = pickle.load((TMP_PATH/'images.pkl').open('rb'))  img_vecs = pickle.load((TMP_PATH/'img_vecs.pkl').open('rb')) 
```

```
 arch = resnet50 
```

```
 n = len(images); n 
```

```
 766876 
```

```
 val_idxs = list(range(n-28650, n)) 
```

Here is a cool trick [ [2:04:06](https://youtu.be/tY0n9OT5_nA%3Ft%3D2h4m6s) ]. We can now create a model data object which specifically is an image classifier data object and we have this thing called `from_names_and_array` I'm not sure if we've used it before but we can pass it a list of file names (all of the file names in ImageNet) and an array of our dependent variables (all of the fast text word vectors). We can then pass in the validation indexes which in this case is just all of the last IDs — we need to make sure that they are the same as ImageNet uses otherwise we will be cheating. Then we pass in `continuous=True` which means this puts a lie again to this image classifier data is now an image regressive data so continuous equals True means don't one hot encode my outputs but treat them just as continuous values. So now we have a model data object that contains all of our file names and for every file name a continuous array representing the word vector for that. So we have data, now we need an architecture and the loss function.

```
 tfms = tfms_from_model(arch, 224, transforms_side_on, max_zoom=1.1)  md = ImageClassifierData. from_names_and_array (PATH, images,  img_vecs, val_idxs=val_idxs, classes= None , tfms=tfms,  continuous= True , bs=256) 
```

```
 x,y = next(iter(md.val_dl)) 
```

Let's create an architecture [ [2:05:26](https://youtu.be/tY0n9OT5_nA%3Ft%3D2h5m26s) ]. We'll revise this next week, but we can use the tricks we've learnt so far and it's actually incredibly simple. Fastai has a `ConvnetBuilder` which is what gets called when you say `ConvLerner.pretrained` and you specify:

*   `f` : the architecture (we are going to use ResNet50)
*   `c` : how many classes you want (in this case, it's not really classes — it's how many outputs you want which is the length of the fast text word vector ie 300).
*   `is_multi` : It is not a multi classification as it is not classification at all.
*   `is_reg` : Yes, it is a regression.
*   `xtra_fc` : What fully connected layers you want. We are just going to add one fully connected hidden layer of a length of 1024\. Why 1024? The last layer of ResNet50 I think is 1024 long, the final output we need is 300 long. We obviously need our penultimate (second to the last) layer to be longer than 300\. Otherwise it's not enough information, so we just picked something a bit bigger. Maybe different numbers would be better but this worked for Jeremy.
*   `ps` : how much dropout you want. Jeremy found that the default dropout, he was consistently under fitting so he just decreased the dropout from 0.5 to 0.2\.

So this is now a convolutional neural network that does not have any softmax or anything like that because it's regression it's just a linear layer at the end and that's our model [ [2:06:55](https://youtu.be/tY0n9OT5_nA%3Ft%3D2h6m55s) ]. We can create a ConvLearner from that model and give it an optimization function. So now all we need is a loss function.

```
 models = ConvnetBuilder(arch, md.c, is_multi= False , is_reg= True ,  xtra_fc=[1024], ps=[0.2,0.2]) 
```

```
 learn = ConvLearner(md, models, precompute= True )  learn.opt_fn = partial(optim.Adam, betas=(0.9,0.99)) 
```

**Loss Function** [ [2:07:38](https://youtu.be/tY0n9OT5_nA%3Ft%3D2h7m38s) ]: Default loss function for regression is L1 loss (the absolute differences) — that is not bad. But unfortunately in really high dimensional spaces (anybody who has studied a bit of machine learning probably knows this) everything is on the outside (in this case, it's 300 dimensional). When everything is on the outside, distance is not meaningless but a little bit awkward. Things tend to be close together or far away, it doesn't really mean much in these really high dimensional spaces where everything is on the edge. What does mean something, though, is that if one thing is on the edge over here, and one thing is on the edge over there, we can form an angle between those vectors and the angle is meaningful. That is why we use cosine similarity when we are looking for how close or far apart things are in high dimensional spaces. If you haven't seen cosine similarity before, it is basically the same as Euclidean distance but it's normalized to be a unit norm (ie divided by the length). So we don't care about the length of the vector, we only care about its angle. There is a bunch of stuff that you could easily learn in a couple of hours but if you haven't seen it before, it's a bit mysterious. For now, just know that loss functions and high dimensional spaces where you are trying to find similarity, you care about angle and you don't care about distance [ [2:09:13](https://youtu.be/tY0n9OT5_nA%3Ft%3D2h9m13s) ]. If you didn't use the following custom loss function, it would still work but it's a little bit less good. Now we have data, architecture, and loss function, therefore, we are done. We can go ahead and fit.

```
 def cos_loss(inp,targ):  return 1 - F.cosine_similarity(inp,targ).mean()  learn.crit = cos_loss 
```

```
 learn.lr_find(start_lr=1e-4, end_lr=1e15) 
```

```
 learn.sched.plot() 
```

```
 lr = 1e-2  wd = 1e-7 
```

We are training on all of ImageNet that is going to take a long time. So `precompute=True` is your friend. Remember `precompute=True` ? That is the thing we've learnt ages ago that caches the output of the final convolutional layer and just trains the fully connected bit. Even with `precompute=True` , it takes about 3 minutes to train an epoch on all of ImageNet. So this is about an hour worth of training, but it's pretty cool that with fastai, we can train a new custom head on all of ImageNet for 40 epochs in an hour or so.

```
 learn.precompute= True 
```

```
 learn.fit(lr, 1, cycle_len=20, wds=wd, use_clr=(20,10)) 
```

```
 _epoch trn_loss val_loss_ 
 0 0.104692 0.125685 
 1 0.112455 0.129307 
 2 0.110631 0.126568 
 3 0.108629 0.127338 
 4 0.110791 0.125033 
 5 0.108859 0.125186 
 6 0.106582 0.123875 
 7 0.103227 0.123945 
 8 0.10396 0.12304 
 9 0.105898 0.124894 
 10 0.10498 0.122582 
 11 0.104983 0.122906 
 12 0.102317 0.121171 
 13 0.10017 0.121816 
 14 0.099454 0.119647 
 15 0.100425 0.120914 
 16 0.097226 0.119724 
 17 0.094666 0.118746 
 18 0.094137 0.118744 
 19 0.090076 0.117908 
```

```
 [0.11790786389489033] 
```

```
 learn.bn_freeze( True ) 
```

```
 learn.fit(lr, 1, cycle_len=20, wds=wd, use_clr=(20,10)) 
```

```
 _epoch trn_loss val_loss_ 
 0 0.104692 0.125685 
 1 0.112455 0.129307 
 2 0.110631 0.126568 
 3 0.108629 0.127338 
 4 0.110791 0.125033 
 5 0.108859 0.125186 
 6 0.106582 0.123875 
 7 0.103227 0.123945 
 8 0.10396 0.12304 
 9 0.105898 0.124894 
 10 0.10498 0.122582 
 11 0.104983 0.122906 
 12 0.102317 0.121171 
 13 0.10017 0.121816 
 14 0.099454 0.119647 
 15 0.100425 0.120914 
 16 0.097226 0.119724 
 17 0.094666 0.118746 
 18 0.094137 0.118744 
 19 0.090076 0.117908 
```

```
 [0.11790786389489033] 
```

```
 lrs = np.array([lr/1000,lr/100,lr]) 
```

```
 learn.precompute= False  learn.freeze_to(1) 
```

```
 learn.save('pre0') 
```

```
 learn.load('pre0') 
```

### Image search

#### Search imagenet classes

At the end of all that, we can now say let's grab the 1000 ImageNet classes, let's predict on our whole validation set, and take a look at a few pictures [ [2:10:26](https://youtu.be/tY0n9OT5_nA%3Ft%3D2h10m26s) ].

```
 syns, wvs = list(zip(*syn_wv_1k))  wvs = np.array(wvs) 
```

```
 %time pred_wv = learn.predict() 
```

```
 CPU times: user 18.4 s, sys: 7.91 s, total: 26.3 s 
 Wall time: 7.17 s 
```

```
 start=300 
```

```
 denorm = md.val_ds.denorm 
```

```
 def show_img(im, figsize= None , ax= None ):  if not ax: fig,ax = plt.subplots(figsize=figsize)  ax.imshow(im)  ax.axis('off')  return ax 
```

```
 def show_imgs(ims, cols, figsize= None ):  fig,axes = plt.subplots(len(ims)//cols, cols, figsize=figsize)  for i,ax in enumerate(axes.flat): show_img(ims[i], ax=ax)  plt.tight_layout() 
```

Because validation set is ordered, tall the stuff of the same type are in the same place.

```
 show_imgs(denorm(md.val_ds[start:start+25][0]), 5, (10,10)) 
```

![](../img/1_exiD0uDeL6xx5EOLdPS3BA.png)

**Nearest neighbor search** [ [2:10:56](https://youtu.be/tY0n9OT5_nA%3Ft%3D2h10m56s) ]: What we can now do is we can now use nearest neighbors search. So nearest neighbors search means here is one300 dimensional vector and here is a whole a lot of other 300 dimensional vectors, which things is it closest to? Normally that takes a very long time because you have to look through every 300 dimensional vector, calculate its distance, and find out how far away it is. But there is an amazing almost unknown library called **NMSLib** that does that incredibly fast. Some of you may have tried other nearest neighbor's libraries, I guarantee this is faster than what you are using — I can tell you that because it's been bench marked by people who do this stuff for a living. This is by far the fastest on every possible dimension. We want to create an index on angular distance, and we need to do it on all of our ImageNet word vectors. Adding a whole batch, create the index, and now we can query a bunch of vectors all at once, get the 10 nearest neighbors. The library uses multi-threading and is absolutely fantastic. You can install from pip ( `pip install nmslib` ) and it just works.

```
 import nmslib 
```

```
 def create_index(a):  index = nmslib.init(space='angulardist')  index.addDataPointBatch(a)  index.createIndex()  return index 
```

```
 def get_knns(index, vecs):  return zip(*index.knnQueryBatch(vecs, k=10, num_threads=4)) 
```

```
 def get_knn(index, vec): return index.knnQuery(vec, k=10) 
```

```
 nn_wvs = create_index(wvs) 
```

It tells you how far away they are and their indexes [ [2:12:13](https://youtu.be/tY0n9OT5_nA%3Ft%3D2h12m13s) ].

```
 idxs,dists = get_knns(nn_wvs, pred_wv) 
```

So now we can go through and print out the top 3 so it turns out that bird actually is a limpkin. Interestingly the fourth one does not say it's a limpkin and Jeremy looked it up. He doesn't know much about birds but everything else is brown with white spots, but the 4th one isn't. So we don't know if that is actually a limpkin or if it is mislabeled but sure as heck it doesn't look like the other birds.

```
 [[classids[syns[id]] for id in ids[:3]]  for ids in idxs[start:start+10]] 
```

```
 [['limpkin', 'oystercatcher', 'spoonbill'], 
 ['limpkin', 'oystercatcher', 'spoonbill'], 
 ['limpkin', 'oystercatcher', 'spoonbill'], 
 ['spoonbill', 'bustard', 'oystercatcher'], 
 ['limpkin', 'oystercatcher', 'spoonbill'], 
 ['limpkin', 'oystercatcher', 'spoonbill'], 
 ['limpkin', 'oystercatcher', 'spoonbill'], 
 ['limpkin', 'oystercatcher', 'spoonbill'], 
 ['limpkin', 'oystercatcher', 'spoonbill'], 
 ['limpkin', 'oystercatcher', 'spoonbill']] 
```

This is not a particularly hard thing to do because there is only a thousand ImageNet classes and it is not doing anything new. But what if we now bring in the entirety of WordNet and we now say which of those 45 thousand things is it closest to?

#### Search all WordMet noun classes

```
 all_syns, all_wvs = list(zip(*syn2wv.items()))  all_wvs = np.array(all_wvs) 
```

```
 nn_allwvs = create_index(all_wvs) 
```

```
 idxs,dists = get_knns(nn_allwvs, pred_wv) 
```

```
 [[classids[all_syns[id]] for id in ids[:3]]  for ids in idxs[start:start+10]] 
```

```
 [['limpkin', 'oystercatcher', 'spoonbill'], 
 ['limpkin', 'oystercatcher', 'spoonbill'], 
 ['limpkin', 'oystercatcher', 'spoonbill'], 
 ['spoonbill', 'bustard', 'oystercatcher'], 
 ['limpkin', 'oystercatcher', 'spoonbill'], 
 ['limpkin', 'oystercatcher', 'spoonbill'], 
 ['limpkin', 'oystercatcher', 'spoonbill'], 
 ['limpkin', 'oystercatcher', 'spoonbill'], 
 ['limpkin', 'oystercatcher', 'spoonbill'], 
 ['limpkin', 'oystercatcher', 'spoonbill']] 
```

Exactly the same result. It is now searching all of the WordNet.

#### Text -&gt; image search [ [2:13:16](https://youtu.be/tY0n9OT5_nA%3Ft%3D2h13m16s) ]

Now let's do something a bit different — which is to take all of our predictions ( `pred_wv` ) so basically take our whole validation set of images and create a KNN index of the image representations because remember, it is predicting things that are meant to be word vectors. Now let's grab the fast text vector for “boat” and boat is not an ImageNet concept — yet we can now find all of the images in our predicted word vectors (ie our validation set) that are closest to the word boat and it works even though it is not something that was ever trained on.

```
 nn_predwv = create_index(pred_wv)  en_vecd = pickle.load(open(TRANS_PATH/'wiki.en.pkl','rb'))  vec = en_vecd['boat'] 
```

```
 idxs,dists = get_knn(nn_predwv, vec)  show_imgs([open_image(PATH/md.val_ds.fnames[i]) for i in idxs[:3]],  3, figsize=(9,3)); 
```

![](../img/1_BLLI7IWFO84BPwB-Y1uCKg.png)

What if we now take engine's vector and boat's vector and take their average and what if we now look in our nearest neighbors for that [ [2:14:04](https://youtu.be/tY0n9OT5_nA%3Ft%3D2h14m4s) ]?

```
 vec = (en_vecd['engine'] + en_vecd['boat'])/2 
```

```
 idxs,dists = get_knn(nn_predwv, vec)  show_imgs([open_image(PATH/md.val_ds.fnames[i]) for i in idxs[:3]],  3, figsize=(9,3)); 
```

![](../img/1_EkwjxE8m8xeX2lDRLIJTCw.png)

These are boats with engines. I mean, yes, the middle one is actually a boat with an engine — it just happens to have wings on as well. By the way, sail is not an ImageNet thing , neither is boat. Here is the average of two things that are not ImageNet things and yet with one exception, it's found us two sailboats.

```
 vec = (en_vecd['sail'] + en_vecd['boat'])/2 
```

```
 idxs,dists = get_knn(nn_predwv, vec)  show_imgs([open_image(PATH/md.val_ds.fnames[i]) for i in idxs[:3]],  3, figsize=(9,3)); 
```

![](../img/1_-7X7LGxPWi1qv8fF1hkCog.png)

#### Image-&gt;image [ [2:14:35](https://youtu.be/tY0n9OT5_nA%3Ft%3D2h14m35s) ]

Okay, let's do something else crazy. Let's open up an image in the validation set. Let's call `predict_array` on that image to get its word vector like thing, and let's do a nearest neighbor search on all the other images.

```
 fname = 'valid/n01440764/ILSVRC2012_val_00007197.JPEG'  img = open_image(PATH/fname)  show_img(img); 
```

![](../img/1_FVCX6O367r2oJXVhR1r-Sg.png)

```
 t_img = md.val_ds.transform(img)  pred = learn.predict_array(t_img[ None ]) 
```

```
 idxs,dists = get_knn(nn_predwv, pred)  show_imgs([open_image(PATH/md.val_ds.fnames[i]) for i in idxs[1:4]],  3, figsize=(9,3)); 
```

![](../img/1_hQ8r1gcwNfh7KlZGjS-lcQ.png)

And here are all the other images of whatever that is. So you can see, this is crazy — we've trained a thing on all of ImageNet in an hour, using a custom head that required basically like two lines fo code, and these things run in 300 milliseconds to do these searches.

Jeremy taught this basic idea last year as well, but it was in Keras, and it was pages and pages of code, and everything took a long time and complicated. And back then, Jeremy said he can't begin to think all of the stuff you could do with this. He doesn't think anybody has really thought deeply about this yet, but he thinks it's fascinating. So go back and read the DeVICE paper because Andrea had a whole bunch of other thoughts and now that it is so easy to do, hopefully people will dig into this now. Jeremy thinks it's crazy and amazing.

Alright, see you next week!
