<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></head><body><p name="759e" id="759e" class="graf graf--p graf-after--h4">Before we do, let’s take a quick look. So we’ve got this super resolution ResNet which just does lots of computation with lots of ResNet blocks and then it does some upsampling and gets our final three channels out.</p><p name="ef25" id="ef25" class="graf graf--p graf-after--p">Then to make life faster, we are going to run tins in parallel. One reason we want to run it in parallel is because Gerardo told us that he has 6 GPUs and this is what his computer looks like right now.</p><figure name="0599" id="0599" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_rPTiAdy8iIVV3twfOG7mHg.png"></figure><p name="8d51" id="8d51" class="graf graf--p graf-after--figure">So I’m sure anybody who has more than one GPU has had this experience before. So how do we get these men working together? All you need to do is to take your PyTorch module and wrap it with <code class="markup--code markup--p-code">nn.DataParallel</code>. Once you’ve done that, it copies it to each of your GPUs and will automatically run it in parallel. It scales pretty well to two GPUs, okay to three GPUs, better than nothing to four GPUs and beyond that, performance does go backwards. By default, it will copy it to all of your GPUs — you can add an array of GPUs otherwise if you want to avoid getting in trouble, for example, I have to share our box with Yannet and if I didn’t put this here, then she would be yelling at me right now or boycotting my class. So this is how you avoid getting into trouble with Yannet.</p><pre name="7cbd" id="7cbd" class="graf graf--pre graf-after--p">m = to_gpu(SrResnet(64, scale))<br>m = nn.DataParallel(m, [0,2])<br>learn = Learner(md, SingleModel(m), opt_fn=optim.Adam)<br>learn.crit = F.mse_loss</pre><p name="cece" id="cece" class="graf graf--p graf-after--pre">One thing to be aware of here is that once you do this, it actually modifies your module [<a href="https://youtu.be/nG3tT31nPmQ?t=48m21s" data-href="https://youtu.be/nG3tT31nPmQ?t=48m21s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">48:21</a>]. So if you now print out your module, let’s say previously it was just an endless sequential, now you’ll find it’s an <code class="markup--code markup--p-code">nn.Sequential </code>embedded inside a module called <code class="markup--code markup--p-code">Module</code>. In other words, if you save something which you had <code class="markup--code markup--p-code">nn.DataParallel</code> and then tried and load it back into something you haven’t <code class="markup--code markup--p-code">nn.DataParallel</code>, it’ll say it doesn’t match up because one of them is embedded inside this Module attribute and the other one isn’t. It may also depend even on which GPU IDs you have had it copy to. Two possible solutions:</p><ol class="postList"><li name="e8cc" id="e8cc" class="graf graf--li graf-after--p">Don’t save the module <code class="markup--code markup--li-code">m</code> but instead save the module attribute <code class="markup--code markup--li-code">m.module</code> because that’s actually the non data parallel bit.</li><li name="1f19" id="1f19" class="graf graf--li graf-after--li">Always put it on the same GPU IDs and then use data parallel and load and save that every time. That’s what I was using.</li></ol><p name="7ef5" id="7ef5" class="graf graf--p graf-after--li">This is an easy thing for me to fix automatically in fast.ai and I’ll do it pretty soon so it will look for that module attribute and deal with it automatically. But for now, we have to do it manually. It’s probably useful to know what’s going on behind the scenes anyway.</p><p name="87f6" id="87f6" class="graf graf--p graf-after--p">So we’ve got our module [<a href="https://youtu.be/nG3tT31nPmQ?t=49m46s" data-href="https://youtu.be/nG3tT31nPmQ?t=49m46s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">49:46</a>]. I find it’ll run 50 or 60% faster on a 1080Ti if you are running on volta, it actually parallelize a bit better. There are much faster ways to parallelize but this is a super easy way.</p><p name="5327" id="5327" class="graf graf--p graf-after--p">We create our learner in the usual way. We can use MSE loss here so that’s just going to compare the pixels of the output to the pixels that we expected. We can run our learning rate finder and we can train it for a while.</p><pre name="9d98" id="9d98" class="graf graf--pre graf-after--p">learn.lr_find(start_lr=1e-5, end_lr=10000)<br>learn.sched.plot()</pre><pre name="16cd" id="16cd" class="graf graf--pre graf-after--pre">31%|███▏      | 225/720 [00:24&lt;00:53,  9.19it/s, loss=0.0482]</pre><figure name="e25a" id="e25a" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_3NGCeWjks8iCWo97ag8KgQ.png"></figure><pre name="9825" id="9825" class="graf graf--pre graf-after--figure">lr=2e-3</pre><pre name="6465" id="6465" class="graf graf--pre graf-after--pre">learn.fit(lr, 1, cycle_len=1, use_clr_beta=(40,10))</pre><pre name="3614" id="3614" class="graf graf--pre graf-after--pre">2%|▏         | 15/720 [00:02&lt;01:52,  6.25it/s, loss=0.042]  <br>epoch      trn_loss   val_loss                                 <br>    0      0.007431   0.008192</pre><pre name="4ceb" id="4ceb" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[array([0.00819])]</em></pre><pre name="e978" id="e978" class="graf graf--pre graf-after--pre">x,y = next(iter(md.val_dl))<br>preds = learn.model(VV(x))</pre><p name="2b51" id="2b51" class="graf graf--p graf-after--pre">Here is our input:</p><pre name="8437" id="8437" class="graf graf--pre graf-after--p">idx=4<br>show_img(y,idx,normed=<strong class="markup--strong markup--pre-strong">False</strong>)</pre><figure name="a115" id="a115" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_eLOmwY4FKA-XU1MjNlw2PA.png"></figure><p name="35a2" id="35a2" class="graf graf--p graf-after--figure">And here is our output.</p><pre name="5498" id="5498" class="graf graf--pre graf-after--p">show_img(preds,idx,normed=<strong class="markup--strong markup--pre-strong">False</strong>);</pre><figure name="7610" id="7610" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_qHj8r-MMhw_koKNxyNFKxQ.png"></figure><p name="b775" id="b775" class="graf graf--p graf-after--figure">And you can see that what we’ve managed to do is to train a very advanced residual convolutional network that’s learnt to blue things. Why is that? Well, because it’s what we asked for. We said to minimize MSE loss. MSE loss between pixels really the best way to do that is just average the pixel i.e. to blur it. So that’s why pixel loss is no good. So we want to use our perceptual loss.</p><pre name="ddf6" id="ddf6" class="graf graf--pre graf-after--p">show_img(x,idx,normed=<strong class="markup--strong markup--pre-strong">True</strong>);</pre><figure name="29b8" id="29b8" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_c2wlq6d0wuNB2xIjaRMcZw.png"></figure><pre name="7438" id="7438" class="graf graf--pre graf-after--figure">x,y = next(iter(md.val_dl))<br>preds = learn.model(VV(x))</pre><pre name="e8b7" id="e8b7" class="graf graf--pre graf-after--pre">show_img(y,idx,normed=<strong class="markup--strong markup--pre-strong">False</strong>)</pre><figure name="6ab9" id="6ab9" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_MEYmY0bflI07lSFsEDMS4Q.png"></figure><pre name="5836" id="5836" class="graf graf--pre graf-after--figure">show_img(preds,idx,normed=<strong class="markup--strong markup--pre-strong">False</strong>);</pre><figure name="7822" id="7822" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_34ihffYGeiFCpgaWIcDOjw.png"></figure><pre name="b3e3" id="b3e3" class="graf graf--pre graf-after--figure">show_img(x,idx);</pre><figure name="6693" id="6693" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_SLZNRUiVAoakl-7YKNuqyw.png"></figure><h4 name="6ba2" id="6ba2" class="graf graf--h4 graf-after--figure">Perceptual loss&nbsp;[50:57]</h4><p name="c03f" id="c03f" class="graf graf--p graf-after--h4">With perceptual loss, we are basically going to take our VGG network and just like we did last week, we are going to find the block index just before we get a maxpool.</p><pre name="69e2" id="69e2" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> icnr(x, scale=2, init=nn.init.kaiming_normal):<br>    new_shape = [int(x.shape[0] / (scale ** 2))] + list(x.shape[1:])<br>    subkernel = torch.zeros(new_shape)<br>    subkernel = init(subkernel)<br>    subkernel = subkernel.transpose(0, 1)<br>    subkernel = subkernel.contiguous().view(subkernel.shape[0],<br>                                            subkernel.shape[1], -1)<br>    kernel = subkernel.repeat(1, 1, scale ** 2)<br>    transposed_shape = [x.shape[1]] + [x.shape[0]] + <br>                          list(x.shape[2:])<br>    kernel = kernel.contiguous().view(transposed_shape)<br>    kernel = kernel.transpose(0, 1)<br>    <strong class="markup--strong markup--pre-strong">return</strong> kernel</pre><pre name="8869" id="8869" class="graf graf--pre graf-after--pre">m_vgg = vgg16(<strong class="markup--strong markup--pre-strong">True</strong>)<br><br>blocks = [i-1 <strong class="markup--strong markup--pre-strong">for</strong> i,o <strong class="markup--strong markup--pre-strong">in</strong> enumerate(children(m_vgg))<br>              <strong class="markup--strong markup--pre-strong">if</strong> isinstance(o,nn.MaxPool2d)]<br>blocks, [m_vgg[i] <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> blocks]</pre><pre name="4f3c" id="4f3c" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">([5, 12, 22, 32, 42],<br> [ReLU(inplace), ReLU(inplace), ReLU(inplace), ReLU(inplace), ReLU(inplace)])</em></pre><p name="eac5" id="eac5" class="graf graf--p graf-after--pre">So here are the ends of each block of the same grid size. If we just print them out, as we’d expect, every one of those is a ReLU module and so in this case these last two blocks are less interesting to us. The grid size there is small enough, and course enough that it’s not as useful for super resolution. So we are just going to use the first three. Just to save unnecessary computation, we are just going to use those first 23 layers of VGG and we’ll throw away the rest. We’ll stick it on the GPU. We are not going to be training this VGG model at all — we are just using it to compare activations. So we’ll stick it in eval mode and we will set it to not trainable.</p><pre name="c557" id="c557" class="graf graf--pre graf-after--p">vgg_layers = children(m_vgg)[:23]<br>m_vgg = nn.Sequential(*vgg_layers).cuda().eval()<br>set_trainable(m_vgg, <strong class="markup--strong markup--pre-strong">False</strong>)</pre><pre name="0d0e" id="0d0e" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> flatten(x): <strong class="markup--strong markup--pre-strong">return</strong> x.view(x.size(0), -1)</pre><p name="e74e" id="e74e" class="graf graf--p graf-after--pre">Just like last week, we will use <code class="markup--code markup--p-code">SaveFeatures</code> class to do a forward hook which saves the output activations at each of those layers [<a href="https://youtu.be/nG3tT31nPmQ?t=52m7s" data-href="https://youtu.be/nG3tT31nPmQ?t=52m7s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">52:07</a>].</p><pre name="2107" id="2107" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">SaveFeatures</strong>():<br>    features=<strong class="markup--strong markup--pre-strong">None</strong><br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, m): <br>        self.hook = m.register_forward_hook(self.hook_fn)<br>    <strong class="markup--strong markup--pre-strong">def</strong> hook_fn(self, module, input, output): self.features = output<br>    <strong class="markup--strong markup--pre-strong">def</strong> remove(self): self.hook.remove()</pre><p name="b62d" id="b62d" class="graf graf--p graf-after--pre">So now we have everything we need to create our perceptual loss or as I call it here <code class="markup--code markup--p-code">FeatureLoss</code> class. We are going to pass in a list of layer IDs, the layers where we want the content loss to be calculated, and a list of weights for each of those layers. We can go through each of those layer IDs and create an object which has the forward hook function to store the activations. So in our forward, then we can just go ahead and call the forward pass of our model with the target (high res image we are trying to create). The reason we do that is because that is going to then call that hook function and store in <code class="markup--code markup--p-code">self.sfs</code> (self dot save features) the activations we want. Now we are going to need to do that for our conv net output as well. So we need to clone these because otherwise the conv net output is going to go ahead and just clobber what I already had. So now we can do the same thing for the conv net output which is the input to the loss function. And so now we’ve got those two things we can zip them all together along with the weights so we’ve got inputs, targets, and weights. Then we can do the L1 loss between the inputs and the targets and multiply by the layer weights. The only other thing I do is I also grab the pixel loss, but I weight it down quite a bit. Most people don’t do this. I haven’t seen papers that do this, but in my opinion, it’s maybe a little bit better because you’ve got the perceptual content loss activation stuff but the really finest level it also cares about the individual pixels. So that’s our loss function.</p><pre name="8d19" id="8d19" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">FeatureLoss</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, m, layer_ids, layer_wgts):<br>        super().__init__()<br>        self.m,self.wgts = m,layer_wgts<br>        self.sfs = [SaveFeatures(m[i]) <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> layer_ids]<br><br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, input, target, sum_layers=<strong class="markup--strong markup--pre-strong">True</strong>):<br>        self.m(VV(target.data))<br>        res = [F.l1_loss(input,target)/100]<br>        targ_feat = [V(o.features.data.clone()) <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> self.sfs]<br>        self.m(input)<br>        res += [F.l1_loss(flatten(inp.features),flatten(targ))*wgt<br>               <strong class="markup--strong markup--pre-strong">for</strong> inp,targ,wgt <strong class="markup--strong markup--pre-strong">in</strong> zip(self.sfs, targ_feat, <br>                                       self.wgts)]<br>        <strong class="markup--strong markup--pre-strong">if</strong> sum_layers: res = sum(res)<br>        <strong class="markup--strong markup--pre-strong">return</strong> res<br>    <br>    <strong class="markup--strong markup--pre-strong">def</strong> close(self):<br>        <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> self.sfs: o.remove()</pre><p name="7869" id="7869" class="graf graf--p graf-after--pre">We create our super resolution ResNet telling it how much to scale up by.</p><pre name="2af4" id="2af4" class="graf graf--pre graf-after--p">m = SrResnet(64, scale)</pre><p name="ce07" id="ce07" class="graf graf--p graf-after--pre">And then we are going to do our <code class="markup--code markup--p-code">icnr</code> initialization of that pixel shuffle convolution [<a href="https://youtu.be/nG3tT31nPmQ?t=54m27s" data-href="https://youtu.be/nG3tT31nPmQ?t=54m27s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">54:27</a>]. This is very boring code, I actually stole it from somebody else. Literally all it does is just say okay, you’ve got some weight tensor <code class="markup--code markup--p-code">x</code> that you want to initialize so we are going to treat it as if it has shape (i.e. number of features) divided by scale squared features in practice. So this might be 2² = 4 because we actually want to just keep one set of then and then copy them four times, so we divide it by four and we create something of that size and we initialize that with, by default, <code class="markup--code markup--p-code">kaiming_normal</code> initialization. Then we just make scale² copies of it. And the rest of it is just kind of moving axes around a little bit. So that’s going to return a new weight matrix where each initialized sub kernel is repeated r² or <code class="markup--code markup--p-code">scale</code>² times. So that details don’t matter very much. All that matters here is that I just looked through to find what was the actual conv layer just before the pixel shuffle and store it away and then I called <code class="markup--code markup--p-code">icnr</code> on its weight matrix to get my new weight matrix. And then I copied that new weight matrix back into that layer.</p><pre name="8b60" id="8b60" class="graf graf--pre graf-after--p">conv_shuffle = m.features[10][0][0]<br>kernel = icnr(conv_shuffle.weight, scale=scale)<br>conv_shuffle.weight.data.copy_(kernel);</pre><p name="f263" id="f263" class="graf graf--p graf-after--pre">As you can see, I went to quite a lot of trouble in this exercise to really try to implement all the best practices [<a href="https://youtu.be/nG3tT31nPmQ?t=56m13s" data-href="https://youtu.be/nG3tT31nPmQ?t=56m13s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">56:13</a>]. I tend to do things a bit one extreme or the other. I show you a really hacky version that only slightly works or I go to the <em class="markup--em markup--p-em">n</em>th degree to make it work really well. So this is a version where I’m claiming that this is pretty much a state of the art implementation. It’s a competition winning or at least my re-implementation of a competition winning approach. The reason I’m doing that is because I think this is one of those rare papers where they actually get a lot of the details right and I want you to get a feel of what it feels like to get all the details right. Remember, getting the details right is the difference between the hideous blurry mess and the pretty exquisite result.</p><pre name="454f" id="454f" class="graf graf--pre graf-after--p">m = to_gpu(m)</pre><pre name="7271" id="7271" class="graf graf--pre graf-after--pre">learn = Learner(md, SingleModel(m), opt_fn=optim.Adam)</pre><pre name="1a69" id="1a69" class="graf graf--pre graf-after--pre">t = torch.load(learn.get_model_path('sr-samp0'), <br>         map_location=<strong class="markup--strong markup--pre-strong">lambda</strong> storage, loc: storage)<br>learn.model.load_state_dict(t, strict=<strong class="markup--strong markup--pre-strong">False</strong>)</pre><pre name="f9af" id="f9af" class="graf graf--pre graf-after--pre">learn.freeze_to(999)</pre><pre name="b270" id="b270" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(10,13): set_trainable(m.features[i], <strong class="markup--strong markup--pre-strong">True</strong>)</pre><pre name="e76a" id="e76a" class="graf graf--pre graf-after--pre">conv_shuffle = m.features[10][2][0]<br>kernel = icnr(conv_shuffle.weight, scale=scale)<br>conv_shuffle.weight.data.copy_(kernel);</pre><p name="f3af" id="f3af" class="graf graf--p graf-after--pre">So we are going do DataParallel on that again [<a href="https://youtu.be/nG3tT31nPmQ?t=57m14s" data-href="https://youtu.be/nG3tT31nPmQ?t=57m14s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">57:14</a>].</p><pre name="c5ea" id="c5ea" class="graf graf--pre graf-after--p">m = nn.DataParallel(m, [0,2])<br>learn = Learner(md, SingleModel(m), opt_fn=optim.Adam)</pre><pre name="613e" id="613e" class="graf graf--pre graf-after--pre">learn.set_data(md)</pre><p name="1f2a" id="1f2a" class="graf graf--p graf-after--pre">We are going to set our criterion to be FeatureLoss using our VGG model, grab the first few blocks and these are sets of layer weights that I found worked pretty well.</p><pre name="3a8c" id="3a8c" class="graf graf--pre graf-after--p">learn.crit = FeatureLoss(m_vgg, blocks[:3], [0.2,0.7,0.1])</pre><pre name="eb82" id="eb82" class="graf graf--pre graf-after--pre">lr=6e-3<br>wd=1e-7</pre><p name="0739" id="0739" class="graf graf--p graf-after--pre">Do a learning rate finder.</p><pre name="66cc" id="66cc" class="graf graf--pre graf-after--p">learn.lr_find(1e-4, 0.1, wds=wd, linear=<strong class="markup--strong markup--pre-strong">True</strong>)</pre><pre name="059b" id="059b" class="graf graf--pre graf-after--pre"> 1%|          | 15/1801 [00:06&lt;12:55,  2.30it/s, loss=0.0965]<br>12%|█▏        | 220/1801 [01:16&lt;09:08,  2.88it/s, loss=0.42]</pre><pre name="736f" id="736f" class="graf graf--pre graf-after--pre">learn.sched.plot(n_skip_end=1)</pre><figure name="5156" id="5156" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_pFxZy5AKqmhC031Q6a71PQ.png"></figure><p name="60ec" id="60ec" class="graf graf--p graf-after--figure">Fit it for a while</p><pre name="f015" id="f015" class="graf graf--pre graf-after--p">learn.fit(lr, 1, cycle_len=2, wds=wd, use_clr=(20,10))</pre><pre name="9a5e" id="9a5e" class="graf graf--pre graf-after--pre">epoch      trn_loss   val_loss                                  <br>    0      0.04523    0.042932  <br>    1      0.043574   0.041242</pre><pre name="06bd" id="06bd" class="graf graf--pre graf-after--pre">[array([0.04124])]</pre><pre name="dc7e" id="dc7e" class="graf graf--pre graf-after--pre">learn.save('sr-samp0')</pre><pre name="0ba4" id="0ba4" class="graf graf--pre graf-after--pre">learn.save('sr-samp1')</pre><p name="efe3" id="efe3" class="graf graf--p graf-after--pre">And I fiddled around for a while trying to get some of these details right. But here is my favorite part of the paper is what happens next. Now that we’ve done it for scale equals 2 — progressive resizing. So progressive resizing is the trick that let us get the best best single computer result for ImageNet training on DAWN bench. It’s this idea of starting small gradually making bigger. I only know of two papers that have used this idea. One is the progressive resizing of GANs paper which allows training a very high resolution GANs and the other one is the EDSR paper. And the cool thing about progressive resizing is not only are your earlier epochs, assuming you’ve got 2x2 smaller, four times faster. You can also make the batch size maybe 3 or 4 times bigger. But more importantly, they are going to generalize better because you are feeding in your model different sized images during training. So we were able to train half as many epochs for ImageNet as most people. Our epochs were faster and there were fewer of them. So progressive resizing is something that, particularly if you are training from scratch (I’m not so sure if it’s useful for fine-tuning transfer learning, but if you are training from scratch), you probably want to do nearly all the time.</p><h4 name="ebe2" id="ebe2" class="graf graf--h4 graf-after--p">Progressive resizing&nbsp;[<a href="https://youtu.be/nG3tT31nPmQ?t=59m7s" data-href="https://youtu.be/nG3tT31nPmQ?t=59m7s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">59:07</a>]</h4><p name="16d2" id="16d2" class="graf graf--p graf-after--h4">So the next step is to go all the way back to the top and change to 4 scale, 32 batch size, restart. I saved the model before I do that.</p><figure name="5c8a" id="5c8a" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_Wm9cQuH2YuzT4zLdjFx5gQ.png"></figure><p name="43c2" id="43c2" class="graf graf--p graf-after--figure">Go back and that’s why there’s a little bit of fussing around in here with reloading because what I needed to do now is I needed to load my saved model back in.</p><figure name="2a1c" id="2a1c" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_sbHDTKhKUuuOqvDz8RayHw.png"></figure><p name="83e5" id="83e5" class="graf graf--p graf-after--figure">But there’s a slight issue which is I now have one more upsampling layer than I used to have to go from 2x2 to 4x4. My loop here is now looping through twice, not once. Therefore, it’s added an extra conv net and an extra pixel shuffle. So how am I going to load in weights for a different network?</p><figure name="eecb" id="eecb" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_bu5wo96iiUbwq3lfABmLSg.png"></figure><p name="ca0d" id="ca0d" class="graf graf--p graf-after--figure">The answer is that I use a very handy thing in PyTorch <code class="markup--code markup--p-code">load_state_dict</code>. This is what <code class="markup--code markup--p-code">lean.load</code> calls behind the scenes. If I pass this parameter <code class="markup--code markup--p-code">strict=False</code> then it says “okay, if you can’t fill in all of the layers, just fill in the layers you can.” So after loading the model back in this way, we are going to end up with something where it’s loaded in all the layers that it can and that one conv layer that’s new is going to be randomly initialized.</p><figure name="089c" id="089c" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_FJ6Ntp4aiKC1Lav6V4Q-9A.png"></figure><p name="63e9" id="63e9" class="graf graf--p graf-after--figure">Then I freeze all my layers and then unfreeze that upsampling part [<a href="https://youtu.be/nG3tT31nPmQ?t=1h45s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h45s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:00:45</a>] Then use <code class="markup--code markup--p-code">icnr</code> on my newly added extra layer. Then I can go ahead and learn again. So then the rest is the same.</p><p name="2c30" id="2c30" class="graf graf--p graf-after--p">If you are trying to replicate this, don’t just run this top to bottom. Realize it involves a bit of jumping around.</p><pre name="5d38" id="5d38" class="graf graf--pre graf-after--p">learn.load('sr-samp1')</pre><pre name="68f5" id="68f5" class="graf graf--pre graf-after--pre">lr=3e-3</pre><pre name="65e8" id="65e8" class="graf graf--pre graf-after--pre">learn.fit(lr, 1, cycle_len=1, wds=wd, use_clr=(20,10))</pre><pre name="581f" id="581f" class="graf graf--pre graf-after--pre">epoch      trn_loss   val_loss                                <br>    0      0.069054   0.06638</pre><pre name="6920" id="6920" class="graf graf--pre graf-after--pre">[array([0.06638])]</pre><pre name="25e8" id="25e8" class="graf graf--pre graf-after--pre">learn.save('sr-samp2')</pre><pre name="57ac" id="57ac" class="graf graf--pre graf-after--pre">learn.unfreeze()</pre><pre name="1cfa" id="1cfa" class="graf graf--pre graf-after--pre">learn.load('sr-samp2')</pre><pre name="fbdb" id="fbdb" class="graf graf--pre graf-after--pre">learn.fit(lr/3, 1, cycle_len=1, wds=wd, use_clr=(20,10))</pre><pre name="793d" id="793d" class="graf graf--pre graf-after--pre">epoch      trn_loss   val_loss           <br>    0      0.06042    0.057613</pre><pre name="0753" id="0753" class="graf graf--pre graf-after--pre">[array([0.05761])]</pre><pre name="f9ee" id="f9ee" class="graf graf--pre graf-after--pre">learn.save('sr1')</pre><pre name="79cb" id="79cb" class="graf graf--pre graf-after--pre">learn.sched.plot_loss()</pre><figure name="8120" id="8120" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_M7lOaEQaa21WBgAXVd8roQ.png"></figure><pre name="59d4" id="59d4" class="graf graf--pre graf-after--figure"><strong class="markup--strong markup--pre-strong">def</strong> plot_ds_img(idx, ax=<strong class="markup--strong markup--pre-strong">None</strong>, figsize=(7,7), normed=<strong class="markup--strong markup--pre-strong">True</strong>):<br>    <strong class="markup--strong markup--pre-strong">if</strong> ax <strong class="markup--strong markup--pre-strong">is</strong> <strong class="markup--strong markup--pre-strong">None</strong>: fig,ax = plt.subplots(figsize=figsize)<br>    im = md.val_ds[idx][0]<br>    <strong class="markup--strong markup--pre-strong">if</strong> normed: im = denorm(im)[0]<br>    <strong class="markup--strong markup--pre-strong">else</strong>:      im = np.rollaxis(to_np(im),0,3)<br>    ax.imshow(im)<br>    ax.axis('off')</pre><pre name="8443" id="8443" class="graf graf--pre graf-after--pre">fig,axes=plt.subplots(6,6,figsize=(20,20))<br><strong class="markup--strong markup--pre-strong">for</strong> i,ax <strong class="markup--strong markup--pre-strong">in</strong> enumerate(axes.flat): <br>    plot_ds_img(i+200,ax=ax, normed=<strong class="markup--strong markup--pre-strong">True</strong>)</pre><figure name="9b60" id="9b60" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_iQrT4lAb6bYt0s65aYEVsA.png"></figure><pre name="a1f1" id="a1f1" class="graf graf--pre graf-after--figure">x,y=md.val_ds[215]</pre><pre name="3959" id="3959" class="graf graf--pre graf-after--pre">y=y[<strong class="markup--strong markup--pre-strong">None</strong>]</pre><pre name="ed63" id="ed63" class="graf graf--pre graf-after--pre">learn.model.eval()<br>preds = learn.model(VV(x[<strong class="markup--strong markup--pre-strong">None</strong>]))<br>x.shape,y.shape,preds.shape</pre><pre name="77c1" id="77c1" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">((3, 72, 72), (1, 3, 288, 288), torch.Size([1, 3, 288, 288]))</em></pre><pre name="f39f" id="f39f" class="graf graf--pre graf-after--pre">learn.crit(preds, V(y), sum_layers=<strong class="markup--strong markup--pre-strong">False</strong>)</pre><pre name="5e53" id="5e53" class="graf graf--pre graf-after--pre">[Variable containing:<br> 1.00000e-03 *<br>   1.1935<br> [torch.cuda.FloatTensor of size 1 (GPU 0)], Variable containing:<br> 1.00000e-03 *<br>   8.5054<br> [torch.cuda.FloatTensor of size 1 (GPU 0)], Variable containing:<br> 1.00000e-02 *<br>   3.4656<br> [torch.cuda.FloatTensor of size 1 (GPU 0)], Variable containing:<br> 1.00000e-03 *<br>   3.8243<br> [torch.cuda.FloatTensor of size 1 (GPU 0)]]</pre><pre name="5bc5" id="5bc5" class="graf graf--pre graf-after--pre">learn.crit.close()</pre><p name="2bd7" id="2bd7" class="graf graf--p graf-after--pre">The longer you train, the better it gets [<a href="https://youtu.be/nG3tT31nPmQ?t=1h1m18s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h1m18s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:01:18</a>]. I ended up training it for about 10 hours, but you’ll still get very good results much more quickly if you’re less patient. So we can try it out and and here is the result. On the left is my pixelated bird and on the right is the upsampled version. It literally invented coloration. But it figured out what kind of bird it is, and it knows what these feathers are meant to look like. So it has imagined a set of feathers which are compatible with these exact pixels which is genius. Same for the back of its head. There is no way you can tell what these blue dots are meant to represent. But if you know that this kind of bird has an array of feathers here, you know that’s what they must be. Then you can figure out whether the feathers would have to be such that when they were pixelated they would end up in these spots. So it literally reverse engineered given its knowledge of this exact species of bird, how it would have to have looked to create this output. This is so amazing. It also knows from all the signs around it that this area here (background) was almost certainly blurred out. So it actually reconstructed blurred vegetation. If it hadn’t have done all of those things, it wouldn’t have gotten such a good loss function. Because in the end, it had to match the activations saying “oh, there’s a feather over here and it’s kind of fluffy looking and it’s in this direction” and all that.</p><pre name="e380" id="e380" class="graf graf--pre graf-after--p">_,axes=plt.subplots(1,2,figsize=(14,7))<br>show_img(x[<strong class="markup--strong markup--pre-strong">None</strong>], 0, ax=axes[0])<br>show_img(preds,0, normed=<strong class="markup--strong markup--pre-strong">True</strong>, ax=axes[1])</pre><figure name="e88c" id="e88c" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_wJTo6Q3kodiPLTyxaLDYZg.png"></figure><p name="ebce" id="ebce" class="graf graf--p graf-after--figure">Well, that brings us to the end of super resolution [<a href="https://youtu.be/nG3tT31nPmQ?t=1h3m18s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h3m18s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:03:18</a>]. Don’t forget to check out the <a href="http://forums.fast.ai/t/ask-jeremy-anything/15646/1" data-href="http://forums.fast.ai/t/ask-jeremy-anything/15646/1" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">ask Jeremy anything</a> thread.</p><h3 name="7f81" id="7f81" class="graf graf--h3 graf-after--p">Ask Jeremy&nbsp;Anything</h3><p name="b14e" id="b14e" class="graf graf--p graf-after--h3"><strong class="markup--strong markup--p-strong">Question</strong>: What are the future plans for fast.ai and this course? Will there be a part 3? If there is a part 3, I would really love to take it [<a href="https://youtu.be/nG3tT31nPmQ?t=1h4m11s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h4m11s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:04:11</a>].</p><p name="d99a" id="d99a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Jeremy</strong>: I’m not quite sure. It’s always hard to guess. I hope there will be some kind of follow-up. Last year, after part 2, one of the students started up a weekly book club going through the Ian Goodfellow deep learning book, and Ian actually came in and presented quite a few of the chapters and there was somebody, an expert, who presented every chapter. That was a really cool part 3. To a large extent, it will depend on you, the community, to come up with ideas and help make them happen, and I’m definitely keen to help. I’ve got a bunch of ideas but I’m nervous about saying them because I’m not sure which ones will happen and which ones won’t. But the more support I have in making things happen that you want to happen from you, the more likely they are to happen.</p><p name="f8fb" id="f8fb" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: What was your experience like starting down the path of entrepreneurship? Have you always been an entrepreneur or did you start at a big company and transition to a startup? Did you go from academia to startups or startups to academia? [<a href="https://youtu.be/nG3tT31nPmQ?t=1h5m13s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h5m13s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:05:13</a>]</p><p name="0962" id="0962" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Jeremy</strong>: No, I was definitely not an academia. I am totally a fake academic. I started at McKinsey and company which is a strategy firm when I was 18 which meant I couldn’t really go to university so it didn’t really turn up. Then spent 8 years in business helping really big companies on strategic questions. I always wanted to be an entrepreneur, planned to only spend two years in McKinsey, only thing I really regret in my life was not sticking to that plan and wasting eight years instead. So two years would have been perfect. But then I went into entrepreneurship, started two companies in Australia. The best part about that was that I didn’t get any funding so all the money that I made was mine or the decisions were mine and my partner’s. I focused entirely on profit and product and customer and service. Whereas I find in San Francisco, I’m glad I came here and so the two of us came here for Kaggle, Anthony and I, and raised ridiculous amount of money 11 million dollar for this really new company. That was really interesting but it’s also really distracting trying to worry about scaling and VC’s wanting to see what your business development plans are and also just not having any real need to actually make a profit. So I had a bit of the same problem at Enlitic where I again raised a lot of money 15 million dollars pretty quickly and a lot of distractions. I think trying to bootstrap your own company and focus on making money by selling something at a profit and then plowing that back into the company, it worked really well. Because within five years, we were making a profit from 3 months in and within 5 years, we were making enough for profit not just to pay all of us and our own wages but also to see my bank account growing and after 10 years sold it for a big chunk of money, not enough that a VC would be excited but enough that I didn’t have to worry about money again. So I think bootstrapping a company is something which people in the Bay Area at least don’t seem to appreciate how good of an idea that is.</p><p name="0cec" id="0cec" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: If you were 25 years old today and still know what you know where would you be looking to use AI? What are you working on right now or looking to work on in the next 2 years [<a href="https://youtu.be/nG3tT31nPmQ?t=1h8m10s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h8m10s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:08:10</a>]?</p><p name="9097" id="9097" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Jeremy</strong>: You should ignore the last part of that. I won’t even answer it. Doesn’t matter where I’m looking. What you should do is leverage your knowledge about your domain. So one of the main reasons we do this is to get people who have backgrounds in recruiting, oil field surveys, journalism, activism, whatever and solve your problems. It’ll be really obvious to you what real problems are and it will be really obvious to you what data you have and where to find it. Those are all the bits that for everybody else that’s really hard. So people who start out with “oh, I know deep learning now I’ll go and find something to apply it to” basically never succeed where else people who are like “oh, I’ve been spending 25 years doing specialized recruiting for legal firms and I know that the key issue is this thing and I know that this piece of data totally solves it and so I’m just going to do that now and I already know who to call or actually start selling it to”. They are the ones who tend to win. If you’ve done nothing but academic stuff, then it’s more maybe about your hobbies and interests. So everybody has hobbies. The main thing I would say is please don’t focus on building tools for data scientists to use or for software engineers to use because every data scientist knows about the market of data scientists whereas only you know about the market for analyzing oil survey world or understanding audiology studies or whatever it is that you do.</p><p name="5bd2" id="5bd2" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: Given what you’ve shown us about applying transfer learning from image recognition to NLP, there looks to be a lot of value in paying attention to all of the developments that happen across the whole ML field and that if you were to focus in one area you might miss out on some great advances in other concentrations. How do you stay aware of all of the advancements across the field while still having time to dig in deep to your specific domains [<a href="https://youtu.be/nG3tT31nPmQ?t=1h10m19s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h10m19s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:10:19</a>]?</p><p name="86ca" id="86ca" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Jeremy</strong>: Yeah, that’s awesome. I mean that’s one of the key messages of this course. Lots of good work’s being done in different places and people are so specialized and most people don’t know about it. If I can get state of the art results in NLP within six months of starting to look at NLP and I think that says more about NLP than it does about me, frankly. It’s kind of like the entrepreneurship thing. You pick the areas you see that you know about and kind of transfer stuff like “oh, we could use deep learning to solve this problem” or in this case, we could use this idea of computer vision to solve that problem. So things like transfer learning, I’m sure there’s like a thousand opportunities for you to do in other field to do what Sebastian and I did in NLP with NLP classification. So the short answer to your question is the way to stay ahead of what’s going on would be to follow my feed of Twitter favorites and my approach is to then follow lots and lots of people on Twitter and put them into the Twitter favorites for you. Literally, every time I come across something interesting, I click favorite. There are two reasons I do it. The first is that when the next course comes along, I go through my favorites to find which things I want to study. The second is so that you can do the same thing. And then which you go deep into, it almost doesn’t matter. I find every time I look at something it turns out to be super interesting and important. So pick something which you feel like solving that problem would be actually useful for some reason and it doesn’t seem to be very popular which is kind of the opposite of what everybody else does. Everybody else works on the problems which everybody else is already working on because they are the ones that seem popular. I can’t quite understand this train of thinking but it seems to be very common.</p><p name="adac" id="adac" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: Is Deep Learning an overkill to use on Tabular data? When is it better to use DL instead of ML on tabular data [<a href="https://youtu.be/nG3tT31nPmQ?t=1h12m46s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h12m46s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:12:46</a>]?</p><p name="a89a" id="a89a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Jeremy</strong>: Is that a real question or did you just put that there so that I would point out that Rachel Thomas just wrote an article? <a href="http://www.fast.ai/2018/04/29/categorical-embeddings/" data-href="http://www.fast.ai/2018/04/29/categorical-embeddings/" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">http://www.fast.ai/2018/04/29/categorical-embeddings/</a></p><p name="efae" id="efae" class="graf graf--p graf-after--p">So Rachel has just written about this and Rachel and I spent a long time talking about it and the short answer is we think it’s great to use deep learning on tabular data. Actually, of all the rich complex important and interesting things that appear in Rachel’s Twitter stream covering everything from the genocide of Rohingya through to latest ethics violations in AI companies, the one by far that got the most attention and engagement from the community was the question about is it called tabular data or structured data. So yeah, ask computer people how to name things and you’ll get plenty of interest. There are some really good links here to stuff from Instacart and Pinterest and other folks who have done some good work in this area. Any of you that went to the Data Institute conference would have seen Jeremy Stanley’s presentation about the really cool work they did at Instacart.</p><p name="3be8" id="3be8" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Rachel</strong>: I relied heavily on lessons 3 and 4 from part 1 in writing this post so much of that may be familiar to you.</p><p name="354b" id="354b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Jeremy</strong>: Rachel asked me during the post like how to tell whether you should use the decision tree ensemble like GBM or random forest or neural net and my answer is I still don’t know. Nobody I’m aware of has done that research in any particularly meaningful way. So there’s a question to be answered there, I guess. My approach has been to try to make both of those things as accessible as possible through fast.ai library so you can try them both and see what works. That’s what I do.</p><p name="ec9f" id="ec9f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: Reinforcement Learning popularity has been on a gradual rise in the recent past. What’s your take on Reinforcement Learning? Would fast.ai consider covering some ground in popular RL techniques in the future [<a href="https://youtu.be/nG3tT31nPmQ?t=1h15m21s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h15m21s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:15:21</a>]?</p><p name="4dc5" id="4dc5" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Jeremy</strong>: I’m still not a believer in reinforcement learning. I think it’s an interesting problem to solve but it’s not at all clear that we have a good way of solving this problem. So the problem, it really is the delayed credit problem. So I want to learn to play pong, I’ve moved up or down and three minutes later I find out whether I won the game of pong — which actions I took were actually useful? So to me, the idea of calculating the gradients of the output with respect to those inputs, the credit is so delayed that those derivatives don’t seem very interesting. I get this question quite regularly in every one of these four courses so far. I’ve always said the same thing. I’m rather pleased that finally recently there’s been some results showing that actually basically random search often does better than reinforcement learning so basically what’s happened is very well-funded companies with vast amounts of computational power throw all of it at reinforcement learning problems and get good results and people then say “oh it’s because of the reinforcement learning” rather than the vast amounts of compute power. Or they use extremely thoughtful and clever algorithms like a combination of convolutional neural nets and Monte Carlo tree search like they did with the Alpha Go stuff to get great results and people incorrectly say “oh that’s because of reinforcement learning” when it wasn’t really reinforcement learning at all. So I’m very interested in solving these kind of more generic optimization type problems rather than just prediction problems and that’s what these delayed credit problems tend to look like. But I don’t think we’ve yet got good enough best practices that I have anything on, ready to teach and say I’ve got to teach you this thing because I think it’s still going to be useful next year. So we’ll keep watching and see what happens.</p><h4 name="fbb6" id="fbb6" class="graf graf--h4 graf-after--p">Super resolution network to a style transfer network [<a href="https://youtu.be/nG3tT31nPmQ?t=1h17m57s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h17m57s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:17:57</a>]</h4><figure name="39a2" id="39a2" class="graf graf--figure graf-after--h4"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_nBoerE_seZP-z5lqL7bsUg.png"></figure><p name="daee" id="daee" class="graf graf--p graf-after--figure">We are going to now turn the super resolution network into a style transfer network. And we’ll do this pretty quickly. We basically already have something. <em class="markup--em markup--p-em">x</em> is my input image and I’m going to have some loss function and I’ve got some neural net again. Instead of a neural net that does a whole a lot of compute and then does upsampling at the end, our input this time is just as big as our output. So we are going to do some downsampling first. Then our computer, and then our upsampling. So that’s the first change we are going to make — we are going to add some downsampling so some stride 2 convolution layers to the front of our network. The second is rather than just comparing <em class="markup--em markup--p-em">yc</em> and <em class="markup--em markup--p-em">x </em>are the same thing here. So we are going to basically say our input image should look like itself by the end. Specifically we are going to compare it by chucking it through VGG and comparing it at one of the activation layers. And then its style should look like some painting which we’ll do just like we did with the Gatys’ approach by looking at the Gram matrix correspondence at a number of layers. So that’s basically it. So that ought to be super straight forward. It’s really combining two things we’ve already done.</p><h4 name="9209" id="9209" class="graf graf--h4 graf-after--p">Style transfer net [<a href="https://youtu.be/nG3tT31nPmQ?t=1h19m19s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h19m19s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:19:19</a>]</h4><p name="3448" id="3448" class="graf graf--p graf-after--h4"><a href="https://github.com/fastai/fastai/blob/master/courses/dl2/style-transfer-net.ipynb" data-href="https://github.com/fastai/fastai/blob/master/courses/dl2/style-transfer-net.ipynb" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Notebook</a></p><p name="d131" id="d131" class="graf graf--p graf-after--p">So all this code starts identical, except we don’t have high res and low res, we just have one size 256.</p><pre name="bbab" id="bbab" class="graf graf--pre graf-after--p">%matplotlib inline<br>%reload_ext autoreload<br>%autoreload 2</pre><pre name="a98a" id="a98a" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">fastai.conv_learner</strong> <strong class="markup--strong markup--pre-strong">import</strong> *<br><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">pathlib</strong> <strong class="markup--strong markup--pre-strong">import</strong> Path<br>torch.cuda.set_device(0)</pre><pre name="3c68" id="3c68" class="graf graf--pre graf-after--pre">torch.backends.cudnn.benchmark=<strong class="markup--strong markup--pre-strong">True</strong></pre><pre name="7e96" id="7e96" class="graf graf--pre graf-after--pre">PATH = Path('data/imagenet')<br>PATH_TRN = PATH/'train'</pre><pre name="ca27" id="ca27" class="graf graf--pre graf-after--pre">fnames_full,label_arr_full,all_labels = folder_source(PATH, 'train')<br>fnames_full = ['/'.join(Path(fn).parts[-2:]) <strong class="markup--strong markup--pre-strong">for</strong> fn <strong class="markup--strong markup--pre-strong">in</strong> fnames_full]<br>list(zip(fnames_full[:5],label_arr_full[:5]))</pre><pre name="0bde" id="0bde" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[('n01440764/n01440764_9627.JPEG', 0),<br> ('n01440764/n01440764_9609.JPEG', 0),<br> ('n01440764/n01440764_5176.JPEG', 0),<br> ('n01440764/n01440764_6936.JPEG', 0),<br> ('n01440764/n01440764_4005.JPEG', 0)]</em></pre><pre name="e50e" id="e50e" class="graf graf--pre graf-after--pre">all_labels[:5]</pre><pre name="7cd0" id="7cd0" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">['n01440764', 'n01443537', 'n01484850', 'n01491361', 'n01494475']</em></pre><pre name="24cd" id="24cd" class="graf graf--pre graf-after--pre">np.random.seed(42)<br><em class="markup--em markup--pre-em"># keep_pct = 1.</em><br><em class="markup--em markup--pre-em"># keep_pct = 0.01</em><br>keep_pct = 0.1<br>keeps = np.random.rand(len(fnames_full)) &lt; keep_pct<br>fnames = np.array(fnames_full, copy=<strong class="markup--strong markup--pre-strong">False</strong>)[keeps]<br>label_arr = np.array(label_arr_full, copy=<strong class="markup--strong markup--pre-strong">False</strong>)[keeps]</pre><pre name="55df" id="55df" class="graf graf--pre graf-after--pre">arch = vgg16<br><em class="markup--em markup--pre-em"># sz,bs = 96,32</em><br>sz,bs = 256,24<br><em class="markup--em markup--pre-em"># sz,bs = 128,32</em></pre><pre name="22f6" id="22f6" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">MatchedFilesDataset</strong>(FilesDataset):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, fnames, y, transform, path):<br>        self.y=y<br>        <strong class="markup--strong markup--pre-strong">assert</strong>(len(fnames)==len(y))<br>        super().__init__(fnames, transform, path)<br>    <strong class="markup--strong markup--pre-strong">def</strong> get_y(self, i): <br>        <strong class="markup--strong markup--pre-strong">return</strong> open_image(os.path.join(self.path, self.y[i]))<br>    <strong class="markup--strong markup--pre-strong">def</strong> get_c(self): <strong class="markup--strong markup--pre-strong">return</strong> 0</pre><pre name="917d" id="917d" class="graf graf--pre graf-after--pre">val_idxs = get_cv_idxs(len(fnames), val_pct=min(0.01/keep_pct, 0.1))<br>((val_x,trn_x),(val_y,trn_y)) = split_by_idx(val_idxs, <br>                                 np.array(fnames), np.array(fnames))<br>len(val_x),len(trn_x)</pre><pre name="4fe4" id="4fe4" class="graf graf--pre graf-after--pre">(12800, 115206)</pre><pre name="2036" id="2036" class="graf graf--pre graf-after--pre">img_fn = PATH/'train'/'n01558993'/'n01558993_9684.JPEG'</pre><pre name="4445" id="4445" class="graf graf--pre graf-after--pre">tfms = tfms_from_model(arch, sz, tfm_y=TfmType.PIXEL)<br>datasets = ImageData.get_ds(MatchedFilesDataset, (trn_x,trn_y), <br>                   (val_x,val_y), tfms, path=PATH_TRN)<br>md = ImageData(PATH, datasets, bs, num_workers=16, classes=<strong class="markup--strong markup--pre-strong">None</strong>)</pre><pre name="48f6" id="48f6" class="graf graf--pre graf-after--pre">denorm = md.val_ds.denorm</pre><pre name="0c8d" id="0c8d" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> show_img(ims, idx, figsize=(5,5), normed=<strong class="markup--strong markup--pre-strong">True</strong>, ax=<strong class="markup--strong markup--pre-strong">None</strong>):<br>    <strong class="markup--strong markup--pre-strong">if</strong> ax <strong class="markup--strong markup--pre-strong">is</strong> <strong class="markup--strong markup--pre-strong">None</strong>: fig,ax = plt.subplots(figsize=figsize)<br>    <strong class="markup--strong markup--pre-strong">if</strong> normed: ims = denorm(ims)<br>    <strong class="markup--strong markup--pre-strong">else</strong>:      ims = np.rollaxis(to_np(ims),1,4)<br>    ax.imshow(np.clip(ims,0,1)[idx])<br>    ax.axis('off')</pre><h4 name="5660" id="5660" class="graf graf--h4 graf-after--pre">Model [<a href="https://youtu.be/nG3tT31nPmQ?t=1h19m30s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h19m30s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:19:30</a>]</h4><p name="5e54" id="5e54" class="graf graf--p graf-after--h4">My model is the same. One thing I did here is I did not do any kind of fancy best practices for this one at all. Partly because there doesn’t seem to be any. There’s been very little follow up in this approach compared to the super resolution stuff. We’ll talk about why in a moment. So you’ll see, this is much more normal looking.</p><pre name="6f0e" id="6f0e" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> conv(ni, nf, kernel_size=3, stride=1, actn=<strong class="markup--strong markup--pre-strong">True</strong>, pad=<strong class="markup--strong markup--pre-strong">None</strong>, <br>         bn=<strong class="markup--strong markup--pre-strong">True</strong>):<br>    <strong class="markup--strong markup--pre-strong">if</strong> pad <strong class="markup--strong markup--pre-strong">is</strong> <strong class="markup--strong markup--pre-strong">None</strong>: pad = kernel_size//2<br>    layers = [nn.Conv2d(ni, nf, kernel_size, stride=stride,<br>                          padding=pad, bias=<strong class="markup--strong markup--pre-strong">not</strong> bn)]<br>    <strong class="markup--strong markup--pre-strong">if</strong> actn: layers.append(nn.ReLU(inplace=<strong class="markup--strong markup--pre-strong">True</strong>))<br>    <strong class="markup--strong markup--pre-strong">if</strong> bn: layers.append(nn.BatchNorm2d(nf))<br>    <strong class="markup--strong markup--pre-strong">return</strong> nn.Sequential(*layers)</pre><p name="7af8" id="7af8" class="graf graf--p graf-after--pre">I’ve got batch norm layers. I don’t have scaling factor here.</p><pre name="8e05" id="8e05" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">ResSequentialCenter</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, layers):<br>        super().__init__()<br>        self.m = nn.Sequential(*layers)</pre><pre name="d18d" id="d18d" class="graf graf--pre graf-after--pre">    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x): <strong class="markup--strong markup--pre-strong">return</strong> x[:, :, 2:-2, 2:-2] + self.m(x)</pre><pre name="852e" id="852e" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> res_block(nf):<br>    <strong class="markup--strong markup--pre-strong">return</strong> ResSequentialCenter([conv(nf, nf, actn=<strong class="markup--strong markup--pre-strong">True</strong>, pad=0), <br>              conv(nf, nf, pad=0)])</pre><p name="af82" id="af82" class="graf graf--p graf-after--pre">I don’t have a pixel shuffle — it’s just using a normal upsampling followed by 1x1 conf. So it’s just more normal.</p><pre name="5179" id="5179" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> upsample(ni, nf):<br>    <strong class="markup--strong markup--pre-strong">return</strong> nn.Sequential(nn.Upsample(scale_factor=2), conv(ni, nf))</pre><p name="4132" id="4132" class="graf graf--p graf-after--pre">One thing they mentioned in the paper is they had a lot of problems with zero padding creating artifacts and the way they solved that was by adding 40 pixel of reflection padding at the start. So I did the same thing and then they used zero padding in their convolutions in their Res blocks. Now if you’ve got zero padding in your convolutions in your Res blocks, then that means that the two parts of your ResNet won’t add up anymore because you’ve lost a pixel from each side on each of your two convolutions. So my <code class="markup--code markup--p-code">ResSequential</code> has become <code class="markup--code markup--p-code">ResSequentialCenter</code> and I’ve removed the last 2 pixels on each side of those good cells. Other than that, this is basically the same as what we had before.</p><pre name="437c" id="437c" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">StyleResnet</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self):<br>        super().__init__()<br>        features = [nn.ReflectionPad2d(40),<br>                    conv(3, 32, 9),<br>                    conv(32, 64, stride=2), conv(64, 128, stride=2)]<br>        <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(5): features.append(res_block(128))<br>        features += [upsample(128, 64), upsample(64, 32),<br>                     conv(32, 3, 9, actn=<strong class="markup--strong markup--pre-strong">False</strong>)]<br>        self.features = nn.Sequential(*features)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x): <strong class="markup--strong markup--pre-strong">return</strong> self.features(x)</pre><h4 name="a39b" id="a39b" class="graf graf--h4 graf-after--pre">Style Image [<a href="https://youtu.be/nG3tT31nPmQ?t=1h21m2s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h21m2s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:21:02</a>]</h4><p name="93a0" id="93a0" class="graf graf--p graf-after--h4">So then we can bring in our starry night picture.</p><pre name="e724" id="e724" class="graf graf--pre graf-after--p">style_fn = PATH/'style'/'starry_night.jpg'<br>style_img = open_image(style_fn)<br>style_img.shape</pre><pre name="1850" id="1850" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(1198, 1513, 3)</em></pre><pre name="ff2f" id="ff2f" class="graf graf--pre graf-after--pre">plt.imshow(style_img);</pre><figure name="c999" id="c999" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_3QN8_RpikQBlk8wwjD9B3w.png"></figure><pre name="cef3" id="cef3" class="graf graf--pre graf-after--figure">h,w,_ = style_img.shape<br>rat = max(sz/h,sz/h)<br>res = cv2.resize(style_img, (int(w*rat), int(h*rat)), interpolation=cv2.INTER_AREA)<br>resz_style = res[:sz,-sz:]</pre><p name="11e8" id="11e8" class="graf graf--p graf-after--pre">We can resize it.</p><pre name="f983" id="f983" class="graf graf--pre graf-after--p">plt.imshow(resz_style);</pre><figure name="5234" id="5234" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_CExkSLE8DWFZM0M3GPkHfg.png"></figure><p name="4b03" id="4b03" class="graf graf--p graf-after--figure">We can throw it through our transformations</p><pre name="e179" id="e179" class="graf graf--pre graf-after--p">style_tfm,_ = tfms[1](resz_style,resz_style)</pre><pre name="374e" id="374e" class="graf graf--pre graf-after--pre">style_tfm = np.broadcast_to(style_tfm[<strong class="markup--strong markup--pre-strong">None</strong>], (bs,)+style_tfm.shape)</pre><p name="8185" id="8185" class="graf graf--p graf-after--pre">Just to make the method a little bit easier for my brain to handle, I took our transform style image which after transformations of 3 x 256 x 256, and I made a mini batch. My batch size is 24 — 24 copies of it. It just maeks it a little bit easier to do the kind of batch arithmetic without worrying about some of the broadcasting. They are not really 24 copies. I used <code class="markup--code markup--p-code">np.broadcast</code> to basically fake 24 pieces.</p><pre name="1e49" id="1e49" class="graf graf--pre graf-after--p">style_tfm.shape</pre><pre name="77d0" id="77d0" class="graf graf--pre graf-after--pre">(24, 3, 256, 256)</pre><h4 name="21ad" id="21ad" class="graf graf--h4 graf-after--pre">Perceptual loss [<a href="https://youtu.be/nG3tT31nPmQ?t=1h21m51s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h21m51s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:21:51</a>]</h4><p name="f45e" id="f45e" class="graf graf--p graf-after--h4">So just like before, we create a VGG, grab the last block. This time we are going to use all of these layers so we keep everything up to the 43rd layer.</p><pre name="3041" id="3041" class="graf graf--pre graf-after--p">m_vgg = vgg16(<strong class="markup--strong markup--pre-strong">True</strong>)</pre><pre name="9a1f" id="9a1f" class="graf graf--pre graf-after--pre">blocks = [i-1 <strong class="markup--strong markup--pre-strong">for</strong> i,o <strong class="markup--strong markup--pre-strong">in</strong> enumerate(children(m_vgg))<br>              <strong class="markup--strong markup--pre-strong">if</strong> isinstance(o,nn.MaxPool2d)]<br>blocks, [m_vgg[i] <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> blocks[1:]]</pre><pre name="955c" id="955c" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">([5, 12, 22, 32, 42],<br> [ReLU(inplace), ReLU(inplace), ReLU(inplace), ReLU(inplace)])</em></pre><pre name="e33e" id="e33e" class="graf graf--pre graf-after--pre">vgg_layers = children(m_vgg)[:43]<br>m_vgg = nn.Sequential(*vgg_layers).cuda().eval()<br>set_trainable(m_vgg, <strong class="markup--strong markup--pre-strong">False</strong>)</pre><pre name="b5db" id="b5db" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> flatten(x): <strong class="markup--strong markup--pre-strong">return</strong> x.view(x.size(0), -1)</pre><pre name="00a6" id="00a6" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">SaveFeatures</strong>():<br>    features=<strong class="markup--strong markup--pre-strong">None</strong><br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, m): <br>        self.hook = m.register_forward_hook(self.hook_fn)<br>    <strong class="markup--strong markup--pre-strong">def</strong> hook_fn(self, module, input, output): self.features = output<br>    <strong class="markup--strong markup--pre-strong">def</strong> remove(self): self.hook.remove()</pre><pre name="54dc" id="54dc" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> ct_loss(input, target): <strong class="markup--strong markup--pre-strong">return</strong> F.mse_loss(input,target)</pre><pre name="5228" id="5228" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> gram(input):<br>        b,c,h,w = input.size()<br>        x = input.view(b, c, -1)<br>        <strong class="markup--strong markup--pre-strong">return</strong> torch.bmm(x, x.transpose(1,2))/(c*h*w)*1e6</pre><pre name="eee5" id="eee5" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> gram_loss(input, target):<br>    <strong class="markup--strong markup--pre-strong">return</strong> F.mse_loss(gram(input), gram(target[:input.size(0)]))</pre><p name="41cb" id="41cb" class="graf graf--p graf-after--pre">So now our combined loss is going to add together a content loss for the third block plus the Gram loss for all of our blocks with different weights. Again, going back to everything being as normal as possible, I’ve gone back to using MSE above. Basically what happened was I had a lot of trouble getting this to train properly. So I gradually removed trick after trick and eventually just went “ok, I’m just gonna make it as bland as possible”.</p><p name="a0cf" id="a0cf" class="graf graf--p graf-after--p">Last week’s Gram matrix was wrong, by the way [<a href="https://youtu.be/nG3tT31nPmQ?t=1h22m37s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h22m37s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:22:37</a>]. It only worked for a batch size of one and we only had a batch size of one so that was fine. I was using matrix multiply which meant that every batch was being compared to every other batch. You actually need to use batch matrix multiple (<code class="markup--code markup--p-code">torch.bmm</code>) which does a matrix multiply per batch. So that’s something to be aware of there.</p><pre name="434a" id="434a" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">CombinedLoss</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, m, layer_ids, style_im, ct_wgt, style_wgts):<br>        super().__init__()<br>        self.m,self.ct_wgt,self.style_wgts = m,ct_wgt,style_wgts<br>        self.sfs = [SaveFeatures(m[i]) <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> layer_ids]<br>        m(VV(style_im))<br>        self.style_feat = [V(o.features.data.clone()) <br>                              <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> self.sfs]</pre><pre name="2900" id="2900" class="graf graf--pre graf-after--pre">    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, input, target, sum_layers=<strong class="markup--strong markup--pre-strong">True</strong>):<br>        self.m(VV(target.data))<br>        targ_feat = self.sfs[2].features.data.clone()<br>        self.m(input)<br>        inp_feat = [o.features <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> self.sfs]<br>        <br>        res = [ct_loss(inp_feat[2],V(targ_feat)) * self.ct_wgt]<br>        res += [gram_loss(inp,targ)*wgt <strong class="markup--strong markup--pre-strong">for</strong> inp,targ,wgt<br>                <strong class="markup--strong markup--pre-strong">in</strong> zip(inp_feat, self.style_feat, self.style_wgts)]<br>        <br>        <strong class="markup--strong markup--pre-strong">if</strong> sum_layers: res = sum(res)<br>        <strong class="markup--strong markup--pre-strong">return</strong> res<br>    <br>    <strong class="markup--strong markup--pre-strong">def</strong> close(self):<br>        <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> self.sfs: o.remove()</pre><p name="05b3" id="05b3" class="graf graf--p graf-after--pre">So I’ve got Gram matrices, I do my MSE loss between the Gram matrices, I weight them by style weights, so I create that ResNet.</p><pre name="01e9" id="01e9" class="graf graf--pre graf-after--p">m = StyleResnet()<br>m = to_gpu(m)</pre><pre name="d2fb" id="d2fb" class="graf graf--pre graf-after--pre">learn = Learner(md, SingleModel(m), opt_fn=optim.Adam)</pre><p name="8ea4" id="8ea4" class="graf graf--p graf-after--pre">I create my combined loss passing in the VGG network, passing in the block IDs, passing in the transformed starry night image, and you’ll see the the very start here, I do a forward pass through my VGG model with that starry night image in order that I can save the features for it. Notice, it’s really important now that I don’t do any data augmentation because I’ve saved the style features for a particular non-augmented version. So if I augmented it, it might make some minor problems. But that’s fine because I’ve got all of ImageNet to deal with. I don’t really need to do data augmentation anyway.</p><pre name="46f3" id="46f3" class="graf graf--pre graf-after--p">learn.crit = CombinedLoss(m_vgg, blocks[1:], style_tfm, 1e4,<br>                             [0.025,0.275,5.,0.2])</pre><pre name="58a8" id="58a8" class="graf graf--pre graf-after--pre">wd=1e-7</pre><pre name="841d" id="841d" class="graf graf--pre graf-after--pre">learn.lr_find(wds=wd)<br>learn.sched.plot(n_skip_end=1)</pre><pre name="7435" id="7435" class="graf graf--pre graf-after--pre">  1%|▏         | 7/482 [00:04&lt;05:32,  1.43it/s, loss=2.48e+04] <br> 53%|█████▎    | 254/482 [02:27&lt;02:12,  1.73it/s, loss=1.13e+12]</pre><figure name="76cb" id="76cb" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_sDFoltJE1s5ugWZndD4k0A.png"></figure><pre name="464f" id="464f" class="graf graf--pre graf-after--figure">lr=5e-3</pre><p name="18e5" id="18e5" class="graf graf--p graf-after--pre">So I’ve got my loss function and I can go ahead and fit [<a href="https://youtu.be/nG3tT31nPmQ?t=1h24m6s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h24m6s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:24:06</a>]. And there is nothing clever here at all.</p><pre name="7b0f" id="7b0f" class="graf graf--pre graf-after--p">learn.fit(lr, 1, cycle_len=1, wds=wd, use_clr=(20,10))</pre><pre name="5313" id="5313" class="graf graf--pre graf-after--pre">epoch      trn_loss   val_loss                               <br>    0      105.351372 105.833994</pre><pre name="7ad2" id="7ad2" class="graf graf--pre graf-after--pre">[array([105.83399])]</pre><pre name="ff47" id="ff47" class="graf graf--pre graf-after--pre">learn.save('style-2')</pre><pre name="4c19" id="4c19" class="graf graf--pre graf-after--pre">x,y=md.val_ds[201]</pre><pre name="757d" id="757d" class="graf graf--pre graf-after--pre">learn.model.eval()<br>preds = learn.model(VV(x[<strong class="markup--strong markup--pre-strong">None</strong>]))<br>x.shape,y.shape,preds.shape</pre><pre name="9293" id="9293" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">((3, 256, 256), (3, 256, 256), torch.Size([1, 3, 256, 256]))</em></pre><p name="597a" id="597a" class="graf graf--p graf-after--pre">At the end, I have my <code class="markup--code markup--p-code">sum_layers=False</code> so I can see what each part looks like and see they are balanced. And I can finally pop it out</p><pre name="31b9" id="31b9" class="graf graf--pre graf-after--p">learn.crit(preds, VV(y[<strong class="markup--strong markup--pre-strong">None</strong>]), sum_layers=<strong class="markup--strong markup--pre-strong">False</strong>)</pre><pre name="a7e5" id="a7e5" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[Variable containing:<br>  53.2221<br> [torch.cuda.FloatTensor of size 1 (GPU 0)], Variable containing:<br>  3.8336<br> [torch.cuda.FloatTensor of size 1 (GPU 0)], Variable containing:<br>  4.0612<br> [torch.cuda.FloatTensor of size 1 (GPU 0)], Variable containing:<br>  5.0639<br> [torch.cuda.FloatTensor of size 1 (GPU 0)], Variable containing:<br>  53.0019<br> [torch.cuda.FloatTensor of size 1 (GPU 0)]]</em></pre><pre name="c3c5" id="c3c5" class="graf graf--pre graf-after--pre">learn.crit.close()</pre><pre name="d2cd" id="d2cd" class="graf graf--pre graf-after--pre">_,axes=plt.subplots(1,2,figsize=(14,7))<br>show_img(x[<strong class="markup--strong markup--pre-strong">None</strong>], 0, ax=axes[0])<br>show_img(preds, 0, ax=axes[1])</pre><figure name="a174" id="a174" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_Hlb0cHXu_IdLZmfBxJIgJA.png"></figure><p name="232c" id="232c" class="graf graf--p graf-after--figure">So I mentioned that should be pretty easy and yet it took me about 4 days because I just found this incredibly fiddly to actually get it to work [<a href="https://youtu.be/nG3tT31nPmQ?t=1h24m26s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h24m26s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:24:26</a>]. So when I finally got up in the morning I said to Rachel “guess what, it trained correctly.” Rachel said “I never thought that was going to happen.” It just looked awful all the time and it’s really about getting the exact right mix of content loss and a style loss and the mix of the layers of the style loss. The worst part was it takes a really long time to train the darn CNN and I didn’t really know how long to train it before I decided it wasn’t doing well. Should I just train it for longer? And I don’t know all the little details didn’t seem to slightly change it but just it would totally fall apart all the time. So I kind of mentioned this partly to say just remember the final answer you see here is after me driving myself crazy all week of nearly always not working until finally the last minute it finally does. Even for things which just seemed like they couldn’t possibly be difficult because that is combining two things we already have working. The other is to be careful about how we interpret what authors claim.</p><figure name="e2ae" id="e2ae" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_6AvLQSM40JcTWE26C4GafA.png"></figure><p name="9e75" id="9e75" class="graf graf--p graf-after--figure">It was so fiddly getting this style transfer to work [<a href="https://youtu.be/nG3tT31nPmQ?t=1h26m10s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h26m10s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:26:10</a>]. After doing it, it left me thinking why did I bother because now I’ve got something that takes hours to create a network that can turn any kind of photo into one specific style. It just seems very unlikely I would want that for anything. The only reason I could think that being useful would be to do some art-y stuff on a video where I wanted to turn every frame into some style. It’s incredibly niche thing to want to do. But when I looked at the paper, the table is saying “oh, we are a thousand times faster than the Gatys’ approach which is just such an obviously meaningless thing to say. Such an incredibly misleading thing to say because it ignores all the hours of training for each individual style and I find this frustrating because groups like this Stanford group clearly know better or ought to know better, but still I guess the academic community encourages people to make these ridiculously grand claims. It also completely ignores this incredibly sensitive fiddly training process so this paper was just so well accepted when it came out. I remember everybody getting on Twitter and saying “wow, you know these Stanford people have found this way of doing style transfer a thousand times faster.” And clearly people saying this were top researchers in the field, clearly none of them actually understood it because nobody said “I don’t see why this is remotely useful, and also I tried it and it was incredibly fiddly to get it all to work.” It’s not until 18 months later I finally coming back to it and kind of thinking like “wait a minute, this is kind of stupid.” So this is the answer, I think, to the question of why haven’t people done follow ups on this to create really amazing best practices and better approaches like with a super resolution part of the paper. And I think the answer is because it’s dumb. So I think super resolution part of the paper is clearly not dumb. And it’s been improved and improved and now we have great super resolution. And I think we can derive from that great noise reduction, great colorization, great slant removal, great interactive artifact removal, etc. So I think there’s a lot of really cool techniques here. It’s also leveraging a lot of stuff that we’ve been learning and getting better and better at.</p><h3 name="90f0" id="90f0" class="graf graf--h3 graf-after--p">Segmentation [<a href="https://youtu.be/nG3tT31nPmQ?t=1h29m13s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h29m13s" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">1:29:13</a>]</h3><figure name="e8d3" id="e8d3" class="graf graf--figure graf-after--h3"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_6iq4f7DtfqsWxsE9ib0SPg.png"></figure><p name="2144" id="2144" class="graf graf--p graf-after--figure">Finally, let’s talk about segmentation. This is from the famous CamVid dataset which is a classic example of an academic segmentation dataset. Basically you can see what we do is we start with a picture (they are actually video frames in this dataset) and we have some labels where they are not actually colors — each one has an ID and the IDs are mapped to colors. So red might be 1, purple might be 2, light pink might be 3 and so all the buildings are one class, all the cars are another class, all the people are another class, all the road is another class, and so on. So what we are actually doing here is multi-class classification for every pixel. You can see, sometimes that multi-class classification really is quite tricky — like these branches. Although, sometimes the labels are really not that great. This is very coarse as you can see. So that’s what we are going to do.</p><p name="6e73" id="6e73" class="graf graf--p graf-after--p">We are going to do segmentation and so it’s a lot like bounding boxes. But rather than just finding a box around each thing, we are actually going to label every single pixel with its class. Really, it’s actually a lot easier because it fits our CNN style so nicely that we can create any CNN where the output is an N by M grid containing the integers from 0 to C where there are C categories. And then we can use cross-entropy loss with a softmax activation and we are done. I could actually stop the class there and you can go and use exactly the same approaches you’ve learnt in lesson 1 and 2 and you’ll get a perfectly okay result. So the first thing to say is this is not actually a terribly hard thing to do. But we are going to try and do it really well.</p><h4 name="6328" id="6328" class="graf graf--h4 graf-after--p">Doing it the simple way [<a href="https://youtu.be/nG3tT31nPmQ?t=1h31m26s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h31m26s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:31:26</a>]</h4><p name="58b7" id="58b7" class="graf graf--p graf-after--h4"><a href="https://github.com/fastai/fastai/blob/master/courses/dl2/carvana.ipynb" data-href="https://github.com/fastai/fastai/blob/master/courses/dl2/carvana.ipynb" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Notebook</a></p><p name="5576" id="5576" class="graf graf--p graf-after--p">Let’s start by doing it the really simple way. And we are going to use Kaggle <a href="https://www.kaggle.com/c/carvana-image-masking-challenge" data-href="https://www.kaggle.com/c/carvana-image-masking-challenge" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Carvana</a> competition and you can download it with Kaggle API as usual.</p><pre name="10c5" id="10c5" class="graf graf--pre graf-after--p">%matplotlib inline<br>%reload_ext autoreload<br>%autoreload 2</pre><pre name="c2dc" id="c2dc" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">fastai.conv_learner</strong> <strong class="markup--strong markup--pre-strong">import</strong> *<br><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">fastai.dataset</strong> <strong class="markup--strong markup--pre-strong">import</strong> *<br><br><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">pathlib</strong> <strong class="markup--strong markup--pre-strong">import</strong> Path<br><strong class="markup--strong markup--pre-strong">import</strong> <strong class="markup--strong markup--pre-strong">json</strong></pre><h4 name="381e" id="381e" class="graf graf--h4 graf-after--pre">Setup</h4><p name="f902" id="f902" class="graf graf--p graf-after--h4">There is a train folder containing bunch of images which is the independent variable and a train_masks folder there’s the dependent variable and they look like below.</p><figure name="84f3" id="84f3" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_3lO9olOrpojAt5C22B5CoQ.png"></figure><p name="78b2" id="78b2" class="graf graf--p graf-after--figure">In this case, just like cats and dogs, we are going simple rather than doing multi-class classification, we are going to do binary classification. But of course multi-class is just the more general version — categorical cross entropy or binary class entropy. There is no differences conceptually, so the dependent variable is just zeros and ones, where else the independent variable is a regular image.</p><p name="da00" id="da00" class="graf graf--p graf-after--p">In order to do this well, it would really help to know what cars look like. Because really what we want to do is to figure out this is a car and its orientation and put white pixels where we expect the car to be based on the picture and their understanding of what cars look like.</p><pre name="7a38" id="7a38" class="graf graf--pre graf-after--p">PATH = Path('data/carvana')<br>list(PATH.iterdir())</pre><pre name="38eb" id="38eb" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[PosixPath('data/carvana/train_masks.csv'),<br> PosixPath('data/carvana/train_masks-128'),<br> PosixPath('data/carvana/sample_submission.csv'),<br> PosixPath('data/carvana/train_masks_png'),<br> PosixPath('data/carvana/train.csv'),<br> PosixPath('data/carvana/train-128'),<br> PosixPath('data/carvana/train'),<br> PosixPath('data/carvana/metadata.csv'),<br> PosixPath('data/carvana/tmp'),<br> PosixPath('data/carvana/models'),<br> PosixPath('data/carvana/train_masks')]</em></pre><pre name="826c" id="826c" class="graf graf--pre graf-after--pre">MASKS_FN = 'train_masks.csv'<br>META_FN = 'metadata.csv'<br>TRAIN_DN = 'train'<br>MASKS_DN = 'train_masks'</pre><pre name="6d64" id="6d64" class="graf graf--pre graf-after--pre">masks_csv = pd.read_csv(PATH/MASKS_FN)<br>masks_csv.head()</pre><figure name="0315" id="0315" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_gSfi6-hcJG8YP3knRDQUAg.png"></figure><p name="174c" id="174c" class="graf graf--p graf-after--figure">The original dataset came with these CSV files as well [<a href="https://youtu.be/nG3tT31nPmQ?t=1h32m44s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h32m44s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:32:44</a>]. I don’t really use them for very much other than getting the list of images from them.</p><pre name="f2cd" id="f2cd" class="graf graf--pre graf-after--p">meta_csv = pd.read_csv(PATH/META_FN)<br>meta_csv.head()</pre><figure name="8173" id="8173" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_yBm5uRoK_sHQrS-ddATg6w.png"></figure><pre name="e876" id="e876" class="graf graf--pre graf-after--figure"><strong class="markup--strong markup--pre-strong">def</strong> show_img(im, figsize=<strong class="markup--strong markup--pre-strong">None</strong>, ax=<strong class="markup--strong markup--pre-strong">None</strong>, alpha=<strong class="markup--strong markup--pre-strong">None</strong>):<br>    <strong class="markup--strong markup--pre-strong">if</strong> <strong class="markup--strong markup--pre-strong">not</strong> ax: fig,ax = plt.subplots(figsize=figsize)<br>    ax.imshow(im, alpha=alpha)<br>    ax.set_axis_off()<br>    <strong class="markup--strong markup--pre-strong">return</strong> ax</pre><pre name="98c4" id="98c4" class="graf graf--pre graf-after--pre">CAR_ID = '00087a6bd4dc'</pre><pre name="b065" id="b065" class="graf graf--pre graf-after--pre">list((PATH/TRAIN_DN).iterdir())[:5]</pre><pre name="38b6" id="38b6" class="graf graf--pre graf-after--pre">[PosixPath('data/carvana/train/5ab34f0e3ea5_15.jpg'),<br> PosixPath('data/carvana/train/de3ca5ec1e59_07.jpg'),<br> PosixPath('data/carvana/train/28d9a149cb02_13.jpg'),<br> PosixPath('data/carvana/train/36a3f7f77e85_12.jpg'),<br> PosixPath('data/carvana/train/843763f47895_08.jpg')]</pre><pre name="1eb0" id="1eb0" class="graf graf--pre graf-after--pre">Image.open(PATH/TRAIN_DN/f'<strong class="markup--strong markup--pre-strong">{CAR_ID}</strong>_01.jpg').resize((300,200))</pre><figure name="b51b" id="b51b" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_KwDMh55dYxo5-ychRK2srQ.png"></figure><pre name="f946" id="f946" class="graf graf--pre graf-after--figure">list((PATH/MASKS_DN).iterdir())[:5]</pre><pre name="0341" id="0341" class="graf graf--pre graf-after--pre">[PosixPath('data/carvana/train_masks/6c0cd487abcd_03_mask.gif'),<br> PosixPath('data/carvana/train_masks/351c583eabd6_01_mask.gif'),<br> PosixPath('data/carvana/train_masks/90fdd8932877_02_mask.gif'),<br> PosixPath('data/carvana/train_masks/28d9a149cb02_10_mask.gif'),<br> PosixPath('data/carvana/train_masks/88bc32b9e1d9_14_mask.gif')]</pre><pre name="3c07" id="3c07" class="graf graf--pre graf-after--pre">Image.open(PATH/MASKS_DN/f'<strong class="markup--strong markup--pre-strong">{CAR_ID}</strong>_01_mask.gif').resize((300,200))</pre><figure name="1e7c" id="1e7c" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_zz4dhkhhF00W9VIQQiiGcg.png"></figure><p name="388e" id="388e" class="graf graf--p graf-after--figure">Each image after the car ID has a 01, 02, etc of which I’ve printed out all 16 of them for one car and as you can see basically those numbers are the 16 orientations of one car [<a href="https://youtu.be/nG3tT31nPmQ?t=1h32m58s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h32m58s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:32:58</a>]. I don’t think anybody in this competition actually used these orientation information. I believe they all kept the car’s images just treated them separately.</p><pre name="d3fa" id="d3fa" class="graf graf--pre graf-after--p">ims = [open_image(PATH/TRAIN_DN/f'<strong class="markup--strong markup--pre-strong">{CAR_ID}</strong>_{i+1:02d}.jpg') <br>          <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(16)]</pre><pre name="58e5" id="58e5" class="graf graf--pre graf-after--pre">fig, axes = plt.subplots(4, 4, figsize=(9, 6))<br><strong class="markup--strong markup--pre-strong">for</strong> i,ax <strong class="markup--strong markup--pre-strong">in</strong> enumerate(axes.flat): show_img(ims[i], ax=ax)<br>plt.tight_layout(pad=0.1)</pre><figure name="c9fd" id="c9fd" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_ran3w5qDWsvfVaoPileaxw.png"></figure><h4 name="b9a4" id="b9a4" class="graf graf--h4 graf-after--figure">Resize and convert [<a href="https://youtu.be/nG3tT31nPmQ?t=1h33m27s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h33m27s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:33:27</a>]</h4><p name="9975" id="9975" class="graf graf--p graf-after--h4">These images are pretty big — over 1000 by 1000 in size and just opening the JPEGs and resizing them is slow. So I processed them all. Also OpenCV can’t handle GIF files so I converted them.</p><p name="f033" id="f033" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: How would somebody get these masks for training initially? <a href="https://www.mturk.com/" data-href="https://www.mturk.com/" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Mechanical turk</a> or something [<a href="https://youtu.be/nG3tT31nPmQ?t=1h33m48s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h33m48s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:33:48</a>]? Yeah, just a lot of boring work. Probably there are some tools that help you with a bit of edge snapping so that the human can do it roughly and then just fine tune the bits it gets wrong. These kinds of labels are expensive. So one of the things I really want to work on is deep learning enhanced interactive labeling tools because that’s clearly something that would help a lot of people.</p><p name="e0f5" id="e0f5" class="graf graf--p graf-after--p">I’ve got a little section here that you can run if you want to. You probably want to. It converts the GIFs into PNGs so just open int up with PIL and then save it as PNG because OpenCV doesn’t have GIF support. As per usual for this kind of stuff, I do it with a ThreadPool so I can take advantage of parallel processing. And then also create a separate directory <code class="markup--code markup--p-code">train-128</code> and <code class="markup--code markup--p-code">train_masks-128</code> which contains the 128 by 128 resized versions of them.</p><p name="ca7c" id="ca7c" class="graf graf--p graf-after--p">This is the kind of stuff that keeps you sane if you do it early in the process. So anytime you get a new dataset, seriously think about creating a smaller version to make life fast. Anytime you find yourself waiting on your computer, try and think of a way to create a smaller version.</p><pre name="d7eb" id="d7eb" class="graf graf--pre graf-after--p">(PATH/'train_masks_png').mkdir(exist_ok=<strong class="markup--strong markup--pre-strong">True</strong>)</pre><pre name="49bf" id="49bf" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> convert_img(fn):<br>    fn = fn.name<br>    Image.open(PATH/'train_masks'/fn).save(PATH/'train_masks_png'/<br>                     f'<strong class="markup--strong markup--pre-strong">{fn[:-4]}</strong>.png')</pre><pre name="6a95" id="6a95" class="graf graf--pre graf-after--pre">files = list((PATH/'train_masks').iterdir())<br><strong class="markup--strong markup--pre-strong">with</strong> ThreadPoolExecutor(8) <strong class="markup--strong markup--pre-strong">as</strong> e: e.map(convert_img, files)</pre><pre name="42f4" id="42f4" class="graf graf--pre graf-after--pre">(PATH/'train_masks-128').mkdir(exist_ok=<strong class="markup--strong markup--pre-strong">True</strong>)</pre><pre name="9b6b" id="9b6b" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> resize_mask(fn):<br>    Image.open(fn).resize((128,128)).save((fn.parent.parent)<br>        /'train_masks-128'/fn.name)<br><br>files = list((PATH/'train_masks_png').iterdir())<br><strong class="markup--strong markup--pre-strong">with</strong> ThreadPoolExecutor(8) <strong class="markup--strong markup--pre-strong">as</strong> e: e.map(resize_img, files)</pre><pre name="0b2b" id="0b2b" class="graf graf--pre graf-after--pre">(PATH/'train-128').mkdir(exist_ok=<strong class="markup--strong markup--pre-strong">True</strong>)</pre><pre name="eb5f" id="eb5f" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> resize_img(fn):<br>    Image.open(fn).resize((128,128)).save((fn.parent.parent)<br>         /'train-128'/fn.name)<br><br>files = list((PATH/'train').iterdir())<br><strong class="markup--strong markup--pre-strong">with</strong> ThreadPoolExecutor(8) <strong class="markup--strong markup--pre-strong">as</strong> e: e.map(resize_img, files)</pre><p name="c665" id="c665" class="graf graf--p graf-after--pre">So after you grab it from Kaggle, you probably want to run this stuff, go away, have lunch, come back and when you are done, you’ll have these smaller directories which we are going to use below 128 by 128 to start with.</p><h4 name="b90f" id="b90f" class="graf graf--h4 graf-after--p">Dataset [<a href="https://youtu.be/nG3tT31nPmQ?t=1h35m33s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h35m33s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:35:33</a>]</h4><pre name="30b1" id="30b1" class="graf graf--pre graf-after--h4">TRAIN_DN = 'train-128'<br>MASKS_DN = 'train_masks-128'<br>sz = 128<br>bs = 64</pre><pre name="6925" id="6925" class="graf graf--pre graf-after--pre">ims = [open_image(PATH/TRAIN_DN<br>            /f'<strong class="markup--strong markup--pre-strong">{CAR_ID}</strong>_{i+1:02d}.jpg') <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(16)]<br>im_masks = [open_image(PATH/MASKS_DN<br>            /f'<strong class="markup--strong markup--pre-strong">{CAR_ID}</strong>_{i+1:02d}_mask.png') <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(16)]</pre><p name="d0be" id="d0be" class="graf graf--p graf-after--pre">So here is a cool trick. If you use the same axis object (<code class="markup--code markup--p-code">ax</code>) to plot an image twice and the second time you use alpha which you might know means transparency in the computer vision world, then you can actually plot the mask over the top of the photo. So here is a nice way to see all the masks on top of the photos for all of the cars in one group.</p><pre name="9454" id="9454" class="graf graf--pre graf-after--p">fig, axes = plt.subplots(4, 4, figsize=(9, 6))<br><strong class="markup--strong markup--pre-strong">for</strong> i,ax <strong class="markup--strong markup--pre-strong">in</strong> enumerate(axes.flat):<br>    ax = show_img(ims[i], ax=ax)<br>    show_img(im_masks[i][...,0], ax=ax, alpha=0.5)<br>plt.tight_layout(pad=0.1)</pre><figure name="a8ff" id="a8ff" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_KuD6ZEQEbZH8Ka3u8tP5Ig.png"></figure><p name="e562" id="e562" class="graf graf--p graf-after--figure">This is the same MatchedFilesDataset we’ve seen twice already. This is all the same code. Here is something important though. If we had something that was in the training set the one on the left, and then the validation had the image on the right, that would be kind of cheating because it’s the same car.</p><figure name="3af9" id="3af9" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*vUDf60cZZxP3gezhcP36-w.png" data-width="260" data-height="99" src="../img/1_vUDf60cZZxP3gezhcP36-w.png"></figure><pre name="74e2" id="74e2" class="graf graf--pre graf-after--figure"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">MatchedFilesDataset</strong>(FilesDataset):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, fnames, y, transform, path):<br>        self.y=y<br>        <strong class="markup--strong markup--pre-strong">assert</strong>(len(fnames)==len(y))<br>        super().__init__(fnames, transform, path)<br>    <strong class="markup--strong markup--pre-strong">def</strong> get_y(self, i): <br>        <strong class="markup--strong markup--pre-strong">return</strong> open_image(os.path.join(self.path, self.y[i]))<br>    <strong class="markup--strong markup--pre-strong">def</strong> get_c(self): <strong class="markup--strong markup--pre-strong">return</strong> 0</pre><pre name="882e" id="882e" class="graf graf--pre graf-after--pre">x_names = np.array([Path(TRAIN_DN)/o <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> masks_csv['img']])<br>y_names = np.array([Path(MASKS_DN)/f'<strong class="markup--strong markup--pre-strong">{o[:-4]}</strong>_mask.png' <br>                       <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> masks_csv['img']])</pre><pre name="4b02" id="4b02" class="graf graf--pre graf-after--pre">len(x_names)//16//5*16</pre><pre name="4e18" id="4e18" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">1008</em></pre><p name="1159" id="1159" class="graf graf--p graf-after--pre">So we use a continuous set of car IDs and since each set is a set of 16, we make sure that’s evenly divisible by 16. So we make sure that our validation set contains different car IDs to our training set. This is the kind of stuff which you’ve got to be careful of. On Kaggle, it’s not so bad — you’ll know about it because you’ll submit your result and you’ll get a very different result on your leaderboard compared to your validation set. But in the real world. you won’t know until you put it in production and send your company bankrupt and lose your job. So you might want to think carefully about your validation set in that case.</p><pre name="c79e" id="c79e" class="graf graf--pre graf-after--p">val_idxs = list(range(1008))<br>((val_x,trn_x),(val_y,trn_y)) = split_by_idx(val_idxs, x_names, <br>                                              y_names)<br>len(val_x),len(trn_x)</pre><pre name="45fc" id="45fc" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(1008, 4080)</em></pre><p name="ed3d" id="ed3d" class="graf graf--p graf-after--pre">Here we are going to use transform type classification (<code class="markup--code markup--p-code">TfmType.CLASS</code>) [<a href="https://youtu.be/nG3tT31nPmQ?t=1h37m3s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h37m3s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:37:03</a>]. It’s basically the same as transform type pixel (<code class="markup--code markup--p-code">TfmType.PIXEL</code>) but if you think about it, with a pixel version if we rotate a little bit then we probably want to average the pixels in between the two, but the classification, obviously we don’t. We use nearest neighbor. So there’s slight difference there. Also for classification, lighting doesn’t kick in, normalization doesn’t kick in to the dependent variable.</p><pre name="42cd" id="42cd" class="graf graf--pre graf-after--p">aug_tfms = [RandomRotate(4, tfm_y=TfmType.CLASS),<br>            RandomFlip(tfm_y=TfmType.CLASS),<br>            RandomLighting(0.05, 0.05)]<br><em class="markup--em markup--pre-em"># aug_tfms = []</em></pre><p name="14f9" id="14f9" class="graf graf--p graf-after--pre">They are already square images, so we don’t have to do any cropping.</p><pre name="6e78" id="6e78" class="graf graf--pre graf-after--p">tfms = tfms_from_model(resnet34, sz, crop_type=CropType.NO, tfm_y=TfmType.CLASS, aug_tfms=aug_tfms)<br>datasets = ImageData.get_ds(MatchedFilesDataset, (trn_x,trn_y), (val_x,val_y), tfms, path=PATH)<br>md = ImageData(PATH, datasets, bs, num_workers=8, classes=<strong class="markup--strong markup--pre-strong">None</strong>)</pre><pre name="f384" id="f384" class="graf graf--pre graf-after--pre">denorm = md.trn_ds.denorm<br>x,y = next(iter(md.aug_dl))<br>x = denorm(x)</pre><p name="8f3d" id="8f3d" class="graf graf--p graf-after--pre">So here you can see different versions of the augmented images — they are moving around a bit, and they are rotating a bit, and so forth.</p><pre name="6cf7" id="6cf7" class="graf graf--pre graf-after--p">fig, axes = plt.subplots(5, 6, figsize=(12, 10))<br><strong class="markup--strong markup--pre-strong">for</strong> i,ax <strong class="markup--strong markup--pre-strong">in</strong> enumerate(axes.flat):<br>    ax=show_img(x[i], ax=ax)<br>    show_img(y[i], ax=ax, alpha=0.5)<br>plt.tight_layout(pad=0.1)</pre><figure name="0886" id="0886" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_ak5NPATO_ayUjsc7UGfEdQ.png"></figure><p name="d0df" id="d0df" class="graf graf--p graf-after--figure">I get a lot of questions during our study group about how do I debug things and fix things that aren’t working. I never have a great answer other than every time I fix a problem is because of stuff like this that I do all the time. I just always print out everything as I go and then the one thing that I screw up always turns out to be the one thing that I forgot to check along the way. The more of this kind of thing you can do the better. If you are not looking at all of your intermediate results, you are going to have troubles.</p><h4 name="e7bd" id="e7bd" class="graf graf--h4 graf-after--p">Model [<a href="https://youtu.be/nG3tT31nPmQ?t=1h38m30s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h38m30s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:38:30</a>]</h4><pre name="c864" id="c864" class="graf graf--pre graf-after--h4"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">Empty</strong>(nn.Module): <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self,x): <strong class="markup--strong markup--pre-strong">return</strong> x<br><br>models = ConvnetBuilder(resnet34, 0, 0, 0, custom_head=Empty())<br>learn = ConvLearner(md, models)<br>learn.summary()</pre><pre name="7b45" id="7b45" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">StdUpsample</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, nin, nout):<br>        super().__init__()<br>        self.conv = nn.ConvTranspose2d(nin, nout, 2, stride=2)<br>        self.bn = nn.BatchNorm2d(nout)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x): <strong class="markup--strong markup--pre-strong">return</strong> self.bn(F.relu(self.conv(x)))</pre><pre name="93cc" id="93cc" class="graf graf--pre graf-after--pre">flatten_channel = Lambda(<strong class="markup--strong markup--pre-strong">lambda</strong> x: x[:,0])</pre><pre name="913a" id="913a" class="graf graf--pre graf-after--pre">simple_up = nn.Sequential(<br>    nn.ReLU(),<br>    StdUpsample(512,256),<br>    StdUpsample(256,256),<br>    StdUpsample(256,256),<br>    StdUpsample(256,256),<br>    nn.ConvTranspose2d(256, 1, 2, stride=2),<br>    flatten_channel<br>)</pre><p name="1815" id="1815" class="graf graf--p graf-after--pre">Given that we want something that knows what cars look like, we probably want to start with a pre-trained ImageNet network. So we are going to start with ResNet34. With <code class="markup--code markup--p-code">ConvnetBuilder</code>, we can grab our ResNet34 and we can add a custom head. The custom head is going to be something that upsamples a bunch of times and we are going to do things really dumb for now which is we’re just going to do a ConvTranspose2d, batch norm, ReLU.</p><p name="6fbe" id="6fbe" class="graf graf--p graf-after--p">This is what I am saying — any of you could have built this without looking at any of this notebook or at least you have the information from previous classes. There is nothing new at all. So at the very end, we have a single filter. Now that’s going to give us something which is batch size by 1 by 128 by 128. But we want something which is batch size by 128 by 128. So we have to remove that unit axis so I’ve got a lambda layer here. Lambda layers are incredibly helpful because without the lambda layer here, which is simply removing that unit axis by just indexing it with a 0, without a lambda layer, I would have to have created a custom class with a custom forward method and so forth. But by creating a lambda layer that does the one custom bit, I can now just chuck it in the Sequential and so that makes life easier.</p><p name="e580" id="e580" class="graf graf--p graf-after--p">PyTorch people are kind of snooty about this approach. Lambda layer is actually something that’s a part of the fastai library not part of the PyTorch library. And literally people on PyTorch discussion board say “yes, we could give people this”, “yes it is only a single line of code” but they never encourage them to use sequential too often. So there you go.</p><p name="8d82" id="8d82" class="graf graf--p graf-after--p">So this is our custom head [<a href="https://youtu.be/nG3tT31nPmQ?t=1h40m36s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h40m36s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:40:36</a>]. So we are going to have a ResNet 34 that goes downsample and then a really simple custom head that very quickly upsamples, and that hopefully will do something. And we are going to use accuracy with a threshold of 0.5 and print out metrics.</p><pre name="6a44" id="6a44" class="graf graf--pre graf-after--p">models = ConvnetBuilder(resnet34, 0, 0, 0, custom_head=simple_up)<br>learn = ConvLearner(md, models)<br>learn.opt_fn=optim.Adam<br>learn.crit=nn.BCEWithLogitsLoss()<br>learn.metrics=[accuracy_thresh(0.5)]</pre></body></html>