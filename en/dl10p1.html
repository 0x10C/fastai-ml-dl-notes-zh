
<!-- saved from url=(0064)file:///C:/Users/asus/Desktop/fastai-ml-dl-notes-zh/en/dl10.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></head><body><hr class="section-divider"><h1 name="af72" id="af72" class="graf graf--h3 graf--leading graf--title">Deep Learning 2: Part 2 Lesson&nbsp;10</h1><p name="2560" id="2560" class="graf graf--p graf-after--h3"><em class="markup--em markup--p-em">My personal notes from </em><a href="http://www.fast.ai/" data-href="http://www.fast.ai/" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener noopener noopener nofollow noopener nofollow noopener nofollow noopener" target="_blank"><em class="markup--em markup--p-em">fast.ai course</em></a><em class="markup--em markup--p-em">. These notes will continue to be updated and improved as I continue to review the course to “really” understand it. Much appreciation to </em><a href="https://twitter.com/jeremyphoward" data-href="https://twitter.com/jeremyphoward" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener noopener noopener nofollow noopener nofollow noopener nofollow noopener" target="_blank"><em class="markup--em markup--p-em">Jeremy</em></a><em class="markup--em markup--p-em"> and </em><a href="https://twitter.com/math_rachel" data-href="https://twitter.com/math_rachel" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener noopener noopener nofollow noopener nofollow noopener nofollow noopener" target="_blank"><em class="markup--em markup--p-em">Rachel</em></a><em class="markup--em markup--p-em"> who gave me this opportunity to learn.</em></p><p name="8f4a" id="8f4a" class="graf graf--p graf-after--p graf--trailing">Lessons: <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" class="markup--anchor markup--p-anchor" target="_blank">1</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" class="markup--anchor markup--p-anchor" target="_blank">2</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" class="markup--anchor markup--p-anchor" target="_blank">3</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" class="markup--anchor markup--p-anchor" target="_blank">4</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" class="markup--anchor markup--p-anchor" target="_blank">5</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" class="markup--anchor markup--p-anchor" target="_blank">6</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" class="markup--anchor markup--p-anchor" target="_blank">7</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" class="markup--anchor markup--p-anchor" target="_blank">8</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" class="markup--anchor markup--p-anchor" target="_blank">9</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" class="markup--anchor markup--p-anchor" target="_blank"><strong class="markup--strong markup--p-strong">10</strong></a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" class="markup--anchor markup--p-anchor" target="_blank">11</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" class="markup--anchor markup--p-anchor" target="_blank">12</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" class="markup--anchor markup--p-anchor" target="_blank">13</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" class="markup--anchor markup--p-anchor" target="_blank">14</a></p><hr class="section-divider"><h4 name="40c1" id="40c1" class="graf graf--h4 graf--leading"><a href="https://youtu.be/h5Tz7gZT9Fo" data-href="https://youtu.be/h5Tz7gZT9Fo" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">Video</a> /&nbsp;<a href="http://forums.fast.ai/t/part-2-lesson-10-wiki/14364/1" data-href="http://forums.fast.ai/t/part-2-lesson-10-wiki/14364/1" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">Forum</a></h4><figure name="a7e0" id="a7e0" class="graf graf--figure graf-after--h4"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_g_wGv7SlgRghedYKSaIJ1w.png"></figure><h4 name="e65a" id="e65a" class="graf graf--h4 graf-after--figure">Review of Last Week&nbsp;[<a href="https://youtu.be/h5Tz7gZT9FoY?t=16s" data-href="https://youtu.be/h5Tz7gZT9FoY?t=16s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">0:16</a>]</h4><ul class="postList"><li name="1c9d" id="1c9d" class="graf graf--li graf-after--h4">Many students are struggling with last week’s material, so if you are finding it difficult, that’s fine. The reason Jeremy put it up there up front is so that we have something to cogitate about, think about, and gradually work towards, so by lesson 14, you’ll get a second crack at it.</li><li name="c0ad" id="c0ad" class="graf graf--li graf-after--li">To understand the pieces, you’ll need to understand the shapes of convolutional layer outputs, receptive fields, and loss functions — which are all the things you’ll need to understand for all of your deep learning studies anyway.</li><li name="6e98" id="6e98" class="graf graf--li graf-after--li">One key thing is that we started out with something simple — a single object classifier, single object bounding box without a classifier, and then single object classifier and bounding box. The bit where we go to multiple objects is actually almost identical to that except we first have to solve the matching problem. We ended up creating far more activations than we need for our our number of ground truth bounding boxes, so we match each ground truth object to a subset of those activations. Once we’ve done that, the loss function that we then do to each matched pair is almost identical to this loss function (i.e. the one for single object classifier and bounding box).</li><li name="9e86" id="9e86" class="graf graf--li graf-after--li">If you are feeling stuck, go back to lesson 8 and make sure you understand Dataset, DataLoader, and most importantly the loss function.</li><li name="b6b2" id="b6b2" class="graf graf--li graf-after--li">So once we have something which can predict the class and bounding box for one object, we went to multiple objects by just creating more activations [<a href="https://youtu.be/h5Tz7gZT9Fo?t=2m40s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=2m40s" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">2:40</a>]. We had to then deal with the matching problem, having dealt with a matching problem, we then moved each of those anchor boxes in and out a little bit and around a little bit, so they tried to line up with particular ground truth objects.</li><li name="8ad3" id="8ad3" class="graf graf--li graf-after--li">We talked about how we took advantage of the convolutional nature of the network to try to have activations that had a receptive field that was similar to the ground truth object we were predicting. Chloe provided the following fantastic picture to talk about what SSD_MultiHead.forward does line by line:</li></ul><figure name="53d0" id="53d0" class="graf graf--figure graf-after--li"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_BbxbH3gWu8RHMTuXZlDasA.png"><figcaption class="imageCaption">by <a href="http://forums.fast.ai/u/chloews" data-href="http://forums.fast.ai/u/chloews" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">Chloe&nbsp;Sultan</a></figcaption></figure><p name="2196" id="2196" class="graf graf--p graf-after--figure">What Chloe’s done here is she’s focused particularly on the dimensions of the tensor at each point in the path as we gradually downsampled using stride 2 convolutions, making sure she understands why those grid sizes happen then understanding how the outputs come out of those.</p><figure name="91cb" id="91cb" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_gwvD-lSxiUH_EFq9n-ofOQ.png"></figure><ul class="postList"><li name="b19e" id="b19e" class="graf graf--li graf-after--figure">This is where you’ve got to remember this <code class="markup--code markup--li-code">pbd.set_trace()</code>. I just went in just before the class and went into <code class="markup--code markup--li-code">SSD_MultiHead.forward</code> and entered <code class="markup--code markup--li-code">pdb.set_trace()</code> and then I ran a single batch. Then I could just print out the size of all these. We make mistakes and that’s why we have debuggers and know how to check things and do things in small little bits along the way.</li><li name="2a2e" id="2a2e" class="graf graf--li graf-after--li">We then talked about increasing <em class="markup--em markup--li-em">k</em> [<a href="https://youtu.be/h5Tz7gZT9Fo?t=5m49s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=5m49s" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">5:49</a>] which is the number of anchor boxes for each convolutional grid cell which we can do with different zooms, aspect ratios, and that gives us a plethora of activations and therefore predicted bounding boxes.</li><li name="f41d" id="f41d" class="graf graf--li graf-after--li">Then we went down to a small number using Non Maximum Suppression.</li><li name="ee14" id="ee14" class="graf graf--li graf-after--li">Non Maximum Suppression is kind of hacky, ugly, and totally heuristic, and we did not even talk about the code because it seems hideous. Somebody actually came up with a paper recently which attempts to do an end-to-end conv net to replace that NMS piece (<a href="https://arxiv.org/abs/1705.02950" data-href="https://arxiv.org/abs/1705.02950" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">https://arxiv.org/abs/1705.02950</a>).</li></ul><figure name="bcb4" id="bcb4" class="graf graf--figure graf-after--li"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_PadSMuPPUl1W0fPhIdylYQ.png"></figure><ul class="postList"><li name="59b5" id="59b5" class="graf graf--li graf-after--figure">Not enough people are reading papers! What we are doing in class now is implementing papers, the papers are the real ground truth. And I think you know from talking to people a lot of the reason people aren’t reading paper is because a lot of people don’g think they are capable of reading papers. They don’t think they are the kind of people that read papers, but you are. You are here. We started looking at a paper last week and we read the words that were in English and we largely understood them. If you look at the picture above carefully, you’ll realize <code class="markup--code markup--li-code">SSD_MultiHead.forward</code> is not doing the same. You might then wonder if this is better. My answer is probably. Because SSD_MultiHead.forward was the first thing I tried just to get something out there. Between this and YOLO3 paper, they are probably much better ways.</li><li name="0e00" id="0e00" class="graf graf--li graf-after--li">One thing you’ll notice in particular they use a smaller k but they have a lot more sets of grids 1x1, 3x3, 5x5, 10x10, 19x19, 38x38 — 8732 per class. A lot more than we had, so that’ll be an interesting thing to experiment with.</li><li name="008d" id="008d" class="graf graf--li graf-after--li">Another thing I noticed is that we had 4x4, 2x2, 1c1 which means there are a lot of overlap — every set fits within every other set. In this case where you’ve got 1, 3, 5, you don’t have that overlap. So it might actually make it easier to learn. There’s lots of interesting you can play with.</li></ul><figure name="9868" id="9868" class="graf graf--figure graf-after--li"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_1AG_zIXUouogXB5--BFuzw.png"></figure><ul class="postList"><li name="f7a0" id="f7a0" class="graf graf--li graf-after--figure">Perhaps most important thing I would recommend is to put the code and the equations next to each other. You are either math person or code person. By having them side by side, you will learn a little bit of the other.</li><li name="92a9" id="92a9" class="graf graf--li graf-after--li">Learning the math is hard because of the notation might seem hard to look up but there are good resources such as <a href="https://en.wikipedia.org/wiki/List_of_mathematical_symbols" data-href="https://en.wikipedia.org/wiki/List_of_mathematical_symbols" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">wikipedia</a>.</li><li name="fcea" id="fcea" class="graf graf--li graf-after--li">Another thing you should try doing is to re-create things that you see in the papers. Here was the key most important figure 1 from the focal loss paper.</li></ul><figure name="d784" id="d784" class="graf graf--figure graf-after--li"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_NJ8DKEP6qwePIi9hGhvhAg.png"></figure><figure name="aa13" id="aa13" class="graf graf--figure graf-after--figure"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_WhwnGf2r6-aboRYEDWrOUA.png"></figure><ul class="postList"><li name="6c30" id="6c30" class="graf graf--li graf-after--figure">I did discover a minor bug in my code last week [<a href="https://youtu.be/h5Tz7gZT9Fo?t=12m14s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=12m14s" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">12:14</a>] — the way I was flattening out the convolutional activations did not line up with how I was using them in the loss function, and fixing that made it quite a bit better.</li></ul><figure name="2d9b" id="2d9b" class="graf graf--figure graf-after--li"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_B_pXi5zpN2EGnhATYo-ZWg.png"></figure><p name="c0bd" id="c0bd" class="graf graf--p graf-after--figure graf--trailing"><strong class="markup--strong markup--p-strong">Question</strong>: Usually, when we down sample, we increase the number of filters, or depth. when we’re doing sampling from 7x7 to 4x4 etc, why are we decreasing the number from 512 to 256? Why not decrease dimension in SSD head? (performance related?) [<a href="https://youtu.be/_ercfViGrgY?t=12m58s" data-href="https://youtu.be/_ercfViGrgY?t=12m58s" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">12:58</a>] We have a number of out paths and we want each one to be the same, so we don’t want each one to have different number of filters and also this is what the papers did so I was trying to match up with that. Having these 256 — it’s a different concept because we are taking advantage of not just the last layer but the layers before that as well. Life is easier if we make them more consistent.</p><hr class="section-divider"><h3 name="0f76" id="0f76" class="graf graf--h3 graf--leading">Natural Language Processing [<a href="https://youtu.be/h5Tz7gZT9Fo?t=14m10s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=14m10s" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">14:10</a>]</h3><h4 name="d499" id="d499" class="graf graf--h4 graf-after--h3">Where we are going&nbsp;:</h4><p name="1599" id="1599" class="graf graf--p graf-after--h4">We have seen in every lesson this idea of taking a pre-trained model, whip off some some stuff on the top, replace it with something new, and get it to do something similar. We’ve kind of dived in a little bit deeper to that to say with <code class="markup--code markup--p-code">ConvLearner.pretrained</code> it had a standard way of sticking stuff on the top which does a particular thing (i.e. classification). Then we learned actually we can stick any PyTorch module we like on the end and have it do anything we like with a <code class="markup--code markup--p-code">custom_head</code> and so suddenly you discover there’s some really interesting things we can do.</p><p name="566d" id="566d" class="graf graf--p graf-after--p">In fact, Yang Lu said “what if we did a different kind of custom head?” and the different custom head was let’s take the original pictures, rotate them, and the make our dependent variable the opposite of that rotation and see if it can learn to un-rotate it. This is a super useful thing, in fact, I think Google photos nowadays has this option that it’ll actually automatically rotate your photos for you. But the cool thing is, as he showed here, you can build that network right now by doing exactly the same as our previous lesson. But your custom head is one that spits out a single number which is how much to rotate by, and your dataset has a dependent variable which is how much you rotated by.</p><figure name="5171" id="5171" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_4MNEdvVzjzHbbtlub98chw.png"><figcaption class="imageCaption"><a href="http://forums.fast.ai/t/fun-with-lesson8-rotation-adjustment-things-you-can-do-without-annotated-dataset/14261/1" data-href="http://forums.fast.ai/t/fun-with-lesson8-rotation-adjustment-things-you-can-do-without-annotated-dataset/14261/1" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">http://forums.fast.ai/t/fun-with-lesson8-rotation-adjustment-things-you-can-do-without-annotated-dataset/14261/1</a></figcaption></figure><p name="8c4d" id="8c4d" class="graf graf--p graf-after--figure">So you suddenly realize with this idea of a backbone plus a custom head, you can do almost anything you can think about [<a href="https://youtu.be/h5Tz7gZT9Fo?t=16m30s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=16m30s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">16:30</a>].</p><ul class="postList"><li name="5141" id="5141" class="graf graf--li graf-after--p">Today, we are going to look at the same idea and see how that applies to NLP.</li><li name="1b4c" id="1b4c" class="graf graf--li graf-after--li">In the next lesson, we are going to go further and say if NLP and computer vision lets you do the same basic ideas, how do we combine the two. We are going to learn about a model that can actually learn to find word structures from images, images from word structures, or images from images. That will form the basis if you wanted to go further of doing things like going from an image to a sentence (i.e. image captioning) or going from a sentence to an image which we kind of started to do, a phrase to image.</li><li name="12fe" id="12fe" class="graf graf--li graf-after--li">From there, we’ve got to go deeper then into computer vision to think what other kinds of things we can do with this idea of pre-trained network plus a custom head. So we will look at various kinds of image enhancement like increasing the resolution of a low-res photo to guess what was missing or adding artistic filters on top of photos, or changing photos of horses into photos of zebras, etc.</li><li name="f2f1" id="f2f1" class="graf graf--li graf-after--li">Then finally that’s going to bring us all the way back to bounding boxes again. To get there, we’re going to first of all learn about segmentation which is not just figuring out where a bounding box is, but figuring out what every single pixel in an image is a part of — so this pixel is a part of a person, this pixel is a part of a car. Then we are going to use that idea, particularly an idea called UNet, which turns out that this idea of UNet, we can apply to bounding boxes — where it’s called feature pyramids. We’ll use that to get really good results with bounding boxes. That’s kind of our path from here. It’s all going to build on each other but take us into lots of different areas.</li></ul><h4 name="1add" id="1add" class="graf graf--h4 graf-after--li">torchtext to fastai.text [<a href="https://youtu.be/h5Tz7gZT9Fo?t=18m56s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=18m56s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">18:56</a>]:</h4><figure name="a4af" id="a4af" class="graf graf--figure graf-after--h4"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_E9I4m0Oj7vpWbf0DlnyFmw.png"></figure><p name="c50b" id="c50b" class="graf graf--p graf-after--figure">For NLP, last part, we relied on a library called torchtext but as good as it was, I’ve since then found the limitation of it too problematic to keep using it. As a lot of you complained on the forums, it’s pretty darn slow partly that’s because it’s not doing parallel processing and partly it’s because it doesn’t remember what you did last time and it does it all over again from scratch. Then it’s hard to do fairly simple things like a lot of you were trying to get into the toxic comment competition on Kaggle which was a multi-label problem and trying to do that with torchtext, I eventually got it working but it took me like a week of hacking away which is kind of ridiculous. To fix all these problems, we’ve created a new library called fastai.text. Fastai.text is a replacement for the combination of torchtext and fastai.nlp. So don’t use fastai.nlp anymore — that’s obsolete. It’s slower, it’s more confusing, it’s less good in every way, but there’s a lot of overlaps. Intentionally, a lot of the classes and functions have the same names, but this is the non-torchtext version.</p><h4 name="aa31" id="aa31" class="graf graf--h4 graf-after--p">IMDb [<a href="https://youtu.be/h5Tz7gZT9Fo?t=20m32s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=20m32s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">20:32</a>]</h4><p name="f3ea" id="f3ea" class="graf graf--p graf-after--h4"><a href="https://github.com/fastai/fastai/blob/master/courses/dl2/imdb.ipynb" data-href="https://github.com/fastai/fastai/blob/master/courses/dl2/imdb.ipynb" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Notebook</a></p><p name="f7b1" id="f7b1" class="graf graf--p graf-after--p">We will work with IMDb again. For those of you who have forgotten, go back and checkout <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" class="markup--anchor markup--p-anchor" target="_blank">lesson 4</a>. This is a dataset of movie reviews, and we used it to find out whether we might enjoy Zombiegeddon or not, and we thought probably my kind of thing.</p><pre name="c17f" id="c17f" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">fastai.text</strong> <strong class="markup--strong markup--pre-strong">import</strong> *<br><strong class="markup--strong markup--pre-strong">import</strong> <strong class="markup--strong markup--pre-strong">html</strong></pre><blockquote name="2e97" id="2e97" class="graf graf--blockquote graf-after--pre">We need to download the IMDB large movie reviews from this site: <a href="http://ai.stanford.edu/~amaas/data/sentiment/" data-href="http://ai.stanford.edu/~amaas/data/sentiment/" class="markup--anchor markup--blockquote-anchor" rel="nofollow noopener" target="_blank">http://ai.stanford.edu/~amaas/data/sentiment/</a> Direct link&nbsp;: <a href="http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz" data-href="http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz" class="markup--anchor markup--blockquote-anchor" rel="nofollow noopener" target="_blank">Link</a></blockquote><pre name="4475" id="4475" class="graf graf--pre graf-after--blockquote">BOS = 'xbos'  <em class="markup--em markup--pre-em"># beginning-of-sentence tag</em><br>FLD = 'xfld'  <em class="markup--em markup--pre-em"># data field tag</em></pre><pre name="7cb2" id="7cb2" class="graf graf--pre graf-after--pre">PATH=Path('data/aclImdb/')</pre><h4 name="b5e5" id="b5e5" class="graf graf--h4 graf-after--pre">Standardize format&nbsp;[<a href="https://youtu.be/h5Tz7gZT9Fo?t=21m27s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=21m27s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">21:27</a>]</h4><p name="e8d7" id="e8d7" class="graf graf--p graf-after--h4">The basic paths for NLP is that we have to take sentences and turn them into numbers, and there is a couple to get there. At the moment, somewhat intentionally, fastai.text does not provide that many helper functions. It’s really designed more to let you handle things in a fairly flexible way.</p><pre name="9637" id="9637" class="graf graf--pre graf-after--p">CLAS_PATH=Path('data/imdb_clas/')<br>CLAS_PATH.mkdir(exist_ok=<strong class="markup--strong markup--pre-strong">True</strong>)</pre><pre name="1e45" id="1e45" class="graf graf--pre graf-after--pre">LM_PATH=Path('data/imdb_lm/')<br>LM_PATH.mkdir(exist_ok=<strong class="markup--strong markup--pre-strong">True</strong>)</pre><p name="54bc" id="54bc" class="graf graf--p graf-after--pre">As you can see here [<a href="https://youtu.be/h5Tz7gZT9Fo?t=21m59s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=21m59s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">21:59</a>], I wrote something called get_texts which goes through each thing in <code class="markup--code markup--p-code">CLASSES</code>. There are three classes in IMDb: negative, positive, and then there’s another folder “unsupervised” which contains the ones they haven’t gotten around to labeling yet — so we will just call that a class for now. So we just go through each one of those classes, then find every file in that folder, and open it up, read it, and chuck it into the end of the array. As you can see, with pathlib, it’s super easy to grab stuff and pull it in, and then the label is just whatever class we are up to so far. We will do that for both training set and test set.</p><pre name="ecb6" id="ecb6" class="graf graf--pre graf-after--p">CLASSES = ['neg', 'pos', 'unsup']</pre><pre name="1893" id="1893" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> get_texts(path):<br>    texts,labels = [],[]<br>    <strong class="markup--strong markup--pre-strong">for</strong> idx,label <strong class="markup--strong markup--pre-strong">in</strong> enumerate(CLASSES):<br>        <strong class="markup--strong markup--pre-strong">for</strong> fname <strong class="markup--strong markup--pre-strong">in</strong> (path/label).glob('*.*'):<br>            texts.append(fname.open('r').read())<br>            labels.append(idx)<br>    <strong class="markup--strong markup--pre-strong">return</strong> np.array(texts),np.array(labels)</pre><pre name="8fe1" id="8fe1" class="graf graf--pre graf-after--pre">trn_texts,trn_labels = get_texts(PATH/'train')<br>val_texts,val_labels = get_texts(PATH/'test')</pre><pre name="bdc3" id="bdc3" class="graf graf--pre graf-after--pre">len(trn_texts),len(val_texts)</pre><pre name="cc13" id="cc13" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(75000, 25000)</em></pre><p name="57f9" id="57f9" class="graf graf--p graf-after--pre">there are 75,000 in train, 25,000 in test. 50,000 in the train set are unsupervised, and we won’t actually be able to use them when we get to the classification. Jeremy found this much easier than torch.text approach of having lots of layers and wrappers because in the end, reading text files is not that hard.</p><pre name="b872" id="b872" class="graf graf--pre graf-after--p">col_names = ['labels','text']</pre><p name="75b8" id="75b8" class="graf graf--p graf-after--pre">One thing that’s always good idea is to sort things randomly [<a href="https://youtu.be/h5Tz7gZT9Fo?t=23m19s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=23m19s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">23:19</a>]. It is useful to know this simple trick for sorting things randomly particularly when you’ve got multiple things you have to sort the same way. In this case, you have labels and <code class="markup--code markup--p-code">texts. np.random.permutation</code>, if you give it an integer, it gives you a random list from 0 up to and not including the number you give it in some random order.</p><pre name="b6f2" id="b6f2" class="graf graf--pre graf-after--p">np.random.seed(42)<br>trn_idx = np.random.permutation(len(trn_texts))<br>val_idx = np.random.permutation(len(val_texts))</pre><p name="3c98" id="3c98" class="graf graf--p graf-after--pre">You can them pass that in as an indexer to give you a list that’s sorted in that random order. So in this case, it is going to sort <code class="markup--code markup--p-code">trn_texts</code> and <code class="markup--code markup--p-code">trn_labels</code> in the same random way. So that’s a useful little idiom to use.</p><pre name="f0aa" id="f0aa" class="graf graf--pre graf-after--p">trn_texts = trn_texts[trn_idx]<br>val_texts = val_texts[val_idx]</pre><pre name="d43d" id="d43d" class="graf graf--pre graf-after--pre">trn_labels = trn_labels[trn_idx]<br>val_labels = val_labels[val_idx]</pre><p name="7251" id="7251" class="graf graf--p graf-after--pre">Now we have our texts and labels sorted, we can create a dataframe from them [<a href="https://youtu.be/h5Tz7gZT9Fo?t=24m7s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=24m7s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">24:07</a>]. Why are we doing this? The reason is because there is a somewhat standard approach starting to appear for text classification datasets which is to have your training set as a CSV file with the labels first, and the text of the NLP documents second. So it basically looks like this:</p><figure name="1e41" id="1e41" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_KUPgEBboQilVi7wcp6RO0Q.png"></figure><pre name="432f" id="432f" class="graf graf--pre graf-after--figure">df_trn = pd.DataFrame({'text':trn_texts, 'labels':trn_labels}, <br>                      columns=col_names)<br>df_val = pd.DataFrame({'text':val_texts, 'labels':val_labels},<br>                      columns=col_names)</pre><pre name="8386" id="8386" class="graf graf--pre graf-after--pre">df_trn[df_trn['labels']!=2].to_csv(CLAS_PATH/'train.csv',<br>                                   header=<strong class="markup--strong markup--pre-strong">False</strong>, index=<strong class="markup--strong markup--pre-strong">False</strong>)<br>df_val.to_csv(CLAS_PATH/'test.csv', header=<strong class="markup--strong markup--pre-strong">False</strong>, index=<strong class="markup--strong markup--pre-strong">False</strong>)</pre><pre name="3fe6" id="3fe6" class="graf graf--pre graf-after--pre">(CLAS_PATH/'classes.txt').open('w')<br>    .writelines(f'<strong class="markup--strong markup--pre-strong">{o}\n</strong>' <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> CLASSES)<br>(CLAS_PATH/'classes.txt').open().readlines()</pre><pre name="5e02" id="5e02" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">['neg\n', 'pos\n', 'unsup\n']</em></pre><p name="e5c9" id="e5c9" class="graf graf--p graf-after--pre">So you have your labels and texts, and then a file called classes.txt which just lists the classes. I say somewhat standard because in a reasonably recent academic paper Yann LeCun and a team of researcher looked at quite a few datasets and they use this format for all of them. So that’s what I started using as well for my recent paper. You’ll find that this notebook, if you put your data into this format, the whole notebook will work every time [<a href="https://youtu.be/h5Tz7gZT9Fo?t=25m17s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=25m17s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">25:17</a>]. So rather than having a thousand different formats, I just said let’s just pick a standard format and your job is to put your data in that format which is the CSV file. The CSV files have no header by default.</p><p name="ac34" id="ac34" class="graf graf--p graf-after--p">You’ll notice at the start, we have two different paths [<a href="https://youtu.be/h5Tz7gZT9Fo?t=25m51s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=25m51s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">25:51</a>]. One was the classification path, and the other was the language model path. In NLP, you’ll see LM all the time. LM means language model. The classification path is going to contain the information that we are going to use to create a sentiment analysis model. The language model path is going to contain the information we need to create a language model. So they are a little bit different. One thing that is different is that when we create the train.csv in the classification path, we remove everything that has a label of 2 because label of 2 is “unsupervised” and we can’t use it.</p><figure name="42e7" id="42e7" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_CdwPQjBC0mmDYXXRQh6xMA.png"></figure><pre name="b093" id="b093" class="graf graf--pre graf-after--figure">trn_texts,val_texts = sklearn.model_selection.train_test_split(<br>    np.concatenate([trn_texts,val_texts]), test_size=0.1)</pre><pre name="aa3e" id="aa3e" class="graf graf--pre graf-after--pre">len(trn_texts), len(val_texts)</pre><pre name="826a" id="826a" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(90000, 10000)</em></pre><p name="a158" id="a158" class="graf graf--p graf-after--pre">The second difference is the labels [<a href="https://youtu.be/h5Tz7gZT9Fo?t=26m51s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=26m51s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">26:51</a>]. For the classification path, the labels are the actual labels, but for the language model, there are no labels so we just use a bunch of zeros and that just makes it a little easier because we can use a consistent dataframe/CSV format.</p><p name="5cd2" id="5cd2" class="graf graf--p graf-after--p">Now the language model, we can create our own validation set, so you’ve probably come across by now, <code class="markup--code markup--p-code">sklearn.model_selection.train_test_split</code> which is a really simple function that grabs a dataset and randomly splits it into a training set and a validation set according to whatever proportion you specify. In this case, we concatenate our classification training and validation together, split it by 10%, now we have 90,000 training, 10,000 validation for our language model. So that’s getting the data in a standard format for our language model and our classifier.</p><pre name="635b" id="635b" class="graf graf--pre graf-after--p">df_trn = pd.DataFrame({'text':trn_texts, 'labels':<br>                       [0]*len(trn_texts)}, columns=col_names)<br>df_val = pd.DataFrame({'text':val_texts, 'labels':<br>                       [0]*len(val_texts)}, columns=col_names)</pre><pre name="0d13" id="0d13" class="graf graf--pre graf-after--pre">df_trn.to_csv(LM_PATH/'train.csv', header=<strong class="markup--strong markup--pre-strong">False</strong>, index=<strong class="markup--strong markup--pre-strong">False</strong>)<br>df_val.to_csv(LM_PATH/'test.csv', header=<strong class="markup--strong markup--pre-strong">False</strong>, index=<strong class="markup--strong markup--pre-strong">False</strong>)</pre><h4 name="ec91" id="ec91" class="graf graf--h4 graf-after--pre">Language model tokens&nbsp;[<a href="https://youtu.be/h5Tz7gZT9Fo?t=28m3s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=28m3s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">28:03</a>]</h4><p name="8aa6" id="8aa6" class="graf graf--p graf-after--h4">The next thing we need to do is tokenization. Tokenization means at this stage, for a document (i.e. a movie review), we have a big long string and we want to turn it into a list of tokens which is similar to a list of words but not quite. For example, <code class="markup--code markup--p-code">don’t</code> we want it to be <code class="markup--code markup--p-code">do</code> and <code class="markup--code markup--p-code">n’t</code>, we probably want full stop to be a token, and so forth. Tokenization is something that we passed off to a terrific library called spaCy — partly terrific because the Australian wrote it and partly terrific because it’s good at what it does. We put a bit of stuff on top of spaCy but the vast majority of the work’s been done by spaCy.</p><pre name="d87d" id="d87d" class="graf graf--pre graf-after--p">chunksize=24000</pre><p name="73ee" id="73ee" class="graf graf--p graf-after--pre">Before we pass it to spaCy, Jeremy wrote this simple <code class="markup--code markup--p-code">fixup</code> function which is each time he’s looked at different datasets (about a dozen in building this), every one had different weird things that needed to be replaced. So here are all the ones he’s come up with so far, and hopefully this will help you out as well. All the entities are html unescaped and there are bunch more things we replace. Have a look at the result of running this on text that you put in and make sure there’s no more weird tokens in there.</p><pre name="1dc5" id="1dc5" class="graf graf--pre graf-after--p">re1 = re.compile(r'  +')</pre><pre name="b258" id="b258" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> fixup(x):<br>   x = x.replace('#39;', "'").replace('amp;', '&amp;')<br>        .replace('#146;', "'").replace('nbsp;', ' ')<br>        .replace('#36;', '$').replace('<strong class="markup--strong markup--pre-strong">\\</strong>n', "<strong class="markup--strong markup--pre-strong">\n</strong>")<br>        .replace('quot;', "'").replace('&lt;br /&gt;', "<strong class="markup--strong markup--pre-strong">\n</strong>")<br>        .replace('<strong class="markup--strong markup--pre-strong">\\</strong>"', '"').replace('&lt;unk&gt;','u_n')<br>        .replace(' @.@ ','.').replace(' @-@ ','-')<br>        .replace('<strong class="markup--strong markup--pre-strong">\\</strong>', ' <strong class="markup--strong markup--pre-strong">\\</strong> ')<br>    <strong class="markup--strong markup--pre-strong">return</strong> re1.sub(' ', html.unescape(x))</pre><pre name="51b0" id="51b0" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> get_texts(df, n_lbls=1):<br>    labels = df.iloc[:,range(n_lbls)].values.astype(np.int64)<br>    texts = f'<strong class="markup--strong markup--pre-strong">\n{BOS}</strong> <strong class="markup--strong markup--pre-strong">{FLD}</strong> 1 ' + df[n_lbls].astype(str)<br>    <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(n_lbls+1, len(df.columns)): <br>        texts += f' <strong class="markup--strong markup--pre-strong">{FLD}</strong> {i-n_lbls} ' + df[i].astype(str)<br>    texts = texts.apply(fixup).values.astype(str)</pre><pre name="e305" id="e305" class="graf graf--pre graf-after--pre">    tok = Tokenizer().proc_all_mp(partition_by_cores(texts))<br>    <strong class="markup--strong markup--pre-strong">return</strong> tok, list(labels)</pre><p name="4be3" id="4be3" class="graf graf--p graf-after--pre"><code class="markup--code markup--p-code">get_all function</code> calls <code class="markup--code markup--p-code">get_texts</code> and get_texts is going to do a few things [<a href="https://youtu.be/h5Tz7gZT9Fo?t=29m40s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=29m40s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">29:40</a>]. One of which is to apply that <code class="markup--code markup--p-code">fixup</code> that we just mentioned.</p><pre name="e925" id="e925" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> get_all(df, n_lbls):<br>    tok, labels = [], []<br>    <strong class="markup--strong markup--pre-strong">for</strong> i, r <strong class="markup--strong markup--pre-strong">in</strong> enumerate(df):<br>        print(i)<br>        tok_, labels_ = get_texts(r, n_lbls)<br>        tok += tok_;<br>        labels += labels_<br>    <strong class="markup--strong markup--pre-strong">return</strong> tok, labels</pre><p name="d5ea" id="d5ea" class="graf graf--p graf-after--pre">Let’s look through this because there is some interesting things to point out [<a href="https://youtu.be/h5Tz7gZT9Fo?t=29m57s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=29m57s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">29:57</a>]. We are going to use pandas to open our train.csv from the language model path, but we are passing in an extra parameter you may not have seen before called <code class="markup--code markup--p-code">chunksize</code>. Python and pandas can both be pretty inefficient when it comes to storing and using text data. So you’ll see that very few people in NLP are working with large corpuses. And Jeremy thinks the part of the reason is that traditional tools made it really difficult — you run out of memory all the time. So this process he is showing us today, he has used on corpuses of over a billion words successfully using this exact code. One of the simple trick is this thing called <code class="markup--code markup--p-code">chunksize</code> with pandas. That that means is that pandas does not return a data frame, but it returns an iterator that we can iterate through chunks of a data frame. That is why we don’t say <code class="markup--code markup--p-code">tok_trn = get_text(df_trn)</code> but instead we call <code class="markup--code markup--p-code">get_all</code> which loops through the data frame but actually what it’s really doing is it’s looping through chunks of the data frame so each of those chunks is basically a data frame representing a subset of the data [<a href="https://youtu.be/h5Tz7gZT9Fo?t=31m5s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=31m5s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">31:05]</a>.</p><p name="64f7" id="64f7" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: When I’m working with NLP data, many times I come across data with foreign texts/characters. Is it better to discard them or keep them [<a href="https://youtu.be/h5Tz7gZT9Fo?t=31m31s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=31m31s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">31:31</a>]? No no, definitely keep them. This whole process is unicode and I’ve actually used this on Chinese text. This is designed to work on pretty much anything. In general, most of the time, it’s not a good idea to remove anything. Old-fashioned NLP approaches tended to do all this like lemmatization and all these normalization steps to get rid things, lower case everything, etc. But that’s throwing away information which you don’t know ahead of time whether it’s useful or not. So don’t throw away information.</p><p name="2da6" id="2da6" class="graf graf--p graf-after--p">So we go through each chunk each of which is a data frame and we call <code class="markup--code markup--p-code">get_texts</code> [<a href="https://youtu.be/h5Tz7gZT9Fo?t=32m19s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=32m19s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">32:19</a>]. <code class="markup--code markup--p-code">get_texts</code> will grab the labels and makes them into integers, and it’s going to grab the texts. A couple things to point out:</p><ul class="postList"><li name="46c3" id="46c3" class="graf graf--li graf-after--p">Before we include the text, we have “beginning of stream” (<code class="markup--code markup--li-code">BOS</code>) token which we defined in the beginning. There’s nothing special about these particular strings of letters — they are just ones I figured don’t appear in normal texts very often. So every text is going to start with <code class="markup--code markup--li-code">‘xbos’</code> — why is that? Because it’s often useful for your model to know when a new text is starting. For example, if it’s a language model, we are going to concatenate all the texts together. So it would be really helpful for it to know all this articles finished and a new one started so I should probably forget some of their context now.</li><li name="e013" id="e013" class="graf graf--li graf-after--li">Ditto is quite often texts have multiple fields like a title and abstract, and then a main document. So by the same token, we’ve got this thing here which lets us actually have multiple fields in our CSV. So this process is designed to be very flexible. Again at the start of each one, we put a special “field starts here” token followed by the number of the field that’s starting here for as many fields as we have. Then we apply <code class="markup--code markup--li-code">fixup</code> to it.</li><li name="b6f6" id="b6f6" class="graf graf--li graf-after--li">Then most importantly [<a href="https://youtu.be/h5Tz7gZT9Fo?t=33m54s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=33m54s" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">33:54</a>], we tokenize it — we tokenize it by doing a “process all multiprocessing” (<code class="markup--code markup--li-code">proc_all_mp</code>). Tokenizing tends to be pretty slow but we’ve all got multiple cores in our machines now, and some of the better machines on AWS can have dozens of cores. spaCy is not very amenable to multi processing but Jeremy finally figured out how to get it to work. The good news is that it’s all wrapped up in this one function now. So all you need to pass to that function is a list of things to tokenize which each part of that list will be tokenized on a different core. There is also a function called <code class="markup--code markup--li-code">partition_by_cores</code> which takes a list and splits it into sublists. The number of sublists is the number of cores that you have in your computer. On Jeremy’s machine without multiprocessing, this takes about an hour and a half, and with multiprocessing, it takes about 2 minutes. So it’s a really hand thing to have. Feel free to look inside it and take advantage of it for your own stuff. Remember, we all have multiple cores even in our laptops and very few things in Python take advantage or it unless you make a bit of an effort to make it work.</li></ul><pre name="71cc" id="71cc" class="graf graf--pre graf-after--li">df_trn = pd.read_csv(LM_PATH/'train.csv', header=<strong class="markup--strong markup--pre-strong">None</strong>, <br>                     chunksize=chunksize)<br>df_val = pd.read_csv(LM_PATH/'test.csv', header=<strong class="markup--strong markup--pre-strong">None</strong>, <br>                     chunksize=chunksize)</pre><pre name="3888" id="3888" class="graf graf--pre graf-after--pre">tok_trn, trn_labels = get_all(df_trn, 1)<br>tok_val, val_labels = get_all(df_val, 1)</pre><pre name="e57d" id="e57d" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">0<br>1<br>2<br>3<br>0</em></pre><pre name="9579" id="9579" class="graf graf--pre graf-after--pre">(LM_PATH/'tmp').mkdir(exist_ok=<strong class="markup--strong markup--pre-strong">True</strong>)</pre><p name="e6bd" id="e6bd" class="graf graf--p graf-after--pre">Here is the result at the end [<a href="https://youtu.be/h5Tz7gZT9Fo?t=35m42s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=35m42s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">35:42</a>]. Beginning of the stream token (<code class="markup--code markup--p-code">xbos</code>), beginning of field number 1 token (<code class="markup--code markup--p-code">xfld 1</code>), and tokenized text. You’ll see that the punctuation is on whole now a separate token.</p><p name="c139" id="c139" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code"><strong class="markup--strong markup--p-strong">t_up</strong></code>: <code class="markup--code markup--p-code">t_up mgm</code> — MGM was originally capitalized. But the interesting thing is that normally people either lowercase everything or they leave the case as is. Now if you leave the case as is, then “SCREW YOU” and “screw you” are two totally different sets of tokens that have to be learnt from scratch. Or if you lowercase them all, then there is no difference at all. So how do you fix this so that you both get a semantic impact of “I’M SHOUTING NOW” but not have to learn the shouted version vs. the normal version. So the idea is to come up with a unique token to mean the next thing is all uppercase. Then we lowercase it, so now whatever used to be uppercase is lowercased, and then we can learn the semantic meaning of all uppercase.</p><p name="612c" id="612c" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code"><strong class="markup--strong markup--p-strong">tk_rep</strong></code>: Similarly, if you have 29&nbsp;<code class="markup--code markup--p-code">!</code> in a row, we don’t learn a separate token for 29 exclamation marks — instead we put in a special token for “the next thing repeats lots of times” and then put the number 29 and an exclamation mark (i.e. <code class="markup--code markup--p-code">tk_rep 29&nbsp;!</code>). So there are a few tricks like that. If you are interested in NLP, have a look at the tokenizer code for these little tricks that Jeremy added in because some of them are kind of fun.</p><pre name="bd40" id="bd40" class="graf graf--pre graf-after--p">' '.join(tok_trn[0])</pre><figure name="ed79" id="ed79" class="graf graf--figure graf--layoutOutsetCenter graf-after--pre" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_avBwSHfjT31_-m28KGf4NA.png"></figure><p name="34df" id="34df" class="graf graf--p graf-after--figure">The nice thing with doing things this way is we can now just <code class="markup--code markup--p-code">np.save</code> that and load it back up later [<a href="https://youtu.be/h5Tz7gZT9Fo?t=37m44s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=37m44s" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">37:44</a>]. We don’t have to recalculate all this stuff each time like we tend to have to do with torchtext or a lot of other libraries. Now that we got it tokenized, the next thing we need to do is to turn it into numbers which we call numericalizing it. The way we numericalize it is very simple.</p><ul class="postList"><li name="e541" id="e541" class="graf graf--li graf-after--p">We make a list list of all the words that appear in some order</li><li name="1199" id="1199" class="graf graf--li graf-after--li">Then we replace every word with its index into that list</li><li name="f18a" id="f18a" class="graf graf--li graf-after--li">The list of all the tokens, we call that the vocabulary.</li></ul><pre name="1d6f" id="1d6f" class="graf graf--pre graf-after--li">np.save(LM_PATH/'tmp'/'tok_trn.npy', tok_trn)<br>np.save(LM_PATH/'tmp'/'tok_val.npy', tok_val)</pre><pre name="32d1" id="32d1" class="graf graf--pre graf-after--pre">tok_trn = np.load(LM_PATH/'tmp'/'tok_trn.npy')<br>tok_val = np.load(LM_PATH/'tmp'/'tok_val.npy')</pre><p name="98ab" id="98ab" class="graf graf--p graf-after--pre">Here is an example of some of the vocabulary [<a href="https://youtu.be/h5Tz7gZT9Fo?t=38m28s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=38m28s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">38:28</a>]. The Counter class in Python is very handy for this. It basically gives us a list of unique items and their counts. Here are the 25 most common things in the vocabulary. Generally speaking, we don’t want every unique token in our vocabulary. If it doesn’t appear at least twice then might just be a spelling mistake or a word we can’t learn anything about it if it doesn’t appear that often. Also the stuff we are going to be learning about so far in this part gets a bit clunky once you’ve got a vocabulary bigger than 60,000. Time permitting, we may look at some work Jeremy has been doing recently on handling larger vocabularies, otherwise that might have to come in a future course. But actually for classification, doing more than about 60,000 words doesn’t seem to help anyway.</p><pre name="7824" id="7824" class="graf graf--pre graf-after--p">freq = Counter(p <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> tok_trn <strong class="markup--strong markup--pre-strong">for</strong> p <strong class="markup--strong markup--pre-strong">in</strong> o)<br>freq.most_common(25)</pre><pre name="29b7" id="29b7" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[('the', 1207984),<br> ('.', 991762),<br> (',', 985975),<br> ('and', 587317),<br> ('a', 583569),<br> ('of', 524362),<br> ('to', 484813),<br> ('is', 393574),<br> ('it', 341627),<br> ('in', 337461),<br> ('i', 308563),<br> ('this', 270705),<br> ('that', 261447),<br> ('"', 236753),<br> ("'s", 221112),<br> ('-', 188249),<br> ('was', 180235),<br> ('\n\n', 178679),<br> ('as', 165610),<br> ('with', 159164),<br> ('for', 158981),<br> ('movie', 157676),<br> ('but', 150203),<br> ('film', 144108),<br> ('you', 124114)]</em></pre><p name="81db" id="81db" class="graf graf--p graf-after--pre">So we are going to limit our vocabulary to 60,000 words, things that appear at least twice [<a href="https://youtu.be/h5Tz7gZT9Fo?t=39m33s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=39m33s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">39:33</a>]. Here is a simple way to do that. Use&nbsp;<code class="markup--code markup--p-code">.most_common</code>, pass in the max vocab size. That’ll sort it by the frequency and if it appears less often than a minimum frequency, then don’t bother with it at all. That gives us <code class="markup--code markup--p-code">itos</code> — that’s the same name that torchtext used and it means integer-to-string. This is just the list of unique tokens in the vocab. We’ll insert two more tokens — a vocab item for unknown (<code class="markup--code markup--p-code">_unk_</code>) and a vocab item for padding (<code class="markup--code markup--p-code">_pad_</code>).</p><pre name="c5dd" id="c5dd" class="graf graf--pre graf-after--p">max_vocab = 60000<br>min_freq = 2</pre><pre name="b194" id="b194" class="graf graf--pre graf-after--pre">itos = [o <strong class="markup--strong markup--pre-strong">for</strong> o,c <strong class="markup--strong markup--pre-strong">in</strong> freq.most_common(max_vocab) <strong class="markup--strong markup--pre-strong">if</strong> c&gt;min_freq]<br>itos.insert(0, '_pad_')<br>itos.insert(0, '_unk_')</pre><p name="3ee1" id="3ee1" class="graf graf--p graf-after--pre">We can then create the dictionary which goes in the opposite direction (string to integer)[<a href="https://youtu.be/h5Tz7gZT9Fo?t=40m19s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=40m19s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">40:19</a>]. That won’t cover everything because we intentionally truncated it down to 60,000 words. If we come across something that is not in the dictionary, we want to replace it with zero for unknown so we can use <code class="markup--code markup--p-code">defaultdict</code> with a lambda function that always returns zero.</p><pre name="35a6" id="35a6" class="graf graf--pre graf-after--p">stoi = collections.defaultdict(<strong class="markup--strong markup--pre-strong">lambda</strong>:0, <br>                               {v:k <strong class="markup--strong markup--pre-strong">for</strong> k,v <strong class="markup--strong markup--pre-strong">in</strong> enumerate(itos)})<br>len(itos)</pre><pre name="547e" id="547e" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">60002</em></pre><p name="e43a" id="e43a" class="graf graf--p graf-after--pre">So now we have our <code class="markup--code markup--p-code">stoi</code> dictionary defined, we can then call that for every word for every sentence [<a href="https://youtu.be/h5Tz7gZT9Fo?t=40m50s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=40m50s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">40:50</a>].</p><pre name="da6e" id="da6e" class="graf graf--pre graf-after--p">trn_lm = np.array([[stoi[o] <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> p] <strong class="markup--strong markup--pre-strong">for</strong> p <strong class="markup--strong markup--pre-strong">in</strong> tok_trn])<br>val_lm = np.array([[stoi[o] <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> p] <strong class="markup--strong markup--pre-strong">for</strong> p <strong class="markup--strong markup--pre-strong">in</strong> tok_val])</pre><p name="baca" id="baca" class="graf graf--p graf-after--pre">Here is our numericalized version:</p><figure name="4851" id="4851" class="graf graf--figure graf--layoutOutsetCenter graf-after--p" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_1VnI0YwW5Lb2N1qFHuOpAA.png"></figure><p name="f856" id="f856" class="graf graf--p graf-after--figure">Of course, the nice thing is we can save that step as well. Each time we get to another step, we can save it. These are not very big files compared to what you are used with images. Text is generally pretty small.</p><p name="063d" id="063d" class="graf graf--p graf-after--p">Very important to also save that vocabulary (<code class="markup--code markup--p-code">itos</code>). The list of numbers means nothing unless you know what each number refers to, and that’s what <code class="markup--code markup--p-code">itos</code> tells you.</p><pre name="98a4" id="98a4" class="graf graf--pre graf-after--p">np.save(LM_PATH/'tmp'/'trn_ids.npy', trn_lm)<br>np.save(LM_PATH/'tmp'/'val_ids.npy', val_lm)<br>pickle.dump(itos, open(LM_PATH/'tmp'/'itos.pkl', 'wb'))</pre><p name="135f" id="135f" class="graf graf--p graf-after--pre">So you save those three things, and later on you can load them back up.</p><pre name="3020" id="3020" class="graf graf--pre graf-after--p">trn_lm = np.load(LM_PATH/'tmp'/'trn_ids.npy')<br>val_lm = np.load(LM_PATH/'tmp'/'val_ids.npy')<br>itos = pickle.load(open(LM_PATH/'tmp'/'itos.pkl', 'rb'))</pre><p name="962e" id="962e" class="graf graf--p graf-after--pre">Now our vocab size is 60,002 and our training language model has 90,000 documents in it.</p><pre name="4c69" id="4c69" class="graf graf--pre graf-after--p">vs=len(itos)<br>vs,len(trn_lm)</pre><pre name="2ae0" id="2ae0" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(60002, 90000)</em></pre><p name="ca8f" id="ca8f" class="graf graf--p graf-after--pre">That’s the preprocessing you do [<a href="https://youtu.be/h5Tz7gZT9Fo?t=42m1s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=42m1s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">42:01</a>]. We can probably wrap a little bit more of that in utility functions if we want to but it’s all pretty straight forward and that exact code will work for any dataset you have once you’ve got it in that CSV format.</p><h4 name="f406" id="f406" class="graf graf--h4 graf-after--p">Pre-training [<a href="https://youtu.be/h5Tz7gZT9Fo?t=42m19s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=42m19s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">42:19</a>]</h4><figure name="1c1f" id="1c1f" class="graf graf--figure graf-after--h4"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_4RrK26gE8W28T8uJBl013g.png"></figure><p name="8c3a" id="8c3a" class="graf graf--p graf-after--figure">Here is kind of a new insight that’s not new at all which is that we’d like to pre-train something. We know from lesson 4 that if we pre-train our classifier by first creating a language model and then fine-tuning that as a classifier, that was helpful. It actually got us a new state-of-the-art result — we got the best IMDb classifier result that had been published by quite a bit. We are not going that far enough though, because IMDb movie reviews are not that different to any other English document; compared to how different they are to a random string or even to a Chinese document. So just like ImageNet allowed us to train things that recognize stuff that kind of looks like pictures, and we could use it on stuff that was nothing to do with ImageNet like satellite images. Why don’t we train a language model that’s good at English and then fine-tune it to be good at movie reviews.</p><p name="7d03" id="7d03" class="graf graf--p graf-after--p">So this basic insight led Jeremy to try building a language model on Wikipedia. Stephen Merity has already processed Wikipedia, found a subset of nearly the most of it, but throwing away the stupid little articles leaving bigger articles. He calls that wikitext103. Jeremy grabbed wikitext103 and trained a language model on it. He used exactly the same approach he’s about to show you for training an IMDb language model, but instead he trained a wikitext103 language model. He saved it and made it available for anybody who wants to use it at <a href="http://files.fast.ai/models/wt103/" data-href="http://files.fast.ai/models/wt103/" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">this URL</a>. The idea now is let’s train an IMDb language model which starts with these weights. Hopefully to you folks, this is an extremely obvious, extremely non-controversial idea because it’s basically what we’ve done in nearly every class so far. But when Jeremy first mentioned this to people in the NLP community June or July of last year, there couldn’t have been less interest and was told it was stupid [<a href="https://youtu.be/h5Tz7gZT9Fo?t=45m3s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=45m3s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">45:03</a>]. Because Jeremy was obstreperous, he ignored them even though they know much more about NLP and tried it anyway. And let’s see what happened.</p><h4 name="6acb" id="6acb" class="graf graf--h4 graf-after--p">wikitext103 conversion [<a href="https://youtu.be/h5Tz7gZT9Fo?t=46m11s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=46m11s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">46:11</a>]</h4><p name="2906" id="2906" class="graf graf--p graf-after--h4">Here is how we do it. Grab the wikitext models. If you do <code class="markup--code markup--p-code">wget -r</code>, it will recursively grab the whole directory which has a few things in it.</p><pre name="0a52" id="0a52" class="graf graf--pre graf-after--p"># ! wget -nH -r -np -P {PATH} <a href="http://files.fast.ai/models/wt103/" data-href="http://files.fast.ai/models/wt103/" class="markup--anchor markup--pre-anchor" rel="nofollow noopener" target="_blank">http://files.fast.ai/models/wt103/</a></pre><p name="1222" id="1222" class="graf graf--p graf-after--pre">We need to make sure that our language model has exactly the same embedding size, number of hidden, and number of layers as Jeremy’s wikitext one did otherwise you can’t load the weights in.</p><pre name="51cd" id="51cd" class="graf graf--pre graf-after--p">em_sz,nh,nl = 400,1150,3</pre><p name="777c" id="777c" class="graf graf--p graf-after--pre">Here are our pre-trained path and our pre-trained language model path.</p><pre name="28af" id="28af" class="graf graf--pre graf-after--p">PRE_PATH = PATH<strong class="markup--strong markup--pre-strong">/</strong>'models'<strong class="markup--strong markup--pre-strong">/</strong>'wt103'<br>PRE_LM_PATH = PRE_PATH<strong class="markup--strong markup--pre-strong">/</strong>'fwd_wt103.h5'</pre><p name="f2ad" id="f2ad" class="graf graf--p graf-after--pre">Let’s go ahead and <code class="markup--code markup--p-code">torch.load</code> in those weights from the forward wikitext103 model. We don’t normally use torch.load, but that’s the PyTorch way of grabbing a file. It basically gives you a dictionary containing the name of the layer and a tensor/array of those weights.</p><p name="b53c" id="b53c" class="graf graf--p graf-after--p">Now the problem is that wikitext language model was built with a certain vocabulary which was not the same as ours [<a href="https://youtu.be/h5Tz7gZT9Fo?t=47m14s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=47m14s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">47:14</a>]. Our #40 is not the same as wikitext103 model’s #40. So we need to map one to the other. That’s very very simple because luckily Jeremy saved <code class="markup--code markup--p-code">itos</code> for the wikitext vocab.</p><pre name="27d4" id="27d4" class="graf graf--pre graf-after--p">wgts = torch.load(PRE_LM_PATH, map_location=<strong class="markup--strong markup--pre-strong">lambda</strong> storage, <br>                  loc: storage)</pre><pre name="580b" id="580b" class="graf graf--pre graf-after--pre">enc_wgts = to_np(wgts['0.encoder.weight'])<br>row_m = enc_wgts.mean(0)</pre></body></html>