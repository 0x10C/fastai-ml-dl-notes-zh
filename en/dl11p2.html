<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></head><body><p name="61fb" id="61fb" class="graf graf--p graf-after--pre">Now we can create our dataset with our X’s and Y’s (i.e. French and English)[<a href="https://youtu.be/tY0n9OT5_nA?t=43m12s" data-href="https://youtu.be/tY0n9OT5_nA?t=43m12s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">43:12</a>]. If you want to translate instead English to French, switch these two around and you’re done.</p><pre name="b69e" id="b69e" class="graf graf--pre graf-after--p">trn_ds = Seq2SeqDataset(fr_trn,en_trn)<br>val_ds = Seq2SeqDataset(fr_val,en_val)</pre><p name="26ca" id="26ca" class="graf graf--p graf-after--pre">Now we need to create DataLoaders [<a href="https://youtu.be/tY0n9OT5_nA?t=43m22s" data-href="https://youtu.be/tY0n9OT5_nA?t=43m22s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">43:22</a>]. We can just grab our data loader and pass in our dataset and batch size. We actually have to transpose the arrays — we won’t go into the details about why, but we can talk about it during the week if you’re interested but have a think about why we might need to transpose their orientation. Since we’ve already done all the pre-processing, there is no point spawning off multiple workers to do augmentation, etc because there is no work to do. So <code class="markup--code markup--p-code">making num_workers=1</code> will save you some time. We have to tell it what our padding index is — that is pretty important because what’s going to happen is that we’ve got different length sentences and fastai will automatically stick them together and pad the shorter ones so that they are all equal length. Remember a tensor has to be rectangular.</p><pre name="cd3c" id="cd3c" class="graf graf--pre graf-after--p">bs=125</pre><pre name="9180" id="9180" class="graf graf--pre graf-after--pre">trn_samp = SortishSampler(en_trn, key=<strong class="markup--strong markup--pre-strong">lambda</strong> x: len(en_trn[x]), <br>                          bs=bs)<br>val_samp = SortSampler(en_val, key=<strong class="markup--strong markup--pre-strong">lambda</strong> x: len(en_val[x]))</pre><pre name="45ef" id="45ef" class="graf graf--pre graf-after--pre">trn_dl = DataLoader(trn_ds, bs, transpose=<strong class="markup--strong markup--pre-strong">True</strong>, transpose_y=<strong class="markup--strong markup--pre-strong">True</strong>, <br>                    num_workers=1, pad_idx=1, pre_pad=<strong class="markup--strong markup--pre-strong">False</strong>, <br>                    sampler=trn_samp)<br>val_dl = DataLoader(val_ds, int(bs*1.6), transpose=<strong class="markup--strong markup--pre-strong">True</strong>, <br>                    transpose_y=<strong class="markup--strong markup--pre-strong">True</strong>, num_workers=1, pad_idx=1,<br>                    pre_pad=<strong class="markup--strong markup--pre-strong">False</strong>, sampler=val_samp)<br>md = ModelData(PATH, trn_dl, val_dl)</pre><p name="d151" id="d151" class="graf graf--p graf-after--pre">In the decoder in particular, we want our padding to be at the end, not at the start [<a href="https://youtu.be/tY0n9OT5_nA?t=44m29s" data-href="https://youtu.be/tY0n9OT5_nA?t=44m29s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">44:29</a>]:</p><ul class="postList"><li name="98e3" id="98e3" class="graf graf--li graf-after--p">Classifier → padding in the beginning. Because we want that final token to represent the last word of the movie review.</li><li name="77d7" id="77d7" class="graf graf--li graf-after--li">Decoder → padding at the end. As you will see, it actually is going to work out a bit better to have the padding at the end.</li></ul><p name="4280" id="4280" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">Sampler [</strong><a href="https://youtu.be/tY0n9OT5_nA?t=44m54s" data-href="https://youtu.be/tY0n9OT5_nA?t=44m54s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong">44:54</strong></a><strong class="markup--strong markup--p-strong">]</strong> Finally, since we’ve got sentences of different lengths coming in and they all have to be put together in a mini-batch to be the same size by padding, we would much prefer that the sentences in a mini-batch are of similar sizes already. Otherwise it is going to be as long as the longest sentence and that is going to end up wasting time and memory. Therefore, we are going to use the sampler tricks that we learnt last time which is the validation set, we are going to ask it to sort everything by length first. Then for the training set, we are going to randomize the order of things but to roughly make it so that things of similar length are about in the same spot.</p><p name="0919" id="0919" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Model Data [</strong><a href="https://youtu.be/tY0n9OT5_nA?t=45m40s" data-href="https://youtu.be/tY0n9OT5_nA?t=45m40s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong">45:40</strong></a><strong class="markup--strong markup--p-strong">]</strong> At this point, we can create a model data object — remember a model data object really does one thing which is it says “I have a training set and a validation set, and an optional test set” and sticks them into a single object. We also has a path so that it has somewhere to store temporary files, models, stuff like that.</p><p name="d364" id="d364" class="graf graf--p graf-after--p">We are not using fastai for very much at all in this example. We used PyTorch compatible Dataset and and DataLoader — behind the scene it is actually using the fastai version because we need it to do the automatic padding for convenience, so there is a few tweaks in fastai version that are a bit faster and a bit more convenient. We are also using fastai’s Samplers, but there is not too much going on here.</p><h4 name="294d" id="294d" class="graf graf--h4 graf-after--p">Architecture [<a href="https://youtu.be/tY0n9OT5_nA?t=46m59s" data-href="https://youtu.be/tY0n9OT5_nA?t=46m59s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">46:59</a>]</h4><figure name="eedd" id="eedd" class="graf graf--figure graf-after--h4"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_IMBl2Aiclyt6PCrg1IQg5A.png"></figure><ul class="postList"><li name="da7d" id="da7d" class="graf graf--li graf-after--figure">The architecture is going to take our sequence of tokens.</li><li name="14c0" id="14c0" class="graf graf--li graf-after--li">It is going to spit them into an encoder (a.k.a. backbone).</li><li name="7d9d" id="7d9d" class="graf graf--li graf-after--li">That is going to spit out the final hidden state which for each sentence, it’s just a single vector.</li></ul><p name="2bf4" id="2bf4" class="graf graf--p graf-after--li">None of this is going to be new [<a href="https://youtu.be/tY0n9OT5_nA?t=47m41s" data-href="https://youtu.be/tY0n9OT5_nA?t=47m41s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">47:41</a>]. That is all going to be using very direct simple techniques that we’ve already learned.</p><ul class="postList"><li name="bef9" id="bef9" class="graf graf--li graf-after--p">Then we are going to take that, and we will spit it into a different RNN which is a decoder. That’s going to have some new stuff because we need something that can go through one word at a time. And it keeps going until it thinks it’s finished the sentence. It doesn’t know how long the sentence is going to be ahead of time. It keeps going until it thinks it’s finished the sentence and then it stops and returns a sentence.</li></ul><pre name="8f62" id="8f62" class="graf graf--pre graf-after--li"><strong class="markup--strong markup--pre-strong">def</strong> create_emb(vecs, itos, em_sz):<br>    emb = nn.Embedding(len(itos), em_sz, padding_idx=1)<br>    wgts = emb.weight.data<br>    miss = []<br>    <strong class="markup--strong markup--pre-strong">for</strong> i,w <strong class="markup--strong markup--pre-strong">in</strong> enumerate(itos):<br>        <strong class="markup--strong markup--pre-strong">try</strong>: wgts[i] = torch.from_numpy(vecs[w]*3)<br>        <strong class="markup--strong markup--pre-strong">except</strong>: miss.append(w)<br>    print(len(miss),miss[5:10])<br>    <strong class="markup--strong markup--pre-strong">return</strong> emb</pre><pre name="32ba" id="32ba" class="graf graf--pre graf-after--pre">nh,nl = 256,2</pre><p name="6d69" id="6d69" class="graf graf--p graf-after--pre">Let’s start with the encoder [<a href="https://youtu.be/tY0n9OT5_nA?t=48m15s" data-href="https://youtu.be/tY0n9OT5_nA?t=48m15s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">48:15</a>]. In terms of the variable naming here, there is identical attributes for encoder and decoder. The encoder version has <code class="markup--code markup--p-code">enc</code> the decoder version has <code class="markup--code markup--p-code">dec</code>.</p><ul class="postList"><li name="d68c" id="d68c" class="graf graf--li graf-after--p"><code class="markup--code markup--li-code">emb_enc</code>: Embeddings for the encoder</li><li name="82eb" id="82eb" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">gru</code>&nbsp;: RNN. GRU and LSTM are nearly the same thing.</li></ul><p name="7106" id="7106" class="graf graf--p graf-after--li">We need to create an embedding layer because remember — what we are being passed is the index of the words into a vocabulary. And we want to grab their fast.text embedding. Then over time, we might want to also fine tune to train that embedding end-to-end.</p><p name="f12b" id="f12b" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">create_emb</code> [<a href="https://youtu.be/tY0n9OT5_nA?t=49m37s" data-href="https://youtu.be/tY0n9OT5_nA?t=49m37s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">49:37</a>]: It is important that you know now how to set the rows and columns for your embedding so the number of rows has to be equal to your vocabulary size — so each vocabulary has a word vector. The size of the embedding is determined by fast.text and fast.text embeddings are size 300. So we have to use size 300 as well otherwise we can’t start out by using their embeddings.</p><p name="69df" id="69df" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">nn.Embedding </code>will initially going to give us a random set of embeddings [<a href="https://youtu.be/tY0n9OT5_nA?t=50m12s" data-href="https://youtu.be/tY0n9OT5_nA?t=50m12s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">50:12</a>]. So we will go through each one of these and if we find it in fast.text, we will replace it with the fast.text embedding. Again, something you should already know is that (<code class="markup--code markup--p-code">emb.weight.data</code>):</p><ul class="postList"><li name="b08a" id="b08a" class="graf graf--li graf-after--p">A PyTorch module that is learnable has <code class="markup--code markup--li-code">weight</code> attribute</li><li name="c64d" id="c64d" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">weight</code> attribute is a <code class="markup--code markup--li-code">Variable</code> that has <code class="markup--code markup--li-code">data</code> attribute</li><li name="76b2" id="76b2" class="graf graf--li graf-after--li">The <code class="markup--code markup--li-code">data</code> attribute is a tensor</li></ul><p name="7e8b" id="7e8b" class="graf graf--p graf-after--li">Now that we’ve got our weight tensor, we can just go through our vocabulary and we can look up the word in our pre-trained vectors and if we find it, we will replace the random weights with that pre-trained vector [<a href="https://youtu.be/tY0n9OT5_nA?t=52m35s" data-href="https://youtu.be/tY0n9OT5_nA?t=52m35s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">52:35</a>]. The random weights have a standard deviation of 1. Our pre-trained vectors has a standard deviation of about 0.3. So again, this is the kind of hacky thing Jeremy does when he is prototyping stuff, he just multiplied it by 3. By the time you see the video of this, we may able to put all this sequence to sequence stuff into the fastai library, you won’t find horrible hacks like that in there (sure hope). But hack away when you are prototyping. Some things won’t be in fast.text in which case, we’ll just keep track of it [<a href="https://youtu.be/tY0n9OT5_nA?t=53m22s" data-href="https://youtu.be/tY0n9OT5_nA?t=53m22s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">53:22</a>]. The print statement is there so that we can see what’s going on (i.e. why are we missing stuff?). Remember we had about 30,000 so we are not missing too many.</p><pre name="f790" id="f790" class="graf graf--pre graf-after--p"><em class="markup--em markup--pre-em">3097 ['l’', "d'", 't_up', 'd’', "qu'"]<br>1285 ["'s", '’s', "n't", 'n’t', ':']</em></pre><p name="0f76" id="0f76" class="graf graf--p graf-after--pre">Jeremy has started doing some stuff around incorporating large vocabulary handling into fastai — it’s not finished yet but hopefully by the time we get here, this kind of stuff will be possible [<a href="https://youtu.be/tY0n9OT5_nA?t=56m50s" data-href="https://youtu.be/tY0n9OT5_nA?t=56m50s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">56:50</a>].</p><pre name="53e4" id="53e4" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">Seq2SeqRNN</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, vecs_enc, itos_enc, em_sz_enc, vecs_dec, <br>                 itos_dec, em_sz_dec, nh, out_sl, nl=2):<br>        super().__init__()<br>        self.nl,self.nh,self.out_sl = nl,nh,out_sl<br>        self.emb_enc = create_emb(vecs_enc, itos_enc, em_sz_enc)<br>        self.emb_enc_drop = nn.Dropout(0.15)<br>        self.gru_enc = nn.GRU(em_sz_enc, nh, num_layers=nl, <br>                              dropout=0.25)<br>        self.out_enc = nn.Linear(nh, em_sz_dec, bias=<strong class="markup--strong markup--pre-strong">False</strong>)<br>        <br>        self.emb_dec = create_emb(vecs_dec, itos_dec, em_sz_dec)<br>        self.gru_dec = nn.GRU(em_sz_dec, em_sz_dec, num_layers=nl, <br>                              dropout=0.1)<br>        self.out_drop = nn.Dropout(0.35)<br>        self.out = nn.Linear(em_sz_dec, len(itos_dec))<br>        self.out.weight.data = self.emb_dec.weight.data<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, inp):<br>        sl,bs = inp.size()<br>        h = self.initHidden(bs)<br>        emb = self.emb_enc_drop(self.emb_enc(inp))<br>        enc_out, h = self.gru_enc(emb, h)<br>        h = self.out_enc(h)<br><br>        dec_inp = V(torch.zeros(bs).long())<br>        res = []<br>        <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(self.out_sl):<br>            emb = self.emb_dec(dec_inp).unsqueeze(0)<br>            outp, h = self.gru_dec(emb, h)<br>            outp = self.out(self.out_drop(outp[0]))<br>            res.append(outp)<br>            dec_inp = V(outp.data.max(1)[1])<br>            <strong class="markup--strong markup--pre-strong">if</strong> (dec_inp==1).all(): <strong class="markup--strong markup--pre-strong">break</strong><br>        <strong class="markup--strong markup--pre-strong">return</strong> torch.stack(res)<br>    <br>    <strong class="markup--strong markup--pre-strong">def</strong> initHidden(self, bs): <br>        <strong class="markup--strong markup--pre-strong">return</strong> V(torch.zeros(self.nl, bs, self.nh))</pre><p name="06f9" id="06f9" class="graf graf--p graf-after--pre">The key thing to know is that encoder takes our inputs and spits out a hidden vector that hopefully will learn to contain all of the information about what that sentence says and how it sets it [<a href="https://youtu.be/tY0n9OT5_nA?t=58m49s" data-href="https://youtu.be/tY0n9OT5_nA?t=58m49s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">58:49</a>]. If it can’t do that, we can’t feed it into a decoder and hope it to spit our our sentence in a different language. So that’s what we want it to learn to do. We are not going to do anything special to make it learn to do that — we are just going to do the three things (data, architecture, loss function) and cross our fingers.</p><p name="d81c" id="d81c" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Decoder [</strong><a href="https://youtu.be/tY0n9OT5_nA?t=59m58s" data-href="https://youtu.be/tY0n9OT5_nA?t=59m58s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong">59:58</strong></a><strong class="markup--strong markup--p-strong">]</strong>: How do we now do the new bit? The basic idea of the new bit is the same. We are going to do exactly the same thing, but we are going to write our own for loop. The for loop is going to do exactly what the for loop inside PyTorch does for encoder, but we are going to do it manually. How big is the for loop? It’s an output sequence length (<code class="markup--code markup--p-code">out_sl</code>) which was something passed to the constructor which is equal to the length of the largest English sentence. Since we are translating into English, so it can’t possibly be longer than that at least in this corpus. If we then used it on some different corpus that was longer, this is going to fail — you could always pass in a different parameter, of course. So the basic idea is the same [<a href="https://youtu.be/tY0n9OT5_nA?t=1h1m6s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h1m6s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:01:06</a>].</p><ul class="postList"><li name="6087" id="6087" class="graf graf--li graf-after--p">We are going to go through and put it through the embedding.</li><li name="3354" id="3354" class="graf graf--li graf-after--li">We are going to stick it through the RNN, dropout, and a linear layer.</li><li name="06f1" id="06f1" class="graf graf--li graf-after--li">We will then append the output to a list which will be stacked into a single tensor and get returned.</li></ul><p name="72a6" id="72a6" class="graf graf--p graf-after--li">Normally, a recurrent neural network works on a whole sequence at a time, but we have a for loop to go through each part of the sequence separately [<a href="https://youtu.be/tY0n9OT5_nA?t=1h1m37s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h1m37s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:01:37</a>]. Wo we have to add a leading unit axis to the start (<code class="markup--code markup--p-code">.unsqueeze(0)</code>) to basicaly say this is a sequence of length one. We are not really taking advantage of the recurrent net much at all — we could easily re-write this with a linear layer.</p><p name="e3e0" id="e3e0" class="graf graf--p graf-after--p">One thing to be aware of is <code class="markup--code markup--p-code">dec_inp</code>[<a href="https://youtu.be/tY0n9OT5_nA?t=1h2m34s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h2m34s" class="markup--anchor markup--p-anchor" rel="nofollow noopener noopener" target="_blank">1:02:34</a>]: <span class="markup--quote markup--p-quote is-other" name="anon_525be9aa639a" data-creator-ids="anon">What is the input to the embedding? The answer is it is the previous word that we translated.</span> The basic idea is if you are trying to translate the 4th word of the new sentence but you don’t know what the third word you just said was, that is going to be really hard. So we are going to feed that in at each time step. What was the previous word at the start? There was none. Specifically, we are going to start out with a beginning of stream token (<code class="markup--code markup--p-code">_bos_</code>) which is zero.</p><p name="a98d" id="a98d" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">outp</code> [<a href="https://youtu.be/tY0n9OT5_nA?t=1h5m24s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h5m24s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:05:24</a>]: it is a tensor whose length is equal to the number of words in our English vocabulary and it contains the probability for every one of those words that it is that word.</p><p name="7115" id="7115" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">outp.data.max</code>&nbsp;: It looks in its tensor to find out which word has the highest probability. <code class="markup--code markup--p-code">max</code> in PyTorch returns two things: the first thing is what is that max probability and the second is what is the index into the array of that max probability. So we want that second item which is the word index with the largest thing.</p><p name="9875" id="9875" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">dec_inp</code>&nbsp;: It contains the word index into the vocabulary of the word. If it’s one (i.e. padding), that means we are done — we reached the end because we finished with a bunch of padding. If it’s not one, let’s go back and continue.</p><p name="2dd1" id="2dd1" class="graf graf--p graf-after--p">Each time, we appended our outputs (not the word but the probabilities) to the list [<a href="https://youtu.be/tY0n9OT5_nA?t=1h6m48s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h6m48s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:06:48</a>] which we stack up into a tensor and we can now go ahead and feed that to a loss function.</p><h4 name="0bd6" id="0bd6" class="graf graf--h4 graf-after--p">Loss function [<a href="https://youtu.be/tY0n9OT5_nA?t=1h7m13s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h7m13s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:07:13</a>]</h4><p name="e70a" id="e70a" class="graf graf--p graf-after--h4">The loss function is categorical cross entropy loss. We have a list of probabilities for each of our classes where the classes are all the words in our English vocab and we have a target which is the correct class (i.e. which is the correct word at this location). There are two tweaks which is why we need to write our own loss function but you can see basically it is going to be cross entropy loss.</p><pre name="15b8" id="15b8" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> seq2seq_loss(input, target):<br>    sl,bs = target.size()<br>    sl_in,bs_in,nc = input.size()<br>    <strong class="markup--strong markup--pre-strong">if</strong> sl&gt;sl_in: input = F.pad(input, (0,0,0,0,0,sl-sl_in))<br>    input = input[:sl]<br>    <strong class="markup--strong markup--pre-strong">return</strong> F.cross_entropy(input.view(-1,nc), target.view(-1))</pre><p name="4a8b" id="4a8b" class="graf graf--p graf-after--pre">Tweaks [<a href="https://youtu.be/tY0n9OT5_nA?t=1h7m40s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h7m40s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:07:40</a>]:</p><ol class="postList"><li name="27b9" id="27b9" class="graf graf--li graf-after--p">If the generated sequence length is shorter than the sequence length of the target, we need to add some padding. PyTorch padding function requires a tuple of 6 to pad a rank 3 tensor (sequence length, batch size, by number of words in the vocab). Each pair represents padding before and after that dimension.</li></ol><p name="f5fd" id="f5fd" class="graf graf--p graf-after--li">2. <code class="markup--code markup--p-code">F.cross_entropy</code> expects a rank 2 tensor, but we have sequence length by batch size, so let’s just flatten out. That is what <code class="markup--code markup--p-code">view(-1,&nbsp;...)</code> does.</p><pre name="6d30" id="6d30" class="graf graf--pre graf-after--p">opt_fn = partial(optim.Adam, betas=(0.8, 0.99))</pre><p name="37be" id="37be" class="graf graf--p graf-after--pre">The difference between&nbsp;<code class="markup--code markup--p-code">.cuda()</code> and <code class="markup--code markup--p-code">to_gpu()</code>&nbsp;: <code class="markup--code markup--p-code">to_gpu</code> will not put to in the GPU if you do not have one. You can also set <code class="markup--code markup--p-code">fastai.core.USE_GPU</code> to <code class="markup--code markup--p-code">false</code> to force it to not use GPU that can be handy for debugging.</p><pre name="cb79" id="cb79" class="graf graf--pre graf-after--p">rnn = Seq2SeqRNN(fr_vecd, fr_itos, dim_fr_vec, en_vecd, en_itos, <br>                 dim_en_vec, nh, enlen_90)<br>learn = RNN_Learner(md, SingleModel(to_gpu(rnn)), opt_fn=opt_fn)<br>learn.crit = seq2seq_loss</pre><pre name="dfe5" id="dfe5" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">3097 ['l’', "d'", 't_up', 'd’', "qu'"]<br>1285 ["'s", '’s', "n't", 'n’t', ':']</em></pre><p name="6e9c" id="6e9c" class="graf graf--p graf-after--pre">We then need something that tells it how to handle learning rate groups so there is a thing called <code class="markup--code markup--p-code">SingleModel</code> that you can pass it to which treats the whole thing as a single learning rate group [<a href="https://youtu.be/tY0n9OT5_nA?t=1h9m40s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h9m40s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:09:40</a>]. So this is the easiest way to turn a PyTorch module into a fastai model.</p><figure name="74f4" id="74f4" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_NW1_lYHLm8R0ML_BWq0QRA.png"></figure><p name="0816" id="0816" class="graf graf--p graf-after--figure">We could just call Learner to turn that into a learner, but if we call RNN_Learner, it does add in <code class="markup--code markup--p-code">save_encoder</code> and <code class="markup--code markup--p-code">load_encoder</code> that can be handy sometimes. In this case, we really could have said <code class="markup--code markup--p-code">Leaner</code> but <code class="markup--code markup--p-code">RNN_Learner</code> also works.</p><pre name="23c2" id="23c2" class="graf graf--pre graf-after--p">learn.lr_find()<br>learn.sched.plot()</pre><figure name="1547" id="1547" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_Fwxxo1lXoIqdM5v24sWfIA.png"></figure><pre name="7b69" id="7b69" class="graf graf--pre graf-after--figure">lr=3e-3<br>learn.fit(lr, 1, cycle_len=12, use_clr=(20,10))</pre><pre name="b0ba" id="b0ba" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss                              <br>    0      5.48978    5.462648  <br>    1      4.616437   4.770539                              <br>    2      4.345884   4.37726                               <br>    3      3.857125   4.136014                              <br>    4      3.612306   3.941867                              <br>    5      3.375064   3.839872                              <br>    6      3.383987   3.708972                              <br>    7      3.224772   3.664173                              <br>    8      3.238523   3.604765                              <br>    9      2.962041   3.587814                              <br>    10     2.96163    3.574888                              <br>    11     2.866477   3.581224</em></pre><pre name="55b4" id="55b4" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[3.5812237]</em></pre><pre name="d550" id="d550" class="graf graf--pre graf-after--pre"><br>learn.save('initial')<br>learn.load('initial')</pre><h4 name="f1e5" id="f1e5" class="graf graf--h4 graf-after--pre">Test [<a href="https://youtu.be/tY0n9OT5_nA?t=1h11m1s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h11m1s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:11:01</a>]</h4><p name="1281" id="1281" class="graf graf--p graf-after--h4">Remember the model attribute of a learner is a standard PyTorch model so we can pass some <code class="markup--code markup--p-code">x</code> which we can grab out of our validation set or you could <code class="markup--code markup--p-code">learn.predict_array</code> or whatever you like to get some predictions. Then we convert those predictions into words by going&nbsp;<code class="markup--code markup--p-code">.max()[1]</code> to grab the index of the highest probability words to get some predictions. Then we can go through a few examples and print out the French, the correct English, and the predicted English for things that are not padding.</p><pre name="ed82" id="ed82" class="graf graf--pre graf-after--p">x,y = next(iter(val_dl))<br>probs = learn.model(V(x))<br>preds = to_np(probs.max(2)[1])<br><br><strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(180,190):<br>    print(' '.join([fr_itos[o] <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> x[:,i] <strong class="markup--strong markup--pre-strong">if</strong> o != 1]))<br>    print(' '.join([en_itos[o] <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> y[:,i] <strong class="markup--strong markup--pre-strong">if</strong> o != 1]))<br>    print(' '.join([en_itos[o] <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> preds[:,i] <strong class="markup--strong markup--pre-strong">if</strong> o!=1]))<br>    print()</pre><pre name="a65c" id="a65c" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">quels facteurs pourraient influer sur le choix de leur emplacement ? _eos_<br>what factors influencetheir location ? _eos_<br>what factors might might influence on the their ? ? _eos_<br><br>qu’ est -ce qui ne peut pas changer ? _eos_<br>what can not change ? _eos_<br>what not change change ? _eos_<br><br>que faites - vous ? _eos_<br>what do you do ? _eos_<br>what do you do ? _eos_<br><br>qui réglemente les pylônes d' antennes ? _eos_<br>who regulates antenna towers ? _eos_<br>who regulates the doors doors ? _eos_<br><br>où sont - ils situés ? _eos_<br>where are they located ? _eos_<br>where are the located ? _eos_<br><br>quelles sont leurs compétences ? _eos_<br>what are their qualifications ? _eos_<br>what are their skills ? _eos_<br><br>qui est victime de harcèlement sexuel ? _eos_<br>who experiences sexual harassment ? _eos_<br>who is victim sexual sexual ? ? _eos_<br><br>quelles sont les personnes qui visitent les communautés autochtones ? _eos_<br>who visits indigenous communities ? _eos_<br>who are people people aboriginal aboriginal ? _eos_<br><br>pourquoi ces trois points en particulier ? _eos_<br>why these specific three ? _eos_<br>why are these two different ? ? _eos_<br><br>pourquoi ou pourquoi pas ? _eos_<br>why or why not ? _eos_<br>why or why not _eos_</em></pre><p name="7a65" id="7a65" class="graf graf--p graf-after--pre">Amazingly enough, this kind of simplest possible written largely from scratch PyTorch module on only fifty thousand sentences is sometimes capable, on validation set, of giving you exactly the right answer. Sometimes the right answer is in slightly different wording, and sometimes sentences that really aren’t grammatically sensible or even have too many question marks. So we are well on the right track. We think you would agree even the simplest possible seq-to-seq trained for a very small number of epochs without any pre-training other than the use of word embeddings is surprisingly good. We are going to improve this later but the message here is even sequence to sequence models you think is simpler than they could possibly work even with less data than you think you could learn from can be surprisingly effective and in certain situations this may be enough for your needs.</p><p name="b22e" id="b22e" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: Would it help to normalize punctuation (e.g. <code class="markup--code markup--p-code">’</code> vs. <code class="markup--code markup--p-code">'</code>)? [<a href="https://youtu.be/tY0n9OT5_nA?t=1h13m10s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h13m10s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:13:10</a>] The answer to this particular case is probably yes — the difference between curly quotes and straight quotes is really semantic. You do have to be very careful though because it may turn out that people using beautiful curly quotes like using more formal language and they are writing in a different way. So if you are going to do some kind of pre-processing like punctuation normalization, you should definitely check your results with and without because nearly always that kind of pre-processing make things worse even when you’re sure it won’t.</p><p name="ecd4" id="ecd4" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: What might be some ways of regularizing these seq2seq models besides dropout and weight decay? [<a href="https://youtu.be/tY0n9OT5_nA?t=1h14m17s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h14m17s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:14:17</a>] Let me think about that during the week. AWD-LSTM which we have been relying a lot has dropouts of many different kinds and there is also a kind of a regularization based on activations and on changes. Jeremy has not seen anybody put anything like that amount of work into regularizing sequence to sequence model and there is a huge opportunity for somebody to do like the AWD-LSTM of seq-to-seq which might be as simple as stealing all the ideas from AWD-LSTM and using them directly in seq-to-seq that would be pretty easy to try. There’s been an interesting paper that Stephen Merity added in the last couple weeks where he used an idea which take all of these different AWD-LSTM hyper parameters and train a bunch of different models and then use a random forest to find out the feature importance — which ones actually matter the most and then figure out how to set them. You could totally use this approach to figure out for sequence to sequence regularization approaches which one is the best and optimize them and that would be amazing. But at the moment, we don’t know if there are additional ideas to sequence to sequence regularization beyond what is in that paper for regular language model.</p><h3 name="aea6" id="aea6" class="graf graf--h3 graf-after--p">Tricks [<a href="https://youtu.be/tY0n9OT5_nA?t=1h16m28s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h16m28s" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">1:16:28</a>]</h3><h4 name="d6c4" id="d6c4" class="graf graf--h4 graf-after--h3"><strong class="markup--strong markup--h4-strong">Trick #1&nbsp;: Go bi-directional</strong></h4><p name="1896" id="1896" class="graf graf--p graf-after--h4">For classification, the approach to bi-directional Jeremy suggested to use is take all of your token sequences, spin them around, train a new language model, and train a new classifier. He also mentioned that wikitext pre-trained model if you replace <code class="markup--code markup--p-code">fwd</code> with <code class="markup--code markup--p-code">bwd</code> in the name, you will get the pre-trained backward model he created for you. Get a set of predictions and then average the predictions just like a normal ensemble. That is how we do bi-dir for that kind of classification. There may be ways to do it end-to-end, but Jeremy hasn’t quite figured them out yet and they are not in fastai yet. So if you figure it out, that’s an interesting line of research. But because we are not doing massive documents where we have to chunk it into separate bits and then pool over them, we can do bi-dir very easily in this case. It is literally as simple as adding <code class="markup--code markup--p-code">bidirectional=True</code> to our encoder. People tend not to do bi-directional for the decoder partly because it is kind of considered cheating but maybe it can work in some situations although it might need to be more of an ensemble approach in the decoder because it’s a bit less obvious. But encoder it’s very simple — <code class="markup--code markup--p-code">bidirectional=True</code> and we now have a second RNN that is going the opposite direction. The second RNN is visiting each token in the opposing order so when we get to the final hidden state, it is the first (i.e. left most) token&nbsp;. But the hidden state is the same size, so the final result is that we end up with a tensor with an extra axis of length 2. Depending on what library you use, often that will be then combined with the number of layers, so if you have 2 layers and bi-directional — that tensor dimension is now length 4. With PyTorch it depends which bit of the process you are looking at as to whether you get a separate result for each layer and/or for each bidirectional bit. You have to look up the documentation and it will tell you input’s output’s tensor sizes appropriate for the number of layers and whether you have <code class="markup--code markup--p-code">bidirectional=True</code>.</p><p name="1fab" id="1fab" class="graf graf--p graf-after--p">In this particular case, you will see all the changes that had to be made [<a href="https://youtu.be/tY0n9OT5_nA?t=1h19m38s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h19m38s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:19:38</a>]. For example&nbsp;,when we added <code class="markup--code markup--p-code">bidirectional=True</code>, the <code class="markup--code markup--p-code">Linear</code> layer now needs number of hidden times 2 (i.e. <code class="markup--code markup--p-code">nh*2</code>) to reflect the fact that we have that second direction in our hidden state. Also in <code class="markup--code markup--p-code">initHidden</code> it’s now <code class="markup--code markup--p-code">self.nl*2</code>.</p><pre name="c1bd" id="c1bd" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">Seq2SeqRNN_Bidir</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, vecs_enc, itos_enc, em_sz_enc, vecs_dec, <br>                 itos_dec, em_sz_dec, nh, out_sl, nl=2):<br>        super().__init__()<br>        self.emb_enc = create_emb(vecs_enc, itos_enc, em_sz_enc)<br>        self.nl,self.nh,self.out_sl = nl,nh,out_sl<br>        self.gru_enc = nn.GRU(em_sz_enc, nh, num_layers=nl,<br>                              dropout=0.25, bidirectional=<strong class="markup--strong markup--pre-strong">True</strong>)<br>        self.out_enc = nn.Linear(nh<strong class="markup--strong markup--pre-strong">*2</strong>, em_sz_dec, bias=<strong class="markup--strong markup--pre-strong">False</strong>)<br>        self.drop_enc = nn.Dropout(0.05)<br>        self.emb_dec = create_emb(vecs_dec, itos_dec, em_sz_dec)<br>        self.gru_dec = nn.GRU(em_sz_dec, em_sz_dec, num_layers=nl,<br>                              dropout=0.1)<br>        self.emb_enc_drop = nn.Dropout(0.15)<br>        self.out_drop = nn.Dropout(0.35)<br>        self.out = nn.Linear(em_sz_dec, len(itos_dec))<br>        self.out.weight.data = self.emb_dec.weight.data<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, inp):<br>        sl,bs = inp.size()<br>        h = self.initHidden(bs)<br>        emb = self.emb_enc_drop(self.emb_enc(inp))<br>        enc_out, h = self.gru_enc(emb, h)<br>        h = h.view(2,2,bs,-1).permute(0,2,1,3)<br>                .contiguous().view(2,bs,-1)<br>        h = self.out_enc(self.drop_enc(h))</pre><pre name="2136" id="2136" class="graf graf--pre graf-after--pre">        dec_inp = V(torch.zeros(bs).long())<br>        res = []<br>        <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(self.out_sl):<br>            emb = self.emb_dec(dec_inp).unsqueeze(0)<br>            outp, h = self.gru_dec(emb, h)<br>            outp = self.out(self.out_drop(outp[0]))<br>            res.append(outp)<br>            dec_inp = V(outp.data.max(1)[1])<br>            <strong class="markup--strong markup--pre-strong">if</strong> (dec_inp==1).all(): <strong class="markup--strong markup--pre-strong">break</strong><br>        <strong class="markup--strong markup--pre-strong">return</strong> torch.stack(res)<br>    <br>    <strong class="markup--strong markup--pre-strong">def</strong> initHidden(self, bs): <br>        <strong class="markup--strong markup--pre-strong">return</strong> V(torch.zeros(self.nl<strong class="markup--strong markup--pre-strong">*2</strong>, bs, self.nh))</pre><p name="2244" id="2244" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">Question</strong>: Why is making the decoder bi-directional considered cheating? [<a href="https://youtu.be/tY0n9OT5_nA?t=1h20m13s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h20m13s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:20:13</a>] It’s not just cheating but we have this loop going on so it is not as simple as having two tensors. Then how do you turn those two separate loops into a final result? After talking about it during the break, Jeremy has gone from “everybody knows it doesn’t work” to “maybe it could work”, but it requires more thought. It is quite possible during the week, he’ll realize it’s a dumb idea, but we’ll think about it.</p><p name="6d51" id="6d51" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: Why do you need to set a range to the loop? [<a href="https://youtu.be/tY0n9OT5_nA?t=1h20m58s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h20m58s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:20:58</a>] Because when we start training, everything is random so <code class="markup--code markup--p-code">if (dec_inp==1).all(): break</code> will probably never be true. Later on, it will pretty much always break out eventually but basically we are going to go forever. It’s really important to remember when you are designing an architecture that when you start, the model knows nothing about anything. So you want to make sure if it’s going to do something at least it’s vaguely sensible.</p><p name="4c6e" id="4c6e" class="graf graf--p graf-after--p">We got 3.58 cross entropy loss with single direction [<a href="https://youtu.be/tY0n9OT5_nA?t=1h21m46s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h21m46s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:21:46</a>]. With bi-direction, we got down to 3.51, so that improved a little. It shouldn’t really slow things down too much. Bi-directional does mean there is a little bit more sequential processing have to happen, but it is generally a good win. In the Google translation model, of the 8 layers, only the first layer is bi-directional because it allows it to do more in parallel so if you create really deep models you may need to think about which ones are bi-directional otherwise we have performance issues.</p><pre name="ebba" id="ebba" class="graf graf--pre graf-after--p">rnn = Seq2SeqRNN_Bidir(fr_vecd, fr_itos, dim_fr_vec, en_vecd,<br>                       en_itos, dim_en_vec, nh, enlen_90)<br>learn = RNN_Learner(md, SingleModel(to_gpu(rnn)), opt_fn=opt_fn)<br>learn.crit = seq2seq_loss</pre><pre name="aee3" id="aee3" class="graf graf--pre graf-after--pre">learn.fit(lr, 1, cycle_len=12, use_clr=(20,10))</pre><pre name="5cac" id="5cac" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss                              <br>    0      4.896942   4.761351  <br>    1      4.323335   4.260878                              <br>    2      3.962747   4.06161                               <br>    3      3.596254   3.940087                              <br>    4      3.432788   3.944787                              <br>    5      3.310895   3.686629                              <br>    6      3.454976   3.638168                              <br>    7      3.093827   3.588456                              <br>    8      3.257495   3.610536                              <br>    9      3.033345   3.540344                              <br>    10     2.967694   3.516766                              <br>    11     2.718945   3.513977</em></pre><pre name="a27d" id="a27d" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[3.5139771]</em></pre><h4 name="0ad2" id="0ad2" class="graf graf--h4 graf-after--pre">Trick #2 Teacher Forcing [<a href="https://youtu.be/tY0n9OT5_nA?t=1h22m39s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h22m39s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:22:39</a>]</h4><p name="76cb" id="76cb" class="graf graf--p graf-after--h4">Now let’s talk about teacher forcing. When a model starts learning, it knows nothing about nothing. So when the model starts learning, it is not going to spit out “Er” at the first step, it is going to spit out some random meaningless word because it doesn’t know anything about German or about English or about the idea of language. And it is going to feed it to the next process as an input and be totally unhelpful. That means, early learning is going to be very difficult because it is feeding in an input that is stupid into a model that knows nothing and somehow it’s going to get better. So it is not asking too much eventually it gets there, but it’s definitely not as helpful as we can be. So what if instead of feeing in the thing I predicted just now, what if we instead we feed in the actual correct word was meant to be. We can’t do that at inference time because by definition we don’t know the correct word - it has to translate it. We can’t require the correct translation in order to do translation.</p><figure name="af01" id="af01" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_DU776SGr1rhYeU7ilIKX9w.png"></figure><p name="9052" id="9052" class="graf graf--p graf-after--figure">So the way it’s set up is we have this thing called <code class="markup--code markup--p-code">pr_force</code> which is probability of forcing [<a href="https://youtu.be/tY0n9OT5_nA?t=1h24m1s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h24m1s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:24:01</a>]. If some random number is less than that probability then we are going to replace our decoder input with the actual correct thing. If we have already gone too far and if it is already longer than the target sequence, we are just going to stop because obviously we can’t give it the correct thing. So you can see how beautiful PyTorch is for this. The key reasons that we switched to PyTorch at this exact point in last year’s class was because Jeremy tried to implement teacher forcing in Keras and TensorFlow and went even more insane than he started. It was weeks of getting nowhere then he saw on Twitter Andrej Karpathy said something about this thing called PyTorch that just came out and it’s really cool. He tried it that day, by the next day, he had teacher forcing. All this stuff of trying to debug things was suddenly so much easier and and this kind of dynamic thing is so much easier. So this is a great example of “hey, I get to use random numbers and if statements”.</p><pre name="8552" id="8552" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">Seq2SeqStepper</strong>(Stepper):<br>    <strong class="markup--strong markup--pre-strong">def</strong> step(self, xs, y, epoch):<br>        self.m.pr_force = (10-epoch)*0.1 <strong class="markup--strong markup--pre-strong">if</strong> epoch&lt;10 <strong class="markup--strong markup--pre-strong">else</strong> 0<br>        xtra = []<br>        output = self.m(*xs, y)<br>        <strong class="markup--strong markup--pre-strong">if</strong> isinstance(output,tuple): output,*xtra = output<br>        self.opt.zero_grad()<br>        loss = raw_loss = self.crit(output, y)<br>        <strong class="markup--strong markup--pre-strong">if</strong> self.reg_fn: loss = self.reg_fn(output, xtra, raw_loss)<br>        loss.backward()<br>        <strong class="markup--strong markup--pre-strong">if</strong> self.clip:   <em class="markup--em markup--pre-em"># Gradient clipping</em><br>            nn.utils.clip_grad_norm(trainable_params_(self.m), <br>                                    self.clip)<br>        self.opt.step()<br>        <strong class="markup--strong markup--pre-strong">return</strong> raw_loss.data[0]</pre><p name="6a94" id="6a94" class="graf graf--p graf-after--pre">Here is the basic idea [<a href="https://youtu.be/tY0n9OT5_nA?t=1h25m29s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h25m29s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:25:29</a>]. At the start of training, let’s set <code class="markup--code markup--p-code">pr_force</code> really high so that nearly always it gets the actual correct previous word and so it has a useful input. Then as we trained a bit more, let’s decrease <code class="markup--code markup--p-code">pr_force</code> so that by the end <code class="markup--code markup--p-code">pr_force</code> is zero and it has to learn properly which is fine because it is now actually feeding in sensible inputs most of the time anyway.</p><pre name="e419" id="e419" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">Seq2SeqRNN_TeacherForcing</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, vecs_enc, itos_enc, em_sz_enc, vecs_dec,<br>                 itos_dec, em_sz_dec, nh, out_sl, nl=2):<br>        super().__init__()<br>        self.emb_enc = create_emb(vecs_enc, itos_enc, em_sz_enc)<br>        self.nl,self.nh,self.out_sl = nl,nh,out_sl<br>        self.gru_enc = nn.GRU(em_sz_enc, nh, num_layers=nl, <br>                              dropout=0.25)<br>        self.out_enc = nn.Linear(nh, em_sz_dec, bias=<strong class="markup--strong markup--pre-strong">False</strong>)<br>        self.emb_dec = create_emb(vecs_dec, itos_dec, em_sz_dec)<br>        self.gru_dec = nn.GRU(em_sz_dec, em_sz_dec, num_layers=nl, <br>                              dropout=0.1)<br>        self.emb_enc_drop = nn.Dropout(0.15)<br>        self.out_drop = nn.Dropout(0.35)<br>        self.out = nn.Linear(em_sz_dec, len(itos_dec))<br>        self.out.weight.data = self.emb_dec.weight.data<br>        self.pr_force = 1.<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, inp, y=<strong class="markup--strong markup--pre-strong">None</strong>):<br>        sl,bs = inp.size()<br>        h = self.initHidden(bs)<br>        emb = self.emb_enc_drop(self.emb_enc(inp))<br>        enc_out, h = self.gru_enc(emb, h)<br>        h = self.out_enc(h)</pre><pre name="07ab" id="07ab" class="graf graf--pre graf-after--pre">        dec_inp = V(torch.zeros(bs).long())<br>        res = []<br>        <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(self.out_sl):<br>            emb = self.emb_dec(dec_inp).unsqueeze(0)<br>            outp, h = self.gru_dec(emb, h)<br>            outp = self.out(self.out_drop(outp[0]))<br>            res.append(outp)<br>            dec_inp = V(outp.data.max(1)[1])<br>            <strong class="markup--strong markup--pre-strong">if</strong> (dec_inp==1).all(): <strong class="markup--strong markup--pre-strong">break</strong><br>            <strong class="markup--strong markup--pre-strong">if</strong> (y <strong class="markup--strong markup--pre-strong">is</strong> <strong class="markup--strong markup--pre-strong">not</strong> <strong class="markup--strong markup--pre-strong">None</strong>) <strong class="markup--strong markup--pre-strong">and</strong> (random.random()&lt;self.pr_force):<br>                <strong class="markup--strong markup--pre-strong">if</strong> i&gt;=len(y): <strong class="markup--strong markup--pre-strong">break</strong><br>                dec_inp = y[i]<br>        <strong class="markup--strong markup--pre-strong">return</strong> torch.stack(res)<br>    <br>    <strong class="markup--strong markup--pre-strong">def</strong> initHidden(self, bs): <br>        <strong class="markup--strong markup--pre-strong">return</strong> V(torch.zeros(self.nl, bs, self.nh))</pre><p name="d6ad" id="d6ad" class="graf graf--p graf-after--pre"><code class="markup--code markup--p-code">pr_force</code>: “probability of forcing”. High in the beginning zero by the end.</p><p name="672d" id="672d" class="graf graf--p graf-after--p">Let’s now write something such that in the training loop, it gradually decreases <code class="markup--code markup--p-code">pr_force</code> [<a href="https://youtu.be/tY0n9OT5_nA?t=1h26m1s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h26m1s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:26:01</a>]. How do we do that? One approach would be to write our own training loop but let’s not do that because we already have a training loop that has progress bars, uses exponential weighted averages to smooth out the losses, keeps track of metrics, and does bunch of things. They also keep track of calling the reset for RNN at the start of the epoch to make sure the hidden state is set to zeros. What we’ve tended to find is that as we start to write some new thing and we need to replace some part of the code, we then add some little hook so that we can all use that hook to make things easier. In this particular case, there is a hook that Jeremy has ended up using all the time which is the hook called the stepper. If you look at the source code, model.py is where our fit function lives which is the lowest level thing that does not require learner or anything much at all — just requires a standard PyTorch model and a model data object. You just need to know how many epochs, a standard PyTorch optimizer, and a standard PyTorch loss function. We hardly ever used in the class, we normally call <code class="markup--code markup--p-code">learn.fit</code>, but <code class="markup--code markup--p-code">learn.fit</code> calls this.</p><figure name="5d07" id="5d07" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_hhksba0Jh8iyWmuC_tPtqg.png"></figure><p name="a435" id="a435" class="graf graf--p graf-after--figure">We have looked at the source code sometime [<a href="https://youtu.be/tY0n9OT5_nA?t=1h27m49s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h27m49s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:27:49</a>]. We’ve seen how it loos through each epoch and that loops through each thing in our batch and calls <code class="markup--code markup--p-code">stepper.step</code>. <code class="markup--code markup--p-code">stepper.step</code> is the thing that is responsible for:</p><ul class="postList"><li name="63a0" id="63a0" class="graf graf--li graf-after--p">calling the model</li><li name="9cff" id="9cff" class="graf graf--li graf-after--li">getting the loss</li><li name="11bb" id="11bb" class="graf graf--li graf-after--li">finding the loss function</li><li name="eeca" id="eeca" class="graf graf--li graf-after--li">calling the optimizer</li></ul><figure name="bf12" id="bf12" class="graf graf--figure graf-after--li"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_dlBOu68q6RyNuQ0opzvxMg.png"></figure><p name="b9d2" id="b9d2" class="graf graf--p graf-after--figure">So by default, <code class="markup--code markup--p-code">stepper.step</code> uses a particular class called <code class="markup--code markup--p-code">Stepper</code> which basically calls the model, zeros the gradient, calls the loss function, calls <code class="markup--code markup--p-code">backward</code>, does gradient clipping if necessary, then calls the optimizer. They are basic steps that back when we looked at “PyTorch from scratch” we had to do. The nice thing is, we can replace that with something else rather than replacing the training loop. If you inherit from <code class="markup--code markup--p-code">Stepper</code>, then write your own version of <code class="markup--code markup--p-code">step</code>&nbsp;, you can just copy and paste the contents of step and add whatever you like. Or if it’s something that you’re going to do before or afterwards, you could even call <code class="markup--code markup--p-code">super.step</code>. In this case, Jeremy rather suspects he has been unnecessarily complicated [<a href="https://youtu.be/tY0n9OT5_nA?t=1h29m12s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h29m12s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:29:12</a>] — he probably could have done something like:</p><pre name="4062" id="4062" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">Seq2SeqStepper</strong>(Stepper):<br>    <strong class="markup--strong markup--pre-strong">def</strong> step(self, xs, y, epoch):<br>        self.m.pr_force = (10-epoch)*0.1 <strong class="markup--strong markup--pre-strong">if</strong> epoch&lt;10 <strong class="markup--strong markup--pre-strong">else</strong> 0<br>        <strong class="markup--strong markup--pre-strong">return</strong> super.step(xs, y, epoch)</pre><p name="16e1" id="16e1" class="graf graf--p graf-after--pre">But as he said, when he is prototyping, he doesn’t think carefully about how to minimize his code — he copied and pasted the contents of the <code class="markup--code markup--p-code">step</code> and he added a single line to the top which was to replace <code class="markup--code markup--p-code">pr_force</code> in the module with something that gradually decreased linearly for the first 10 epochs, and after 10 epochs, it is zero. So total hack but good enough to try it out. The nice thing is that everything else is the same except for the addition of these three lines:</p><pre name="38fe" id="38fe" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">            if</strong> (y <strong class="markup--strong markup--pre-strong">is</strong> <strong class="markup--strong markup--pre-strong">not</strong> <strong class="markup--strong markup--pre-strong">None</strong>) <strong class="markup--strong markup--pre-strong">and</strong> (random.random()&lt;self.pr_force):<br>                <strong class="markup--strong markup--pre-strong">if</strong> i&gt;=len(y): <strong class="markup--strong markup--pre-strong">break</strong><br>                dec_inp = y[i]</pre></body></html>