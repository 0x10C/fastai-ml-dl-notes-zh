
<!-- saved from url=(0063)file:///C:/Users/asus/Desktop/fastai-ml-dl-notes-zh/en/dl3.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></head><body><hr class="section-divider"><h1 name="4ebe" id="4ebe" class="graf graf--h3 graf--leading graf--title">Deep Learning 2: Part 1 Lesson&nbsp;3</h1><p name="839f" id="839f" class="graf graf--p graf-after--h3"><em class="markup--em markup--p-em">My personal notes from </em><a href="http://www.fast.ai/" data-href="http://www.fast.ai/" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener nofollow noopener" target="_blank"><em class="markup--em markup--p-em">fast.ai course</em></a><em class="markup--em markup--p-em">. These notes will continue to be updated and improved as I continue to review the course to “really” understand it. Much appreciation to </em><a href="https://twitter.com/jeremyphoward" data-href="https://twitter.com/jeremyphoward" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener nofollow noopener" target="_blank"><em class="markup--em markup--p-em">Jeremy</em></a><em class="markup--em markup--p-em"> and </em><a href="https://twitter.com/math_rachel" data-href="https://twitter.com/math_rachel" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener nofollow noopener" target="_blank"><em class="markup--em markup--p-em">Rachel</em></a><em class="markup--em markup--p-em"> who gave me this opportunity to learn.</em></p><p name="8804" id="8804" class="graf graf--p graf-after--p graf--trailing">Lessons: <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" class="markup--anchor markup--p-anchor" target="_blank">1</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" class="markup--anchor markup--p-anchor" target="_blank">2</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" class="markup--anchor markup--p-anchor" target="_blank"><strong class="markup--strong markup--p-strong">3</strong></a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" class="markup--anchor markup--p-anchor" target="_blank">4</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" class="markup--anchor markup--p-anchor" target="_blank">5</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" class="markup--anchor markup--p-anchor" target="_blank">6</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" class="markup--anchor markup--p-anchor" target="_blank">7</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" class="markup--anchor markup--p-anchor" target="_blank">8</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" class="markup--anchor markup--p-anchor" target="_blank">9</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" class="markup--anchor markup--p-anchor" target="_blank">10</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" class="markup--anchor markup--p-anchor" target="_blank">11</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" class="markup--anchor markup--p-anchor" target="_blank">12</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" class="markup--anchor markup--p-anchor" target="_blank">13</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" class="markup--anchor markup--p-anchor" target="_blank">14</a></p><hr class="section-divider"><h3 name="e2f8" id="e2f8" class="graf graf--h3 graf--leading"><a href="http://forums.fast.ai/t/wiki-lesson-3/9401/1" data-href="http://forums.fast.ai/t/wiki-lesson-3/9401/1" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">Lesson 3</a></h3><h4 name="6b99" id="6b99" class="graf graf--h4 graf-after--h3">Helpful materials created by students:</h4><ul class="postList"><li name="fc13" id="fc13" class="graf graf--li graf-after--h4"><a href="https://github.com/reshamas/fastai_deeplearn_part1/blob/master/tools/aws_ami_gpu_setup.md" data-href="https://github.com/reshamas/fastai_deeplearn_part1/blob/master/tools/aws_ami_gpu_setup.md" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">AWS how-to</a></li><li name="0deb" id="0deb" class="graf graf--li graf-after--li"><a href="https://github.com/reshamas/fastai_deeplearn_part1/blob/master/tools/tmux.md" data-href="https://github.com/reshamas/fastai_deeplearn_part1/blob/master/tools/tmux.md" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">Tmux</a></li><li name="71cc" id="71cc" class="graf graf--li graf-after--li"><a href="https://medium.com/@apiltamang/case-study-a-world-class-image-classifier-for-dogs-and-cats-err-anything-9cf39ee4690e" data-href="https://medium.com/@apiltamang/case-study-a-world-class-image-classifier-for-dogs-and-cats-err-anything-9cf39ee4690e" class="markup--anchor markup--li-anchor" target="_blank">Lesson 2 summary</a></li><li name="6b77" id="6b77" class="graf graf--li graf-after--li"><a href="https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0" data-href="https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">Learning rate finder</a></li><li name="12c5" id="12c5" class="graf graf--li graf-after--li"><a href="https://towardsdatascience.com/a-practitioners-guide-to-pytorch-1d0f6a238040" data-href="https://towardsdatascience.com/a-practitioners-guide-to-pytorch-1d0f6a238040" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">PyTorch</a></li><li name="6f0d" id="6f0d" class="graf graf--li graf-after--li"><a href="https://miguel-data-sc.github.io/2017-11-05-first/" data-href="https://miguel-data-sc.github.io/2017-11-05-first/" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">Learning rate vs. Batch size</a></li><li name="39da" id="39da" class="graf graf--li graf-after--li"><a href="https://medium.com/@radekosmulski/do-smoother-areas-of-the-error-surface-lead-to-better-generalization-b5f93b9edf5b" data-href="https://medium.com/@radekosmulski/do-smoother-areas-of-the-error-surface-lead-to-better-generalization-b5f93b9edf5b" class="markup--anchor markup--li-anchor" target="_blank">Smoother area of the error surface vs. generalization</a></li><li name="7e5b" id="7e5b" class="graf graf--li graf-after--li"><a href="https://medium.com/@init_27/convolutional-neural-network-in-5-minutes-8f867eb9ca39" data-href="https://medium.com/@init_27/convolutional-neural-network-in-5-minutes-8f867eb9ca39" class="markup--anchor markup--li-anchor" target="_blank">Convolutional Neural Network in 5 minutes</a></li><li name="77c6" id="77c6" class="graf graf--li graf-after--li"><a href="http://teleported.in/posts/decoding-resnet-architecture/" data-href="http://teleported.in/posts/decoding-resnet-architecture/" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">Decoding ResNet Architecture</a></li><li name="1e2e" id="1e2e" class="graf graf--li graf-after--li"><a href="https://medium.com/@apiltamang" data-href="https://medium.com/@apiltamang" class="markup--anchor markup--li-anchor" target="_blank">Yet Another ResNet Tutorial</a></li></ul><h4 name="f475" id="f475" class="graf graf--h4 graf-after--li">Where we go from&nbsp;here:</h4><figure name="4062" id="4062" class="graf graf--figure graf-after--h4"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_w03TpHU-IgKy5GsLuMYxzw.png"></figure><h3 name="68b0" id="68b0" class="graf graf--h3 graf-after--figure">Review [<a href="https://youtu.be/9C06ZPF8Uuc?t=8m24s" data-href="https://youtu.be/9C06ZPF8Uuc?t=8m24s" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">08:24</a>]:</h3><h4 name="3d37" id="3d37" class="graf graf--h4 graf-after--h3">Kaggle CLI&nbsp;: How to download data&nbsp;1:</h4><p name="bdf2" id="bdf2" class="graf graf--p graf-after--h4"><a href="https://github.com/floydwch/kaggle-cli" data-href="https://github.com/floydwch/kaggle-cli" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Kaggle CLI</a> is a good tool to use when you are downloading from Kaggle. Because it is downloading data from Kaggle website (through screen scraping), it breaks when the website changes. When that happens, run <code class="markup--code markup--p-code">pip install kaggle-cli --upgrade</code>.</p><p name="822f" id="822f" class="graf graf--p graf-after--p">Then you can run:</p><pre name="5ff8" id="5ff8" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">$ kg download -u &lt;username&gt; -p &lt;password&gt; -c &lt;competition&gt;</code></pre><p name="3b47" id="3b47" class="graf graf--p graf-after--pre">Replace <code class="markup--code markup--p-code">&lt;username&gt;</code>, <code class="markup--code markup--p-code">&lt;password&gt;</code> with your credential and <code class="markup--code markup--p-code">&lt;competition&gt;</code> is what follows <code class="markup--code markup--p-code">/c/</code> in the URL. For example, if you are trying to download dog breed data from <code class="markup--code markup--p-code u-paddingRight0 u-marginRight0">https://www.kaggle.com<strong class="markup--strong markup--p-strong">/c/</strong>dog-breed-identification</code> the command would look like:</p><pre name="4de8" id="4de8" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">$ kg download -u john.doe -p mypassword -c </code>dog-breed-identification</pre><p name="6a3b" id="6a3b" class="graf graf--p graf-after--pre">Make sure you had clicked on the <code class="markup--code markup--p-code">Download</code> button from your computer once and accepted the rules:</p><figure name="c6ef" id="c6ef" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_NE_vFqUgrq_ZY-Ez8lYD1Q.png"></figure><h4 name="73b8" id="73b8" class="graf graf--h4 graf-after--figure">CurWget (Chrome extension): How to download data&nbsp;2:</h4><figure name="7a2a" id="7a2a" class="graf graf--figure graf-after--h4"><a href="https://chrome.google.com/webstore/detail/curlwget/jmocjfidanebdlinpbcdkcmgdifblncg" data-href="https://chrome.google.com/webstore/detail/curlwget/jmocjfidanebdlinpbcdkcmgdifblncg" class="graf-imageAnchor" data-action="image-link" data-action-observe-only="true" rel="nofollow"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_dpgcElfgbBLg-LKqyQHOBQ.png"></a></figure><h4 name="ce77" id="ce77" class="graf graf--h4 graf-after--figure">Quick Dogs vs. Cats&nbsp;[<a href="https://youtu.be/9C06ZPF8Uuc?t=13m39s" data-href="https://youtu.be/9C06ZPF8Uuc?t=13m39s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">13:39</a>]</h4><pre name="40e5" id="40e5" class="graf graf--pre graf-after--h4"><strong class="markup--strong markup--pre-strong">from</strong> fastai.conv_learner <strong class="markup--strong markup--pre-strong">import</strong> * <br>PATH = 'data/dogscats/'<br>sz=224; bs=64</pre><p name="6091" id="6091" class="graf graf--p graf-after--pre">Often the notebook assumes that your data is in <code class="markup--code markup--p-code">data</code> folder. But maybe you want to put them somewhere else. In that case, you can use symbolic link (symlink for short):</p><figure name="d52d" id="d52d" class="graf graf--figure graf--layoutOutsetCenter graf-after--p" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_f835x3bUfRPT9pFaqjvutw.png"></figure><p name="9c07" id="9c07" class="graf graf--p graf-after--figure">Here is an end to end process to get a state of the art result for dogs vs. cats:</p><figure name="a91d" id="a91d" class="graf graf--figure graf--layoutOutsetCenter graf-after--p" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_ItxElIWV6hU9f_fwEZ9jMQ.png"><figcaption class="imageCaption">Quick Dogs v&nbsp;Cats</figcaption></figure><h4 name="9208" id="9208" class="graf graf--h4 graf-after--figure">A little further analysis:</h4><pre name="1884" id="1884" class="graf graf--pre graf-after--h4">data = ImageClassifierData.from_paths(PATH, tfms= tfms, bs=bs, test_name='test')</pre><ul class="postList"><li name="07ec" id="07ec" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">from_paths</code>&nbsp;: Indicates that subfolder names are the labels. If your <code class="markup--code markup--li-code">train</code> folder or <code class="markup--code markup--li-code">valid</code> folder has a different name, you can send <code class="markup--code markup--li-code">trn_name</code> and <code class="markup--code markup--li-code">val_name</code> argument.</li><li name="7c3b" id="7c3b" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">test_name</code>&nbsp;: If you want to submit to Kaggle competition, you will need to fill in the name of the folder where the test set is.</li></ul><pre name="64d6" id="64d6" class="graf graf--pre graf-after--li">learn = ConvLearner.pretrained(resnet50, data)</pre><ul class="postList"><li name="a15f" id="a15f" class="graf graf--li graf-after--pre">Notice that we did not set <code class="markup--code markup--li-code">pre_compue=True</code>. It is just a shortcut which caches some of the intermediate steps that do not have to be recalculated each time. If you are at all confused about it, you can just leave it off.</li><li name="57d5" id="57d5" class="graf graf--li graf-after--li">Remember, when <code class="markup--code markup--li-code">pre_compute=True</code>&nbsp;, data augmentation does not work.</li></ul><pre name="3fad" id="3fad" class="graf graf--pre graf-after--li">learn.unfreeze() <br>learn.<strong class="markup--strong markup--pre-strong">bn_freeze</strong>(<strong class="markup--strong markup--pre-strong">True</strong>) <br>%time learn.fit([1e-5, 1e-4,1e-2], 1, cycle_len=1)</pre><ul class="postList"><li name="c01d" id="c01d" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">bn_freeze</code>&nbsp;: If you are using a bigger deeper model like ResNet50 or ResNext101 (anything with number bigger than 34) on a dataset that is very similar to ImageNet (i.e. side-on photos of standard object whose size is similar to ImageNet between 200–500 pixels), you should add this line. We will learn more in the second half of the course, but it is causing the batch normalization moving averages to not be updated.</li></ul><h4 name="d940" id="d940" class="graf graf--h4 graf-after--li"><a href="https://github.com/fastai/fastai/blob/master/courses/dl1/keras_lesson1.ipynb" data-href="https://github.com/fastai/fastai/blob/master/courses/dl1/keras_lesson1.ipynb" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">How to use other libraries — Keras</a>&nbsp;[<a href="https://youtu.be/9C06ZPF8Uuc?t=20m2s" data-href="https://youtu.be/9C06ZPF8Uuc?t=20m2s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">20:02</a>]</h4><p name="cc4e" id="cc4e" class="graf graf--p graf-after--h4">It is important to understand how to use libraries other than Fast.ai. Keras is a good example to look at because just like Fast.ai sits on top of PyTorch, it sits on top of varieties of libraries such as TensorFlow, MXNet, CNTK, etc.</p><p name="7ba0" id="7ba0" class="graf graf--p graf-after--p">If you want to run <a href="https://github.com/fastai/fastai/blob/master/courses/dl1/keras_lesson1.ipynb" data-href="https://github.com/fastai/fastai/blob/master/courses/dl1/keras_lesson1.ipynb" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">the notebook</a>, run <code class="markup--code markup--p-code">pip install tensorflow-gpu keras</code></p><ol class="postList"><li name="1205" id="1205" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Define data generators</strong></li></ol><pre name="e01b" id="e01b" class="graf graf--pre graf-after--li">train_data_dir = f'{PATH}train' <br>validation_data_dir = f'{PATH}valid'</pre><pre name="b3f6" id="b3f6" class="graf graf--pre graf-after--pre">train_datagen = ImageDataGenerator(rescale=1. / 255,<br>    shear_range=0.2, zoom_range=0.2, horizontal_flip=True)</pre><pre name="318d" id="318d" class="graf graf--pre graf-after--pre">test_datagen = ImageDataGenerator(rescale=1. / 255)</pre><pre name="37b8" id="37b8" class="graf graf--pre graf-after--pre">train_generator = train_datagen.flow_from_directory(train_data_dir,<br>    target_size=(sz, sz),<br>    batch_size=batch_size, class_mode='binary')</pre><pre name="5938" id="5938" class="graf graf--pre graf-after--pre">validation_generator = test_datagen.flow_from_directory(<br>    validation_data_dir,<br>    shuffle=False,<br>    target_size=(sz, sz),<br>    batch_size=batch_size, class_mode='binary')</pre><ul class="postList"><li name="50e2" id="50e2" class="graf graf--li graf-after--pre">The idea of train folder and validation folder with subfolders with the label names is commonly done, and Keras also does it.</li><li name="6623" id="6623" class="graf graf--li graf-after--li">Keras requires much more code and many more parameters to be set.</li><li name="da38" id="da38" class="graf graf--li graf-after--li">Rather than creating a single data object, in Keras you define <code class="markup--code markup--li-code">DataGenerator</code> and specify what kind of data augmentation we want it to do and also what kind of normalization to do. In other words, in Fast.ai, we can just say “whatever ResNet50 requires, just do that for me please” but in Keras, you need to know what is expected. There is no standard set of augmentations.</li><li name="910c" id="910c" class="graf graf--li graf-after--li">You have to then create a validation data generator in which you are responsible to create a generator that does not have data augmentation. And you also have to tell it not to shuffle the dataset for validation because otherwise you cannot keep track of how well you are doing.</li></ul><p name="7940" id="7940" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">2. Create a model</strong></p><pre name="6a75" id="6a75" class="graf graf--pre graf-after--p">base_model = ResNet50(weights='imagenet', include_top=False)<br>x = base_model.output<br>x = GlobalAveragePooling2D()(x)<br>x = Dense(1024, activation='relu')(x)<br>predictions = Dense(1, activation='sigmoid')(x)</pre><ul class="postList"><li name="d334" id="d334" class="graf graf--li graf-after--pre">The reason Jeremy used ResNet50 for Quick Dogs and Cats was because Keras does not have ResNet34. We want to compare apple to apple.</li><li name="4dd6" id="4dd6" class="graf graf--li graf-after--li">You cannot ask it to construct a model that is suitable for a particular dataset, so you have to do it by hand.</li><li name="348f" id="348f" class="graf graf--li graf-after--li">First you create a base model, then you construct layers you want to add on top of it.</li></ul><p name="7ebb" id="7ebb" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">3. Freeze layers and compile</strong></p><pre name="e7e4" id="e7e4" class="graf graf--pre graf-after--p">model = Model(inputs=base_model.input, outputs=predictions)</pre><pre name="bb85" id="bb85" class="graf graf--pre graf-after--pre">for layer in base_model.layers: layer.trainable = False</pre><pre name="1d23" id="1d23" class="graf graf--pre graf-after--pre">model.compile(optimizer='rmsprop', loss='binary_crossentropy', <br>    metrics=['accuracy'])</pre><ul class="postList"><li name="59a9" id="59a9" class="graf graf--li graf-after--pre">Loop through layers and freeze them manually by calling <code class="markup--code markup--li-code">layer.trainable=False</code></li><li name="24af" id="24af" class="graf graf--li graf-after--li">You need to compile a model</li><li name="01ce" id="01ce" class="graf graf--li graf-after--li">Pass the type of optimizer, loss, and metrics</li></ul><p name="088e" id="088e" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">4. Fit</strong></p><pre name="136e" id="136e" class="graf graf--pre graf-after--p">model.fit_generator(train_generator, <strong class="markup--strong markup--pre-strong">train_generator.n//batch_size</strong>,<br>    epochs=3, <strong class="markup--strong markup--pre-strong">workers=4</strong>, validation_data=validation_generator,<br>    validation_steps=validation_generator.n // batch_size)</pre><ul class="postList"><li name="05dd" id="05dd" class="graf graf--li graf-after--pre">Keras expects to know how many batches there are per epoch.</li><li name="0ef1" id="0ef1" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">workers</code>&nbsp;: how many processors to use</li></ul><p name="ae0a" id="ae0a" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">5. Fine-tune: Unfreeze some layers, compile, then fit again</strong></p><pre name="92d7" id="92d7" class="graf graf--pre graf-after--p">split_at = 140</pre><pre name="69eb" id="69eb" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">for</strong> layer <strong class="markup--strong markup--pre-strong">in</strong> model.layers[:split_at]: layer.trainable = <strong class="markup--strong markup--pre-strong">False</strong><br><strong class="markup--strong markup--pre-strong">for</strong> layer <strong class="markup--strong markup--pre-strong">in</strong> model.layers[split_at:]: layer.trainable = <strong class="markup--strong markup--pre-strong">True</strong></pre><pre name="3dda" id="3dda" class="graf graf--pre graf-after--pre">model.compile(optimizer='rmsprop', loss='binary_crossentropy',<br>    metrics=['accuracy'])</pre><pre name="edaf" id="edaf" class="graf graf--pre graf-after--pre">%%time model.fit_generator(train_generator, <br>    train_generator.n // batch_size, epochs=1, workers=3,<br>    validation_data=validation_generator,<br>    validation_steps=validation_generator.n // batch_size)</pre><p name="286c" id="286c" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">Pytorch</strong> — If you want to deploy to mobile devices, PyTorch is still very early.</p><p name="345b" id="345b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Tensorflow</strong> — If you want to convert things you learned in this class, do more work with Keras, but it would take a bit more work and is hard to get the same level of results. Maybe there will be TensorFlow compatible version of Fast.ai in future. We will see.</p><h4 name="5c36" id="5c36" class="graf graf--h4 graf-after--p">Create Submission file for Kaggle&nbsp;[<a href="https://youtu.be/9C06ZPF8Uuc?t=32m45s" data-href="https://youtu.be/9C06ZPF8Uuc?t=32m45s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">32:45</a>]</h4><p name="5d2b" id="5d2b" class="graf graf--p graf-after--h4">To create the submission files, we need two pieces of information:</p><ul class="postList"><li name="dbd4" id="dbd4" class="graf graf--li graf-after--p"><code class="markup--code markup--li-code">data.classes</code>&nbsp;: contains all the different classes</li><li name="8eb9" id="8eb9" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">data.test_ds.fnames</code>&nbsp;: test file names</li></ul><pre name="fa78" id="fa78" class="graf graf--pre graf-after--li">log_preds, y = learn.TTA(is_test=True)<br>probs = np.exp(log_preds)</pre><p name="3874" id="3874" class="graf graf--p graf-after--pre">It is always good idea to use <code class="markup--code markup--p-code">TTA:</code></p><ul class="postList"><li name="f606" id="f606" class="graf graf--li graf-after--p"><code class="markup--code markup--li-code">is_test=True</code>&nbsp;: it will give you predictions on the test set rather than the validation set</li><li name="49e0" id="49e0" class="graf graf--li graf-after--li">By default, PyTorch models will give you back the log of the predictions, so you need to do <code class="markup--code markup--li-code">np.exp(log_preds)</code> to get the probability.</li></ul><pre name="718f" id="718f" class="graf graf--pre graf-after--li">ds = pd.DataFrame(probs)<br>ds.columns = data.classes</pre><ul class="postList"><li name="5df5" id="5df5" class="graf graf--li graf-after--pre">Create Pandas <code class="markup--code markup--li-code">DataFrame</code></li><li name="0883" id="0883" class="graf graf--li graf-after--li">Set the column name as <code class="markup--code markup--li-code">data.classes</code></li></ul><pre name="f817" id="f817" class="graf graf--pre graf-after--li">ds.insert(0, 'id', [o[5:-4] <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> data.test_ds.fnames])</pre><ul class="postList"><li name="3938" id="3938" class="graf graf--li graf-after--pre">Insert a new column at position zero named <code class="markup--code markup--li-code">id</code>. Remove first 5 and last 4 letters since we just need IDs (a file name looks like <code class="markup--code markup--li-code">test/0042d6bf3e5f3700865886db32689436.jpg</code>)</li></ul><pre name="cd32" id="cd32" class="graf graf--pre graf-after--li">ds.head()</pre><figure name="9776" id="9776" class="graf graf--figure graf--layoutOutsetCenter graf-after--pre" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_S6mkbwDsXs2ERYI3Eygvig.png"></figure><pre name="4d87" id="4d87" class="graf graf--pre graf-after--figure">SUBM = f'{PATH}sub/' <br>os.makedirs(SUBM, exist_ok=<strong class="markup--strong markup--pre-strong">True</strong>) <br>ds.to_csv(f'{SUBM}subm.gz', compression='gzip', index=<strong class="markup--strong markup--pre-strong">False</strong>)</pre><ul class="postList"><li name="92d9" id="92d9" class="graf graf--li graf-after--pre">Now you can call <code class="markup--code markup--li-code">ds.to_csv</code> to create a CSV file and <code class="markup--code markup--li-code">compression='gzip'</code> will zip it up on the server.</li></ul><pre name="cdc3" id="cdc3" class="graf graf--pre graf-after--li">FileLink(f'{SUBM}subm.gz')</pre><ul class="postList"><li name="62db" id="62db" class="graf graf--li graf-after--pre">You can use Kaggle CLI to submit from the server directly, or you can use <code class="markup--code markup--li-code">FileLink</code> which will give you a link to download the file from the server to your computer.</li></ul><h4 name="a52e" id="a52e" class="graf graf--h4 graf-after--li">Individual prediction [<a href="https://youtu.be/9C06ZPF8Uuc?t=39m32s" data-href="https://youtu.be/9C06ZPF8Uuc?t=39m32s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">39:32</a>]</h4><p name="646b" id="646b" class="graf graf--p graf-after--h4">What if we want to run a single image through a model to get a prediction?</p><pre name="5bf3" id="5bf3" class="graf graf--pre graf-after--p">fn = data.val_ds.fnames[0]; fn</pre><pre name="9355" id="9355" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">'train/</em><code class="markup--code markup--pre-code"><em class="markup--em markup--pre-em">001513dfcb2ffafc82cccf4d8bbaba97.jpg</em></code><em class="markup--em markup--pre-em">'</em></pre><pre name="8a42" id="8a42" class="graf graf--pre graf-after--pre">Image.open(PATH + fn)</pre><figure name="b5ca" id="b5ca" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_1eb6vEpa8SOrxaoNNs7f0g.png"></figure><ul class="postList"><li name="8ef6" id="8ef6" class="graf graf--li graf-after--figure">We will pick a first file from the validation set.</li></ul><p name="bee9" id="bee9" class="graf graf--p graf-after--li">This is the shortest way to get a prediction:</p><pre name="ccaa" id="ccaa" class="graf graf--pre graf-after--p">trn_tfms, val_tfms = tfms_from_model(arch, sz)</pre><pre name="31d2" id="31d2" class="graf graf--pre graf-after--pre">im = val_tfms(Image.open(PATH+fn)<br> <br>preds = learn.predict_array(im[None])</pre><pre name="12be" id="12be" class="graf graf--pre graf-after--pre">np.argmax(preds)</pre><ul class="postList"><li name="d917" id="d917" class="graf graf--li graf-after--pre">Image must be transformed. <code class="markup--code markup--li-code">tfms_from_model</code> returns training transforms and validation transforms. In this case, we will use validation transform.</li><li name="d1d1" id="d1d1" class="graf graf--li graf-after--li">Everything that gets passed to or returned from a model is generally assumed to be in a mini-batch. Here we only have one image, but we have to turn that into a mini-batch of a single image. In other words, we need to create a tensor that is not just <code class="markup--code markup--li-code">[rows, columns, channels]</code>&nbsp;, but <code class="markup--code markup--li-code">[number of images, rows, columns, channels]</code>.</li><li name="4f24" id="4f24" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">im[None]</code>&nbsp;: Numpy trick to add additional unit axis to the start.</li></ul><h4 name="f1db" id="f1db" class="graf graf--h4 graf-after--li">Theory: What is actually going on behind the scenes with convolutional neural network&nbsp;[<a href="https://youtu.be/9C06ZPF8Uuc?t=42m17s" data-href="https://youtu.be/9C06ZPF8Uuc?t=42m17s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">42:17</a>]</h4><ul class="postList"><li name="22f8" id="22f8" class="graf graf--li graf-after--h4">We saw a little bit of theory in Lesson 1 — <a href="http://setosa.io/ev/image-kernels/" data-href="http://setosa.io/ev/image-kernels/" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">http://setosa.io/ev/image-kernels/</a></li><li name="b101" id="b101" class="graf graf--li graf-after--li">Convolution is something where we have a little matrix (nearly always 3x3 in deep learning) and multiply every element of that matrix by every element of 3x3 section of an image and add them all together to get the result of that convolution at one point.</li></ul><p name="1a70" id="1a70" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">Otavio’s fantastic visualization (he created Word Lens):</strong></p><figure name="26e7" id="26e7" class="graf graf--figure graf--iframe graf-after--p"><iframe data-width="854" data-height="480" width="700" height="393" data-src="/media/4870874ee9b944bcf928658fb1767280?postId=74b0ef79e56" data-media-id="4870874ee9b944bcf928658fb1767280" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Fi.ytimg.com%2Fvi%2FOqm9vsf_hvU%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="../img/saved_resource.html"></iframe><iframe data-width="854" data-height="480" width="700" height="393" src="../img/4870874ee9b944bcf928658fb1767280.html" data-media-id="4870874ee9b944bcf928658fb1767280" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Fi.ytimg.com%2Fvi%2FOqm9vsf_hvU%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen="" frameborder="0"></iframe></figure><p name="d6a2" id="d6a2" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Jeremy’s visualization: </strong><a href="https://github.com/fastai/fastai/blob/master/courses/dl1/excel/conv-example.xlsx" data-href="https://github.com/fastai/fastai/blob/master/courses/dl1/excel/conv-example.xlsx" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong">Spreadsheet</strong></a><strong class="markup--strong markup--p-strong"> [</strong><a href="https://youtu.be/9C06ZPF8Uuc?t=49m51s" data-href="https://youtu.be/9C06ZPF8Uuc?t=49m51s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong">49:51</strong></a><strong class="markup--strong markup--p-strong">]</strong></p><figure name="6cb1" id="6cb1" class="graf graf--figure graf--layoutOutsetCenter graf-after--p" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_AUQDWjcwS2Yt7Id0WyXCaQ.png"><figcaption class="imageCaption">I used <a href="https://office.live.com/start/Excel.aspx?ui=en-US&amp;rs=US" data-href="https://office.live.com/start/Excel.aspx?ui=en-US&amp;rs=US" class="markup--anchor markup--figure-anchor" rel="nofollow noopener noopener nofollow noopener" target="_blank">https://office.live.com/start/Excel.aspx</a></figcaption></figure><ul class="postList"><li name="adf2" id="adf2" class="graf graf--li graf-after--figure">This data is from MNIST</li><li name="24b0" id="24b0" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Activation</strong>: A number that is calculated by applying some kind of linear operation to some numbers in the input.</li><li name="03a2" id="03a2" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Rectified Linear Unit (ReLU)</strong>: Throw away negative — i.e. MAX(0, x)</li><li name="ab1d" id="ab1d" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Filter/Kernel:</strong> A 3x3 slice of a 3D tensor you used for convolution</li><li name="1def" id="1def" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Tensor:</strong> Multidimensional array or matrix Hidden Layer A layer that is neither input nor output</li><li name="fe35" id="fe35" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Max pooling:</strong> A (2,2) max pooling will halve the resolution in both height and width — think of it as a summary</li><li name="3402" id="3402" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Fully connected layer:</strong> Give a weight to each and every single activation and calculate the sum product. Weight matrix is as big as the entire input.</li><li name="7c6b" id="7c6b" class="graf graf--li graf-after--li">Note: There are many things you can do after the max pooling layer. One of them is to do another max pool across the entire size. In older architectures or structured data, we do fully connected layer. <span class="markup--quote markup--li-quote is-other" name="anon_c3c6755e8a2b" data-creator-ids="anon">Architecture that make heavy use of fully connected layers are prone to overfitting and are slower. ResNet and ResNext do not use very large fully connected layers.</span></li></ul><p name="60d7" id="60d7" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">Question</strong>: What happens if the input had 3 channels? [<a href="https://youtu.be/9C06ZPF8Uuc?t=1h5m30s" data-href="https://youtu.be/9C06ZPF8Uuc?t=1h5m30s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:05:30</a>] It will look something similar to the Conv1 layer which has 2 channels — therefore, filters have 2 channels per filter. Pre-trained ImageNet models use 3 channels. Some of the techniques you can use when you do when you do have less than 3 channel is to either duplicate one of the channels to make it 3, or if you have 2, then get an average and consider that as the third channel. If you have 4 channels, you could add extra level to the convolutional kernel with all zeros.</p><h4 name="dcb5" id="dcb5" class="graf graf--h4 graf-after--p">What happens next? [<a href="https://youtu.be/9C06ZPF8Uuc?t=1h8m47s" data-href="https://youtu.be/9C06ZPF8Uuc?t=1h8m47s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:08:47</a>]</h4><p name="950f" id="950f" class="graf graf--p graf-after--h4">We have gotten as far as fully connected layer (it does classic matrix product). In the excel sheet, there is one activation. If we want to look at which one of ten digit the input is, we actually want to calculate 10 numbers.</p><p name="a141" id="a141" class="graf graf--p graf-after--p">Let’s look at an example where we are trying to predict whether a picture is a cat, a dog, or a plane, or fish, or a building. Our goal is:</p><ol class="postList"><li name="6de5" id="6de5" class="graf graf--li graf-after--p">Take output from the fully connected layer (no ReLU so there may be negatives)</li><li name="56c4" id="56c4" class="graf graf--li graf-after--li">Calculate 5 numbers where each of them is between 0 and 1 and they add up to 1.</li></ol><p name="858f" id="858f" class="graf graf--p graf-after--li">To do this, we need a different kind of activation function (a function applied to an activation).</p><p name="eb1b" id="eb1b" class="graf graf--p graf-after--p">Why do we need non-lineality? If you stack multiple linear layers, it is still just a linear layer. By adding non-linear layers, we can fit arbitrarily complex shapes. The non-linear activation function we used was ReLU.</p><h4 name="4977" id="4977" class="graf graf--h4 graf-after--p">Softmax [<a href="https://youtu.be/9C06ZPF8Uuc?t=1h14m8s" data-href="https://youtu.be/9C06ZPF8Uuc?t=1h14m8s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">01:14:08</a>]</h4><p name="6d11" id="6d11" class="graf graf--p graf-after--h4">Softmax only ever occurs in the final layer. It outputs numbers between 0 and 1, and they add up to 1. In theory, this is not strictly necessary — we could ask out neural net to learn a set of kernels which give probabilities that line up as closely as possible with what we want. In general with deep learning, if you can construct your architecture so that the desired characteristics are as easy to express as possible, you will end up with better models (learn more quickly and with less parameters).</p><figure name="dfd7" id="dfd7" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_YMxLuqkvhuR_ef_3K8iHyw.png"></figure><ol class="postList"><li name="8f14" id="8f14" class="graf graf--li graf-after--figure">Get rid of negatives by <code class="markup--code markup--li-code">e^x</code> because we cannot have negative probabilities. It also accentuates the value difference (2.85&nbsp;: 4.08 → 17.25&nbsp;: 59.03)</li></ol><p name="e650" id="e650" class="graf graf--p graf-after--li">All the math that you need to be familiar with to do deep learning:</p><figure name="033c" id="033c" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_83US55BbMSX1dNuKCd8f1A.png"></figure><p name="3ba1" id="3ba1" class="graf graf--p graf-after--figure">2. We then add up the <code class="markup--code markup--p-code">exp</code> column (182.75), and divide the <code class="markup--code markup--p-code">e^x</code> by the sum. The result will always be positive since we divided positive by positive. Each number will be between 0 and 1, and the total will be 1.</p><p name="7386" id="7386" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: What kind of activation function do we use if we want to classify the picture as cat and dog? [<a href="https://youtu.be/9C06ZPF8Uuc?t=1h20m27s" data-href="https://youtu.be/9C06ZPF8Uuc?t=1h20m27s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:20:27</a>] It so happens that we are going to do that right now. One reason we might want to do that is to do multi-label classification.</p><h3 name="5487" id="5487" class="graf graf--h3 graf-after--p">Planet Competition [<a href="https://youtu.be/9C06ZPF8Uuc?t=1h20m54s" data-href="https://youtu.be/9C06ZPF8Uuc?t=1h20m54s" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">01:20:54</a>]</h3><p name="847e" id="847e" class="graf graf--p graf-after--h3"><a href="https://github.com/fastai/fastai/blob/master/courses/dl1/lesson2-image_models.ipynb" data-href="https://github.com/fastai/fastai/blob/master/courses/dl1/lesson2-image_models.ipynb" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Notebook</a> / <a href="https://www.kaggle.com/c/planet-understanding-the-amazon-from-space" data-href="https://www.kaggle.com/c/planet-understanding-the-amazon-from-space" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Kaggle page</a></p><blockquote name="2c63" id="2c63" class="graf graf--blockquote graf-after--p">I would definitely recommend anthropomorphizing your activation functions. They have personalities. [<a href="https://youtu.be/9C06ZPF8Uuc?t=1h22m21s" data-href="https://youtu.be/9C06ZPF8Uuc?t=1h22m21s" class="markup--anchor markup--blockquote-anchor" rel="nofollow noopener" target="_blank">1:22:21</a>]</blockquote><p name="cfb8" id="cfb8" class="graf graf--p graf-after--blockquote">Softmax does not like to predicting multiple things. It wants to pick one thing.</p><p name="36bf" id="36bf" class="graf graf--p graf-after--p">Fast.ai library will automatically switch into multi-label mode if there is more than one label. So you do not have to do anything. But here is what happens behind the scene:</p><pre name="91bf" id="91bf" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">planet</strong> <strong class="markup--strong markup--pre-strong">import</strong> f2<br><br>metrics=[f2]<br>f_model = resnet34</pre><pre name="d5ac" id="d5ac" class="graf graf--pre graf-after--pre">label_csv = f'<strong class="markup--strong markup--pre-strong">{PATH}</strong>train_v2.csv'<br>n = len(list(open(label_csv)))-1<br>val_idxs = get_cv_idxs(n)</pre><pre name="1e98" id="1e98" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> get_data(sz):<br>    tfms = tfms_from_model(f_model, sz,<br>        aug_tfms=<strong class="markup--strong markup--pre-strong">transforms_top_down</strong>, max_zoom=1.05)</pre><pre name="db27" id="db27" class="graf graf--pre graf-after--pre">    <strong class="markup--strong markup--pre-strong">return</strong> ImageClassifierData.<strong class="markup--strong markup--pre-strong">from_csv</strong>(PATH, 'train-jpg',<br>               label_csv, tfms=tfms, suffix='.jpg',<br>               val_idxs=val_idxs, test_name='test-jpg')</pre><pre name="2bad" id="2bad" class="graf graf--pre graf-after--pre">data = get_data(256)</pre><ul class="postList"><li name="c93f" id="c93f" class="graf graf--li graf-after--pre">Multi-label classification cannot be done with Keras style approach where subfolder is the name of the label. So we use <code class="markup--code markup--li-code">from_csv</code></li><li name="aca6" id="aca6" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">transform_top_down</code>&nbsp;: it does more than just a vertical flip. There are 8 possible symmetries for a square — it can be rotated through 0, 90, 180, 270 degrees and for each of those, it can be flipped (<strong class="markup--strong markup--li-strong">dihedral</strong> group of eight)</li></ul><pre name="c29a" id="c29a" class="graf graf--pre graf-after--li">x,y = next(iter(data.val_dl))</pre><ul class="postList"><li name="f2a8" id="f2a8" class="graf graf--li graf-after--pre">We had seen <code class="markup--code markup--li-code">data.val_ds</code>&nbsp;, <code class="markup--code markup--li-code">test_ds</code>, <code class="markup--code markup--li-code">train_ds</code>(<code class="markup--code markup--li-code">ds</code>: dataset) for which you can get an individual image by <code class="markup--code markup--li-code">data.train_ds[0]</code>, for example.</li><li name="e332" id="e332" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">dl</code> is a data loader which will give you a mini-batch, specifically <em class="markup--em markup--li-em">transformed</em> mini-batch. With a data loader, you cannot ask for a particular mini-batch; you can only get back the <code class="markup--code markup--li-code">next</code> mini-batch. In Python, it is called “generator” or “iterator”. PyTorch really leverages modern Python methodologies.</li></ul><blockquote name="3429" id="3429" class="graf graf--pullquote graf-after--li"><a href="https://youtu.be/9C06ZPF8Uuc?t=1h27m45s" data-href="https://youtu.be/9C06ZPF8Uuc?t=1h27m45s" class="markup--anchor markup--pullquote-anchor" rel="nofollow noopener" target="_blank">If you know Python well, PyTorch comes very naturally. If you don’t know Python well, PyTorch is a good reason to learn Python&nbsp;well.</a></blockquote><ul class="postList"><li name="265a" id="265a" class="graf graf--li graf-after--pullquote"><code class="markup--code markup--li-code">x</code>&nbsp;: a mini-batch of images, <code class="markup--code markup--li-code">y</code>&nbsp;: a mini-batch of labels.</li></ul><p name="b778" id="b778" class="graf graf--p graf-after--li">If you are never sure what arguments a function takes, hit <code class="markup--code markup--p-code">shift+tab</code>&nbsp;.</p><pre name="273f" id="273f" class="graf graf--pre graf-after--p">list(zip(data.classes, y[0]))<br><br><em class="markup--em markup--pre-em">[('agriculture', 1.0),<br> ('artisinal_mine', 0.0),<br> ('bare_ground', 0.0),<br> ('blooming', 0.0),<br> ('blow_down', 0.0),<br> ('clear', 1.0),<br> ('cloudy', 0.0),<br> ('conventional_mine', 0.0),<br> ('cultivation', 0.0),<br> ('habitation', 0.0),<br> ('haze', 0.0),<br> ('partly_cloudy', 0.0),<br> ('primary', 1.0),<br> ('road', 0.0),<br> ('selective_logging', 0.0),<br> ('slash_burn', 1.0),<br> ('water', 1.0)]</em></pre><p name="685d" id="685d" class="graf graf--p graf-after--pre">Behind the scenes, PyTorch and fast.ai are turning our labels into one-hot-encoded labels. If the actual label is dog, it will look like:</p><figure name="5d4d" id="5d4d" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_u6f0xuCoSDDIz5zDrdTO5A.png"></figure><p name="7058" id="7058" class="graf graf--p graf-after--figure">We take the difference between <code class="markup--code markup--p-code">actuals</code> and <code class="markup--code markup--p-code">softmax</code>&nbsp;, add them up to say how much error there is (i.e. loss function) [<a href="https://youtu.be/9C06ZPF8Uuc?t=1h31m2s" data-href="https://youtu.be/9C06ZPF8Uuc?t=1h31m2s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:31:02</a>].</p><p name="9893" id="9893" class="graf graf--p graf-after--p">One-hot-encoding is terribly inefficient for storing, so we will store an index value (single integer) rather than 0’s and 1’s for the target value (<code class="markup--code markup--p-code">y</code>) [<a href="https://youtu.be/9C06ZPF8Uuc?t=1h31m21s" data-href="https://youtu.be/9C06ZPF8Uuc?t=1h31m21s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:31:21</a>]. If you look at the <code class="markup--code markup--p-code">y</code> values for the dog breeds competition, you won’t actually see a big lists of 1’s and 0's, but you will wee a single integer. And internally, PyTorch is converting the index to one-hot-encoded vector (even though you will literally never see it). PyTorch has different loss functions for ones that are one hot encoded and others that are not — but these details are hidden by the fast.ai library so you do not have to worry about it. But the cool thing to realize is that we are doing exactly the same thing for both single label classification and multi label classification.</p><p name="234a" id="234a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: Does it make sense to change the base of log for softmax?[<a href="https://youtu.be/9C06ZPF8Uuc?t=1h32m55s" data-href="https://youtu.be/9C06ZPF8Uuc?t=1h32m55s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">01:32:55</a>] No, changing the base is just a linear scaling which neural net can learn easily:</p><figure name="ee45" id="ee45" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_WUqrfSxhd4dBKSgknPWV6w.png"></figure><pre name="c330" id="c330" class="graf graf--pre graf-after--figure">plt.imshow(data.val_ds.denorm(to_np(x))[0]*1.4);</pre><figure name="825b" id="825b" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_--s7Fu1xm8tYWTGgcE93lg.png"></figure><ul class="postList"><li name="2d5e" id="2d5e" class="graf graf--li graf-after--figure"><code class="markup--code markup--li-code">*1.4</code>&nbsp;: The image was washed out, so making it more visible (“brightening it up a bit”). Images are just matrices of numbers, so we can do things like this.</li><li name="7a45" id="7a45" class="graf graf--li graf-after--li">It is good to experiment images like this because these images are not at all like ImageNet. The vast majority of things you do involving convolutional neural net will not actually be anything like ImageNet (medical imaging, classifying different kinds of steel tube, satellite images, etc)</li></ul><pre name="7f78" id="7f78" class="graf graf--pre graf-after--li">sz=64</pre><pre name="522d" id="522d" class="graf graf--pre graf-after--pre">data = get_data(sz)<br>data = data.resize(int(sz*1.3), 'tmp')</pre><ul class="postList"><li name="1fd2" id="1fd2" class="graf graf--li graf-after--pre">We will not use <code class="markup--code markup--li-code">sz=64</code> for cats and dogs competition because we started with pre-trained ImageNet network which starts off nearly perfect. If we re-trained the whole set with 64 by 64 images, we would destroy the weights that are already very good. Remember, most of ImageNet models are trained with 224 by 224 or 299 by 299 images.</li><li name="1cfa" id="1cfa" class="graf graf--li graf-after--li">There is no images in ImageNet that looks like the one above. And only the first couple layers are useful to us. So starting out with smaller images works well in this case.</li></ul><pre name="7735" id="7735" class="graf graf--pre graf-after--li">learn = ConvLearner.pretrained(f_model, data, metrics=metrics)</pre><pre name="9474" id="9474" class="graf graf--pre graf-after--pre">lrf=learn.lr_find() <br>learn.sched.plot()</pre><figure name="cc89" id="cc89" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_PyRr1RqxJkvTp0zX9xZkmA.png"></figure><pre name="dbf7" id="dbf7" class="graf graf--pre graf-after--figure">lr = 0.2<br>learn.fit(lr, 3, cycle_len=1, cycle_mult=2)</pre><pre name="bfb0" id="bfb0" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[ 0.       0.14882  0.13552  0.87878]                        <br>[ 1.       0.14237  0.13048  0.88251]                        <br>[ 2.       0.13675  0.12779  0.88796]                        <br>[ 3.       0.13528  0.12834  0.88419]                        <br>[ 4.       0.13428  0.12581  0.88879]                        <br>[ 5.       0.13237  0.12361  0.89141]                        <br>[ 6.       0.13179  0.12472  0.8896 ]</em></pre><pre name="de5e" id="de5e" class="graf graf--pre graf-after--pre">lrs = np.array(<strong class="markup--strong markup--pre-strong">[lr/9, lr/3, lr]</strong>)</pre><pre name="1691" id="1691" class="graf graf--pre graf-after--pre">learn.unfreeze()<br>learn.fit(lrs, 3, cycle_len=1, cycle_mult=2)</pre><pre name="bb87" id="bb87" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[ 0.       0.12534  0.10926  0.90892]                        <br>[ 1.       0.12035  0.10086  0.91635]                        <br>[ 2.       0.11001  0.09792  0.91894]                        <br>[ 3.       0.1144   0.09972  0.91748]                        <br>[ 4.       0.11055  0.09617  0.92016]                        <br>[ 5.       0.10348  0.0935   0.92267]                        <br>[ 6.       0.10502  0.09345  0.92281]</em></pre><ul class="postList"><li name="8505" id="8505" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">[lr/9, lr/3, lr]</code> — this is because the images are unlike ImageNet image and earlier layers are probably not as close to what they need to be.</li></ul><pre name="9411" id="9411" class="graf graf--pre graf-after--li">learn.sched.plot_loss()</pre><figure name="a3d1" id="a3d1" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_3fdnGg4PsQq3mpbM943fmw.png"></figure><pre name="63c9" id="63c9" class="graf graf--pre graf-after--figure"><strong class="markup--strong markup--pre-strong">sz = 128</strong><br>learn.set_data(get_data(sz))<br>learn.freeze()<br>learn.fit(lr, 3, cycle_len=1, cycle_mult=2)</pre><pre name="210a" id="210a" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[ 0.       0.09729  0.09375  0.91885]                         <br>[ 1.       0.10118  0.09243  0.92075]                         <br>[ 2.       0.09805  0.09143  0.92235]                         <br>[ 3.       0.09834  0.09134  0.92263]                         <br>[ 4.       0.096    0.09046  0.9231 ]                         <br>[ 5.       0.09584  0.09035  0.92403]                         <br>[ 6.       0.09262  0.09059  0.92358]</em></pre><pre name="b14a" id="b14a" class="graf graf--pre graf-after--pre">learn.unfreeze()<br>learn.fit(lrs, 3, cycle_len=1, cycle_mult=2)<br>learn.save(f'{sz}')</pre><pre name="c7e6" id="c7e6" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[ 0.       0.09623  0.08693  0.92696]                         <br>[ 1.       0.09371  0.08621  0.92887]                         <br>[ 2.       0.08919  0.08296  0.93113]                         <br>[ 3.       0.09221  0.08579  0.92709]                         <br>[ 4.       0.08994  0.08575  0.92862]                         <br>[ 5.       0.08729  0.08248  0.93108]                         <br>[ 6.       0.08218  0.08315  0.92971]</em></pre><pre name="a7ae" id="a7ae" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">sz = 256</strong><br>learn.set_data(get_data(sz))<br>learn.freeze()<br>learn.fit(lr, 3, cycle_len=1, cycle_mult=2)</pre><pre name="6203" id="6203" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[ 0.       0.09161  0.08651  0.92712]                         <br>[ 1.       0.08933  0.08665  0.92677]                         <br>[ 2.       0.09125  0.08584  0.92719]                         <br>[ 3.       0.08732  0.08532  0.92812]                         <br>[ 4.       0.08736  0.08479  0.92854]                         <br>[ 5.       0.08807  0.08471  0.92835]                         <br>[ 6.       0.08942  0.08448  0.9289 ]</em></pre><pre name="5094" id="5094" class="graf graf--pre graf-after--pre">learn.unfreeze()<br>learn.fit(lrs, 3, cycle_len=1, cycle_mult=2)<br>learn.save(f'{sz}')</pre><pre name="397e" id="397e" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[ 0.       0.08932  0.08218  0.9324 ]                         <br>[ 1.       0.08654  0.08195  0.93313]                         <br>[ 2.       0.08468  0.08024  0.93391]                         <br>[ 3.       0.08596  0.08141  0.93287]                         <br>[ 4.       0.08211  0.08152  0.93401]                         <br>[ 5.       0.07971  0.08001  0.93377]                         <br>[ 6.       0.07928  0.0792   0.93554]</em></pre><pre name="e1ab" id="e1ab" class="graf graf--pre graf-after--pre">log_preds,y = learn.TTA()<br>preds = np.mean(np.exp(log_preds),0)<br>f2(preds,y)</pre><pre name="9906" id="9906" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">0.93626519738612801</em></pre><p name="e7fd" id="e7fd" class="graf graf--p graf-after--pre">A couple of questions people have asked what this does [<a href="https://youtu.be/9C06ZPF8Uuc?t=1h38m46s" data-href="https://youtu.be/9C06ZPF8Uuc?t=1h38m46s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">01:38:46</a>]:</p><pre name="3cbe" id="3cbe" class="graf graf--pre graf-after--p">data = data.resize(int(sz*1.3), 'tmp')</pre><p name="f7df" id="f7df" class="graf graf--p graf-after--pre">When we specify what transforms to apply, we send a size:</p><pre name="8438" id="8438" class="graf graf--pre graf-after--p">tfms = tfms_from_model(f_model, sz,<br>        aug_tfms=transforms_top_down, max_zoom=1.05)</pre><p name="2090" id="2090" class="graf graf--p graf-after--pre">One of the things the data loader does is to resize the images on-demand. This has nothing to do with <code class="markup--code markup--p-code">data.resize</code>&nbsp;. If the initial image is 1000 by 1000, reading that JPEG and resizing it to 64 by 64 take more time than training the convolutional net. <code class="markup--code markup--p-code">data.resize</code> tells it that we will not use images bigger than <code class="markup--code markup--p-code">sz*1.3</code> so go through once and create new JPEGs of this size. Since images are rectangular, so new JPEGs whose smallest edge is <code class="markup--code markup--p-code">sz*1.3</code> (center-cropped). It will save you a lot of time.</p><pre name="763e" id="763e" class="graf graf--pre graf-after--p">metrics=[f2]</pre><p name="1afa" id="1afa" class="graf graf--p graf-after--pre">Instead of <code class="markup--code markup--p-code">accuacy</code>, we used <a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html" data-href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">F-beta</a> for this notebook — it is a way of weighing false negatives and false positives. The reason we are using it is because this particular Kaggle competition wants to use it. Take a look at <a href="https://github.com/fastai/fastai/blob/master/courses/dl1/planet.py" data-href="https://github.com/fastai/fastai/blob/master/courses/dl1/planet.py" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">planet.py</a> to see how you can create your own metrics function. This is what gets printed out at the end <code class="markup--code markup--p-code u-paddingRight0 u-marginRight0">[ 0. 0.08932 0.08218 <strong class="markup--strong markup--p-strong">0.9324 </strong>]</code></p><h4 name="ca7b" id="ca7b" class="graf graf--h4 graf-after--p">Activation function for multi-label classification [<a href="https://youtu.be/9C06ZPF8Uuc?t=1h44m25s" data-href="https://youtu.be/9C06ZPF8Uuc?t=1h44m25s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">01:44:25</a>]</h4><p name="46bb" id="46bb" class="graf graf--p graf-after--h4">Activation function for multi-label classification is called <strong class="markup--strong markup--p-strong">sigmoid.</strong></p><figure name="24db" id="24db" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_j7dLkIwvXr6bs6MUzFaN2w.png"></figure><figure name="9759" id="9759" class="graf graf--figure graf-after--figure"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_p8VFZrnPgWgVpUf62ZPuuA.png"></figure><p name="de3f" id="de3f" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Question</strong>: Why don’t we start training with differential learning rate rather than training the last layers alone? [<a href="https://youtu.be/9C06ZPF8Uuc?t=1h50m30s" data-href="https://youtu.be/9C06ZPF8Uuc?t=1h50m30s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">01:50:30</a>]</p><figure name="0c18" id="0c18" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_2Ocl12SOFKZ75iV4bqg-OQ.png"></figure><p name="e62f" id="e62f" class="graf graf--p graf-after--figure">You can skip training just the last layer and go straight to differential learning rates, but you probably do not want to. Convolutional layers all contain pre-trained weights, so they are not random — for things that are close to ImageNet, they are really good; for things that are not close to ImageNet, they are better than nothing. All of our fully connected layers, however, are totally random. Therefore, you would always want to make the fully connected weights better than random by training them a bit first. Otherwise if you go straight to unfreeze, then you are actually going to be fiddling around with those early layer weights when the later ones are still random — which is probably not what you want.</p><p name="f364" id="f364" class="graf graf--p graf-after--p">Question: When you use the differential learning rates, do those three learning rates spread evenly across the layers? [<a href="https://youtu.be/9C06ZPF8Uuc?t=1h55m35s" data-href="https://youtu.be/9C06ZPF8Uuc?t=1h55m35s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">01:55:35</a>] We will talk more about this later in the course but the fast.ai library, there is a concept of “layer groups”. In something like ResNet50, there are hundreds of layers and you probably do not want to write hundreds of learning rates, so the library decided for you how to split them and the last one always refers to just the fully connected layers that we have randomly initialized and added.</p><h4 name="b63f" id="b63f" class="graf graf--h4 graf-after--p">Visualizing the layers [<a href="https://youtu.be/9C06ZPF8Uuc?t=1h56m42s" data-href="https://youtu.be/9C06ZPF8Uuc?t=1h56m42s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">01:56:42</a>]</h4><pre name="fa9a" id="fa9a" class="graf graf--pre graf-after--h4">learn.summary()</pre><pre name="44a0" id="44a0" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[('Conv2d-1',<br>  OrderedDict([('input_shape', [-1, 3, 64, 64]),<br>               ('output_shape', [-1, 64, 32, 32]),<br>               ('trainable', False),<br>               ('nb_params', 9408)])),<br> ('BatchNorm2d-2',<br>  OrderedDict([('input_shape', [-1, 64, 32, 32]),<br>               ('output_shape', [-1, 64, 32, 32]),<br>               ('trainable', False),<br>               ('nb_params', 128)])),<br> ('ReLU-3',<br>  OrderedDict([('input_shape', [-1, 64, 32, 32]),<br>               ('output_shape', [-1, 64, 32, 32]),<br>               ('nb_params', 0)])),<br> ('MaxPool2d-4',<br>  OrderedDict([('input_shape', [-1, 64, 32, 32]),<br>               ('output_shape', [-1, 64, 16, 16]),<br>               ('nb_params', 0)])),<br> ('Conv2d-5',<br>  OrderedDict([('input_shape', [-1, 64, 16, 16]),<br>               ('output_shape', [-1, 64, 16, 16]),<br>               ('trainable', False),<br>               ('nb_params', 36864)]))</em><br> ...</pre><ul class="postList"><li name="3dc7" id="3dc7" class="graf graf--li graf--startsWithSingleQuote graf-after--pre"><code class="markup--code markup--li-code u-paddingRight0 u-marginRight0">‘input_shape’, [-1, <strong class="markup--strong markup--li-strong">3, 64, 64</strong>]</code> — PyTorch lists channel before the image size. Some of the GPU computations run faster when it is in that order. This is done behind scene by the transformation step.</li><li name="d0fd" id="d0fd" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">-1</code>&nbsp;: indicates however big the batch size is. Keras uses <code class="markup--code markup--li-code">None</code>&nbsp;.</li><li name="d1c7" id="d1c7" class="graf graf--li graf--startsWithSingleQuote graf-after--li"><code class="markup--code markup--li-code">‘output_shape’, [-1, 64, 32, 32]</code> — 64 is the number of kernels</li></ul><p name="ae4f" id="ae4f" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">Question</strong>: Learning rate finder for a very small dataset returned strange number and the plot was empty [<a href="https://youtu.be/9C06ZPF8Uuc?t=1h58m57s" data-href="https://youtu.be/9C06ZPF8Uuc?t=1h58m57s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">01:58:57</a>] — The learning rate finder will go through a mini-batch at a time. If you have a tiny dataset, there is just not enough mini-batches. So the trick is to make your batch size very small like 4 or 8.</p><h3 name="afc0" id="afc0" class="graf graf--h3 graf-after--p">Structured Data [<a href="https://youtu.be/9C06ZPF8Uuc?t=1h59m48s" data-href="https://youtu.be/9C06ZPF8Uuc?t=1h59m48s" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">01:59:48</a>]</h3><p name="c697" id="c697" class="graf graf--p graf-after--h3">There are two types of dataset we use in machine learning:</p><ul class="postList"><li name="6452" id="6452" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Unstructured</strong> — Audio, images, natural language text where all of the things inside an object are all the same kind of things — pixels, amplitude of waveform, or words.</li><li name="9c54" id="9c54" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Structured </strong>— Profit and loss statement, information about a Facebook user where each column is structurally quite different. “Structured” refers to columnar data as you might find in a database or a spreadsheet where different columns represent different kinds of things, and each row represents an observation.</li></ul><p name="52af" id="52af" class="graf graf--p graf-after--li">Structured data is often ignored in academics because it is pretty hard to get published in fancy conference proceedings if you have a better logistics model. But it is the thing that makes the world goes round, makes everybody money and efficiency. We will not ignore it because we are doing practical deep learning, and Kaggle does not either because people put prize money up on Kaggle to solve real-world problems:</p><ul class="postList"><li name="8027" id="8027" class="graf graf--li graf-after--p"><a href="https://www.kaggle.com/c/favorita-grocery-sales-forecasting" data-href="https://www.kaggle.com/c/favorita-grocery-sales-forecasting" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">Corporación Favorita Grocery Sales Forecasting</a> — which is currently running</li><li name="a3cb" id="a3cb" class="graf graf--li graf-after--li"><a href="https://www.kaggle.com/c/rossmann-store-sales" data-href="https://www.kaggle.com/c/rossmann-store-sales" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">Rossmann Store Sales</a> — almost identical to above but completed competition.</li></ul><h4 name="4620" id="4620" class="graf graf--h4 graf-after--li">Rossmann Store Sale [<a href="https://youtu.be/9C06ZPF8Uuc?t=2h2m42s" data-href="https://youtu.be/9C06ZPF8Uuc?t=2h2m42s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">02:02:42</a>]</h4><p name="3e4b" id="3e4b" class="graf graf--p graf-after--h4"><a href="https://github.com/fastai/fastai/blob/master/courses/dl1/lesson3-rossman.ipynb" data-href="https://github.com/fastai/fastai/blob/master/courses/dl1/lesson3-rossman.ipynb" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Notebook</a></p><pre name="9081" id="9081" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">fastai.structured</strong> <strong class="markup--strong markup--pre-strong">import</strong> *<br><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">fastai.column_data</strong> <strong class="markup--strong markup--pre-strong">import</strong> *<br>np.set_printoptions(threshold=50, edgeitems=20)<br><br>PATH='data/rossmann/'</pre><ul class="postList"><li name="2ad5" id="2ad5" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">fastai.structured</code> — not PyTorch specific and also used in machine learning course doing random forests with no PyTorch at all. It can used on its own without any of the other parts of Fast.ai library.</li><li name="ece5" id="ece5" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">fastai.column_data</code> — allows us to do Fast.ai and PyTorch stuff with columnar structured data.</li><li name="bff2" id="bff2" class="graf graf--li graf-after--li">For structured data need to use <strong class="markup--strong markup--li-strong">Pandas</strong> a lot. Pandas is an attempt to replicate R’s data frames in Python (If you are not familiar with Pandas, here is a good book — <a href="http://shop.oreilly.com/product/0636920050896.do" data-href="http://shop.oreilly.com/product/0636920050896.do" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">Python for Data Analysis, 2nd Edition</a>)</li></ul><p name="5ba3" id="5ba3" class="graf graf--p graf-after--li">There are a lot of data pre-processing This notebook contains the entire pipeline from the third place winner (<a href="https://arxiv.org/abs/1604.06737" data-href="https://arxiv.org/abs/1604.06737" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Entity Embeddings of Categorical Variables</a>). Data processing is not covered in this course, but is covered in machine learning course in some detail because feature engineering is very important.</p><h4 name="3587" id="3587" class="graf graf--h4 graf-after--p">Looking at CSV&nbsp;files</h4><pre name="55cc" id="55cc" class="graf graf--pre graf-after--h4">table_names = ['train', 'store', 'store_states', 'state_names', <br>               'googletrend', 'weather', 'test']</pre><pre name="8e80" id="8e80" class="graf graf--pre graf-after--pre">tables = [pd.read_csv(f'{PATH}{fname}.csv', low_memory=False) for fname in table_names]</pre><pre name="38b2" id="38b2" class="graf graf--pre graf-after--pre">for t in tables: display(t.head())</pre><figure name="066b" id="066b" class="graf graf--figure graf--layoutOutsetCenter graf-after--pre" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_v0D2IcWqBOVyhGRo8Mv1gA.png"></figure><ul class="postList"><li name="e10d" id="e10d" class="graf graf--li graf-after--figure"><code class="markup--code markup--li-code">StoreType</code> — you often get datasets where some columns contain “code”. It really does not matter what the code means. Stay away from learning too much about it and see what the data says first.</li></ul><h4 name="de85" id="de85" class="graf graf--h4 graf-after--li">Joining tables</h4><p name="fc8d" id="fc8d" class="graf graf--p graf-after--h4">This is a relational dataset, and you have join quite a few tables together — which is easy to do with Pandas’ <code class="markup--code markup--p-code">merge</code>:</p><pre name="6107" id="6107" class="graf graf--pre graf-after--p">def join_df(left, right, left_on, right_on=None, suffix='_y'):<br>    if right_on is None: right_on = left_on<br>    <br>    return left.<strong class="markup--strong markup--pre-strong">merge</strong>(right, how='left', left_on=left_on,<br>        right_on=right_on, suffixes=("", suffix))</pre><p name="0d7c" id="0d7c" class="graf graf--p graf-after--pre">From Fast.ai library:</p><pre name="ef5d" id="ef5d" class="graf graf--pre graf-after--p">add_datepart(train, "Date", drop=False)</pre><ul class="postList"><li name="4e25" id="4e25" class="graf graf--li graf-after--pre">Take a date and pull out a bunch of columns such as “day of week”, “start of a quarter”, “month of year” and so on and add them all to the dataset.</li><li name="2183" id="2183" class="graf graf--li graf-after--li">Duration section will calculate things like how long until the next holiday, how long it has been since the last holiday, etc.</li></ul><pre name="bae1" id="bae1" class="graf graf--pre graf-after--li">joined.to_feather(f'{PATH}joined')</pre><ul class="postList"><li name="4daf" id="4daf" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">to_feather</code>&nbsp;: Saves a Pandas’ data frame into a “feather” format which takes it as it sits in RAM and dumps it to the disk. So it is really really fast. Ecuadorian grocery competition has 350 million records, so you will care about how long it takes to save.</li></ul><h4 name="8e28" id="8e28" class="graf graf--h4 graf-after--li">Next week</h4><ul class="postList"><li name="3c31" id="3c31" class="graf graf--li graf-after--h4">split columns into two types: categorical and continuous. Categorical column will be represented as one hot encoding, and continuous column gets fed into fully connected layer as is.</li><li name="83d6" id="83d6" class="graf graf--li graf-after--li">categorical: store #1 and store #2 are not numerically related to each other. Similarly, day of week Monday (day 0) and Tuesday (day 1).</li><li name="e3c3" id="e3c3" class="graf graf--li graf-after--li">continuous: Things like distance in kilometers to the nearest competitor is a number we treat numerically.</li><li name="cf04" id="cf04" class="graf graf--li graf-after--li graf--trailing"><code class="markup--code markup--li-code">ColumnarModelData</code></li></ul><hr class="section-divider"><p name="1b53" id="1b53" class="graf graf--p graf--leading graf--trailing">Lessons: <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" class="markup--anchor markup--p-anchor" target="_blank">1</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" class="markup--anchor markup--p-anchor" target="_blank">2</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" class="markup--anchor markup--p-anchor" target="_blank"><strong class="markup--strong markup--p-strong">3</strong></a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" class="markup--anchor markup--p-anchor" target="_blank">4</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" class="markup--anchor markup--p-anchor" target="_blank">5</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" class="markup--anchor markup--p-anchor" target="_blank">6</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" class="markup--anchor markup--p-anchor" target="_blank">7</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" class="markup--anchor markup--p-anchor" target="_blank">8</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" class="markup--anchor markup--p-anchor" target="_blank">9</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" class="markup--anchor markup--p-anchor" target="_blank">10</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" class="markup--anchor markup--p-anchor" target="_blank">11</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" class="markup--anchor markup--p-anchor" target="_blank">12</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" class="markup--anchor markup--p-anchor" target="_blank">13</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" class="markup--anchor markup--p-anchor" target="_blank">14</a></p></body></html>