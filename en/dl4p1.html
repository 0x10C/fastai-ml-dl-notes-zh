
<!-- saved from url=(0063)file:///C:/Users/asus/Desktop/fastai-ml-dl-notes-zh/en/dl4.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></head><body><hr class="section-divider"><h1 name="91ea" id="91ea" class="graf graf--h3 graf--leading graf--title">Deep Learning 2: Part 1 Lesson&nbsp;4</h1><p name="2560" id="2560" class="graf graf--p graf-after--h3"><em class="markup--em markup--p-em">My personal notes from </em><a href="http://www.fast.ai/" data-href="http://www.fast.ai/" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener nofollow noopener" target="_blank"><em class="markup--em markup--p-em">fast.ai course</em></a><em class="markup--em markup--p-em">. These notes will continue to be updated and improved as I continue to review the course to “really” understand it. Much appreciation to </em><a href="https://twitter.com/jeremyphoward" data-href="https://twitter.com/jeremyphoward" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener nofollow noopener" target="_blank"><em class="markup--em markup--p-em">Jeremy</em></a><em class="markup--em markup--p-em"> and </em><a href="https://twitter.com/math_rachel" data-href="https://twitter.com/math_rachel" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener nofollow noopener" target="_blank"><em class="markup--em markup--p-em">Rachel</em></a><em class="markup--em markup--p-em"> who gave me this opportunity to learn.</em></p><p name="5056" id="5056" class="graf graf--p graf-after--p graf--trailing">Lessons: <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" class="markup--anchor markup--p-anchor" target="_blank">1</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" class="markup--anchor markup--p-anchor" target="_blank">2</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" class="markup--anchor markup--p-anchor" target="_blank">3</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" class="markup--anchor markup--p-anchor" target="_blank"><strong class="markup--strong markup--p-strong">4</strong></a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" class="markup--anchor markup--p-anchor" target="_blank">5</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" class="markup--anchor markup--p-anchor" target="_blank">6</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" class="markup--anchor markup--p-anchor" target="_blank">7</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" class="markup--anchor markup--p-anchor" target="_blank">8</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" class="markup--anchor markup--p-anchor" target="_blank">9</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" class="markup--anchor markup--p-anchor" target="_blank">10</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" class="markup--anchor markup--p-anchor" target="_blank">11</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" class="markup--anchor markup--p-anchor" target="_blank">12</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" class="markup--anchor markup--p-anchor" target="_blank">13</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" class="markup--anchor markup--p-anchor" target="_blank">14</a></p><hr class="section-divider"><h3 name="f1df" id="f1df" class="graf graf--h3 graf--leading"><a href="http://forums.fast.ai/t/wiki-lesson-4/9402/1" data-href="http://forums.fast.ai/t/wiki-lesson-4/9402/1" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">Lesson 4</a></h3><p name="2cf5" id="2cf5" class="graf graf--p graf-after--h3">Articles by students:</p><ul class="postList"><li name="b371" id="b371" class="graf graf--li graf-after--p"><a href="https://techburst.io/improving-the-way-we-work-with-learning-rate-5e99554f163b" data-href="https://techburst.io/improving-the-way-we-work-with-learning-rate-5e99554f163b" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">Improving the way we work with learning rate</a></li><li name="f222" id="f222" class="graf graf--li graf-after--li"><a href="http://teleported.in/posts/cyclic-learning-rate/" data-href="http://teleported.in/posts/cyclic-learning-rate/" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">The Cyclical Learning Rate technique</a></li><li name="747d" id="747d" class="graf graf--li graf-after--li"><a href="https://medium.com/38th-street-studios/exploring-stochastic-gradient-descent-with-restarts-sgdr-fa206c38a74e" data-href="https://medium.com/38th-street-studios/exploring-stochastic-gradient-descent-with-restarts-sgdr-fa206c38a74e" class="markup--anchor markup--li-anchor" target="_blank">Exploring Stochastic Gradient Descent with Restarts (SGDR)</a></li><li name="4395" id="4395" class="graf graf--li graf-after--li"><a href="https://towardsdatascience.com/transfer-learning-using-differential-learning-rates-638455797f00" data-href="https://towardsdatascience.com/transfer-learning-using-differential-learning-rates-638455797f00" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">Transfer Learning using differential learning rates</a></li><li name="5b26" id="5b26" class="graf graf--li graf-after--li"><a href="https://medium.com/@ArjunRajkumar/getting-computers-to-see-better-than-humans-346d96634f73" data-href="https://medium.com/@ArjunRajkumar/getting-computers-to-see-better-than-humans-346d96634f73" class="markup--anchor markup--li-anchor" target="_blank">Getting Computers To See Better Than Humans</a></li></ul><figure name="f89a" id="f89a" class="graf graf--figure graf-after--li"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_D0WqPCX7RfOL47TOEfkzYg.png"></figure><h4 name="a058" id="a058" class="graf graf--h4 graf-after--figure">Dropout [04:59]</h4><pre name="95c4" id="95c4" class="graf graf--pre graf-after--h4">learn = ConvLearner.pretrained(arch, data, ps=0.5, precompute=True)</pre><ul class="postList"><li name="eb0e" id="eb0e" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">precompute=True</code>&nbsp;: Pre-compute the activations that come out of the last convolutional layer. Remember, activation is a number that is calculated based on some weights/parameter that makes up kernels/filters, and they get applied to the previous layer’s activations or inputs.</li></ul><pre name="6718" id="6718" class="graf graf--pre graf-after--li">learn </pre><pre name="018c" id="018c" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">Sequential(<br>  (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True)<br>  (1): Dropout(p=0.5)<br>  (2): Linear(in_features=1024, out_features=512)<br>  (3): ReLU()<br>  (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True)<br>  (5): Dropout(p=0.5)<br>  (6): Linear(in_features=512, out_features=120)<br>  (7): LogSoftmax()<br>)</em></pre><p name="dfcb" id="dfcb" class="graf graf--p graf-after--pre"><code class="markup--code markup--p-code">learn</code> — This will display the layers we added at the end. These are the layers we train when <code class="markup--code markup--p-code">precompute=True</code></p><p name="d953" id="d953" class="graf graf--p graf-after--p">(0), (4): <code class="markup--code markup--p-code">BatchNorm</code> will be covered in the last lesson</p><p name="a154" id="a154" class="graf graf--p graf-after--p">(1), (5): <code class="markup--code markup--p-code">Dropout</code></p><p name="1e90" id="1e90" class="graf graf--p graf-after--p">(2):<code class="markup--code markup--p-code">Linear</code> layer simply means a matrix multiply. This is a matrix which has 1024 rows and 512 columns, so it will take in 1024 activations and spit out 512 activations.</p><p name="8a1d" id="8a1d" class="graf graf--p graf-after--p">(3):<code class="markup--code markup--p-code">ReLU</code> — just replace negatives with zero</p><p name="7888" id="7888" class="graf graf--p graf-after--p">(6): <code class="markup--code markup--p-code">Linear</code> — the second linear layer that takes those 512 activations from the previous linear layer and put them through a new matrix multiply 512 by 120 and outputs 120 activations</p><p name="b57e" id="b57e" class="graf graf--p graf-after--p">(7): <code class="markup--code markup--p-code">Softmax</code> — The activation function that returns numbers that adds up to 1 and each of them is between 0 and 1:</p><figure name="b01a" id="b01a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*PNRoFZeNc0DfGyqsq-S7sA.png" data-width="150" data-height="48" src="../img/1_PNRoFZeNc0DfGyqsq-S7sA.png"></figure><p name="c563" id="c563" class="graf graf--p graf-after--figure">For minor numerical precision reasons, it turns out to be better to tahe the log of the softmax than softmax directly [<a href="https://youtu.be/gbceqO8PpBg?t=15m3s" data-href="https://youtu.be/gbceqO8PpBg?t=15m3s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">15:03</a>]. That is why when we get predictions out of our models, we have to do <code class="markup--code markup--p-code">np.exp(log_preds)</code>.</p><h4 name="fa77" id="fa77" class="graf graf--h4 graf-after--p">What is <code class="markup--code markup--h4-code">Dropout</code> and what is <code class="markup--code markup--h4-code">p</code>?&nbsp;[<a href="https://youtu.be/gbceqO8PpBg?t=8m17s" data-href="https://youtu.be/gbceqO8PpBg?t=8m17s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">08:17</a>]</h4><pre name="c4f7" id="c4f7" class="graf graf--pre graf-after--h4"><em class="markup--em markup--pre-em">Dropout(p=0.5)</em></pre><figure name="efb5" id="efb5" class="graf graf--figure graf--layoutOutsetCenter graf-after--pre" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_iF4XC8gg608IUouSRI5VrA.png"></figure><p name="a271" id="a271" class="graf graf--p graf-after--figure">If we applied dropout with <code class="markup--code markup--p-code">p=0.5</code> to <code class="markup--code markup--p-code">Conv2</code> layer, it would look like the above. We go through, pick an activation, and delete it with 50% chance. So <code class="markup--code markup--p-code">p=0.5</code> is the probability of deleting that cell. Output does not actually change by very much, just a little bit.</p><p name="4653" id="4653" class="graf graf--p graf-after--p">Randomly throwing away half of the activations in a layer has an interesting effect. An important thing to note is for each mini-batch, we throw away a different random half of activations in that layer. It forces it to not overfit. In other words, when a particular activation that learned just that exact dog or exact cat gets dropped out, the model has to try and find a representation that continues to work even as random half of the activations get thrown away every time.</p><p name="ad4e" id="ad4e" class="graf graf--p graf-after--p">This has been absolutely critical in making modern deep learning work and just about solve the problem of generalization. Geoffrey Hinton and his colleagues came up with this idea loosely inspired by the way the brain works.</p><ul class="postList"><li name="12c1" id="12c1" class="graf graf--li graf-after--p"><code class="markup--code markup--li-code">p=0.01</code> will throw away 1% of the activations. It will not change things up very much at all, and will not prevent overfitting (not generalized).</li><li name="248f" id="248f" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">p=0.99</code> will throw away 99% of the activations. Not going to overfit and great for generalization, but will kill your accuracy.</li><li name="f436" id="f436" class="graf graf--li graf-after--li">By default, the first layer is <code class="markup--code markup--li-code">0.25</code> and second layer is <code class="markup--code markup--li-code">0.5</code>[17:54]. If you find it is overfitting, start bumping it up — try setting all to <code class="markup--code markup--li-code">0.5</code>, still overfitting, try <code class="markup--code markup--li-code">0.7</code> etc. If you are under-fitting, you can try making it lower but is unlikely you would need to make it much lower.</li><li name="559a" id="559a" class="graf graf--li graf-after--li">ResNet34 has less parameters so it does not overfit as much, but for bigger architecture like ResNet50, you often need to increase dropout.</li></ul><p name="437c" id="437c" class="graf graf--p graf-after--li">Have you wondered why the validation losses better than the training losses particularly early in the training? [<a href="https://youtu.be/gbceqO8PpBg?t=12m32s" data-href="https://youtu.be/gbceqO8PpBg?t=12m32s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">12:32</a>] This is because we turn off dropout when we run inference (i.e. making prediction) on the validation set. We want to be using the best model we can.</p><p name="9af0" id="9af0" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: Do you have to do anything to accommodate for the fact that you are throwing away activations? [<a href="https://youtu.be/gbceqO8PpBg?t=13m26s" data-href="https://youtu.be/gbceqO8PpBg?t=13m26s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">13:26</a>] We do not, but PyTorch does two things when you say <code class="markup--code markup--p-code">p=0.5</code>. It throws away half of the activations, and it doubles all the activations that are already there so that average activation does not change.</p><p name="37b6" id="37b6" class="graf graf--p graf-after--p">In Fast.ai, you can pass in <code class="markup--code markup--p-code">ps</code> which is the <code class="markup--code markup--p-code">p</code> value for all of the added layers. It will not change the dropout in the pre-trained network since it should have been already trained with some appropriate level of dropout:</p><pre name="799b" id="799b" class="graf graf--pre graf-after--p">learn = ConvLearner.pretrained(arch, data, <strong class="markup--strong markup--pre-strong">ps=0.5</strong>, precompute=True)</pre><p name="b348" id="b348" class="graf graf--p graf-after--pre">You can remove dropout by setting <code class="markup--code markup--p-code">ps=0.</code> but even after a couple epochs, we start to massively overfit (training loss ≪ validation loss):</p><pre name="a819" id="a819" class="graf graf--pre graf-after--p">[2.      <strong class="markup--strong markup--pre-strong">0.3521</strong>   <strong class="markup--strong markup--pre-strong">0.55247</strong>  0.84189]</pre><p name="6e71" id="6e71" class="graf graf--p graf-after--pre">When <code class="markup--code markup--p-code">ps=0.</code>&nbsp;, dropout layers are not even added to the model:</p><pre name="28c5" id="28c5" class="graf graf--pre graf-after--p">Sequential(<br>  (0): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True)<br>  (1): Linear(in_features=4096, out_features=512)<br>  (2): ReLU()<br>  (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True)<br>  (4): Linear(in_features=512, out_features=120)<br>  (5): LogSoftmax()<br>)</pre><p name="29d2" id="29d2" class="graf graf--p graf-after--pre">You may have noticed, it has been adding two <code class="markup--code markup--p-code">Linear</code> layers [<a href="https://youtu.be/gbceqO8PpBg?t=16m19s" data-href="https://youtu.be/gbceqO8PpBg?t=16m19s" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">16:19</a>]. We do not have to do that. There is <code class="markup--code markup--p-code">xtra_fc</code> parameter you can set. Note: you do need at least one which takes the output of the convolutional layer (4096 in this example) and turns it into the number of classes (120 dog breeds):</p><pre name="b25f" id="b25f" class="graf graf--pre graf-after--p">learn = ConvLearner.pretrained(arch, data, ps=0., precompute=True, <br>            <strong class="markup--strong markup--pre-strong">xtra_fc=[]</strong>); learn </pre><pre name="b092" id="b092" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">Sequential(<br>  (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True)<br>  (1): Linear(in_features=1024, out_features=120)<br>  (2): LogSoftmax()<br>)</em></pre><pre name="2bb9" id="2bb9" class="graf graf--pre graf-after--pre">learn = ConvLearner.pretrained(arch, data, ps=0., precompute=True, <br>            <strong class="markup--strong markup--pre-strong">xtra_fc=[700, 300]</strong>); learn</pre><pre name="fdf2" id="fdf2" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">Sequential(<br>  (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True)<br>  (1): Linear(in_features=1024, out_features=</em><strong class="markup--strong markup--pre-strong"><em class="markup--em markup--pre-em">700</em></strong><em class="markup--em markup--pre-em">)<br>  (2): ReLU()<br>  (3): BatchNorm1d(700, eps=1e-05, momentum=0.1, affine=True)<br>  (4): Linear(in_features=700, out_features=</em><strong class="markup--strong markup--pre-strong"><em class="markup--em markup--pre-em">300</em></strong><em class="markup--em markup--pre-em">)<br>  (5): ReLU()<br>  (6): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True)<br>  (7): Linear(in_features=300, out_features=120)<br>  (8): LogSoftmax()<br>)</em></pre><p name="e998" id="e998" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">Question</strong>: Is there a particular way in which you can determine if it is overfitted? [<a href="https://youtu.be/gbceqO8PpBg?t=19m53s" data-href="https://youtu.be/gbceqO8PpBg?t=19m53s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">19:53</a>]. Yes, you can see the training loss is much lower than the validation loss. You cannot tell if it is <em class="markup--em markup--p-em">too</em> overfitted. Zero overfitting is not generally optimal. The only thing you are trying to do is to get the validation loss low, so you need to play around with a few things and see what makes the validation loss low. You will get a feel for it overtime for your particular problem what too much overfitting looks like.</p><p name="ac9f" id="ac9f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: Why does average activation matter? [<a href="https://youtu.be/gbceqO8PpBg?t=21m15s" data-href="https://youtu.be/gbceqO8PpBg?t=21m15s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">21:15</a>] If we just deleted a half of activations, the next activation who takes them as input will also get halved, and everything after that. For example, fluffy ears are fluffy if this is greater than 0.6, and now it is only fluffy if it is greater than 0.3 — which is changing the meaning. The goal here is delete activations without changing the meanings.</p><p name="a141" id="a141" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: Can we have different level of dropout by layer? [<a href="https://youtu.be/gbceqO8PpBg?t=22m41s" data-href="https://youtu.be/gbceqO8PpBg?t=22m41s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">22:41</a>] Yes, that is why it is called <code class="markup--code markup--p-code">ps</code>:</p><pre name="ab2b" id="ab2b" class="graf graf--pre graf-after--p">learn = ConvLearner.pretrained(arch, data, ps=[0., 0.2],<br>            precompute=True, xtra_fc=[512]); learn</pre><pre name="6364" id="6364" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">Sequential(<br>  (0): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True)<br>  (1): Linear(in_features=4096, out_features=512)<br>  (2): ReLU()<br>  (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True)<br>  (4): Dropout(p=0.2)<br>  (5): Linear(in_features=512, out_features=120)<br>  (6): LogSoftmax()<br>)</em></pre><ul class="postList"><li name="1935" id="1935" class="graf graf--li graf-after--pre">There is no rule of thumb for when earlier or later layers should have different amounts of dropout yet.</li><li name="52cc" id="52cc" class="graf graf--li graf-after--li">If in doubt, use the same dropout for every fully connected layer.</li><li name="bfa1" id="bfa1" class="graf graf--li graf-after--li">Often people only put dropout on the very last linear layer.</li></ul><p name="04a1" id="04a1" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">Question</strong>: Why monitor loss and not accuracy? [<a href="https://youtu.be/gbceqO8PpBg?t=23m53s" data-href="https://youtu.be/gbceqO8PpBg?t=23m53s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">23:53</a>] Loss is the only thing that we can see for both the validation set and the training set. As we learn later, the loss is the thing that we are actually optimizing so it is easier to monitor and understand what that means.</p><p name="3c27" id="3c27" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: Do we need to adjust the learning rate after adding dropouts?[<a href="https://youtu.be/gbceqO8PpBg?t=24m33s" data-href="https://youtu.be/gbceqO8PpBg?t=24m33s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">24:33</a>] It does not seem to impact the learning rate enough to notice. In theory, it might but not enough to affect us.</p><h4 name="57ea" id="57ea" class="graf graf--h4 graf-after--p">Structured and Time Series Data&nbsp;[<a href="https://youtu.be/gbceqO8PpBg?t=25m3s" data-href="https://youtu.be/gbceqO8PpBg?t=25m3s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">25:03</a>]</h4><p name="13a3" id="13a3" class="graf graf--p graf-after--h4"><a href="https://github.com/fastai/fastai/blob/master/courses/dl1/lesson3-rossman.ipynb" data-href="https://github.com/fastai/fastai/blob/master/courses/dl1/lesson3-rossman.ipynb" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Notebook</a> / <a href="https://www.kaggle.com/c/rossmann-store-sales" data-href="https://www.kaggle.com/c/rossmann-store-sales" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Kaggle</a></p><figure name="9076" id="9076" class="graf graf--figure graf--layoutOutsetCenter graf-after--p" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_-yc7uZaE44dDVOB850I9-A.png"></figure><p name="6085" id="6085" class="graf graf--p graf-after--figure">There are two types of columns:</p><ul class="postList"><li name="1504" id="1504" class="graf graf--li graf-after--p">Categorical — It has a number of “levels” e.g. StoreType, Assortment</li><li name="a6de" id="a6de" class="graf graf--li graf-after--li">Continuous — It has a number where differences or ratios of that numbers have some kind of meanings e.g. CompetitionDistance</li></ul><pre name="d667" id="d667" class="graf graf--pre graf-after--li">cat_vars = ['Store', 'DayOfWeek', 'Year', 'Month', 'Day',<br>            'StateHoliday', 'CompetitionMonthsOpen', 'Promo2Weeks',<br>            'StoreType', 'Assortment', 'PromoInterval', <br>            'CompetitionOpenSinceYear', 'Promo2SinceYear', 'State',<br>            'Week', 'Events', 'Promo_fw', 'Promo_bw', <br>            'StateHoliday_fw', 'StateHoliday_bw', <br>            'SchoolHoliday_fw', 'SchoolHoliday_bw']</pre><pre name="496a" id="496a" class="graf graf--pre graf-after--pre">contin_vars = ['CompetitionDistance', 'Max_TemperatureC', <br>               'Mean_TemperatureC', 'Min_TemperatureC', <br>               'Max_Humidity', 'Mean_Humidity', 'Min_Humidity', <br>               'Max_Wind_SpeedKm_h', 'Mean_Wind_SpeedKm_h', <br>               'CloudCover', 'trend', 'trend_DE', <br>               'AfterStateHoliday', 'BeforeStateHoliday', 'Promo', <br>               'SchoolHoliday']</pre><pre name="995b" id="995b" class="graf graf--pre graf-after--pre">n = len(joined); n</pre><ul class="postList"><li name="236a" id="236a" class="graf graf--li graf-after--pre">Numbers like <code class="markup--code markup--li-code">Year</code>&nbsp;, <code class="markup--code markup--li-code">Month</code>, although we could treat them as continuous, we do not have to. If we decide to make <code class="markup--code markup--li-code">Year</code> a categorical variable, we are telling our neural net that for every different “level”of <code class="markup--code markup--li-code">Year</code> (2000, 2001, 2002), you can treat it totally differently; where-else if we say it is continuous, it has to come up with some kind of smooth function to fit them. So often things that actually are continuous but do not have many distinct levels (e.g. <code class="markup--code markup--li-code">Year</code>, <code class="markup--code markup--li-code">DayOfWeek</code>), it often works better to treat them as categorical.</li><li name="8bca" id="8bca" class="graf graf--li graf-after--li">Choosing categorical vs. continuous variable is a modeling decision you get to make. In summary, if it is categorical in the data, it has to be categorical. If it is continuous in the data, you get to pick whether to make it continuous or categorical in the model.</li><li name="1daa" id="1daa" class="graf graf--li graf-after--li">Generally, floating point numbers are hard to make categorical as there are many levels (we call number of levels “<strong class="markup--strong markup--li-strong">Cardinality</strong>” — e.g. the cardinality of the day of week variable is 7).</li></ul><p name="3cfa" id="3cfa" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">Question</strong>: Do you ever <em class="markup--em markup--p-em">bin</em> continuous variables?[<a href="https://youtu.be/gbceqO8PpBg?t=31m2s" data-href="https://youtu.be/gbceqO8PpBg?t=31m2s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">31:02</a>] Jeremy does not bin variables but one thing we could do with, say max temperature, is to group into 0–10, 10–20, 20–30, and call that categorical. Interestingly, a paper just came out last week in which a group of researchers found that sometimes binning can be helpful.</p><p name="f58f" id="f58f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: If you are using year as a category, what happens when a model encounters a year it has never seen before? [<a href="https://youtu.be/gbceqO8PpBg?t=31m47s" data-href="https://youtu.be/gbceqO8PpBg?t=31m47s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">31:47</a>] We will get there, but the short answer is that it will be treated as an unknown category. Pandas has a special category called unknown and if it sees a category it has not seen before, it gets treated as unknown.</p><pre name="6afe" id="6afe" class="graf graf--pre graf-after--p">for v in cat_vars: <br>    joined[v] = joined[v].astype('category').cat.as_ordered()</pre><pre name="7134" id="7134" class="graf graf--pre graf-after--pre">for v in contin_vars:<br>    joined[v] = joined[v].astype('float32')</pre><pre name="4d4c" id="4d4c" class="graf graf--pre graf-after--pre">dep = 'Sales'<br>joined = joined[cat_vars+contin_vars+[dep, 'Date']].copy()</pre><ul class="postList"><li name="5115" id="5115" class="graf graf--li graf-after--pre">Loop through <code class="markup--code markup--li-code">cat_vars</code> and turn applicable data frame columns into categorical columns.</li><li name="e18c" id="e18c" class="graf graf--li graf-after--li">Loop through <code class="markup--code markup--li-code">contin_vars</code> and set them as <code class="markup--code markup--li-code">float32</code> (32 bit floating point) because that is what PyTorch expects.</li></ul><h4 name="2ecb" id="2ecb" class="graf graf--h4 graf-after--li">Start with a small sample&nbsp;[<a href="https://youtu.be/gbceqO8PpBg?t=34m29s" data-href="https://youtu.be/gbceqO8PpBg?t=34m29s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">34:29</a>]</h4><pre name="92ec" id="92ec" class="graf graf--pre graf-after--h4">idxs = get_cv_idxs(n, val_pct=150000/n) <br>joined_samp = joined.iloc[idxs].set_index("Date") <br>samp_size = len(joined_samp); samp_size</pre><figure name="a9f6" id="a9f6" class="graf graf--figure graf--layoutOutsetCenter graf-after--pre" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_dHlXaLjRQSGyrG9pGkWMMQ.png"></figure><p name="6171" id="6171" class="graf graf--p graf-after--figure">Here is what our data looks like. Even though we set some of the columns as “category” (e.g. ‘StoreType’, ‘Year’), Pandas still display as string in the notebook.</p><pre name="a083" id="a083" class="graf graf--pre graf-after--p">df, y, nas, mapper = proc_df(joined_samp, 'Sales', do_scale=True)<br>yl = np.log(y)</pre><p name="3e0f" id="3e0f" class="graf graf--p graf-after--pre"><code class="markup--code markup--p-code">proc_df</code> (process data frame) — A function in Fast.ai that does a few things:</p><ol class="postList"><li name="60cd" id="60cd" class="graf graf--li graf-after--p">Pulls out the dependent variable, puts it into a separate variable, and deletes it from the original data frame. In other words, <code class="markup--code markup--li-code">df</code> do not have <code class="markup--code markup--li-code">Sales</code> column, and <code class="markup--code markup--li-code">y</code> only contains <code class="markup--code markup--li-code">Sales</code> column.</li><li name="96d7" id="96d7" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">do_scale</code>&nbsp;: Neural nets really like to have the input data to all be somewhere around zero with a standard deviation of somewhere around 1. So we take our data, subtract the mean, and divide by the standard deviation to make that happen. It returns a special object which keeps track of what mean and standard deviation it used for that normalization so you can do the same to the test set later (<code class="markup--code markup--li-code">mapper</code>).</li><li name="583c" id="583c" class="graf graf--li graf-after--li">It also handles missing values — for categorical variable, it becomes ID: 0 and other categories become 1, 2, 3, and so on. For continuous variable, it replaces the missing value with the median and create a new boolean column that says whether it was missing or not.</li></ol><figure name="df31" id="df31" class="graf graf--figure graf--layoutOutsetCenter graf-after--li" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_Zs6ASJF8iaAe3cduCmLYKw.png"></figure><p name="0b3c" id="0b3c" class="graf graf--p graf-after--figure">After processing, year 2014 for example becomes 2 since categorical variables have been replaced with contiguous integers starting at zero. The reason for that is, we are going to put them into a matrix later, and we would not want the matrix to be 2014 rows long when it could just be two rows.</p><p name="4ccc" id="4ccc" class="graf graf--p graf-after--p">Now we have a data frame which does not contain the dependent variable and where everything is a number. That is where we need to get to to do deep learning. Check out Machine Learning course on further details. Another thing that is covered in Machine Learning course is validation sets. In this case, we need to predict the next two weeks of sales therefore we should create a validation set which is the last two weeks of our training set:</p><pre name="80f3" id="80f3" class="graf graf--pre graf-after--p">val_idx = np.flatnonzero((df.index&lt;=datetime.datetime(2014,9,17)) &amp;<br>              (df.index&gt;=datetime.datetime(2014,8,1)))</pre><ul class="postList"><li name="16b9" id="16b9" class="graf graf--li graf-after--pre"><a href="http://www.fast.ai/2017/11/13/validation-sets/" data-href="http://www.fast.ai/2017/11/13/validation-sets/" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">How (and why) to create a good validation set</a></li></ul><h4 name="460a" id="460a" class="graf graf--h4 graf-after--li">Let’s get straight to the deep learning action&nbsp;[<a href="https://youtu.be/gbceqO8PpBg?t=39m48s" data-href="https://youtu.be/gbceqO8PpBg?t=39m48s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">39:48</a>]</h4><p name="c2b2" id="c2b2" class="graf graf--p graf-after--h4">For any Kaggle competitions, it is important that you have a strong understanding of your metric — how you are going to be judged. In <a href="https://www.kaggle.com/c/rossmann-store-sales#evaluation" data-href="https://www.kaggle.com/c/rossmann-store-sales#evaluation" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">this competition</a>, we are going to be judged on Root Mean Square Percentage Error (RMSPE).</p><figure name="60bc" id="60bc" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_a7mJ5VCeuAxagGrHOq6ekQ.png"></figure><pre name="8e3b" id="8e3b" class="graf graf--pre graf-after--figure">def inv_y(a): return np.exp(a)</pre><pre name="6820" id="6820" class="graf graf--pre graf-after--pre">def exp_rmspe(y_pred, targ):<br>    targ = inv_y(targ)<br>    pct_var = (targ - inv_y(y_pred))/targ<br>    return math.sqrt((pct_var**2).mean())</pre><pre name="464f" id="464f" class="graf graf--pre graf-after--pre">max_log_y = np.max(yl)<br>y_range = (0, max_log_y*1.2)</pre><ul class="postList"><li name="3ae5" id="3ae5" class="graf graf--li graf-after--pre">When you take the log of the data, getting the root mean squared error will actually get you the root mean square percentage error.</li></ul><pre name="4cdc" id="4cdc" class="graf graf--pre graf-after--li">md = <strong class="markup--strong markup--pre-strong">ColumnarModelData.from_data_frame</strong>(PATH, val_idx, df, <br>         yl.astype(np.float32), cat_flds=cat_vars, bs=128, <br>         test_df=df_test)</pre><ul class="postList"><li name="eba0" id="eba0" class="graf graf--li graf-after--pre">As per usual, we will start by creating model data object which has a validation set, training set, and optional test set built into it. From that, we will get a learner, we will then optionally call <code class="markup--code markup--li-code">lr_find</code>, then call <code class="markup--code markup--li-code">learn.fit</code> and so forth.</li><li name="00ee" id="00ee" class="graf graf--li graf-after--li">The difference here is we are not using <code class="markup--code markup--li-code">ImageClassifierData.from_csv</code> or&nbsp;<code class="markup--code markup--li-code">.from_paths</code>, we need a different kind of model data called <code class="markup--code markup--li-code">ColumnarModelData</code> and we call <code class="markup--code markup--li-code">from_data_frame</code>.</li><li name="9dfc" id="9dfc" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">PATH</code>&nbsp;: Specifies where to store model files etc</li><li name="9d93" id="9d93" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">val_idx</code>&nbsp;: A list of the indexes of the rows that we want to put in the validation set</li><li name="b318" id="b318" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">df</code>&nbsp;: data frame that contains independent variable</li><li name="5f8a" id="5f8a" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">yl</code>&nbsp;: We took the dependent variable <code class="markup--code markup--li-code">y</code> returned by <code class="markup--code markup--li-code">proc_df</code> and took the log of that (i.e. <code class="markup--code markup--li-code">np.log(y)</code>)</li><li name="6549" id="6549" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">cat_flds</code>&nbsp;: which columns to be treated as categorical. Remember, by this time, everything is a number, so unless we specify, it will treat them all as continuous.</li></ul><p name="2dab" id="2dab" class="graf graf--p graf-after--li">Now we have a standard model data object which we are familiar with and contains <code class="markup--code markup--p-code">train_dl</code>, <code class="markup--code markup--p-code">val_dl</code>&nbsp;, <code class="markup--code markup--p-code">train_ds</code>&nbsp;, <code class="markup--code markup--p-code">val_ds</code>&nbsp;, etc.</p><pre name="e09c" id="e09c" class="graf graf--pre graf-after--p">m = md.get_learner(emb_szs, len(df.columns)-len(cat_vars),<br>                   0.04, 1, [1000,500], [0.001,0.01], <br>                   y_range=y_range)</pre><ul class="postList"><li name="02a6" id="02a6" class="graf graf--li graf-after--pre">Here, we are asking it to create a learner that is suitable for our model data.</li><li name="4a55" id="4a55" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">0.04</code>&nbsp;: how much dropout to use</li><li name="0fe9" id="0fe9" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">[1000,500]</code>&nbsp;: how many activations to have in each layer</li><li name="b709" id="b709" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">[0.001,0.01]</code>&nbsp;: how many dropout to use at later layers</li></ul><h4 name="0e6d" id="0e6d" class="graf graf--h4 graf-after--li">Key New Concept: Embeddings [<a href="https://youtu.be/gbceqO8PpBg?t=45m39s" data-href="https://youtu.be/gbceqO8PpBg?t=45m39s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">45:39</a>]</h4><p name="0833" id="0833" class="graf graf--p graf-after--h4">Let’s forget about categorical variables for a moment:</p><figure name="a4a9" id="a4a9" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_T604NRtHHBkBWFvWoovlUw.png"></figure><p name="9864" id="9864" class="graf graf--p graf-after--figure">Remember, you never want to put ReLU in the last layer because softmax needs negatives to create low probabilities.</p><h4 name="64fb" id="64fb" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Simple view of fully connected neural net&nbsp;[</strong><a href="https://youtu.be/gbceqO8PpBg?t=49m13s" data-href="https://youtu.be/gbceqO8PpBg?t=49m13s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--h4-strong">49:13</strong></a><strong class="markup--strong markup--h4-strong">]:</strong></h4><figure name="456b" id="456b" class="graf graf--figure graf-after--h4"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_5D0_nDy0K0QLKFHTD07gcQ.png"></figure><p name="c30f" id="c30f" class="graf graf--p graf-after--figure">For regression problems (not classification), you can even skip the softmax layer.</p><h4 name="2a52" id="2a52" class="graf graf--h4 graf-after--p">Categorical variables [<a href="https://youtu.be/gbceqO8PpBg?t=50m49s" data-href="https://youtu.be/gbceqO8PpBg?t=50m49s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">50:49</a>]</h4><p name="663e" id="663e" class="graf graf--p graf-after--h4">We create a new matrix of 7 rows and as many columns as we choose (4, for example) and fill it with floating numbers. To add “Sunday” to our rank 1 tensor with continuous variables, we do a look up to this matrix, which will return 4 floating numbers, and we use them as “Sunday”.</p><figure name="61fe" id="61fe" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_cAgCy5HfD0rvPDg2dQITeg.png"></figure><p name="b1f0" id="b1f0" class="graf graf--p graf-after--figure">Initially, these numbers are random. But we can put them through a neural net and update them in a way that reduces the loss. In other words, this matrix is just another bunch of weights in our neural net. And matrices of this type are called “<strong class="markup--strong markup--p-strong">embedding matrices</strong>”. An embedding matrix is something where we start out with an integer between zero and maximum number of levels of that category. We index into the matrix to find a particular row, and we append it to all of our continuous variables, and everything after that is just the same as before (linear → ReLU → etc).</p><p name="e7c8" id="e7c8" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: What do those 4 numbers represent?[<a href="https://youtu.be/gbceqO8PpBg?t=55m12s" data-href="https://youtu.be/gbceqO8PpBg?t=55m12s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">55:12</a>] We will learn more about that when we look at collaborative filtering, but for now, they are just parameters that we are learning that happen to end up giving us a good loss. We will discover later that these particular parameters often are human interpretable and quite interesting but that a side effect.</p><p name="ad1f" id="ad1f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: Do you have good heuristics for the dimensionality of the embedding matrix? [<a href="https://youtu.be/gbceqO8PpBg?t=55m57s" data-href="https://youtu.be/gbceqO8PpBg?t=55m57s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">55:57</a>] I sure do! Let’s take a look.</p><pre name="e71d" id="e71d" class="graf graf--pre graf-after--p">cat_sz = [(c, len(joined_samp[c].cat.categories)+1) <br>             <strong class="markup--strong markup--pre-strong">for</strong> c <strong class="markup--strong markup--pre-strong">in</strong> cat_vars]<br>cat_sz</pre><pre name="ce64" id="ce64" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[('Store', 1116),<br> ('DayOfWeek', 8),<br> ('Year', 4),<br> ('Month', 13),<br> ('Day', 32),<br> ('StateHoliday', 3),<br> ('CompetitionMonthsOpen', 26),<br> ('Promo2Weeks', 27),<br> ('StoreType', 5),<br> ('Assortment', 4),<br> ('PromoInterval', 4),<br> ('CompetitionOpenSinceYear', 24),<br> ('Promo2SinceYear', 9),<br> ('State', 13),<br> ('Week', 53),<br> ('Events', 22),<br> ('Promo_fw', 7),<br> ('Promo_bw', 7),<br> ('StateHoliday_fw', 4),<br> ('StateHoliday_bw', 4),<br> ('SchoolHoliday_fw', 9),<br> ('SchoolHoliday_bw', 9)]</em></pre><ul class="postList"><li name="6ec2" id="6ec2" class="graf graf--li graf-after--pre">Here is a list of every categorical variable and its cardinality.</li><li name="9a49" id="9a49" class="graf graf--li graf-after--li">Even if there were no missing values in the original data, you should still set aside one for unknown just in case.</li><li name="cdb2" id="cdb2" class="graf graf--li graf-after--li">The rule of thumb for determining the embedding size is the cardinality size divided by 2, but no bigger than 50.</li></ul><pre name="d440" id="d440" class="graf graf--pre graf-after--li">emb_szs = [(c, min(50, (c+1)//2)) <strong class="markup--strong markup--pre-strong">for</strong> _,c <strong class="markup--strong markup--pre-strong">in</strong> cat_sz]<br>emb_szs</pre><pre name="c5fe" id="c5fe" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[(1116, 50),<br> (8, 4),<br> (4, 2),<br> (13, 7),<br> (32, 16),<br> (3, 2),<br> (26, 13),<br> (27, 14),<br> (5, 3),<br> (4, 2),<br> (4, 2),<br> (24, 12),<br> (9, 5),<br> (13, 7),<br> (53, 27),<br> (22, 11),<br> (7, 4),<br> (7, 4),<br> (4, 2),<br> (4, 2),<br> (9, 5),<br> (9, 5)]</em></pre><p name="2b25" id="2b25" class="graf graf--p graf-after--pre">Then pass the embedding size to the learner:</p><pre name="c974" id="c974" class="graf graf--pre graf-after--p">m = md.get_learner(emb_szs, len(df.columns)-len(cat_vars), 0.04, 1,<br>                   [1000,500], [0.001,0.01], y_range=y_range)</pre><p name="ded3" id="ded3" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">Question</strong>: Is there a way to initialize embedding matrices besides random? [<a href="https://youtu.be/gbceqO8PpBg?t=58m14s" data-href="https://youtu.be/gbceqO8PpBg?t=58m14s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">58:14</a>] We will probably talk about pre-trained more later in the course, but the basic idea is if somebody else at Rossmann had already trained a neural network to predict cheese sales, you may as well start with their embedding matrix of stores to predict liquor sales. This is what happens, for example, at Pinterest and Instacart. Instacart uses this technique for routing their shoppers, and Pinterest uses it for deciding what to display on a webpage. They have embedding matrices of products/stores that get shared in the organization so people do not have to train new ones.</p><p name="59ba" id="59ba" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: What is the advantage of using embedding matrices over one-hot-encoding? [<a href="https://youtu.be/gbceqO8PpBg?t=59m23s" data-href="https://youtu.be/gbceqO8PpBg?t=59m23s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">59:23</a>] For the day of week example above, instead of the 4 numbers, we could have easily passed 7 numbers (e.g. [0, 1, 0, 0, 0, 0, 0] for Sunday). That also is a list of floats and that would totally work — and that is how, generally speaking, categorical variables have been used in statistics for many years (called “dummy variables”). The problem is, the concept of Sunday could only ever be associated with a single floating-point number. So it gets this kind of linear behavior — it says Sunday is more or less of a single thing. With embeddings, Sunday is a concept in four dimensional space. What we tend to find happen is that these embedding vectors tend to get these rich semantic concepts. For example, if it turns out that weekends have a different behavior, you tend to see that Saturday and Sunday will have some particular number higher.</p><blockquote name="e3db" id="e3db" class="graf graf--pullquote graf-after--p"><span class="markup--quote markup--pullquote-quote is-other" name="anon_276bd5ab1363" data-creator-ids="anon">By having higher dimensionality vector rather than just a single number, it gives the deep learning network a chance to learn these rich representations.</span></blockquote><p name="7aba" id="7aba" class="graf graf--p graf-after--pullquote">The idea of an embedding is what is called a “distributed representation” — the most fundamental concept of neural networks. This is the idea that a concept in neural network has a high dimensional representation which can be hard to interpret. These numbers in this vector does not even have to have just one meaning. It could mean one thing if this is low and that one is high, and something else if that one is high and that one is low because it is going through this rich nonlinear function. It is this rich representation that allows it to learn such interesting relationships.</p><p name="5d09" id="5d09" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: Are embeddings suitable for certain types of variables? [<a href="https://youtu.be/gbceqO8PpBg?t=1h2m45s" data-href="https://youtu.be/gbceqO8PpBg?t=1h2m45s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">01:02:45</a>] Embedding is suitable for any categorical variables. The only thing it cannot work well for would be something with too high cardinality. If you had 600,000 rows and a variable had 600,000 levels, that is just not a useful categorical variable. But in general, the third winner in this competition really decided that everything that was not too high cardinality, they put them all as categorical. The good rule of thumb is if you can make a categorical variable, you may as well because that way it can learn this rich distributed representation; where else if you leave it as continuous, the most it can do is to try and find a single functional form that fits it well.</p><h4 name="6d38" id="6d38" class="graf graf--h4 graf-after--p">Matrix algebra behind the scene [<a href="https://youtu.be/gbceqO8PpBg?t=1h4m47s" data-href="https://youtu.be/gbceqO8PpBg?t=1h4m47s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">01:04:47</a>]</h4><p name="0770" id="0770" class="graf graf--p graf-after--h4">Looking up an embedding with an index is identical to doing a matrix product between a one-hot encoded vector and the embedding matrix. But doing so is terribly inefficient, so modern libraries implement this as taking an integer and doing a look up into an array.</p><figure name="2874" id="2874" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_psxpwtr5bw55lKxVV_y81w.png"></figure><p name="ef9d" id="ef9d" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Question</strong>: Could you touch on using dates and times as categorical and how that affects seasonality? [<a href="https://youtu.be/gbceqO8PpBg?t=1h6m59s" data-href="https://youtu.be/gbceqO8PpBg?t=1h6m59s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">01:06:59</a>] There is a Fast.ai function called <code class="markup--code markup--p-code">add_datepart</code> which takes a data frame and a column name. It optionally removes the column from the data frame and replaces it with lots of column representing all of the useful information about that date such as day of week, day of month, month of year, etc (basically everything Pandas gives us).</p><pre name="c695" id="c695" class="graf graf--pre graf-after--p">add_datepart(weather, "Date", drop=False)<br>add_datepart(googletrend, "Date", drop=False)<br>add_datepart(train, "Date", drop=False)<br>add_datepart(test, "Date", drop=False)</pre><figure name="c6f9" id="c6f9" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_OJQ53sO6WXh0C-rzw1QyJg.png"></figure><p name="cbb6" id="cbb6" class="graf graf--p graf-after--figure">So for example, day of week now becomes eight rows by four columns embedding matrix. Conceptually this allows our model to create some interesting time series models. If there is something that has a seven day period cycle that goes up on Mondays and down on Wednesdays but only for daily and only in Berlin, it can totally do that — it has all the information it needs. This is a fantastic way to deal with time series. You just need to make sure that the cycle indicator in your time series exists as a column. If you did not have a column called day of week, it would be very difficult for the neural network to learn to do mod seven and look up in an embedding matrix. It is not impossible but really hard. If you are predicting sales of beverages in San Francisco, you probably want a list of when the ball game is on at AT&amp;T park because that is going to to impact how many people are drinking beer in SoMa. So you need to make sure that the basic indicators or periodicity is in your data, and as long as they are there, neural net is going to learn to use them.</p><h4 name="b9c7" id="b9c7" class="graf graf--h4 graf-after--p">Learner [<a href="https://youtu.be/gbceqO8PpBg?t=1h10m13s" data-href="https://youtu.be/gbceqO8PpBg?t=1h10m13s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">01:10:13</a>]</h4><pre name="ea5a" id="ea5a" class="graf graf--pre graf-after--h4">m = md.get_learner(emb_szs, len(df.columns)-len(cat_vars), 0.04, 1,<br>                   [1000,500], [0.001,0.01], y_range=y_range)<br>lr = 1e-3</pre><ul class="postList"><li name="6003" id="6003" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">emb_szs</code>&nbsp;: embedding size</li><li name="2bf9" id="2bf9" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">len(df.columns)-len(cat_vars)</code>&nbsp;: number of continuous variables in the data frame</li><li name="3ce0" id="3ce0" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">0.04</code>&nbsp;: embedding matrix has its own dropout and this is the dropout rate</li><li name="0932" id="0932" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">1</code>&nbsp;: how many outputs we want to create (output of the last linear layer)</li><li name="74b9" id="74b9" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">[1000, 500]</code>&nbsp;: number of activations in the first linear layer, and the second linear layer</li><li name="9442" id="9442" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">[0.001, 0.01]</code>&nbsp;: dropout in the first linear layer, and the second linear layer</li><li name="7177" id="7177" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">y_range</code>&nbsp;: we will not worry about that for now</li></ul><pre name="2283" id="2283" class="graf graf--pre graf-after--li">m.fit(lr, 3, metrics=[exp_rmspe])</pre><pre name="2ee9" id="2ee9" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">A Jupyter Widget</em></pre><pre name="0b7a" id="0b7a" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[ 0.       0.02479  0.02205  </em><strong class="markup--strong markup--pre-strong"><em class="markup--em markup--pre-em">0.19309</em></strong><em class="markup--em markup--pre-em">]                          <br>[ 1.       0.02044  0.01751  </em><strong class="markup--strong markup--pre-strong"><em class="markup--em markup--pre-em">0.18301</em></strong><em class="markup--em markup--pre-em">]                          <br>[ 2.       0.01598  0.01571  </em><strong class="markup--strong markup--pre-strong"><em class="markup--em markup--pre-em">0.17248</em></strong><em class="markup--em markup--pre-em">]</em></pre><ul class="postList"><li name="cbe1" id="cbe1" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">metrics</code>&nbsp;: this is a custom metric which specifies a function to be called at the end of every epoch and prints out a result</li></ul><pre name="164b" id="164b" class="graf graf--pre graf-after--li">m.fit(lr, 1, metrics=[exp_rmspe], cycle_len=1)</pre><pre name="a134" id="a134" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[ 0.       0.00676  0.01041  0.09711]   </em></pre><p name="31e2" id="31e2" class="graf graf--p graf-after--pre">By using all of the training data, we achieved a RMSPE around 0.09711. There is a big difference between public leader board and private leader board, but we are certainly in the top end of this competition.</p><p name="74fd" id="74fd" class="graf graf--p graf-after--p">So this is a technique for dealing with time series and structured data. Interestingly, compared to the group that used this technique (<a href="https://arxiv.org/abs/1604.06737" data-href="https://arxiv.org/abs/1604.06737" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Entity Embeddings of Categorical Variables</a>), the second place winner did way more feature engineering. The winners of this competition were actually subject matter experts in logistics sales forecasting so they had their own code to create lots and lots of features. Folks at Pinterest who build a very similar model for recommendations also said that when they switched from gradient boosting machines to deep learning, they did way less feature engineering and it was much simpler model which requires less maintenance. So this is one of the big benefits of using this approach to deep learning — you can get state of the art results but with a lot less work.</p><p name="58fb" id="58fb" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: Are we using any time series in any of these? [<a href="https://youtu.be/gbceqO8PpBg?t=1h15m1s" data-href="https://youtu.be/gbceqO8PpBg?t=1h15m1s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">01:15:01</a>] Indirectly, yes. As we just saw, we have a day of week, month of year, etc in our columns and most of them are being treated as categories, so we are building a distributed representation of January, Sunday, and so on. We are not using any classic time series techniques, all we are doing is true fully connected layers in a neural net. The embedding matrix is able to deal with things like day of week periodicity in a much richer way than than any standard time series techniques.</p><p name="0210" id="0210" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong> regarding the difference between image models and this model [<a href="https://youtu.be/gbceqO8PpBg?t=1h15m59s" data-href="https://youtu.be/gbceqO8PpBg?t=1h15m59s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">01:15:59</a>]: There is a difference in a way we are calling <code class="markup--code markup--p-code">get_learner</code>. In imaging we just did <code class="markup--code markup--p-code">Learner.trained</code> and pass the data:</p><pre name="61b4" id="61b4" class="graf graf--pre graf-after--p">learn = ConvLearner.pretrained(arch, data, ps=0., precompute=True)</pre><p name="2b30" id="2b30" class="graf graf--p graf-after--pre">For these kinds of models, in fact for a lot of the models, the model we build depends on the data. In this case, we need to know what embedding matrices we have. So in this case, the data objects creates the learner (upside down to what we have seen before):</p><pre name="254a" id="254a" class="graf graf--pre graf-after--p">m = md.get_learner(emb_szs, len(df.columns)-len(cat_vars), 0.04, 1,<br>                   [1000,500], [0.001,0.01], y_range=y_range)</pre><p name="5401" id="5401" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">Summary of steps </strong>(if you want to use this for your own dataset) [<a href="https://youtu.be/gbceqO8PpBg?t=1h17m56s" data-href="https://youtu.be/gbceqO8PpBg?t=1h17m56s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">01:17:56</a>]:</p><p name="a3c8" id="a3c8" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Step 1</strong>. List categorical variable names, and list continuous variable names, and put them in a Pandas data frame</p><p name="d3c4" id="d3c4" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Step 2</strong>. Create a list of which row indexes you want in your validation set</p><p name="07b3" id="07b3" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Step 3</strong>. Call this exact line of code:</p><pre name="c800" id="c800" class="graf graf--pre graf-after--p">md = ColumnarModelData.from_data_frame(PATH, val_idx, df, <br>         yl.astype(np.float32), cat_flds=cat_vars, bs=128, <br>         test_df=df_test)</pre><p name="bfff" id="bfff" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">Step 4</strong>. Create a list of how big you want each embedding matrix to be</p><p name="4755" id="4755" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Step 5</strong>. Call <code class="markup--code markup--p-code">get_learner</code> — you can use these exact parameters to start with:</p><pre name="1687" id="1687" class="graf graf--pre graf-after--p">m = md.get_learner(emb_szs, len(df.columns)-len(cat_vars), 0.04, 1,<br>                   [1000,500], [0.001,0.01], y_range=y_range)</pre><p name="9b71" id="9b71" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">Step 6</strong>. Call <code class="markup--code markup--p-code">m.fit</code></p><p name="a465" id="a465" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: How to use data augmentation for this type of data, and how does dropout work? [<a href="https://youtu.be/gbceqO8PpBg?t=1h18m59s" data-href="https://youtu.be/gbceqO8PpBg?t=1h18m59s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">01:18:59</a>] No idea. Jeremy thinks it has to be domain-specific, but he has never seen any paper or anybody in industry doing data augmentation with structured data and deep learning. He thinks it can be done but has not seen it done. What dropout is doing is exactly the same as before.</p><p name="efe8" id="efe8" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: What is the downside? Almost no one is using this. Why not? [<a href="https://youtu.be/gbceqO8PpBg?t=1h20m41s" data-href="https://youtu.be/gbceqO8PpBg?t=1h20m41s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">01:20:41</a>] Basically the answer is as we discussed before, no one in academia almost is working on this because it is not something that people publish on. As a result, there have not been really great examples people could look at and say “oh here is a technique that works well so let’s have our company implement it”. But perhaps equally importantly, until now with this Fast.ai library, there has not been any way to do it conveniently. If you wanted to implement one of these models, you had to write all the custom code yourself. There are a lot of big commercial and scientific opportunity to use this and solve problems that previously haven’t been solved very well.</p><h3 name="7e8f" id="7e8f" class="graf graf--h3 graf-after--p">Natural Language Processing [<a href="https://youtu.be/gbceqO8PpBg?t=1h23m37s" data-href="https://youtu.be/gbceqO8PpBg?t=1h23m37s" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">01:23:37</a>]</h3><p name="6bc8" id="6bc8" class="graf graf--p graf-after--h3">The most up-and-coming area of deep learning and it is two or three years behind computer vision. The state of software and some of the concepts is much less mature than it is for computer vision. One of the things you find in NLP is there are particular problems you can solve and they have particular names. There is a particular kind of problem in NLP called “language modeling” and it has a very specific definition — it means build a model where given a few words of a sentence, can you predict what the next word is going to be.</p><h4 name="e25d" id="e25d" class="graf graf--h4 graf-after--p">Language Modeling [<a href="https://youtu.be/gbceqO8PpBg?t=1h25m48s" data-href="https://youtu.be/gbceqO8PpBg?t=1h25m48s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">01:25:48</a>]</h4><p name="2be8" id="2be8" class="graf graf--p graf-after--h4"><a href="https://github.com/fastai/fastai/blob/master/courses/dl1/lang_model-arxiv.ipynb" data-href="https://github.com/fastai/fastai/blob/master/courses/dl1/lang_model-arxiv.ipynb" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Notebook</a></p><p name="46e4" id="46e4" class="graf graf--p graf-after--p">Here we have 18 months worth of papers from arXiv (arXiv.org) and this is an example:</p><pre name="3446" id="3446" class="graf graf--pre graf-after--p">' '.join(md.trn_ds[0].text[:150])</pre><pre name="f241" id="f241" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">'&lt;cat&gt; csni &lt;summ&gt; the exploitation of mm - wave bands is one of the key - enabler for 5 g mobile \n radio networks . however , the introduction of mm - wave technologies in cellular \n networks is not straightforward due to harsh propagation conditions that limit \n the mm - wave access availability . mm - wave technologies require high - gain antenna \n systems to compensate for high path loss and limited power . as a consequence , \n directional transmissions must be used for cell discovery and synchronization \n processes : this can lead to a non - negligible access delay caused by the \n exploration of the cell area with multiple transmissions along different \n directions . \n    the integration of mm - wave technologies and conventional wireless access \n networks with the objective of speeding up the cell search process requires new \n'</em></pre><ul class="postList"><li name="6018" id="6018" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">&lt;cat&gt;</code> — category of the paper. CSNI is Computer Science and Networking</li><li name="f948" id="f948" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">&lt;summ&gt;</code> — abstract of the paper</li></ul><p name="1dc4" id="1dc4" class="graf graf--p graf-after--li">Here are what the output of a trained language model looks like. We did simple little tests in which you pass some priming text and see what the model thinks should come next:</p><pre name="b2ed" id="b2ed" class="graf graf--pre graf-after--p">sample_model(m, "&lt;CAT&gt; csni &lt;SUMM&gt; algorithms that")</pre><pre name="551c" id="551c" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">...use the same network as a single node are not able to achieve the same performance as the traditional network - based routing algorithms . in this paper , we propose a novel routing scheme for routing protocols in wireless networks . the proposed scheme is based ...</em></pre><p name="9687" id="9687" class="graf graf--p graf-after--pre">It learned by reading arXiv papers that somebody who is writing about computer networking would talk like this. Remember, it started out not knowing English at all. It started out with an embedding matrix for every word in English that was random. By reading lots of arXiv papers, it learned what kind of words followed others.</p><p name="7d02" id="7d02" class="graf graf--p graf-after--p">Here we tried specifying a category to be computer vision:</p><pre name="448a" id="448a" class="graf graf--pre graf-after--p">sample_model(m, "&lt;CAT&gt; cscv &lt;SUMM&gt; algorithms that")</pre><pre name="a135" id="a135" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">...use the same data to perform image classification are increasingly being used to improve the performance of image classification algorithms . in this paper , we propose a novel method for image classification using a deep convolutional neural network ( cnn ) . the proposed method is ...</em></pre><p name="5dad" id="5dad" class="graf graf--p graf-after--pre">It not only learned how to write English pretty well, but also after you say something like “convolutional neural network” you should then use parenthesis to specify an acronym “(CNN)”.</p><pre name="bff1" id="bff1" class="graf graf--pre graf-after--p">sample_model(m,"&lt;CAT&gt; cscv &lt;SUMM&gt; algorithms. &lt;TITLE&gt; on ")</pre><pre name="c90b" id="c90b" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">...the performance of deep learning for image classification &lt;eos&gt;</em></pre><pre name="fb16" id="fb16" class="graf graf--pre graf-after--pre">sample_model(m,"&lt;CAT&gt; csni &lt;SUMM&gt; algorithms. &lt;TITLE&gt; on ")</pre><pre name="230f" id="230f" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">...the performance of wireless networks &lt;eos&gt;</em></pre><pre name="8905" id="8905" class="graf graf--pre graf-after--pre">sample_model(m,"&lt;CAT&gt; cscv &lt;SUMM&gt; algorithms. &lt;TITLE&gt; towards ")</pre><pre name="a989" id="a989" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">...a new approach to image classification &lt;eos&gt;</em></pre><pre name="40ae" id="40ae" class="graf graf--pre graf-after--pre">sample_model(m,"&lt;CAT&gt; csni &lt;SUMM&gt; algorithms. &lt;TITLE&gt; towards ")</pre><pre name="c96e" id="c96e" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">...a new approach to the analysis of wireless networks &lt;eos&gt;</em></pre></body></html>