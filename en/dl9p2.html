<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></head><body><p name="66c7" id="66c7" class="graf graf--p graf-after--pre">Now you can see a list of all the assignments [<a href="https://youtu.be/0frKXR-2PBY?t=1h1m5s" data-href="https://youtu.be/0frKXR-2PBY?t=1h1m5s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:01:05</a>]. Anywhere that has <code class="markup--code markup--p-code">gt_overlap &lt; 0.5</code> gets assigned background. The three row-wise max anchor box has high number to force the assignments. Now we can combine these values to classes:</p><pre name="313f" id="313f" class="graf graf--pre graf-after--p">gt_clas = clas[gt_idx]; gt_clas</pre><pre name="cfab" id="cfab" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">Variable containing:<br>  8<br>  8<br>  8<br>  8<br>  8<br>  8<br>  8<br>  8<br> 10<br> 10<br>  8<br> 17<br> 10<br> 10<br>  8<br>  8<br>[torch.cuda.LongTensor of size 16 (GPU 0)]</em></pre><p name="73df" id="73df" class="graf graf--p graf-after--pre">Then add a threshold and finally comes up with the three classes that are being predicted:</p><pre name="9817" id="9817" class="graf graf--pre graf-after--p">thresh = 0.5<br>pos = gt_overlap &gt; thresh<br>pos_idx = torch.nonzero(pos)[:,0]<br>neg_idx = torch.nonzero(1-pos)[:,0]<br>pos_idx</pre><pre name="0b8b" id="0b8b" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em"> 11<br> 13<br> 14<br>[torch.cuda.LongTensor of size 3 (GPU 0)]</em></pre><p name="3aa2" id="3aa2" class="graf graf--p graf-after--pre">And here are what each of these anchor boxes is meant to be predicting:</p><pre name="3a09" id="3a09" class="graf graf--pre graf-after--p">gt_clas[1-pos] = len(id2cat)<br>[id2cat[o] <strong class="markup--strong markup--pre-strong">if</strong> o&lt;len(id2cat) <strong class="markup--strong markup--pre-strong">else</strong> 'bg' <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> gt_clas.data]</pre><pre name="6a65" id="6a65" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">['bg',<br> 'bg',<br> 'bg',<br> 'bg',<br> 'bg',<br> 'bg',<br> 'bg',<br> 'bg',<br> 'bg',<br> 'bg',<br> 'bg',<br> 'sofa',<br> 'bg',<br> 'diningtable',<br> 'chair',<br> 'bg']</em></pre><p name="c233" id="c233" class="graf graf--p graf-after--pre">So that was the matching stage [<a href="https://youtu.be/0frKXR-2PBY?t=1h2m29s" data-href="https://youtu.be/0frKXR-2PBY?t=1h2m29s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:02:29</a>]. For L1 loss, we can:</p><ol class="postList"><li name="1dc4" id="1dc4" class="graf graf--li graf-after--p">take the activations which matched (<code class="markup--code markup--li-code">pos_idx = [11, 13, 14]</code>)</li><li name="9131" id="9131" class="graf graf--li graf-after--li">subtract from those the ground truth bounding boxes</li><li name="9713" id="9713" class="graf graf--li graf-after--li">take the absolute value of the difference</li><li name="7f64" id="7f64" class="graf graf--li graf-after--li">take the mean of that.</li></ol><p name="483c" id="483c" class="graf graf--p graf-after--li">For classifications, we can just do a cross entropy</p><pre name="4c9b" id="4c9b" class="graf graf--pre graf-after--p">gt_bbox = bbox[gt_idx]<br>loc_loss = ((a_ic[pos_idx] - gt_bbox[pos_idx]).abs()).mean()<br>clas_loss  = F.cross_entropy(b_clasi, gt_clas)<br>loc_loss,clas_loss</pre><pre name="0d14" id="0d14" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(Variable containing:<br> 1.00000e-02 *<br>   6.5887<br> [torch.cuda.FloatTensor of size 1 (GPU 0)], Variable containing:<br>  1.0331<br> [torch.cuda.FloatTensor of size 1 (GPU 0)])</em></pre><p name="8150" id="8150" class="graf graf--p graf-after--pre">We will end up with 16 predicted bounding boxes, most of them will be background. If you are wondering what it predicts in terms of bounding box of background, the answer is it totally ignores it.</p><pre name="7536" id="7536" class="graf graf--pre graf-after--p">fig, axes = plt.subplots(3, 4, figsize=(16, 12))<br><strong class="markup--strong markup--pre-strong">for</strong> idx,ax <strong class="markup--strong markup--pre-strong">in</strong> enumerate(axes.flat):<br>    ima=md.val_ds.ds.denorm(to_np(x))[idx]<br>    bbox,clas = get_y(y[0][idx], y[1][idx])<br>    ima=md.val_ds.ds.denorm(to_np(x))[idx]<br>    bbox,clas = get_y(bbox,clas); bbox,clas<br>    a_ic = actn_to_bb(b_bb[idx], anchors)<br>    torch_gt(ax, ima, a_ic, b_clas[idx].max(1)[1], <br>             b_clas[idx].max(1)[0].sigmoid(), 0.01)<br>plt.tight_layout()</pre><figure name="bcf4" id="bcf4" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_8azTUd1Ujf3FQSMBwIXgAw.png"></figure><h4 name="558e" id="558e" class="graf graf--h4 graf-after--figure">Tweak 1. How do we interpret the activations [<a href="https://youtu.be/0frKXR-2PBY?t=1h4m16s" data-href="https://youtu.be/0frKXR-2PBY?t=1h4m16s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:04:16</a>]?</h4><p name="cc2a" id="cc2a" class="graf graf--p graf-after--h4">The way we interpret the activation is defined here:</p><pre name="b02b" id="b02b" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> actn_to_bb(actn, anchors):<br>    actn_bbs = torch.tanh(actn)<br>    actn_centers = (actn_bbs[:,:2]/2 * grid_sizes) + anchors[:,:2]<br>    actn_hw = (actn_bbs[:,2:]/2+1) * anchors[:,2:]<br>    <strong class="markup--strong markup--pre-strong">return</strong> hw2corners(actn_centers, actn_hw)</pre><p name="2a9b" id="2a9b" class="graf graf--p graf-after--pre">We grab the activations, we stick them through <code class="markup--code markup--p-code">tanh</code> (remember <code class="markup--code markup--p-code">tanh</code> is the same shape as sigmoid except it is scaled to be between -1 and 1) which forces it to be within that range. We then grab the actual position of the anchor boxes, and we will move them around according to the value of the activations divided by two (<code class="markup--code markup--p-code">actn_bbs[:,:2]/2</code>). In other words, each predicted bounding box can be moved by up to 50% of a grid size from where its default position is. Ditto for its height and width — it can be up to twice as big or half as big as its default size.</p><h4 name="d59f" id="d59f" class="graf graf--h4 graf-after--p">Tweak 2. We actually use binary cross entropy loss instead of cross entropy [<a href="https://youtu.be/0frKXR-2PBY?t=1h5m36s" data-href="https://youtu.be/0frKXR-2PBY?t=1h5m36s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:05:36</a>]</h4><pre name="960e" id="960e" class="graf graf--pre graf-after--h4"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">BCE_Loss</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, num_classes):<br>        super().__init__()<br>        self.num_classes = num_classes<br><br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, pred, targ):<br>        t = one_hot_embedding(targ, self.num_classes+1)<br>        t = V(t[:,:-1].contiguous())<em class="markup--em markup--pre-em">#.cpu()</em><br>        x = pred[:,:-1]<br>        w = self.get_weight(x,t)<br>        <strong class="markup--strong markup--pre-strong">return</strong> F.binary_cross_entropy_with_logits(x, t, w, <br>                            size_average=<strong class="markup--strong markup--pre-strong">False</strong>)/self.num_classes<br>    <br>    <strong class="markup--strong markup--pre-strong">def</strong> get_weight(self,x,t): <strong class="markup--strong markup--pre-strong">return</strong> <strong class="markup--strong markup--pre-strong">None</strong></pre><p name="820e" id="820e" class="graf graf--p graf-after--pre">Binary cross entropy is what we normally use for multi-label classification. Like in the planet satellite competition, each satellite image could have multiple things. If it has multiple things in it, you cannot use softmax because softmax really encourages just one thing to have the high number. In our case, each anchor box can only have one object associated with it, so it is not for that reason that we are avoiding softmax. It is something else — which is it is possible for an anchor box to have nothing associated with it. There are two ways to handle this idea of “background”; one would be to say background is just a class, so let’s use softmax and just treat background as one of the classes that the softmax could predict. A lot of people have done it this way. But that is a really hard thing to ask neural network to do [<a href="https://youtu.be/0frKXR-2PBY?t=1h5m52s" data-href="https://youtu.be/0frKXR-2PBY?t=1h5m52s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:06:52</a>] — it is basically asking whether this grid cell does not have any of the 20 objects that I am interested with Jaccard overlap of more than 0.5. It is a really hard to thing to put into a single computation. On the other hand, what if we just asked for each class; “is it a motorbike?” “is it a bus?”, “ is it a person?” etc and if all the answer is no, consider that background. That is the way we do it here. It is not that we can have multiple true labels, but we can have zero.</p><p name="29b4" id="29b4" class="graf graf--p graf-after--p">In <code class="markup--code markup--p-code">forward</code>&nbsp;:</p><ol class="postList"><li name="482a" id="482a" class="graf graf--li graf-after--p">First we take the one hot embedding of the target (at this stage, we do have the idea of background)</li><li name="0e7c" id="0e7c" class="graf graf--li graf-after--li">Then we remove the background column (the last one) which results in a vector either of all zeros or one one.</li><li name="7ba1" id="7ba1" class="graf graf--li graf-after--li">Use binary cross-entropy predictions.</li></ol><p name="6a88" id="6a88" class="graf graf--p graf-after--li">This is a minor tweak, but it is the kind of minor tweak that Jeremy wants you to think about and understand because it makes a really big difference to your training and when there is some increment over a previous paper, it would be something like this [<a href="https://youtu.be/0frKXR-2PBY?t=1h8m25s" data-href="https://youtu.be/0frKXR-2PBY?t=1h8m25s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:08:25</a>]. It is important to understand what this is doing and more importantly why.</p><p name="9ad6" id="9ad6" class="graf graf--p graf-after--p">So now we have [<a href="https://youtu.be/0frKXR-2PBY?t=1h9m39s" data-href="https://youtu.be/0frKXR-2PBY?t=1h9m39s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:09:39</a>]:</p><ul class="postList"><li name="3ec1" id="3ec1" class="graf graf--li graf-after--p">A custom loss function</li><li name="d4bf" id="d4bf" class="graf graf--li graf-after--li">A way to calculate Jaccard index</li><li name="882e" id="882e" class="graf graf--li graf-after--li">A way to convert activations to bounding box</li><li name="71d6" id="71d6" class="graf graf--li graf-after--li">A way to map anchor boxes to ground truth</li></ul><p name="9d33" id="9d33" class="graf graf--p graf-after--li">Now all it’s left is SSD loss function.</p><h4 name="5fe0" id="5fe0" class="graf graf--h4 graf-after--p">SSD Loss Function [<a href="https://youtu.be/0frKXR-2PBY?t=1h9m55s" data-href="https://youtu.be/0frKXR-2PBY?t=1h9m55s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:09:55</a>]</h4><pre name="96a6" id="96a6" class="graf graf--pre graf-after--h4"><strong class="markup--strong markup--pre-strong">def</strong> ssd_1_loss(b_c,b_bb,bbox,clas,print_it=<strong class="markup--strong markup--pre-strong">False</strong>):<br>    bbox,clas = get_y(bbox,clas)<br>    a_ic = actn_to_bb(b_bb, anchors)<br>    overlaps = jaccard(bbox.data, anchor_cnr.data)<br>    gt_overlap,gt_idx = map_to_ground_truth(overlaps,print_it)<br>    gt_clas = clas[gt_idx]<br>    pos = gt_overlap &gt; 0.4<br>    pos_idx = torch.nonzero(pos)[:,0]<br>    gt_clas[1-pos] = len(id2cat)<br>    gt_bbox = bbox[gt_idx]<br>    loc_loss = ((a_ic[pos_idx] - gt_bbox[pos_idx]).abs()).mean()<br>    clas_loss  = loss_f(b_c, gt_clas)<br>    <strong class="markup--strong markup--pre-strong">return</strong> loc_loss, clas_loss<br><br><strong class="markup--strong markup--pre-strong">def</strong> ssd_loss(pred,targ,print_it=<strong class="markup--strong markup--pre-strong">False</strong>):<br>    lcs,lls = 0.,0.<br>    <strong class="markup--strong markup--pre-strong">for</strong> b_c,b_bb,bbox,clas <strong class="markup--strong markup--pre-strong">in</strong> zip(*pred,*targ):<br>        loc_loss,clas_loss = ssd_1_loss(b_c,b_bb,bbox,clas,print_it)<br>        lls += loc_loss<br>        lcs += clas_loss<br>    <strong class="markup--strong markup--pre-strong">if</strong> print_it: print(f'loc: <strong class="markup--strong markup--pre-strong">{lls.data[0]}</strong>, clas: <strong class="markup--strong markup--pre-strong">{lcs.data[0]}</strong>')<br>    <strong class="markup--strong markup--pre-strong">return</strong> lls+lcs</pre><p name="8c47" id="8c47" class="graf graf--p graf-after--pre">The <code class="markup--code markup--p-code">ssd_loss</code> function which is what we set as the criteria, it loops through each image in the mini-batch and call <code class="markup--code markup--p-code">ssd_1_loss</code> function (i.e. SSD loss for one image).</p><p name="98d0" id="98d0" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">ssd_1_loss</code> is where it is all happening. It begins by de-structuring <code class="markup--code markup--p-code">bbox</code> and <code class="markup--code markup--p-code">clas</code>. Let’s take a closer look at <code class="markup--code markup--p-code">get_y</code> [<a href="https://youtu.be/0frKXR-2PBY?t=1h10m38s" data-href="https://youtu.be/0frKXR-2PBY?t=1h10m38s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:10:38</a>]:</p><pre name="b8d2" id="b8d2" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> get_y(bbox,clas):<br>    bbox = bbox.view(-1,4)/sz<br>    bb_keep = ((bbox[:,2]-bbox[:,0])&gt;0).nonzero()[:,0]<br>    <strong class="markup--strong markup--pre-strong">return</strong> bbox[bb_keep],clas[bb_keep]</pre><p name="d4b5" id="d4b5" class="graf graf--p graf-after--pre">A lot of code you find on the internet does not work with mini-batches. It only does one thing at a time which we don’t want. In this case, all these functions (<code class="markup--code markup--p-code">get_y</code>, <code class="markup--code markup--p-code">actn_to_bb</code>, <code class="markup--code markup--p-code">map_to_ground_truth</code>) is working on, not exactly a mini-batch at a time, but a whole bunch of ground truth objects at a time. The data loader is being fed a mini-batch at a time to do the convolutional layers. Because we can have <em class="markup--em markup--p-em">different numbers of ground truth objects in each image</em> but a tensor has to be the strict rectangular shape, fastai automatically pads it with zeros (any target values that are shorter) [<a href="https://youtu.be/0frKXR-2PBY?t=1h11m8s" data-href="https://youtu.be/0frKXR-2PBY?t=1h11m8s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:11:08</a>]. This was something that was added recently and super handy, but that does mean that you then have to make sure that you get rid of those zeros. So <code class="markup--code markup--p-code">get_y</code> gets rid of any of the bounding boxes that are just padding.</p><ol class="postList"><li name="f229" id="f229" class="graf graf--li graf-after--p">Get rid of the padding</li><li name="0e2d" id="0e2d" class="graf graf--li graf-after--li">Turn the activations to bounding boxes</li><li name="953a" id="953a" class="graf graf--li graf-after--li">Do the Jaccard</li><li name="fa65" id="fa65" class="graf graf--li graf-after--li">Do map_to_ground_truth</li><li name="26b9" id="26b9" class="graf graf--li graf-after--li">Check that there is an overlap greater than something around 0.4~0.5 (different papers use different values for this)</li><li name="aa5a" id="aa5a" class="graf graf--li graf-after--li">Find the indices of things that matched</li><li name="bad6" id="bad6" class="graf graf--li graf-after--li">Assign background class for the ones that did not match</li><li name="a55d" id="a55d" class="graf graf--li graf-after--li">Then finally get L1 loss for the localization, binary cross entropy loss for the classification, and return them which gets added in <code class="markup--code markup--li-code">ssd_loss</code></li></ol><h4 name="b35e" id="b35e" class="graf graf--h4 graf-after--li">Training [<a href="https://youtu.be/0frKXR-2PBY?t=1h12m47s" data-href="https://youtu.be/0frKXR-2PBY?t=1h12m47s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:12:47</a>]</h4><pre name="0f37" id="0f37" class="graf graf--pre graf-after--h4">learn.crit = ssd_loss<br>lr = 3e-3<br>lrs = np.array([lr/100,lr/10,lr])</pre><pre name="8eb3" id="8eb3" class="graf graf--pre graf-after--pre">learn.lr_find(lrs/1000,1.)<br>learn.sched.plot(1)</pre><pre name="5ef8" id="5ef8" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss                            <br>    0      44.232681  21476.816406</em></pre><figure name="9862" id="9862" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_V8J7FkreIVG7tKxGQQRV2Q.png"></figure><pre name="c8ad" id="c8ad" class="graf graf--pre graf-after--figure">learn.lr_find(lrs/1000,1.)<br>learn.sched.plot(1)</pre><pre name="6db9" id="6db9" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss                            <br>    0      86.852668  32587.789062</em></pre><figure name="939e" id="939e" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_-q583mkIy-e3k6dz5HmkYw.png"></figure><pre name="b198" id="b198" class="graf graf--pre graf-after--figure">learn.fit(lr, 1, cycle_len=5, use_clr=(20,10))</pre><pre name="e101" id="e101" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss                            <br>    0      45.570843  37.099854 <br>    1      37.165911  32.165031                           <br>    2      33.27844   30.990122                           <br>    3      31.12054   29.804482                           <br>    4      29.305789  28.943184</em></pre><pre name="3fb3" id="3fb3" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[28.943184]</em></pre><pre name="f2a9" id="f2a9" class="graf graf--pre graf-after--pre">learn.fit(lr, 1, cycle_len=5, use_clr=(20,10))</pre><pre name="e4ee" id="e4ee" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss                            <br>    0      43.726979  33.803085 <br>    1      34.771754  29.012939                           <br>    2      30.591864  27.132868                           <br>    3      27.896905  26.151638                           <br>    4      25.907382  25.739273</em></pre><pre name="e44e" id="e44e" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[25.739273]</em></pre><pre name="b505" id="b505" class="graf graf--pre graf-after--pre">learn.save('0')</pre><pre name="e878" id="e878" class="graf graf--pre graf-after--pre">learn.load('0')</pre><h4 name="6949" id="6949" class="graf graf--h4 graf-after--pre">Result [<a href="https://youtu.be/0frKXR-2PBY?t=1h13m16s" data-href="https://youtu.be/0frKXR-2PBY?t=1h13m16s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:13:16</a>]</h4><figure name="3657" id="3657" class="graf graf--figure graf-after--h4"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_8azTUd1Ujf3FQSMBwIXgAw.png"></figure><p name="efec" id="efec" class="graf graf--p graf-after--figure">In practice, we want to remove the background and also add some threshold for probabilities, but it is on the right track. The potted plant image, the result is not surprising as all of our anchor boxes were small (4x4 grid). To go from here to something that is going to be more accurate, all we are going to do is to create way more anchor boxes.</p><p name="c496" id="c496" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: For the multi-label classification, why aren’t we multiplying the categorical loss by a constant like we did before [<a href="https://youtu.be/0frKXR-2PBY?t=1h15m20s" data-href="https://youtu.be/0frKXR-2PBY?t=1h15m20s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:15:20</a>]? Great question. It is because later on it will turn out we do not need to.</p><h4 name="5ece" id="5ece" class="graf graf--h4 graf-after--p">More anchors! [<a href="https://youtu.be/0frKXR-2PBY?t=1h14m47s" data-href="https://youtu.be/0frKXR-2PBY?t=1h14m47s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:14:47</a>]</h4><p name="3906" id="3906" class="graf graf--p graf-after--h4">There are 3 ways to do this:</p><ol class="postList"><li name="9e13" id="9e13" class="graf graf--li graf-after--p">Create anchor boxes of different sizes (zoom):</li></ol><figure name="ce4b" id="ce4b" class="graf graf--figure graf--layoutOutsetRow is-partialWidth graf-after--li" style="width: 33.668%;" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_OtrTSJqBXyjeypKehik1CQ.png"></figure><figure name="b8d9" id="b8d9" class="graf graf--figure graf--layoutOutsetRowContinue is-partialWidth graf-after--figure" style="width: 33.133%;" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_YG5bCP3O-jVhaQX_wuiSSg.png"></figure><figure name="02ff" id="02ff" class="graf graf--figure graf--layoutOutsetRowContinue is-partialWidth graf-after--figure" style="width: 33.2%;" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_QCo0wOgJKXDBYNlmE7zUmA.png"><figcaption class="imageCaption" style="width: 301.205%; left: -201.205%;">From left (1x1, 2x2, 4x4 grids of anchor boxes). Notice that some of the anchor box is bigger than the original&nbsp;image.</figcaption></figure><p name="225d" id="225d" class="graf graf--p graf-after--figure">2. Create anchor boxes of different aspect ratios:</p><figure name="fdae" id="fdae" class="graf graf--figure graf--layoutOutsetRow is-partialWidth graf-after--p" style="width: 33.333%;" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_ko8vZK4RD8H2l4u1hXCQZQ.png"></figure><figure name="0b2d" id="0b2d" class="graf graf--figure graf--layoutOutsetRowContinue is-partialWidth graf-after--figure" style="width: 33.1%;" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_3rvuvY6Fu2S6eoN3nK1QWg.png"></figure><figure name="508b" id="508b" class="graf graf--figure graf--layoutOutsetRowContinue is-partialWidth graf-after--figure" style="width: 33.567%;" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_bWZwFqf2Bv-ZbW-KedNO0Q.png"></figure><p name="8881" id="8881" class="graf graf--p graf-after--figure">3. Use more convolutional layers as sources of anchor boxes (the boxes are randomly jittered so that we can see ones that are overlapping [<a href="https://youtu.be/0frKXR-2PBY?t=1h16m28s" data-href="https://youtu.be/0frKXR-2PBY?t=1h16m28s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:16:28</a>]):</p><figure name="f455" id="f455" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_LwFOFtmawmpqp6VDc56RmA.png"></figure><p name="305a" id="305a" class="graf graf--p graf-after--figure">Combining these approaches, you can create lots of anchor boxes (Jeremy said he wouldn’t print it, but here it is):</p><figure name="fd17" id="fd17" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_ymt8L0CCKMd9SG82SemdIA.png"></figure><pre name="14d2" id="14d2" class="graf graf--pre graf-after--figure">anc_grids = [4, 2, 1]<br>anc_zooms = [0.75, 1., 1.3]<br>anc_ratios = [(1., 1.), (1., 0.5), (0.5, 1.)]<br><br>anchor_scales = [(anz*i,anz*j) <strong class="markup--strong markup--pre-strong">for</strong> anz <strong class="markup--strong markup--pre-strong">in</strong> anc_zooms <br>                                    <strong class="markup--strong markup--pre-strong">for</strong> (i,j) <strong class="markup--strong markup--pre-strong">in</strong> anc_ratios]<br>k = len(anchor_scales)<br>anc_offsets = [1/(o*2) <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> anc_grids]</pre><pre name="2e8f" id="2e8f" class="graf graf--pre graf-after--pre">anc_x = np.concatenate([np.repeat(np.linspace(ao, 1-ao, ag), ag)<br>                        <strong class="markup--strong markup--pre-strong">for</strong> ao,ag <strong class="markup--strong markup--pre-strong">in</strong> zip(anc_offsets,anc_grids)])<br>anc_y = np.concatenate([np.tile(np.linspace(ao, 1-ao, ag), ag)<br>                        <strong class="markup--strong markup--pre-strong">for</strong> ao,ag <strong class="markup--strong markup--pre-strong">in</strong> zip(anc_offsets,anc_grids)])<br>anc_ctrs = np.repeat(np.stack([anc_x,anc_y], axis=1), k, axis=0)</pre><pre name="908b" id="908b" class="graf graf--pre graf-after--pre">anc_sizes = np.concatenate([np.array([[o/ag,p/ag] <br>              <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(ag*ag) <strong class="markup--strong markup--pre-strong">for</strong> o,p <strong class="markup--strong markup--pre-strong">in</strong> anchor_scales])<br>                 <strong class="markup--strong markup--pre-strong">for</strong> ag <strong class="markup--strong markup--pre-strong">in</strong> anc_grids])<br>grid_sizes = V(np.concatenate([np.array([ 1/ag <br>              <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(ag*ag) <strong class="markup--strong markup--pre-strong">for</strong> o,p <strong class="markup--strong markup--pre-strong">in</strong> anchor_scales])<br>                  <strong class="markup--strong markup--pre-strong">for</strong> ag <strong class="markup--strong markup--pre-strong">in</strong> anc_grids]), <br>                      requires_grad=<strong class="markup--strong markup--pre-strong">False</strong>).unsqueeze(1)<br>anchors = V(np.concatenate([anc_ctrs, anc_sizes], axis=1), <br>              requires_grad=<strong class="markup--strong markup--pre-strong">False</strong>).float()<br>anchor_cnr = hw2corners(anchors[:,:2], anchors[:,2:])</pre><p name="43cc" id="43cc" class="graf graf--p graf-after--pre"><code class="markup--code markup--p-code">anchors</code>&nbsp;: middle and height, width</p><p name="1b94" id="1b94" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">anchor_cnr</code>&nbsp;: top left and bottom right corners</p><h4 name="ce3e" id="ce3e" class="graf graf--h4 graf-after--p">Review of key concept [<a href="https://youtu.be/0frKXR-2PBY?t=1h18m" data-href="https://youtu.be/0frKXR-2PBY?t=1h18m" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:18:00</a>]</h4><figure name="a3db" id="a3db" class="graf graf--figure graf-after--h4"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_C67J9RhTAiz9MCD-ebpp_w.png"></figure><ul class="postList"><li name="cdc9" id="cdc9" class="graf graf--li graf-after--figure">We have a vector of ground truth (sets of 4 bounding box coordinates and a class)</li><li name="eb81" id="eb81" class="graf graf--li graf-after--li">We have a neural net that takes some input and spits out some output activations</li><li name="1fb5" id="1fb5" class="graf graf--li graf-after--li">Compare the activations and the ground truth, calculate a loss, find the derivative of that, and adjust weights according to the derivative times a learning rate.</li><li name="90af" id="90af" class="graf graf--li graf-after--li">We need a loss function that can take ground truth and activation and spit out a number that says how good these activations are. To do this, we need to take each one of <code class="markup--code markup--li-code">m</code> ground truth objects and decide which set of <code class="markup--code markup--li-code">(4+c)</code> activations is responsible for that object [<a href="https://youtu.be/0frKXR-2PBY?t=1h21m58s" data-href="https://youtu.be/0frKXR-2PBY?t=1h21m58s" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">1:21:58</a>] — which one we should be comparing to decide whether the class is correct and bounding box is close or not (matching problem).</li><li name="48c8" id="48c8" class="graf graf--li graf-after--li">Since we are using SSD approach, so it is not arbitrary which ones we match up [<a href="https://youtu.be/0frKXR-2PBY?t=1h23m18s" data-href="https://youtu.be/0frKXR-2PBY?t=1h23m18s" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">1:23:18</a>]. We want to match up the set of activations whose receptive field has the maximum density from where the real object is.</li><li name="c1a1" id="c1a1" class="graf graf--li graf-after--li">The loss function needs to be some consistent task. If in the first image, the top left object corresponds with the first 4+c activations, and in the second image, we threw things around and suddenly it’s now going with the last 4+c activations, the neural net doesn’t know what to learn.</li><li name="3723" id="3723" class="graf graf--li graf-after--li">Once matching problem is resolved, the rest is just the same as the single object detection.</li></ul><p name="7654" id="7654" class="graf graf--p graf-after--li">Architectures:</p><ul class="postList"><li name="2c45" id="2c45" class="graf graf--li graf-after--p">YOLO — the last layer is fully connected (no concept of geometry)</li><li name="639f" id="639f" class="graf graf--li graf-after--li">SSD — the last layer is convolutional</li></ul><h4 name="db54" id="db54" class="graf graf--h4 graf-after--li">k (zooms x ratios)[<a href="https://youtu.be/0frKXR-2PBY?t=1h29m39s" data-href="https://youtu.be/0frKXR-2PBY?t=1h29m39s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:29:39</a>]</h4><p name="c68a" id="c68a" class="graf graf--p graf-after--h4">For every grid cell which can be different sizes, we can have different orientations and zooms representing different anchor boxes which are just like conceptual ideas that every one of anchor boxes is associated with one set of <code class="markup--code markup--p-code">4+c</code> activations in our model. So however many anchor boxes we have, we need to have that times <code class="markup--code markup--p-code">(4+c)</code> activations. That does not mean that each convolutional layer needs that many activations. Because 4x4 convolutional layer already has 16 sets of activations, the 2x2 layer has 4 sets of activations, and finally 1x1 has one set. So we basically get 1 + 4 + 16 for free. So we only needs to know <code class="markup--code markup--p-code">k</code> where <code class="markup--code markup--p-code">k</code> is the number of zooms by the number of aspect ratios. Where else, the grids, we will get for free through our architecture.</p><h4 name="ca16" id="ca16" class="graf graf--h4 graf-after--p">Model Architecture [<a href="https://youtu.be/0frKXR-2PBY?t=1h31m10s" data-href="https://youtu.be/0frKXR-2PBY?t=1h31m10s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:31:10</a>]</h4><pre name="4a50" id="4a50" class="graf graf--pre graf-after--h4">drop=0.4<br><br><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">SSD_MultiHead</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, k, bias):<br>        super().__init__()<br>        self.drop = nn.Dropout(drop)<br>        self.sconv0 = StdConv(512,256, stride=1, drop=drop)<br>        self.sconv1 = StdConv(256,256, drop=drop)<br>        self.sconv2 = StdConv(256,256, drop=drop)<br>        self.sconv3 = StdConv(256,256, drop=drop)<br>        self.out1 = OutConv(k, 256, bias)<br>        self.out2 = OutConv(k, 256, bias)<br>        self.out3 = OutConv(k, 256, bias)<br><br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x):<br>        x = self.drop(F.relu(x))<br>        x = self.sconv0(x)<br>        x = self.sconv1(x)<br>        o1c,o1l = self.out1(x)<br>        x = self.sconv2(x)<br>        o2c,o2l = self.out2(x)<br>        x = self.sconv3(x)<br>        o3c,o3l = self.out3(x)<br>        <strong class="markup--strong markup--pre-strong">return</strong> [torch.cat([o1c,o2c,o3c], dim=1),<br>                torch.cat([o1l,o2l,o3l], dim=1)]<br><br>head_reg4 = SSD_MultiHead(k, -4.)<br>models = ConvnetBuilder(f_model, 0, 0, 0, custom_head=head_reg4)<br>learn = ConvLearner(md, models)<br>learn.opt_fn = optim.Adam</pre><p name="5fd9" id="5fd9" class="graf graf--p graf-after--pre">The model is nearly identical to what we had before. But we have a number of stride 2 convolutions which is going to take us through to 4x4, 2x2, and 1x1 (each stride 2 convolution halves our grid size in both directions).</p><ul class="postList"><li name="5dc6" id="5dc6" class="graf graf--li graf-after--p">After we do our first convolution to get to 4x4, we will grab a set of outputs from that because we want to save away the 4x4 anchors.</li><li name="16d7" id="16d7" class="graf graf--li graf-after--li">Once we get to 2x2, we grab another set of now 2x2 anchors</li><li name="f368" id="f368" class="graf graf--li graf-after--li">Then finally we get to 1x1</li><li name="afc1" id="afc1" class="graf graf--li graf-after--li">We then concatenate them all together, which gives us the correct number of activations (one activation for every anchor box).</li></ul><h4 name="5984" id="5984" class="graf graf--h4 graf-after--li">Training [<a href="https://youtu.be/0frKXR-2PBY?t=1h32m50s" data-href="https://youtu.be/0frKXR-2PBY?t=1h32m50s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:32:50</a>]</h4><pre name="676d" id="676d" class="graf graf--pre graf-after--h4">learn.crit = ssd_loss<br>lr = 1e-2<br>lrs = np.array([lr/100,lr/10,lr])</pre><pre name="1dca" id="1dca" class="graf graf--pre graf-after--pre">learn.lr_find(lrs/1000,1.)<br>learn.sched.plot(n_skip_end=2)</pre><figure name="1150" id="1150" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_jB_OxbaTmMXHbkeXE4G0SQ.png"></figure><pre name="a580" id="a580" class="graf graf--pre graf-after--figure">learn.fit(lrs, 1, cycle_len=4, use_clr=(20,8))</pre><pre name="0d46" id="0d46" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss                            <br>    0      15.124349  15.015433 <br>    1      13.091956  10.39855                            <br>    2      11.643629  9.4289                              <br>    3      10.532467  8.822998</em></pre><pre name="296f" id="296f" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[8.822998]</em></pre><pre name="7b9f" id="7b9f" class="graf graf--pre graf-after--pre">learn.save('tmp')</pre><pre name="7493" id="7493" class="graf graf--pre graf-after--pre">learn.freeze_to(-2)<br>learn.fit(lrs/2, 1, cycle_len=4, use_clr=(20,8))</pre><pre name="98c2" id="98c2" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss                            <br>    0      9.821056   10.335152 <br>    1      9.419633   11.834093                           <br>    2      8.78818    7.907762                            <br>    3      8.219976   7.456364</em></pre><pre name="727f" id="727f" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[7.4563637]</em></pre><pre name="2c78" id="2c78" class="graf graf--pre graf-after--pre">x,y = next(iter(md.val_dl))<br>y = V(y)<br>batch = learn.model(V(x))<br>b_clas,b_bb = batch<br>x = to_np(x)<br><br>fig, axes = plt.subplots(3, 4, figsize=(16, 12))<br><strong class="markup--strong markup--pre-strong">for</strong> idx,ax <strong class="markup--strong markup--pre-strong">in</strong> enumerate(axes.flat):<br>    ima=md.val_ds.ds.denorm(x)[idx]<br>    bbox,clas = get_y(y[0][idx], y[1][idx])<br>    a_ic = actn_to_bb(b_bb[idx], anchors)<br>    torch_gt(ax, ima, a_ic, b_clas[idx].max(1)[1], <br>             b_clas[idx].max(1)[0].sigmoid(), <strong class="markup--strong markup--pre-strong">0.2</strong>)<br>plt.tight_layout()</pre><p name="bac0" id="bac0" class="graf graf--p graf-after--pre">Here, we printed out those detections with at least probability of <code class="markup--code markup--p-code">0.2</code>&nbsp;. Some of them look pretty hopeful but others not so much.</p><figure name="bc4f" id="bc4f" class="graf graf--figure graf--layoutOutsetCenter graf-after--p" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_l168j5d3fWBZLST3XLPD6A.png"></figure><h3 name="acc2" id="acc2" class="graf graf--h3 graf-after--figure">History of object detection [<a href="https://youtu.be/0frKXR-2PBY?t=1h33m43s" data-href="https://youtu.be/0frKXR-2PBY?t=1h33m43s" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">1:33:43</a>]</h3><figure name="16b5" id="16b5" class="graf graf--figure graf-after--h3"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_bQPvoI0soxtlBt1cEZlzcQ.png"></figure><p name="8ece" id="8ece" class="graf graf--p graf-after--figure"><a href="https://arxiv.org/abs/1312.2249" data-href="https://arxiv.org/abs/1312.2249" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Scalable Object Detection using Deep Neural Networks</a></p><ul class="postList"><li name="c5f6" id="c5f6" class="graf graf--li graf-after--p">When people refer to the multi-box method, they are talking about this paper.</li><li name="1caf" id="1caf" class="graf graf--li graf-after--li">This was the paper that came up with the idea that we can have a loss function that has this matching process and then you can use that to do object detection. So everything since that time has been trying to figure out how to make this better.</li></ul><p name="29bb" id="29bb" class="graf graf--p graf-after--li"><a href="https://arxiv.org/abs/1506.01497" data-href="https://arxiv.org/abs/1506.01497" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a></p><ul class="postList"><li name="3858" id="3858" class="graf graf--li graf-after--p">In parallel, Ross Girshick was going down a totally different direction. He had these two-stage process where the first stage used the classical computer vision approaches to find edges and changes of gradients to guess which parts of the image may represent distinct objects. Then fit each of those into a convolutional neural network which was basically designed to figure out if that is the kind of object we are interested in.</li><li name="9e50" id="9e50" class="graf graf--li graf-after--li">R-CNN and Fast R-CNN are hybrid of traditional computer vision and deep learning.</li><li name="3b77" id="3b77" class="graf graf--li graf-after--li">What Ross and his team then did was they took the multibox idea and replaced the traditional non-deep learning computer vision part of their two stage process with the conv net. So now they have two conv nets: one for region proposals (all of the things that might be objects) and the second part was the same as his earlier work.</li></ul><p name="9a15" id="9a15" class="graf graf--p graf-after--li"><a href="https://arxiv.org/abs/1506.02640" data-href="https://arxiv.org/abs/1506.02640" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">You Only Look Once: Unified, Real-Time Object Detection</a></p><p name="cb78" id="cb78" class="graf graf--p graf-after--p"><a href="https://arxiv.org/abs/1512.02325" data-href="https://arxiv.org/abs/1512.02325" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">SSD: Single Shot MultiBox Detector</a></p><ul class="postList"><li name="9e7d" id="9e7d" class="graf graf--li graf-after--p">At similar time these paper came out. Both of these did something pretty cool which is they achieved similar performance as the Faster R-CNN but with 1 stage.</li><li name="ea38" id="ea38" class="graf graf--li graf-after--li">They took the multibox idea and they tried to figure out how to deal with messy outputs. The basic ideas were to use, for example, hard negative mining where they would go through and find all of the matches that did not look that good and throw them away, use very tricky and complex data augmentation methods, and all kind of hackery. But they got them to work pretty well.</li></ul><p name="a3f4" id="a3f4" class="graf graf--p graf-after--li"><a href="https://arxiv.org/abs/1708.02002" data-href="https://arxiv.org/abs/1708.02002" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Focal Loss for Dense Object Detection</a> (RetinaNet)</p><ul class="postList"><li name="e6d1" id="e6d1" class="graf graf--li graf-after--p">Then something really cool happened late last year which is this thing called focal loss.</li><li name="883d" id="883d" class="graf graf--li graf-after--li">They actually realized why this messy thing wasn’t working. When we look at an image, there are 3 different granularities of convolutional grid (4x4, 2x2, 1x1) [<a href="https://youtu.be/0frKXR-2PBY?t=1h37m28s" data-href="https://youtu.be/0frKXR-2PBY?t=1h37m28s" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">1:37:28</a>]. The 1x1 is quite likely to have a reasonable overlap with some object because most photos have some kind of main subject. On the other hand, in the 4x4 grid cells, the most of 16 anchor boxes are not going to have a much of an overlap with anything. So if somebody was to say to you “$20 bet, what do you reckon this little clip is?” and you are not sure, you will say “background” because most of the time, it is the background.</li></ul><p name="05da" id="05da" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">Question</strong>: I understand why we have a 4x4 grid of receptive fields with 1 anchor box each to coarsely localize objects in the image. But what I think I’m missing is why we need multiple receptive fields at different sizes. The first version already included 16 receptive fields, each with a single anchor box associated. With the additions, there are now many more anchor boxes to consider. Is this because you constrained how much a receptive field could move or scale from its original size? Or is there another reason? [<a href="https://youtu.be/0frKXR-2PBY?t=1h38m47s" data-href="https://youtu.be/0frKXR-2PBY?t=1h38m47s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:38:47</a>] It is kind of backwards. The reason Jeremy did the constraining was because he knew he was going to be adding more boxes later. But really, the reason is that the Jaccard overlap between one of those 4x4 grid cells and a picture where a single object that takes up most of the image is never going to be 0.5. The intersection is much smaller than the union because the object is too big. So for this general idea to work where we are saying you are responsible for something that you have better than 50% overlap with, we need anchor boxes which will on a regular basis have a 50% or higher overlap which means we need to have a variety of sizes, shapes, and scales. This all happens in the loss function. The vast majority of the interesting stuff in all of the object detection is the loss function.</p><h4 name="bfb0" id="bfb0" class="graf graf--h4 graf-after--p">Focal Loss [<a href="https://youtu.be/0frKXR-2PBY?t=1h40m38s" data-href="https://youtu.be/0frKXR-2PBY?t=1h40m38s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:40:38</a>]</h4><figure name="aa98" id="aa98" class="graf graf--figure graf-after--h4"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_6Bood7G6dUuhigy9cxkZ-Q.png"></figure><p name="7caf" id="7caf" class="graf graf--p graf-after--figure">The key thing is this very first picture. The blue line is the binary cross entropy loss. If the answer is not a motorbike [<a href="https://youtu.be/0frKXR-2PBY?t=1h41m46s" data-href="https://youtu.be/0frKXR-2PBY?t=1h41m46s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:41:46</a>], and I said “I think it’s not a motorbike and I am 60% sure” with the blue line, the loss is still about 0.5 which is pretty bad. So if we want to get our loss down, then for all these things which are actually back ground, we have to be saying “I am sure that is background”, “I am sure it’s not a motorbike, or a bus, or a person” — because if I don’t say we are sure it is not any of these things, then we still get loss.</p><p name="75ba" id="75ba" class="graf graf--p graf-after--p">That is why the motorbike example did not work [<a href="https://youtu.be/0frKXR-2PBY?t=1h42m39s" data-href="https://youtu.be/0frKXR-2PBY?t=1h42m39s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:42:39</a>]. Because even when it gets to lower right corner and it wants to say “I think it’s a motorbike”, there is no payoff for it to say so. If it is wrong, it gets killed. And the vast majority of the time, it is background. Even if it is not background, it is not enough just to say “it’s not background” — you have to say which of the 20 things it is.</p><p name="0405" id="0405" class="graf graf--p graf-after--p">So the trick is to trying to find a different loss function [<a href="https://youtu.be/0frKXR-2PBY?t=1h44m" data-href="https://youtu.be/0frKXR-2PBY?t=1h44m" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:44:00</a>] that looks more like the purple line. Focal loss is literally just a scaled cross entropy loss. Now if we say “I’m&nbsp;.6 sure it’s not a motorbike” then the loss function will say “good for you! no worries” [<a href="https://youtu.be/0frKXR-2PBY?t=1h44m42s" data-href="https://youtu.be/0frKXR-2PBY?t=1h44m42s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:44:42</a>].</p><p name="e20f" id="e20f" class="graf graf--p graf-after--p">The actual contribution of this paper is to add <code class="markup--code markup--p-code">(1 − pt)^γ</code> to the start of the equation [<a href="https://youtu.be/0frKXR-2PBY?t=1h45m6s" data-href="https://youtu.be/0frKXR-2PBY?t=1h45m6s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:45:06</a>] which sounds like nothing but actually people have been trying to figure out this problem for years. When you come across a paper like this which is game-changing, you shouldn’t assume you are going to have to write thousands of lines of code. Very often it is one line of code, or the change of a single constant, or adding log to a single place.</p><p name="3d32" id="3d32" class="graf graf--p graf-after--p">A couple of terrific things about this paper [<a href="https://youtu.be/0frKXR-2PBY?t=1h46m8s" data-href="https://youtu.be/0frKXR-2PBY?t=1h46m8s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:46:08</a>]:</p><ul class="postList"><li name="4dce" id="4dce" class="graf graf--li graf-after--p">Equations are written in a simple manner</li><li name="b4f3" id="b4f3" class="graf graf--li graf-after--li">They “refactor”</li></ul><h4 name="021e" id="021e" class="graf graf--h4 graf-after--li">Implementing Focal Loss [<a href="https://youtu.be/0frKXR-2PBY?t=1h49m27s" data-href="https://youtu.be/0frKXR-2PBY?t=1h49m27s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:49:27</a>]:</h4><figure name="a6b9" id="a6b9" class="graf graf--figure graf-after--h4"><img class="graf-image" data-image-id="1*wIp0HYEWPnkiuxLeCfEiAg.png" data-width="429" data-height="36" src="../img/1_wIp0HYEWPnkiuxLeCfEiAg.png"></figure><p name="0a90" id="0a90" class="graf graf--p graf-after--figure">Remember, -log(pt) is the cross entropy loss and focal loss is just a scaled version. When we defined the binomial cross entropy loss, you may have noticed that there was a weight which by default was none:</p><pre name="2c64" id="2c64" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">BCE_Loss</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, num_classes):<br>        super().__init__()<br>        self.num_classes = num_classes<br><br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, pred, targ):<br>        t = one_hot_embedding(targ, self.num_classes+1)<br>        t = V(t[:,:-1].contiguous())<em class="markup--em markup--pre-em">#.cpu()</em><br>        x = pred[:,:-1]<br>        w = self.get_weight(x,t)<br>        <strong class="markup--strong markup--pre-strong">return</strong> F.binary_cross_entropy_with_logits(x, t, w, <br>                          size_average=<strong class="markup--strong markup--pre-strong">False</strong>)/self.num_classes<br>    <br>    <strong class="markup--strong markup--pre-strong">def</strong> get_weight(self,x,t): <strong class="markup--strong markup--pre-strong">return</strong> <strong class="markup--strong markup--pre-strong">None</strong></pre><p name="0e89" id="0e89" class="graf graf--p graf-after--pre">When you call <code class="markup--code markup--p-code">F.binary_cross_entropy_with_logits</code>, you can pass in the weight. Since we just wanted to multiply a cross entropy by something, we can just define <code class="markup--code markup--p-code">get_weight</code>. Here is the entirety of focal loss [<a href="https://youtu.be/0frKXR-2PBY?t=1h50m23s" data-href="https://youtu.be/0frKXR-2PBY?t=1h50m23s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:50:23</a>]:</p><pre name="1cbf" id="1cbf" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">FocalLoss</strong>(BCE_Loss):<br>    <strong class="markup--strong markup--pre-strong">def</strong> get_weight(self,x,t):<br>        alpha,gamma = 0.25,2.<br>        p = x.sigmoid()<br>        pt = p*t + (1-p)*(1-t)<br>        w = alpha*t + (1-alpha)*(1-t)<br>        <strong class="markup--strong markup--pre-strong">return</strong> w * (1-pt).pow(gamma)</pre><p name="9651" id="9651" class="graf graf--p graf-after--pre">If you were wondering why alpha and gamma are 0.25 and 2, here is another excellent thing about this paper, because they tried lots of different values and found that these work well:</p><figure name="9bed" id="9bed" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_qFPRvFHQMQplSJGp3QLiNA.png"></figure><h4 name="6ed2" id="6ed2" class="graf graf--h4 graf-after--figure">Training [<a href="https://youtu.be/0frKXR-2PBY?t=1h51m25s" data-href="https://youtu.be/0frKXR-2PBY?t=1h51m25s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:51:25</a>]</h4><pre name="ebda" id="ebda" class="graf graf--pre graf-after--h4">learn.lr_find(lrs/1000,1.)<br>learn.sched.plot(n_skip_end=2)</pre><figure name="5e64" id="5e64" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_lQPSR3V2IXbxOpcgNE-U-Q.png"></figure><pre name="cf7e" id="cf7e" class="graf graf--pre graf-after--figure">learn.fit(lrs, 1, cycle_len=10, use_clr=(20,10))</pre><pre name="a909" id="a909" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss                            <br>    0      24.263046  28.975235 <br>    1      20.459562  16.362392                           <br>    2      17.880827  14.884829                           <br>    3      15.956896  13.676485                           <br>    4      14.521345  13.134197                           <br>    5      13.460941  12.594139                           <br>    6      12.651842  12.069849                           <br>    7      11.944972  11.956457                           <br>    8      11.385798  11.561226                           <br>    9      10.988802  11.362164</em></pre><pre name="f3db" id="f3db" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[11.362164]</em></pre><pre name="08b0" id="08b0" class="graf graf--pre graf-after--pre">learn.save('fl0')<br>learn.load('fl0')</pre><pre name="5836" id="5836" class="graf graf--pre graf-after--pre">learn.freeze_to(-2)<br>learn.fit(lrs/4, 1, cycle_len=10, use_clr=(20,10))</pre><pre name="911e" id="911e" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss                            <br>    0      10.871668  11.615532 <br>    1      10.908461  11.604334                           <br>    2      10.549796  11.486127                           <br>    3      10.130961  11.088478                           <br>    4      9.70691    10.72144                            <br>    5      9.319202   10.600481                           <br>    6      8.916653   10.358334                           <br>    7      8.579452   10.624706                           <br>    8      8.274838   10.163422                           <br>    9      7.994316   10.108068</em></pre><pre name="44c8" id="44c8" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[10.108068]</em></pre><pre name="268f" id="268f" class="graf graf--pre graf-after--pre">learn.save('drop4')<br>learn.load('drop4')</pre><pre name="eed6" id="eed6" class="graf graf--pre graf-after--pre">plot_results(0.75)</pre><figure name="3481" id="3481" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_G4HCc1mpkvHFqbhrb5Uwpw.png"></figure><p name="bc8c" id="bc8c" class="graf graf--p graf-after--figure">This time things are looking quite a bit better. So our last step, for now, is to basically figure out how to pull out just the interesting ones.</p><h4 name="dc8a" id="dc8a" class="graf graf--h4 graf-after--p">Non Maximum Suppression [<a href="https://youtu.be/0frKXR-2PBY?t=1h52m15s" data-href="https://youtu.be/0frKXR-2PBY?t=1h52m15s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:52:15</a>]</h4><p name="f1db" id="f1db" class="graf graf--p graf-after--h4">All we are going to do is we are going to go through every pair of these bounding boxes and if they overlap by more than some amount, say 0.5, using Jaccard and they are both predicting the same class, we are going to assume they are the same thing and we are going to pick the one with higher <code class="markup--code markup--p-code">p</code> value.</p><p name="6313" id="6313" class="graf graf--p graf-after--p">It is really boring code, Jeremy didn’t write it himself and copied somebody else’s. No reason particularly to go through it.</p><pre name="1f12" id="1f12" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> nms(boxes, scores, overlap=0.5, top_k=100):<br>    keep = scores.new(scores.size(0)).zero_().long()<br>    <strong class="markup--strong markup--pre-strong">if</strong> boxes.numel() == 0: <strong class="markup--strong markup--pre-strong">return</strong> keep<br>    x1 = boxes[:, 0]<br>    y1 = boxes[:, 1]<br>    x2 = boxes[:, 2]<br>    y2 = boxes[:, 3]<br>    area = torch.mul(x2 - x1, y2 - y1)<br>    v, idx = scores.sort(0)  <em class="markup--em markup--pre-em"># sort in ascending order</em><br>    idx = idx[-top_k:]  <em class="markup--em markup--pre-em"># indices of the top-k largest vals</em><br>    xx1 = boxes.new()<br>    yy1 = boxes.new()<br>    xx2 = boxes.new()<br>    yy2 = boxes.new()<br>    w = boxes.new()<br>    h = boxes.new()<br><br>    count = 0<br>    <strong class="markup--strong markup--pre-strong">while</strong> idx.numel() &gt; 0:<br>        i = idx[-1]  <em class="markup--em markup--pre-em"># index of current largest val</em><br>        keep[count] = i<br>        count += 1<br>        <strong class="markup--strong markup--pre-strong">if</strong> idx.size(0) == 1: <strong class="markup--strong markup--pre-strong">break</strong><br>        idx = idx[:-1]  <em class="markup--em markup--pre-em"># remove kept element from view</em><br>        <em class="markup--em markup--pre-em"># load bboxes of next highest vals</em><br>        torch.index_select(x1, 0, idx, out=xx1)<br>        torch.index_select(y1, 0, idx, out=yy1)<br>        torch.index_select(x2, 0, idx, out=xx2)<br>        torch.index_select(y2, 0, idx, out=yy2)<br>        <em class="markup--em markup--pre-em"># store element-wise max with next highest score</em><br>        xx1 = torch.clamp(xx1, min=x1[i])<br>        yy1 = torch.clamp(yy1, min=y1[i])<br>        xx2 = torch.clamp(xx2, max=x2[i])<br>        yy2 = torch.clamp(yy2, max=y2[i])<br>        w.resize_as_(xx2)<br>        h.resize_as_(yy2)<br>        w = xx2 - xx1<br>        h = yy2 - yy1<br>        <em class="markup--em markup--pre-em"># check sizes of xx1 and xx2.. after each iteration</em><br>        w = torch.clamp(w, min=0.0)<br>        h = torch.clamp(h, min=0.0)<br>        inter = w*h<br>        <em class="markup--em markup--pre-em"># IoU = i / (area(a) + area(b) - i)</em><br>        rem_areas = torch.index_select(area, 0, idx)  <br>        <em class="markup--em markup--pre-em"># load remaining areas)</em><br>        union = (rem_areas - inter) + area[i]<br>        IoU = inter/union  <em class="markup--em markup--pre-em"># store result in iou</em><br>        <em class="markup--em markup--pre-em"># keep only elements with an IoU &lt;= overlap</em><br>        idx = idx[IoU.le(overlap)]<br>    <strong class="markup--strong markup--pre-strong">return</strong> keep, count</pre><pre name="6e66" id="6e66" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> show_nmf(idx):<br>    ima=md.val_ds.ds.denorm(x)[idx]<br>    bbox,clas = get_y(y[0][idx], y[1][idx])<br>    a_ic = actn_to_bb(b_bb[idx], anchors)<br>    clas_pr, clas_ids = b_clas[idx].max(1)<br>    clas_pr = clas_pr.sigmoid()<br><br>    conf_scores = b_clas[idx].sigmoid().t().data<br><br>    out1,out2,cc = [],[],[]<br>    <strong class="markup--strong markup--pre-strong">for</strong> cl <strong class="markup--strong markup--pre-strong">in</strong> range(0, len(conf_scores)-1):<br>        c_mask = conf_scores[cl] &gt; 0.25<br>        <strong class="markup--strong markup--pre-strong">if</strong> c_mask.sum() == 0: <strong class="markup--strong markup--pre-strong">continue</strong><br>        scores = conf_scores[cl][c_mask]<br>        l_mask = c_mask.unsqueeze(1).expand_as(a_ic)<br>        boxes = a_ic[l_mask].view(-1, 4)<br>        ids, count = nms(boxes.data, scores, 0.4, 50)<br>        ids = ids[:count]<br>        out1.append(scores[ids])<br>        out2.append(boxes.data[ids])<br>        cc.append([cl]*count)<br>    cc = T(np.concatenate(cc))<br>    out1 = torch.cat(out1)<br>    out2 = torch.cat(out2)<br><br>    fig, ax = plt.subplots(figsize=(8,8))<br>    torch_gt(ax, ima, out2, cc, out1, 0.1)</pre><pre name="26af" id="26af" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(12): show_nmf(i)</pre><figure name="fc05" id="fc05" class="graf graf--figure graf--layoutOutsetCenter graf-after--pre" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_MXk2chJJEcjOz8hMn1ZsOQ.png"></figure><figure name="d53d" id="d53d" class="graf graf--figure graf--layoutOutsetCenter graf-after--figure" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_Fj9fK3G6iXBsGI_XJrxXyg.png"></figure><figure name="2958" id="2958" class="graf graf--figure graf--layoutOutsetCenter graf-after--figure" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_6p3dm-i-YxC9QkxouHJdoA.png"></figure><figure name="75fa" id="75fa" class="graf graf--figure graf--layoutOutsetCenter graf-after--figure" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_nkEpAd2_H4lG1vQfnCJn4Q.png"></figure><figure name="cbef" id="cbef" class="graf graf--figure graf--layoutOutsetCenter graf-after--figure" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_THGq5C21NaP92vw5E_QNdA.png"></figure><figure name="b2b2" id="b2b2" class="graf graf--figure graf--layoutOutsetCenter graf-after--figure" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_0wckbiUSax2JpBlgJxJ05g.png"></figure><figure name="a1f1" id="a1f1" class="graf graf--figure graf--layoutOutsetCenter graf-after--figure" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_EWbNGEQFvYMgC4PSaLe8Ww.png"></figure><figure name="5cba" id="5cba" class="graf graf--figure graf--layoutOutsetCenter graf-after--figure" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_vTRCVjln4vkma1R6eBeSwA.png"></figure><figure name="4351" id="4351" class="graf graf--figure graf--layoutOutsetCenter graf-after--figure" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_3Q01FZuzfptkYrekJiGm1g.png"></figure><figure name="b40b" id="b40b" class="graf graf--figure graf--layoutOutsetCenter graf-after--figure" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_-cD3LQIG9FnyJbt0cnpbNg.png"></figure><figure name="ead3" id="ead3" class="graf graf--figure graf--layoutOutsetCenter graf-after--figure" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_Hkgs1u9PFH9ZrTKL8YBW2Q.png"></figure><figure name="bb83" id="bb83" class="graf graf--figure graf--layoutOutsetCenter graf-after--figure" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_uyTNlp61jcyaW9knbnNSEw.png"></figure><p name="55b5" id="55b5" class="graf graf--p graf-after--figure">There are some things still to fix here [<a href="https://youtu.be/0frKXR-2PBY?t=1h53m43s" data-href="https://youtu.be/0frKXR-2PBY?t=1h53m43s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:53:43</a>]. The trick will be to use something called feature pyramid. That is what we are going to do in lesson 14.</p><h4 name="5eed" id="5eed" class="graf graf--h4 graf-after--p">Talking a little more about SSD paper [<a href="https://youtu.be/0frKXR-2PBY?t=1h54m3s" data-href="https://youtu.be/0frKXR-2PBY?t=1h54m3s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:54:03</a>]</h4><p name="b19e" id="b19e" class="graf graf--p graf-after--h4">When this paper came out, Jeremy was excited because this and YOLO were the first kind of single-pass good quality object detection method that come along. There has been this continuous repetition of history in the deep learning world which is things that involve multiple passes of multiple different pieces, over time, particularly where they involve some non-deep learning pieces (like R-CNN did), over time, they always get turned into a single end-to-end deep learning model. So I tend to ignore them until that happens because that’s the point where people have figured out how to show this as a deep learning model, as soon as they do that they generally end up something much faster and much more accurate. So SSD and YOLO were really important.</p><p name="0629" id="0629" class="graf graf--p graf-after--p">The model is 4 paragraphs. Papers are really concise which means you need to read them pretty carefully. Partly, though, you need to know which bits to read carefully. The bits where they say “here we are going to prove the error bounds on this model,” you could ignore that because you don’t care about proving error bounds. But the bit which says here is what the model is, you need to read real carefully.</p><p name="18c6" id="18c6" class="graf graf--p graf-after--p">Jeremy reads a section <strong class="markup--strong markup--p-strong">2.1 Model</strong> [<a href="https://youtu.be/0frKXR-2PBY?t=1h56m37s" data-href="https://youtu.be/0frKXR-2PBY?t=1h56m37s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:56:37</a>]</p><p name="1d2d" id="1d2d" class="graf graf--p graf-after--p">If you jump straight in and read a paper like this, these 4 paragraphs would probably make no sense. But now that we’ve gone through it, you read those and hopefully thinking “oh that’s just what Jeremy said, only they sad it better than Jeremy and less words [<a href="https://youtu.be/0frKXR-2PBY?t=2h37s" data-href="https://youtu.be/0frKXR-2PBY?t=2h37s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:00:37</a>]. If you start to read a paper and go “what the heck”, the trick is to then start reading back over the citations.</p><p name="9414" id="9414" class="graf graf--p graf-after--p">Jeremy reads <strong class="markup--strong markup--p-strong">Matching strategy</strong> and <strong class="markup--strong markup--p-strong">Training objective</strong> (a.k.a. Loss function)[<a href="https://youtu.be/0frKXR-2PBY?t=2h1m44s" data-href="https://youtu.be/0frKXR-2PBY?t=2h1m44s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:01:44</a>]</p><h4 name="11d8" id="11d8" class="graf graf--h4 graf-after--p">Some paper tips [<a href="https://youtu.be/0frKXR-2PBY?t=2h2m34s" data-href="https://youtu.be/0frKXR-2PBY?t=2h2m34s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">2:02:34</a>]</h4><p name="6731" id="6731" class="graf graf--p graf-after--h4"><a href="https://arxiv.org/pdf/1312.2249.pdf" data-href="https://arxiv.org/pdf/1312.2249.pdf" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Scalable Object Detection using Deep Neural Networks</a></p><ul class="postList"><li name="20f5" id="20f5" class="graf graf--li graf--startsWithDoubleQuote graf-after--p">“Training objective” is loss function</li><li name="1d70" id="1d70" class="graf graf--li graf-after--li">Double bars and two 2’s like this means Mean Squared Error</li></ul><figure name="4909" id="4909" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*LubBtX9ODFMBgI34bFHtdw.png" data-width="446" data-height="73" src="../img/1_LubBtX9ODFMBgI34bFHtdw.png"></figure><ul class="postList"><li name="72d3" id="72d3" class="graf graf--li graf-after--figure">log(c) and log(1-c), and x and (1-x) they are all the pieces for binary cross entropy:</li></ul><figure name="ba87" id="ba87" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*3Xq3HB72jsVKI7uHOHzRDQ.png" data-width="562" data-height="86" src="../img/1_3Xq3HB72jsVKI7uHOHzRDQ.png"></figure><p name="2118" id="2118" class="graf graf--p graf-after--figure graf--trailing">This week, go through the code and go through the paper and see what is going on. Remember what Jeremy did to make it easier for you was he took that loss function, he copied it into a cell and split it up so that each bit was in a separate cell. Then after every sell, he printed or plotted that value. Hopefully this is a good starting point.</p><hr class="section-divider"><p name="27d5" id="27d5" class="graf graf--p graf--leading graf--trailing">Lessons: <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" class="markup--anchor markup--p-anchor" target="_blank">1</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" class="markup--anchor markup--p-anchor" target="_blank">2</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" class="markup--anchor markup--p-anchor" target="_blank">3</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" class="markup--anchor markup--p-anchor" target="_blank">4</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" class="markup--anchor markup--p-anchor" target="_blank">5</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" class="markup--anchor markup--p-anchor" target="_blank">6</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" class="markup--anchor markup--p-anchor" target="_blank">7</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" class="markup--anchor markup--p-anchor" target="_blank">8</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" class="markup--anchor markup--p-anchor" target="_blank"><strong class="markup--strong markup--p-strong">9</strong></a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" class="markup--anchor markup--p-anchor" target="_blank">10</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" class="markup--anchor markup--p-anchor" target="_blank">11</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" class="markup--anchor markup--p-anchor" target="_blank">12</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" class="markup--anchor markup--p-anchor" target="_blank">13</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" class="markup--anchor markup--p-anchor" target="_blank">14</a></p></body></html>