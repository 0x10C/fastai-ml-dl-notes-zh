<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></head><body><p name="9d2d" id="9d2d" class="graf graf--p graf-after--p">Let’s see how this works [<a href="https://youtu.be/tY0n9OT5_nA?t=1h37m9s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h37m9s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:37:09</a>]. We are going to take the last layer’s hidden state and we are going to stick it into a linear layer. Then we are going to stick it into a nonlinear activation, then we are going to do a matrix multiply. So if you think about it — a linear layer, nonlinear activation, matrix multiple — it’s a neural net. It is a neural net with one hidden layer. Stick it into a softmax and then we can use that to weight our encoder outputs. Now rather than just taking the last encoder output, we have the whole tensor of all of the encoder outputs which we just weight by this neural net we created.</p><p name="1183" id="1183" class="graf graf--p graf-after--p">In Python, <code class="markup--code markup--p-code">A @ B</code> is the matrix product, <code class="markup--code markup--p-code">A * B</code> the element-wise product</p><h4 name="ccd3" id="ccd3" class="graf graf--h4 graf-after--p">Papers [<a href="https://youtu.be/tY0n9OT5_nA?t=1h38m18s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h38m18s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:38:18</a>]</h4><ul class="postList"><li name="a4b0" id="a4b0" class="graf graf--li graf-after--h4"><a href="https://arxiv.org/abs/1409.0473" data-href="https://arxiv.org/abs/1409.0473" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">Neural Machine Translation by Jointly Learning to Align and Translate</a> — One amazing paper that originally introduced this idea of attention as well as a couple of key things which have really changed how people work in this field. They say area of attention has been used not just for text but for things like reading text out of pictures or doing various things with computer vi sion.</li><li name="d991" id="d991" class="graf graf--li graf-after--li"><a href="https://arxiv.org/abs/1412.7449" data-href="https://arxiv.org/abs/1412.7449" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">Grammar as a Foreign Language</a> — The second paper which Geoffrey Hinton was involved in that used this idea of RNN with attention to try to replace rules based grammar with an RNN which automatically tagged each word based on the grammar. It turned out to do it better than any rules based system which today seems obvious but at that time it was considered really surprising. They are summary of how attention works which is really nice and concise.</li></ul><p name="d35b" id="d35b" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">Question</strong>: Could you please explain attention again? [<a href="https://youtu.be/tY0n9OT5_nA?t=1h39m46s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h39m46s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:39:46</a>] Sure! Let’s go back and look at our original encoder.</p><figure name="8d45" id="8d45" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_NsX_t2WEEjsPcVyWOrcXFA.png"></figure><p name="bec9" id="bec9" class="graf graf--p graf-after--figure">The RNN spits out two things: it spits out a list of the state after every time step (<code class="markup--code markup--p-code">enc_out</code>), and it also tells you the state at the last time step (<code class="markup--code markup--p-code">h</code>)and we used the state at the last time step to create the input state for our decoder which is one vector <code class="markup--code markup--p-code">s</code> below:</p><figure name="b726" id="b726" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_QQiYoum-_J9Rm7DEQCElxA.png"></figure><p name="51f9" id="51f9" class="graf graf--p graf-after--figure">But we know that it’s creating a vector at every time steps (orange arrows), so wouldn’t it be nice to use them all? But wouldn’t it be nice to use the one or ones that’s most relevant to translating the word we are translating now? So wouldn’t it be nice to be able to take a weighted average of the hidden state at each time step weighted by whatever is the appropriate weight right now. For example, “liebte” would definitely be time step #2 is what it’s all about because that is the word I’m translating. So how do we get a list of weights that is suitable fore the word we are training right now? The answer is by training a neural net to figure out the list of weights. So anytime we want to figure out how to train a little neural net that does any task, the easiest way, normally always to do that is to include it in your module and train it in line with everything else. The minimal possible neural net is something that contains two layers and one nonlinear activation function, so <code class="markup--code markup--p-code">self.l2</code> is one linear layer.</p><figure name="4759" id="4759" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_fnTtr-UiW5JtNy8M9q1mkg.png"></figure><p name="8986" id="8986" class="graf graf--p graf-after--figure">In fact, instead of a linear layer, we can even just grab a random matrix if we do not care about bias [<a href="https://youtu.be/tY0n9OT5_nA?t=1h42m18s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h42m18s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:42:18</a>]. <code class="markup--code markup--p-code">self.W1</code> is a random tensor wrapped up in a <code class="markup--code markup--p-code">Parameter</code>.</p><p name="8015" id="8015" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">Parameter</code>&nbsp;: Remember, a <code class="markup--code markup--p-code">Parameter</code> is identical to PyTorch <code class="markup--code markup--p-code">Variable</code> but it just tells PyTorch “I want you to learn the weights for this please.” [<a href="https://youtu.be/tY0n9OT5_nA?t=1h42m35s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h42m35s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:42:35</a>]</p><p name="d5b1" id="d5b1" class="graf graf--p graf-after--p">So when we start out our decoder, let’s take the current hidden state of the decoder, put that into a linear layer (<code class="markup--code markup--p-code">self.l2</code>) because what is the information we use to decide what words we should focus on next — the only information we have to go on is what the decoder’s hidden state is now. So let’s grab that:</p><ul class="postList"><li name="32a8" id="32a8" class="graf graf--li graf-after--p">put it into the linear layer (<code class="markup--code markup--li-code">self.l2</code>)</li><li name="b54a" id="b54a" class="graf graf--li graf-after--li">put it through a non-linearity (<code class="markup--code markup--li-code">F.tanh</code>)</li><li name="0783" id="0783" class="graf graf--li graf-after--li">put it through one more nonlinear layer (<code class="markup--code markup--li-code">u @ self.V</code> doesn’t have a bias in it so it’s just matrix multiply)</li><li name="f948" id="f948" class="graf graf--li graf-after--li">put that through softmax</li></ul><p name="5710" id="5710" class="graf graf--p graf-after--li">That’s it — a little neural net. It doesn’t do anything. It’s just a neural net and no neural nets do anything they are just linear layers with nonlinear activations with random weights. But it starts to do something if we give it a job to do. In this case, the job we give it to do is to say don’t just take the final state but now let’s use all of the encoder states and let’s take all of them and multiply them by the output of that little neural net. So given that the things in this little neural net are learnable weights, hopefully it’s going to learn to weight those encoder hidden states by something useful. That is all neural net ever does is we give it some random weights to start with and a job to do, and hope that it learns to do the job. It turns out, it does.</p><figure name="9371" id="9371" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_jOVVKAFwMxGt9v6WEqgLXw.png"></figure><p name="2e00" id="2e00" class="graf graf--p graf-after--figure">Everything else in here is identical to what it was before. We have teacher forcing, it’s not bi-directional, so we can see how this goes.</p><pre name="cab4" id="cab4" class="graf graf--pre graf-after--p">rnn = Seq2SeqAttnRNN(fr_vecd, fr_itos, dim_fr_vec, en_vecd, en_itos, dim_en_vec, nh, enlen_90)<br>learn = RNN_Learner(md, SingleModel(to_gpu(rnn)), opt_fn=opt_fn)<br>learn.crit = seq2seq_loss<br>lr=2e-3</pre><pre name="baa5" id="baa5" class="graf graf--pre graf-after--pre">learn.fit(lr, 1, cycle_len=15, use_clr=(20,10), <br>          stepper=Seq2SeqStepper)</pre><pre name="19fc" id="19fc" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss                              <br>    0      3.882168   11.125291 <br>    1      3.599992   6.667136                              <br>    2      3.236066   5.552943                              <br>    3      3.050283   4.919096                              <br>    4      2.99024    4.500383                              <br>    5      3.07999    4.000295                              <br>    6      2.891087   4.024115                              <br>    7      2.854725   3.673913                              <br>    8      2.979285   3.590668                              <br>    9      3.109851   3.459867                              <br>    10     2.92878    3.517598                              <br>    11     2.778292   3.390253                              <br>    12     2.795427   3.388423                              <br>    13     2.809757   3.353334                              <br>    14     2.6723     3.368584</em></pre><pre name="5460" id="5460" class="graf graf--pre graf-after--pre">[3.3685837]</pre><p name="b4e4" id="b4e4" class="graf graf--p graf-after--pre">Teacher forcing had 3.49 and now with nearly exactly the same thing but we’ve got this little minimal neural net figuring out what weightings to give our inputs and we are down to 3.37. Remember, these loss are logs, so <code class="markup--code markup--p-code">e^3.37</code> is quite a significant change.</p><pre name="cc25" id="cc25" class="graf graf--pre graf-after--p">learn.save('attn')</pre><h4 name="1cca" id="1cca" class="graf graf--h4 graf-after--pre">Test [<a href="https://youtu.be/tY0n9OT5_nA?t=1h45m37s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h45m37s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:45:37</a>]</h4><pre name="8d2a" id="8d2a" class="graf graf--pre graf-after--h4">x,y = next(iter(val_dl))<br>probs,attns = learn.model(V(x),ret_attn=<strong class="markup--strong markup--pre-strong">True</strong>)<br>preds = to_np(probs.max(2)[1])</pre><pre name="f2fe" id="f2fe" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(180,190):<br>    print(' '.join([fr_itos[o] <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> x[:,i] <strong class="markup--strong markup--pre-strong">if</strong> o != 1]))<br>    print(' '.join([en_itos[o] <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> y[:,i] <strong class="markup--strong markup--pre-strong">if</strong> o != 1]))<br>    print(' '.join([en_itos[o] <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> preds[:,i] <strong class="markup--strong markup--pre-strong">if</strong> o!=1]))<br>    print()</pre><pre name="b7a3" id="b7a3" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">quels facteurs pourraient influer sur le choix de leur emplacement ? _eos_<br>what factors influencetheir location ? _eos_<br>what factors might influence the their their their ? _eos_</em></pre><pre name="1168" id="1168" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">qu’ est -ce qui ne peut pas changer ? _eos_<br>what can not change ? _eos_<br>what can not change change ? _eos_</em></pre><pre name="4f6f" id="4f6f" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">que faites - vous ? _eos_<br>what do you do ? _eos_<br>what do you do ? _eos_</em></pre><pre name="2284" id="2284" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">qui réglemente les pylônes d' antennes ? _eos_<br>who regulates antenna towers ? _eos_<br>who regulates the lights ? ? _eos_</em></pre><pre name="5a9a" id="5a9a" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">où sont - ils situés ? _eos_<br>where are they located ? _eos_<br>where are they located ? _eos_</em></pre><pre name="dcfb" id="dcfb" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">quelles sont leurs compétences ? _eos_<br>what are their qualifications ? _eos_<br>what are their skills ? _eos_</em></pre><pre name="4807" id="4807" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">qui est victime de harcèlement sexuel ? _eos_<br>who experiences sexual harassment ? _eos_<br>who is victim sexual sexual ? _eos_</em></pre><pre name="f69f" id="f69f" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">quelles sont les personnes qui visitent les communautés autochtones ? _eos_<br>who visits indigenous communities ? _eos_<br>who is people people aboriginal people ? _eos_</em></pre><pre name="1609" id="1609" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">pourquoi ces trois points en particulier ? _eos_<br>why these specific three ? _eos_<br>why are these three three ? ? _eos_</em></pre><pre name="f840" id="f840" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">pourquoi ou pourquoi pas ? _eos_<br>why or why not ? _eos_<br>why or why not ? _eos_</em></pre><p name="5188" id="5188" class="graf graf--p graf-after--pre">Not bad. It’s still not perfect but quite a few of them are correct and again considering that we are asking it to learn about the very idea of language for two different languages and how to translate them between the two, and grammar, and vocabulary, and we only have 50,000 sentences and a lot of the words only appear once, I would say this is actually pretty amazing.</p><p name="9e37" id="9e37" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question: </strong>Why do we use tanh instead of ReLU for the attention mini net? [<a href="https://youtu.be/tY0n9OT5_nA?t=1h46m23s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h46m23s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:46:23</a>] I don’t quite remember — it’s been a while since I looked at it. You should totally try using value and see how it goes. Obviously tanh the key difference is that it can go in each direction and it’s limited both at the top and the bottom. I know very often for the gates inside RNNs, LSTMs, and GRUs, tanh often works out better but it’s been about a year since I actually looked at that specific question so I’ll look at it during the week. The short answer is you should try a different activation function and see if you can get a better result.</p><blockquote name="ed74" id="ed74" class="graf graf--blockquote graf-after--p">From Lesson 7 [<a href="https://youtu.be/H3g26EVADgY?t=44m6s" data-href="https://youtu.be/H3g26EVADgY?t=44m6s" class="markup--anchor markup--blockquote-anchor" rel="nofollow noopener" target="_blank">44:06</a>]: As we have seen last week, tanh is forcing the value to be between -1 and 1. Since we are multiplying by this weight matrix again and again, we would worry that relu (since it is unbounded) might have more gradient explosion problem. Having said that, you can specify RNNCell to use different nonlineality whose default is tanh and ask it to use relu if you wanted to.</blockquote><h4 name="d511" id="d511" class="graf graf--h4 graf-after--blockquote">Visualization [<a href="https://youtu.be/tY0n9OT5_nA?t=1h47m12s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h47m12s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:47:12</a>]</h4><p name="a9cc" id="a9cc" class="graf graf--p graf-after--h4">What we can do also is we can grab the attentions out of the model by adding return attention parameter to <code class="markup--code markup--p-code">forward</code> function. You can put anything you’d like in <code class="markup--code markup--p-code">forward</code> function argument. So we added a return attention parameter, false by default because obviously the training loop it doesn’t know anything about it but then we just had something here says if return attention, then stick the attentions on as well (<code class="markup--code markup--p-code">if ret_attn: res = res,torch.stack(attns)</code>). The attentions is simply the value <code class="markup--code markup--p-code">a</code> just chuck it on a list (<code class="markup--code markup--p-code">attns.append(a)</code>). We can now call the model with return attention equals true and get back the probabilities and the attentions [<a href="https://youtu.be/tY0n9OT5_nA?t=1h47m53s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h47m53s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:47:53</a>]:</p><pre name="7b56" id="7b56" class="graf graf--pre graf-after--p">probs,attns = learn.model(V(x),ret_attn=<strong class="markup--strong markup--pre-strong">True</strong>)</pre><p name="245e" id="245e" class="graf graf--p graf-after--pre">We can now draw pictures, at each time step, of the attention.</p><pre name="3a90" id="3a90" class="graf graf--pre graf-after--p">attn = to_np(attns[...,180])</pre><pre name="800e" id="800e" class="graf graf--pre graf-after--pre">fig, axes = plt.subplots(3, 3, figsize=(15, 10))<br><strong class="markup--strong markup--pre-strong">for</strong> i,ax <strong class="markup--strong markup--pre-strong">in</strong> enumerate(axes.flat):<br>    ax.plot(attn[i])</pre><figure name="ebb1" id="ebb1" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_CSG2P8oBICyPDBnoT9z7Wg.png"></figure><p name="c0b4" id="c0b4" class="graf graf--p graf-after--figure">When you are Chris Olah and Shan Carter, you make things that looks like ☟when you are Jeremy Howard, the exact same information looks like ☝︎[<a href="https://youtu.be/tY0n9OT5_nA?t=1h48m24s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h48m24s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:48:24</a>]. You can see at each different time step, we have a different attention.</p><figure name="aa8d" id="aa8d" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_zOxcT0Nib1_VtVkyQN4FQQ.png"></figure><p name="9c69" id="9c69" class="graf graf--p graf-after--figure">It’s very important when you try to build something like this, you don’t really know if it’s not working right because if it’s not working (as per usual Jeremy’s first 12 attempts of this were broken) and they were broken in a sense that it wasn’t really learning anything useful. Therefore, it was giving equal attention to everything and it wasn’t worse — it just wasn’t much better. Until you actually find ways to visualize the thing in a way that you know what it ought to look like ahead of time, you don’t really know if it’s working [<a href="https://youtu.be/tY0n9OT5_nA?t=1h49m16s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h49m16s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:49:16</a>]. So it’s really important that you try to find ways to check your intermediate steps in your outputs.</p><p name="6032" id="6032" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: What is the loss function of the attentional neural network? [<a href="https://youtu.be/tY0n9OT5_nA?t=1h49m31s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h49m31s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:49:31</a>] No, there is no loss function for the attentional neural network. It is trained end-to-end. It is just sitting inside our decoder loop. The loss function for the decoder loop is the same loss function because the result contains exactly same thing as before — the probabilities of the words. How come the mini neural net learning something? Because in order to make the outputs better and better, it would be great if it made the weights of weighted-average better and better. So part of creating our output is to please do a good job of finding a good set of weights and if it doesn’t do a good job of finding good set of weights, then the loss function won’t improve from that bit. So end-to-end learning means you throw in everything you can into one loss function and the gradients of all the different parameters point in a direction that says “hey, you know if you had put more weight over there, it would have been better.” And thanks to the magic of the chain rule, it knows to put more weight over there, change the parameter in the matrix multiply a little, etc. That is the magic of end-to-end learning. It is a very understandable question but you have to realize there is nothing particular about this code that says this particular bits are separate mini neural network anymore than the GRU is a separate little neural network, or a linear layer is a separate little function. It’s all ends up pushed into one output which is a bunch of probabilities which ends up in one loss function that returns a single number that says this either was or wasn’t a good translation. So thanks to the magic of the chain rule, we then back propagate little updates to all the parameters to make them a little bit better. This is a big, weird, counterintuitive idea and it’s totally okay if it’s a bit mind-bending. It is the bit where even back to lesson 1 “how did we make it find dogs vs. cats?” — we didn’t. All we did was we said “this is our data, this is our architecture, this is our loss function. Please back propagate into the weights to make them better and after you’ve made them better a while, it will start finding cats from dogs.” In this case (i.e. translation), we haven’t used somebody else’s convolutional network architecture. We said “here is a custom architecture which we hope is going to be particularly good at this problem.” Even without this custom architecture, it was still okay. But we made it in a way that made more sense or we think it ought to do worked even better. But at no point, did we do anything different other than say “here is a data, here is an architecture, here is a loss function — go and find the parameters please” And it did it because that’s what neural nets do.</p><p name="d5d9" id="d5d9" class="graf graf--p graf-after--p">So that is sequence-to-sequence learning [<a href="https://youtu.be/tY0n9OT5_nA?t=1h53m19s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h53m19s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:53:19</a>].</p><ul class="postList"><li name="5de6" id="5de6" class="graf graf--li graf-after--p">If you want to encode an image into a CNN backbone of some kind, and then pass that into a decoder which is like RNN with attention, and you make your y-values the actual correct caption of each of those image, you will end up with an image caption generator.</li><li name="2fb5" id="2fb5" class="graf graf--li graf-after--li">If you do the same thing with videos and captions, you will end up with a video caption generator.</li><li name="b552" id="b552" class="graf graf--li graf-after--li">If you do the same thing with 3D CT scan and radiology reports, you will end up with a radiology report generator.</li><li name="f410" id="f410" class="graf graf--li graf-after--li">If you do the same thing with Github issues and people’s chosen summaries of them, you’ll get a Github issue summary generator.</li></ul><blockquote name="fc6c" id="fc6c" class="graf graf--blockquote graf-after--li">Seq-to-seq is magical but they work [<a href="https://youtu.be/tY0n9OT5_nA?t=1h54m7s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h54m7s" class="markup--anchor markup--blockquote-anchor" rel="nofollow noopener" target="_blank">1:54:07</a>]. And I don’t feel like people have begun to scratch the surface of how to use seq-to-seq models in their own domains. Not being a Github person, it would never have occurred to me that “it would be kind of cool to start with some issue and automatically create a summary”. But now, of course, next time I go into Github, I want to see a summary written there for me. I don’t want to write my own commit message. Why should I write my own summary of the code review when I finished adding comments to lots of lines — it should do that for me as well. Now I’m thinking Github so behind, it could be doing this stuff. So what are the thing in your industry? You could start with a sequence and generate something from it. I can’t begin to imagine. Again, it is a fairly new area and the tools for it are not easy to use — they are not even built into fastai yet. Hopefully there will be soon. I don’t think anybody knows what the opportunities are.</blockquote><h3 name="6e04" id="6e04" class="graf graf--h3 graf-after--blockquote">Devise [<a href="https://youtu.be/tY0n9OT5_nA?t=1h55m23s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h55m23s" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">1:55:23</a>]</h3><p name="1fb9" id="1fb9" class="graf graf--p graf-after--h3"><a href="https://github.com/fastai/fastai/blob/master/courses/dl2/devise.ipynb" data-href="https://github.com/fastai/fastai/blob/master/courses/dl2/devise.ipynb" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Notebook</a> / <a href="http://papers.nips.cc/paper/5204-devise-a-deep-visual-semantic-embedding-model.pdf" data-href="http://papers.nips.cc/paper/5204-devise-a-deep-visual-semantic-embedding-model.pdf" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Paper</a></p><p name="5d8b" id="5d8b" class="graf graf--p graf-after--p">We are going to do something bringing together for the first time our two little worlds we focused on — text and images [<a href="https://youtu.be/tY0n9OT5_nA?t=1h55m49s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h55m49s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:55:49</a>]. This idea came up in a paper by an extraordinary deep learning practitioner and researcher named Andrea Frome. Andrea was at Google at the time and her crazy idea was words can have a distributed representation, a space, which particularly at that time was just word vectors. And images can be represented in a space. In the end, if we have a fully connected layer, they ended up as a vector representation. Could we merge the two? Could we somehow encourage the vector space that the images end up with be the same vector space that the words are in? And if we could do that, what would that mean? What could we do with that? So what could we do with that covers things like well, what if I’m wrong what if I’m predicting that this image is a beagle and I predict jumbo jet and Yannet’s model predicts corgi. The normal loss function says that Yannet’s and Jeremy’s models are equally good (i.e. they are both wrong). But what if we could somehow say though you know what corgi is closer to beagle than it is to jumbo jets. So Yannet’s model is better than Jeremy’s. We should be able to do that because in word vector space, beagle and corgi are pretty close together but jumbo jet not so much. So it would give us a nice situation where hopefully our inferences would be wrong in saner ways if they are wrong. It would also allow us to search for things that are not in ImageNet Synset ID (i.e. a category in ImageNet). Why did we have to train a whole new model to find dog vs. cats when we already have something that found corgis and tabbies. Why can’t we just say find me dogs? If we had trained it in word vector space, we totally could because they are word vector, we can find things with the right image vector and so forth. We will look at some cool things we can do with it in a moment but first of all let’s train a model where this model is not learning a category (one hot encoded ID) where every category is equally far from every other category, let’s instead train a model where we’re finding a dependent variable which is a word vector. so What word vector? Obviously the word vector for the word you want. So if it’s corgi, let’s train it to create a word vector that’s the corgi word vector, and if it’s a jumbo jet, let’s train it with a dependent variable that says this is the word vector for a jumbo jet.</p><pre name="0500" id="0500" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">fastai.conv_learner</strong> <strong class="markup--strong markup--pre-strong">import</strong> *<br>torch.backends.cudnn.benchmark=<strong class="markup--strong markup--pre-strong">True</strong><br><br><strong class="markup--strong markup--pre-strong">import</strong> <strong class="markup--strong markup--pre-strong">fastText</strong> <strong class="markup--strong markup--pre-strong">as</strong> <strong class="markup--strong markup--pre-strong">ft</strong></pre><pre name="a6f9" id="a6f9" class="graf graf--pre graf-after--pre">PATH = Path('data/imagenet/')<br>TMP_PATH = PATH/'tmp'<br>TRANS_PATH = Path('data/translate/')<br>PATH_TRN = PATH/'train'</pre><p name="bbe2" id="bbe2" class="graf graf--p graf-after--pre">It is shockingly easy [<a href="https://youtu.be/tY0n9OT5_nA?t=1h59m17s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h59m17s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:59:17</a>]. Let’s grab the fast text word vectors again, load them in (we only need English this time).</p><pre name="30b6" id="30b6" class="graf graf--pre graf-after--p">ft_vecs = ft.load_model(str((TRANS_PATH/'wiki.en.bin')))</pre><pre name="c911" id="c911" class="graf graf--pre graf-after--pre">np.corrcoef(ft_vecs.get_word_vector('jeremy'), <br>            ft_vecs.get_word_vector('Jeremy'))</pre><pre name="7aa7" id="7aa7" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">array([[1.     , 0.60866],<br>       [0.60866, 1.     ]])</em></pre><p name="8dd9" id="8dd9" class="graf graf--p graf-after--pre">So for example, “jeremy” and “Jeremy” have a correlation of&nbsp;.6.</p><pre name="e8a1" id="e8a1" class="graf graf--pre graf-after--p">np.corrcoef(ft_vecs.get_word_vector('banana'), <br>            ft_vecs.get_word_vector('Jeremy'))</pre><pre name="ea1f" id="ea1f" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">array([[1.     , 0.14482],<br>       [0.14482, 1.     ]])</em></pre><p name="68c1" id="68c1" class="graf graf--p graf-after--pre">Jeremy doesn’t like bananas at all, and “banana” and “Jeremy”&nbsp;.14. So words that you would expect to be correlated are correlated and words that should be as far away from each other as possible, unfortunately, they are still slightly correlated but not so much [<a href="https://youtu.be/tY0n9OT5_nA?t=1h59m41s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h59m41s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:59:41</a>].</p><h4 name="c597" id="c597" class="graf graf--h4 graf-after--p">Map ImageNet classes to word&nbsp;vectors</h4><p name="755f" id="755f" class="graf graf--p graf-after--h4">Let’s now grab all of the ImageNet classes because we actually want to know which one is corgi and which one is jumbo jet.</p><pre name="4ad4" id="4ad4" class="graf graf--pre graf-after--p">ft_words = ft_vecs.get_words(include_freq=<strong class="markup--strong markup--pre-strong">True</strong>)<br>ft_word_dict = {k:v <strong class="markup--strong markup--pre-strong">for</strong> k,v <strong class="markup--strong markup--pre-strong">in</strong> zip(*ft_words)}<br>ft_words = sorted(ft_word_dict.keys(), key=<strong class="markup--strong markup--pre-strong">lambda</strong> x: ft_word_dict[x])</pre><pre name="3550" id="3550" class="graf graf--pre graf-after--pre">len(ft_words)</pre><pre name="841a" id="841a" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">2519370</em></pre><pre name="6863" id="6863" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">fastai.io</strong> <strong class="markup--strong markup--pre-strong">import</strong> get_data</pre><p name="b4bd" id="b4bd" class="graf graf--p graf-after--pre">We have a list of all of those up on files.fast.ai that we can grab them.</p><pre name="28c7" id="28c7" class="graf graf--pre graf-after--p">CLASSES_FN = 'imagenet_class_index.json'<br>get_data(f'http://files.fast.ai/models/{CLASSES_FN}', <br>         TMP_PATH/CLASSES_FN)</pre><p name="f2ef" id="f2ef" class="graf graf--p graf-after--pre">Let’s also grab a list of all of the nouns in English which Jeremy made available here:</p><pre name="5ef8" id="5ef8" class="graf graf--pre graf-after--p">WORDS_FN = 'classids.txt'<br>get_data(f'http://files.fast.ai/data/{WORDS_FN}', PATH/WORDS_FN)</pre><p name="08d7" id="08d7" class="graf graf--p graf-after--pre">So we have the names of each of the thousand ImageNet classes and all of the nouns in English according to WordNet which is a popular thing for representing what words are and are not. We can now load that list of ImageNet classes, turn that into a dictionary, so <code class="markup--code markup--p-code">classids_1k</code> contains the class IDs for the 1000 images that are in the competition dataset.</p><pre name="9af1" id="9af1" class="graf graf--pre graf-after--p">class_dict = json.load((TMP_PATH/CLASSES_FN).open())<br>classids_1k = dict(class_dict.values())<br>nclass = len(class_dict); nclass</pre><pre name="8b3c" id="8b3c" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">1000</em></pre><p name="4b83" id="4b83" class="graf graf--p graf-after--pre">Here is an example. A “tench” apparently is a kind of fish.</p><pre name="6242" id="6242" class="graf graf--pre graf-after--p">class_dict['0']</pre><pre name="de10" id="de10" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">['n01440764', 'tench']</em></pre><p name="6928" id="6928" class="graf graf--p graf-after--pre">Let’s do the same thing for all those WordNet nouns [<a href="https://youtu.be/tY0n9OT5_nA?t=2h1m11s" data-href="https://youtu.be/tY0n9OT5_nA?t=2h1m11s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:01:11</a>]. It turns out that ImageNet is using WordNet class names so that makes it nice and easy to map between the two.</p><pre name="5bff" id="5bff" class="graf graf--pre graf-after--p">classid_lines = (PATH/WORDS_FN).open().readlines()<br>classid_lines[:5]</pre><pre name="bbf7" id="bbf7" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">['n00001740 entity\n',<br> 'n00001930 physical_entity\n',<br> 'n00002137 abstraction\n',<br> 'n00002452 thing\n',<br> 'n00002684 object\n']</em></pre><pre name="5fc9" id="5fc9" class="graf graf--pre graf-after--pre">classids = dict(l.strip().split() <strong class="markup--strong markup--pre-strong">for</strong> l <strong class="markup--strong markup--pre-strong">in</strong> classid_lines)<br>len(classids),len(classids_1k)</pre><pre name="1917" id="1917" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(82115, 1000)</em></pre><p name="a58c" id="a58c" class="graf graf--p graf-after--pre">So these are our two worlds — we have the ImageNet thousand and we have the 82,000 which are in WordNet.</p><pre name="7e7d" id="7e7d" class="graf graf--pre graf-after--p">lc_vec_d = {w.lower(): ft_vecs.get_word_vector(w) <strong class="markup--strong markup--pre-strong">for</strong> w <br>                           <strong class="markup--strong markup--pre-strong">in</strong> ft_words[-1000000:]}</pre><p name="e883" id="e883" class="graf graf--p graf-after--pre">So we want to map the two together which is as simple as creating a couple of dictionaries to map them based on the Synset ID or the WordNet ID.</p><pre name="a745" id="a745" class="graf graf--pre graf-after--p">syn_wv = [(k, lc_vec_d[v.lower()]) <strong class="markup--strong markup--pre-strong">for</strong> k,v <strong class="markup--strong markup--pre-strong">in</strong> classids.items()<br>          <strong class="markup--strong markup--pre-strong">if</strong> v.lower() <strong class="markup--strong markup--pre-strong">in</strong> lc_vec_d]<br>syn_wv_1k = [(k, lc_vec_d[v.lower()]) <strong class="markup--strong markup--pre-strong">for</strong> k,v <strong class="markup--strong markup--pre-strong">in</strong> classids_1k.items()<br>          <strong class="markup--strong markup--pre-strong">if</strong> v.lower() <strong class="markup--strong markup--pre-strong">in</strong> lc_vec_d]<br>syn2wv = dict(syn_wv)<br>len(syn2wv)</pre><pre name="ede6" id="ede6" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">49469</em></pre><p name="95ed" id="95ed" class="graf graf--p graf-after--pre">What we need to do now is grab the 82,000 nouns in WordNet and try and look them up in fast text. We’ve managed to look up 49,469 of them in fast text. We now have a dictionary that goes from synset ID which is what WordNet calls them to word vectors. We also have the same thing specifically for the 1k ImageNet classes.</p><pre name="fc98" id="fc98" class="graf graf--pre graf-after--p">pickle.dump(syn2wv, (TMP_PATH/'syn2wv.pkl').open('wb'))<br>pickle.dump(syn_wv_1k, (TMP_PATH/'syn_wv_1k.pkl').open('wb'))</pre><pre name="b392" id="b392" class="graf graf--pre graf-after--pre">syn2wv = pickle.load((TMP_PATH/'syn2wv.pkl').open('rb'))<br>syn_wv_1k = pickle.load((TMP_PATH/'syn_wv_1k.pkl').open('rb'))</pre><p name="c6f0" id="c6f0" class="graf graf--p graf-after--pre">Now we grab all of the ImageNet which you can download from Kaggle now [<a href="https://youtu.be/tY0n9OT5_nA?t=2h2m54s" data-href="https://youtu.be/tY0n9OT5_nA?t=2h2m54s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:02:54</a>]. If you look at the Kaggle ImageNet localization competition, that contains the entirety of the ImageNet classifications as well.</p><pre name="4e72" id="4e72" class="graf graf--pre graf-after--p">images = []<br>img_vecs = []</pre><pre name="3daf" id="3daf" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">for</strong> d <strong class="markup--strong markup--pre-strong">in</strong> (PATH/'train').iterdir():<br>    <strong class="markup--strong markup--pre-strong">if</strong> d.name <strong class="markup--strong markup--pre-strong">not</strong> <strong class="markup--strong markup--pre-strong">in</strong> syn2wv: <strong class="markup--strong markup--pre-strong">continue</strong><br>    vec = syn2wv[d.name]<br>    <strong class="markup--strong markup--pre-strong">for</strong> f <strong class="markup--strong markup--pre-strong">in</strong> d.iterdir():<br>        images.append(str(f.relative_to(PATH)))<br>        img_vecs.append(vec)</pre><pre name="d187" id="d187" class="graf graf--pre graf-after--pre">n_val=0<br><strong class="markup--strong markup--pre-strong">for</strong> d <strong class="markup--strong markup--pre-strong">in</strong> (PATH/'valid').iterdir():<br>    <strong class="markup--strong markup--pre-strong">if</strong> d.name <strong class="markup--strong markup--pre-strong">not</strong> <strong class="markup--strong markup--pre-strong">in</strong> syn2wv: <strong class="markup--strong markup--pre-strong">continue</strong><br>    vec = syn2wv[d.name]<br>    <strong class="markup--strong markup--pre-strong">for</strong> f <strong class="markup--strong markup--pre-strong">in</strong> d.iterdir():<br>        images.append(str(f.relative_to(PATH)))<br>        img_vecs.append(vec)<br>        n_val += 1</pre><pre name="e84e" id="e84e" class="graf graf--pre graf-after--pre">n_val</pre><pre name="12a9" id="12a9" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">28650</em></pre><p name="ee3d" id="ee3d" class="graf graf--p graf-after--pre">It has a validation set of 28,650 items in it. For every image in ImageNet, we can grab its fast text word vector using the synset to word vector (<code class="markup--code markup--p-code">syn2wv</code>) and we can stick that into the image vectors array (<code class="markup--code markup--p-code">img_vecs</code>), stack that all up into a single matrix and save that away.</p><pre name="a70c" id="a70c" class="graf graf--pre graf-after--p">img_vecs = np.stack(img_vecs)<br>img_vecs.shape</pre><p name="1bd7" id="1bd7" class="graf graf--p graf-after--pre">Now what we have is something for every ImageNet image, we also have the fast text word vector that it is associated with [<a href="https://youtu.be/tY0n9OT5_nA?t=2h3m43s" data-href="https://youtu.be/tY0n9OT5_nA?t=2h3m43s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:03:43</a>] by looking up the synset ID → WordNet → Fast text → word vector.</p><pre name="5f9d" id="5f9d" class="graf graf--pre graf-after--p">pickle.dump(images, (TMP_PATH/'images.pkl').open('wb'))<br>pickle.dump(img_vecs, (TMP_PATH/'img_vecs.pkl').open('wb'))</pre><pre name="6c87" id="6c87" class="graf graf--pre graf-after--pre">images = pickle.load((TMP_PATH/'images.pkl').open('rb'))<br>img_vecs = pickle.load((TMP_PATH/'img_vecs.pkl').open('rb'))</pre><pre name="53db" id="53db" class="graf graf--pre graf-after--pre">arch = resnet50</pre><pre name="532e" id="532e" class="graf graf--pre graf-after--pre">n = len(images); n</pre><pre name="8968" id="8968" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">766876</em></pre><pre name="146e" id="146e" class="graf graf--pre graf-after--pre">val_idxs = list(range(n-28650, n))</pre><p name="4b9e" id="4b9e" class="graf graf--p graf-after--pre">Here is a cool trick [<a href="https://youtu.be/tY0n9OT5_nA?t=2h4m6s" data-href="https://youtu.be/tY0n9OT5_nA?t=2h4m6s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:04:06</a>]. We can now create a model data object which specifically is an image classifier data object and we have this thing called <code class="markup--code markup--p-code">from_names_and_array</code> I’m not sure if we’ve used it before but we can pass it a list of file names (all of the file names in ImageNet) and an array of our dependent variables (all of the fast text word vectors). We can then pass in the validation indexes which in this case is just all of the last IDs — we need to make sure that they are the same as ImageNet uses otherwise we will be cheating. Then we pass in <code class="markup--code markup--p-code">continuous=True</code> which means this puts a lie again to this image classifier data is now an image regressive data so continuous equals True means don’t one hot encode my outputs but treat them just as continuous values. So now we have a model data object that contains all of our file names and for every file name a continuous array representing the word vector for that. So we have data, now we need an architecture and the loss function.</p><pre name="ac43" id="ac43" class="graf graf--pre graf-after--p">tfms = tfms_from_model(arch, 224, transforms_side_on, max_zoom=1.1)<br>md = ImageClassifierData.<strong class="markup--strong markup--pre-strong">from_names_and_array</strong>(PATH, images,  <br>        img_vecs, val_idxs=val_idxs, classes=<strong class="markup--strong markup--pre-strong">None</strong>, tfms=tfms,<br>        continuous=<strong class="markup--strong markup--pre-strong">True</strong>, bs=256)</pre><pre name="c264" id="c264" class="graf graf--pre graf-after--pre">x,y = next(iter(md.val_dl))</pre><p name="f8d2" id="f8d2" class="graf graf--p graf-after--pre">Let’s create an architecture [<a href="https://youtu.be/tY0n9OT5_nA?t=2h5m26s" data-href="https://youtu.be/tY0n9OT5_nA?t=2h5m26s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:05:26</a>]. We’ll revise this next week, but we can use the tricks we’ve learnt so far and it’s actually incredibly simple. Fastai has a <code class="markup--code markup--p-code">ConvnetBuilder</code> which is what gets called when you say <code class="markup--code markup--p-code">ConvLerner.pretrained</code> and you specify:</p><ul class="postList"><li name="13a8" id="13a8" class="graf graf--li graf-after--p"><code class="markup--code markup--li-code">f</code>: the architecture (we are going to use ResNet50)</li><li name="ce29" id="ce29" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">c</code>: how many classes you want (in this case, it’s not really classes — it’s how many outputs you want which is the length of the fast text word vector i.e. 300).</li><li name="7b85" id="7b85" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">is_multi</code>: It is not a multi classification as it is not classification at all.</li><li name="9922" id="9922" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">is_reg</code>: Yes, it is a regression.</li><li name="8c1d" id="8c1d" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">xtra_fc</code>&nbsp;: What fully connected layers you want. We are just going to add one fully connected hidden layer of a length of 1024. Why 1024? The last layer of ResNet50 I think is 1024 long, the final output we need is 300 long. We obviously need our penultimate (second to the last) layer to be longer than 300. Otherwise it’s not enough information, so we just picked something a bit bigger. Maybe different numbers would be better but this worked for Jeremy.</li><li name="e1f6" id="e1f6" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">ps</code>&nbsp;: how much dropout you want. Jeremy found that the default dropout, he was consistently under fitting so he just decreased the dropout from 0.5 to 0.2.</li></ul><p name="f179" id="f179" class="graf graf--p graf-after--li">So this is now a convolutional neural network that does not have any softmax or anything like that because it’s regression it’s just a linear layer at the end and that’s our model [<a href="https://youtu.be/tY0n9OT5_nA?t=2h6m55s" data-href="https://youtu.be/tY0n9OT5_nA?t=2h6m55s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:06:55</a>]. We can create a ConvLearner from that model and give it an optimization function. So now all we need is a loss function.</p><pre name="9662" id="9662" class="graf graf--pre graf-after--p">models = ConvnetBuilder(arch, md.c, is_multi=<strong class="markup--strong markup--pre-strong">False</strong>, is_reg=<strong class="markup--strong markup--pre-strong">True</strong>, <br>             xtra_fc=[1024], ps=[0.2,0.2])</pre><pre name="df16" id="df16" class="graf graf--pre graf-after--pre">learn = ConvLearner(md, models, precompute=<strong class="markup--strong markup--pre-strong">True</strong>)<br>learn.opt_fn = partial(optim.Adam, betas=(0.9,0.99))</pre><p name="12d1" id="12d1" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">Loss Function</strong> [<a href="https://youtu.be/tY0n9OT5_nA?t=2h7m38s" data-href="https://youtu.be/tY0n9OT5_nA?t=2h7m38s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:07:38</a>]: Default loss function for regression is L1 loss (the absolute differences) — that is not bad. But unfortunately in really high dimensional spaces (anybody who has studied a bit of machine learning probably knows this) everything is on the outside (in this case, it’s 300 dimensional). When everything is on the outside, distance is not meaningless but a little bit awkward. Things tend to be close together or far away, it doesn’t really mean much in these really high dimensional spaces where everything is on the edge. What does mean something, though, is that if one thing is on the edge over here, and one thing is on the edge over there, we can form an angle between those vectors and the angle is meaningful. That is why we use cosine similarity when we are looking for how close or far apart things are in high dimensional spaces. If you haven’t seen cosine similarity before, it is basically the same as Euclidean distance but it’s normalized to be a unit norm (i.e. divided by the length). So we don’t care about the length of the vector, we only care about its angle. There is a bunch of stuff that you could easily learn in a couple of hours but if you haven’t seen it before, it’s a bit mysterious. For now, just know that loss functions and high dimensional spaces where you are trying to find similarity, you care about angle and you don’t care about distance [<a href="https://youtu.be/tY0n9OT5_nA?t=2h9m13s" data-href="https://youtu.be/tY0n9OT5_nA?t=2h9m13s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:09:13</a>]. If you didn’t use the following custom loss function, it would still work but it’s a little bit less good. Now we have data, architecture, and loss function, therefore, we are done. We can go ahead and fit.</p><pre name="f274" id="f274" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> cos_loss(inp,targ):<br>    <strong class="markup--strong markup--pre-strong">return</strong> 1 - F.cosine_similarity(inp,targ).mean()<br>learn.crit = cos_loss</pre><pre name="8604" id="8604" class="graf graf--pre graf-after--pre">learn.lr_find(start_lr=1e-4, end_lr=1e15)</pre><pre name="ef44" id="ef44" class="graf graf--pre graf-after--pre">learn.sched.plot()</pre><pre name="3d61" id="3d61" class="graf graf--pre graf-after--pre">lr = 1e-2<br>wd = 1e-7</pre><p name="54cd" id="54cd" class="graf graf--p graf-after--pre">We are training on all of ImageNet that is going to take a long time. So <code class="markup--code markup--p-code">precompute=True</code> is your friend. Remember <code class="markup--code markup--p-code">precompute=True</code>? That is the thing we’ve learnt ages ago that caches the output of the final convolutional layer and just trains the fully connected bit. Even with <code class="markup--code markup--p-code">precompute=True</code>, it takes about 3 minutes to train an epoch on all of ImageNet. So this is about an hour worth of training, but it’s pretty cool that with fastai, we can train a new custom head on all of ImageNet for 40 epochs in an hour or so.</p><pre name="9aae" id="9aae" class="graf graf--pre graf-after--p">learn.precompute=<strong class="markup--strong markup--pre-strong">True</strong></pre><pre name="0370" id="0370" class="graf graf--pre graf-after--pre">learn.fit(lr, 1, cycle_len=20, wds=wd, use_clr=(20,10))</pre><pre name="a105" id="a105" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss                                  <br>    0      0.104692   0.125685  <br>    1      0.112455   0.129307                                 <br>    2      0.110631   0.126568                                 <br>    3      0.108629   0.127338                                 <br>    4      0.110791   0.125033                                 <br>    5      0.108859   0.125186                                 <br>    6      0.106582   0.123875                                 <br>    7      0.103227   0.123945                                 <br>    8      0.10396    0.12304                                  <br>    9      0.105898   0.124894                                 <br>    10     0.10498    0.122582                                 <br>    11     0.104983   0.122906                                 <br>    12     0.102317   0.121171                                  <br>    13     0.10017    0.121816                                  <br>    14     0.099454   0.119647                                  <br>    15     0.100425   0.120914                                  <br>    16     0.097226   0.119724                                  <br>    17     0.094666   0.118746                                  <br>    18     0.094137   0.118744                                  <br>    19     0.090076   0.117908</em></pre><pre name="c43b" id="c43b" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[0.11790786389489033]</em></pre><pre name="4894" id="4894" class="graf graf--pre graf-after--pre">learn.bn_freeze(<strong class="markup--strong markup--pre-strong">True</strong>)</pre><pre name="fc1e" id="fc1e" class="graf graf--pre graf-after--pre">learn.fit(lr, 1, cycle_len=20, wds=wd, use_clr=(20,10))</pre><pre name="bac5" id="bac5" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss                                  <br>    0      0.104692   0.125685  <br>    1      0.112455   0.129307                                 <br>    2      0.110631   0.126568                                 <br>    3      0.108629   0.127338                                 <br>    4      0.110791   0.125033                                 <br>    5      0.108859   0.125186                                 <br>    6      0.106582   0.123875                                 <br>    7      0.103227   0.123945                                 <br>    8      0.10396    0.12304                                  <br>    9      0.105898   0.124894                                 <br>    10     0.10498    0.122582                                 <br>    11     0.104983   0.122906                                 <br>    12     0.102317   0.121171                                  <br>    13     0.10017    0.121816                                  <br>    14     0.099454   0.119647                                  <br>    15     0.100425   0.120914                                  <br>    16     0.097226   0.119724                                  <br>    17     0.094666   0.118746                                  <br>    18     0.094137   0.118744                                  <br>    19     0.090076   0.117908</em></pre><pre name="ddc2" id="ddc2" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[0.11790786389489033]</em></pre><pre name="70e1" id="70e1" class="graf graf--pre graf-after--pre">lrs = np.array([lr/1000,lr/100,lr])</pre><pre name="6cfb" id="6cfb" class="graf graf--pre graf-after--pre">learn.precompute=<strong class="markup--strong markup--pre-strong">False</strong><br>learn.freeze_to(1)</pre><pre name="404b" id="404b" class="graf graf--pre graf-after--pre">learn.save('pre0')</pre><pre name="2c47" id="2c47" class="graf graf--pre graf-after--pre">learn.load('pre0')</pre><h3 name="1ae7" id="1ae7" class="graf graf--h3 graf-after--pre">Image search</h3><h4 name="5a9e" id="5a9e" class="graf graf--h4 graf-after--h3">Search imagenet&nbsp;classes</h4><p name="7eea" id="7eea" class="graf graf--p graf-after--h4">At the end of all that, we can now say let’s grab the 1000 ImageNet classes, let’s predict on our whole validation set, and take a look at a few pictures [<a href="https://youtu.be/tY0n9OT5_nA?t=2h10m26s" data-href="https://youtu.be/tY0n9OT5_nA?t=2h10m26s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:10:26</a>].</p><pre name="5e0f" id="5e0f" class="graf graf--pre graf-after--p">syns, wvs = list(zip(*syn_wv_1k))<br>wvs = np.array(wvs)</pre><pre name="da52" id="da52" class="graf graf--pre graf-after--pre">%time pred_wv = learn.predict()</pre><pre name="9568" id="9568" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">CPU times: user 18.4 s, sys: 7.91 s, total: 26.3 s<br>Wall time: 7.17 s</em></pre><pre name="6f55" id="6f55" class="graf graf--pre graf-after--pre">start=300</pre><pre name="03a5" id="03a5" class="graf graf--pre graf-after--pre">denorm = md.val_ds.denorm</pre><pre name="7083" id="7083" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> show_img(im, figsize=<strong class="markup--strong markup--pre-strong">None</strong>, ax=<strong class="markup--strong markup--pre-strong">None</strong>):<br>    <strong class="markup--strong markup--pre-strong">if</strong> <strong class="markup--strong markup--pre-strong">not</strong> ax: fig,ax = plt.subplots(figsize=figsize)<br>    ax.imshow(im)<br>    ax.axis('off')<br>    <strong class="markup--strong markup--pre-strong">return</strong> ax</pre><pre name="e2c3" id="e2c3" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> show_imgs(ims, cols, figsize=<strong class="markup--strong markup--pre-strong">None</strong>):<br>    fig,axes = plt.subplots(len(ims)//cols, cols, figsize=figsize)<br>    <strong class="markup--strong markup--pre-strong">for</strong> i,ax <strong class="markup--strong markup--pre-strong">in</strong> enumerate(axes.flat): show_img(ims[i], ax=ax)<br>    plt.tight_layout()</pre><p name="bf46" id="bf46" class="graf graf--p graf-after--pre">Because validation set is ordered, tall the stuff of the same type are in the same place.</p><pre name="da5f" id="da5f" class="graf graf--pre graf-after--p">show_imgs(denorm(md.val_ds[start:start+25][0]), 5, (10,10))</pre><figure name="c31f" id="c31f" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_exiD0uDeL6xx5EOLdPS3BA.png"></figure><p name="d8a5" id="d8a5" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Nearest neighbor search </strong>[<a href="https://youtu.be/tY0n9OT5_nA?t=2h10m56s" data-href="https://youtu.be/tY0n9OT5_nA?t=2h10m56s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:10:56</a>]: What we can now do is we can now use nearest neighbors search. So nearest neighbors search means here is one300 dimensional vector and here is a whole a lot of other 300 dimensional vectors, which things is it closest to? Normally that takes a very long time because you have to look through every 300 dimensional vector, calculate its distance, and find out how far away it is. But there is an amazing almost unknown library called <strong class="markup--strong markup--p-strong">NMSLib</strong> that does that incredibly fast. Some of you may have tried other nearest neighbor’s libraries, I guarantee this is faster than what you are using — I can tell you that because it’s been bench marked by people who do this stuff for a living. This is by far the fastest on every possible dimension. We want to create an index on angular distance, and we need to do it on all of our ImageNet word vectors. Adding a whole batch, create the index, and now we can query a bunch of vectors all at once, get the 10 nearest neighbors. The library uses multi-threading and is absolutely fantastic. You can install from pip (<code class="markup--code markup--p-code">pip install nmslib</code>) and it just works.</p><pre name="ef34" id="ef34" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">import</strong> <strong class="markup--strong markup--pre-strong">nmslib</strong></pre><pre name="9f2b" id="9f2b" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> create_index(a):<br>    index = nmslib.init(space='angulardist')<br>    index.addDataPointBatch(a)<br>    index.createIndex()<br>    <strong class="markup--strong markup--pre-strong">return</strong> index</pre><pre name="64fd" id="64fd" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> get_knns(index, vecs):<br>     <strong class="markup--strong markup--pre-strong">return</strong> zip(*index.knnQueryBatch(vecs, k=10, num_threads=4))</pre><pre name="07a0" id="07a0" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> get_knn(index, vec): <strong class="markup--strong markup--pre-strong">return</strong> index.knnQuery(vec, k=10)</pre><pre name="a7f1" id="a7f1" class="graf graf--pre graf-after--pre">nn_wvs = create_index(wvs)</pre><p name="8aa9" id="8aa9" class="graf graf--p graf-after--pre">It tells you how far away they are and their indexes [<a href="https://youtu.be/tY0n9OT5_nA?t=2h12m13s" data-href="https://youtu.be/tY0n9OT5_nA?t=2h12m13s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:12:13</a>].</p><pre name="2b14" id="2b14" class="graf graf--pre graf-after--p">idxs,dists = get_knns(nn_wvs, pred_wv)</pre><p name="9794" id="9794" class="graf graf--p graf-after--pre">So now we can go through and print out the top 3 so it turns out that bird actually is a limpkin. Interestingly the fourth one does not say it’s a limpkin and Jeremy looked it up. He doesn’t know much about birds but everything else is brown with white spots, but the 4th one isn’t. So we don’t know if that is actually a limpkin or if it is mislabeled but sure as heck it doesn’t look like the other birds.</p><pre name="7b8d" id="7b8d" class="graf graf--pre graf-after--p">[[classids[syns[id]] <strong class="markup--strong markup--pre-strong">for</strong> id <strong class="markup--strong markup--pre-strong">in</strong> ids[:3]] <br>                         <strong class="markup--strong markup--pre-strong">for</strong> ids <strong class="markup--strong markup--pre-strong">in</strong> idxs[start:start+10]]</pre><pre name="8e93" id="8e93" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[['limpkin', 'oystercatcher', 'spoonbill'],<br> ['limpkin', 'oystercatcher', 'spoonbill'],<br> ['limpkin', 'oystercatcher', 'spoonbill'],<br> ['spoonbill', 'bustard', 'oystercatcher'],<br> ['limpkin', 'oystercatcher', 'spoonbill'],<br> ['limpkin', 'oystercatcher', 'spoonbill'],<br> ['limpkin', 'oystercatcher', 'spoonbill'],<br> ['limpkin', 'oystercatcher', 'spoonbill'],<br> ['limpkin', 'oystercatcher', 'spoonbill'],<br> ['limpkin', 'oystercatcher', 'spoonbill']]</em></pre><p name="9dce" id="9dce" class="graf graf--p graf-after--pre">This is not a particularly hard thing to do because there is only a thousand ImageNet classes and it is not doing anything new. But what if we now bring in the entirety of WordNet and we now say which of those 45 thousand things is it closest to?</p><h4 name="d4bc" id="d4bc" class="graf graf--h4 graf-after--p">Search all WordMet noun&nbsp;classes</h4><pre name="4ed5" id="4ed5" class="graf graf--pre graf-after--h4">all_syns, all_wvs = list(zip(*syn2wv.items()))<br>all_wvs = np.array(all_wvs)</pre><pre name="cb4d" id="cb4d" class="graf graf--pre graf-after--pre">nn_allwvs = create_index(all_wvs)</pre><pre name="2b27" id="2b27" class="graf graf--pre graf-after--pre">idxs,dists = get_knns(nn_allwvs, pred_wv)</pre><pre name="d157" id="d157" class="graf graf--pre graf-after--pre">[[classids[all_syns[id]] <strong class="markup--strong markup--pre-strong">for</strong> id <strong class="markup--strong markup--pre-strong">in</strong> ids[:3]] <br>                             <strong class="markup--strong markup--pre-strong">for</strong> ids <strong class="markup--strong markup--pre-strong">in</strong> idxs[start:start+10]]</pre><pre name="80aa" id="80aa" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[['limpkin', 'oystercatcher', 'spoonbill'],<br> ['limpkin', 'oystercatcher', 'spoonbill'],<br> ['limpkin', 'oystercatcher', 'spoonbill'],<br> ['spoonbill', 'bustard', 'oystercatcher'],<br> ['limpkin', 'oystercatcher', 'spoonbill'],<br> ['limpkin', 'oystercatcher', 'spoonbill'],<br> ['limpkin', 'oystercatcher', 'spoonbill'],<br> ['limpkin', 'oystercatcher', 'spoonbill'],<br> ['limpkin', 'oystercatcher', 'spoonbill'],<br> ['limpkin', 'oystercatcher', 'spoonbill']]</em></pre><p name="ba81" id="ba81" class="graf graf--p graf-after--pre">Exactly the same result. It is now searching all of the WordNet.</p><h4 name="f233" id="f233" class="graf graf--h4 graf-after--p">Text -&gt; image search [<a href="https://youtu.be/tY0n9OT5_nA?t=2h13m16s" data-href="https://youtu.be/tY0n9OT5_nA?t=2h13m16s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">2:13:16</a>]</h4><p name="a2b4" id="a2b4" class="graf graf--p graf-after--h4">Now let’s do something a bit different — which is to take all of our predictions (<code class="markup--code markup--p-code">pred_wv</code>) so basically take our whole validation set of images and create a KNN index of the image representations because remember, it is predicting things that are meant to be word vectors. Now let’s grab the fast text vector for “boat” and boat is not an ImageNet concept — yet we can now find all of the images in our predicted word vectors (i.e. our validation set) that are closest to the word boat and it works even though it is not something that was ever trained on.</p><pre name="72e3" id="72e3" class="graf graf--pre graf-after--p">nn_predwv = create_index(pred_wv)<br>en_vecd = pickle.load(open(TRANS_PATH/'wiki.en.pkl','rb'))<br>vec = en_vecd['boat']</pre><pre name="bb76" id="bb76" class="graf graf--pre graf-after--pre">idxs,dists = get_knn(nn_predwv, vec)<br>show_imgs([open_image(PATH/md.val_ds.fnames[i]) <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> idxs[:3]],<br>                      3, figsize=(9,3));</pre><figure name="5791" id="5791" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_BLLI7IWFO84BPwB-Y1uCKg.png"></figure><p name="f197" id="f197" class="graf graf--p graf-after--figure">What if we now take engine’s vector and boat’s vector and take their average and what if we now look in our nearest neighbors for that [<a href="https://youtu.be/tY0n9OT5_nA?t=2h14m4s" data-href="https://youtu.be/tY0n9OT5_nA?t=2h14m4s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:14:04</a>]?</p><pre name="f02f" id="f02f" class="graf graf--pre graf-after--p">vec = (en_vecd['engine'] + en_vecd['boat'])/2 </pre><pre name="0551" id="0551" class="graf graf--pre graf-after--pre">idxs,dists = get_knn(nn_predwv, vec)<br>show_imgs([open_image(PATH/md.val_ds.fnames[i]) <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> idxs[:3]],<br>                      3, figsize=(9,3));</pre><figure name="4ace" id="4ace" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_EkwjxE8m8xeX2lDRLIJTCw.png"></figure><p name="d7be" id="d7be" class="graf graf--p graf-after--figure">These are boats with engines. I mean, yes, the middle one is actually a boat with an engine — it just happens to have wings on as well. By the way, sail is not an ImageNet thing&nbsp;, neither is boat. Here is the average of two things that are not ImageNet things and yet with one exception, it’s found us two sailboats.</p><pre name="1f6a" id="1f6a" class="graf graf--pre graf-after--p">vec = (en_vecd['sail'] + en_vecd['boat'])/2</pre><pre name="deb4" id="deb4" class="graf graf--pre graf-after--pre">idxs,dists = get_knn(nn_predwv, vec)<br>show_imgs([open_image(PATH/md.val_ds.fnames[i]) <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> idxs[:3]],<br>                      3, figsize=(9,3));</pre><figure name="32ff" id="32ff" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_-7X7LGxPWi1qv8fF1hkCog.png"></figure><h4 name="a464" id="a464" class="graf graf--h4 graf-after--figure">Image-&gt;image [<a href="https://youtu.be/tY0n9OT5_nA?t=2h14m35s" data-href="https://youtu.be/tY0n9OT5_nA?t=2h14m35s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">2:14:35</a>]</h4><p name="419f" id="419f" class="graf graf--p graf-after--h4">Okay, let’s do something else crazy. Let’s open up an image in the validation set. Let’s call <code class="markup--code markup--p-code">predict_array</code> on that image to get its word vector like thing, and let’s do a nearest neighbor search on all the other images.</p><pre name="78d8" id="78d8" class="graf graf--pre graf-after--p">fname = 'valid/n01440764/ILSVRC2012_val_00007197.JPEG'<br>img = open_image(PATH/fname)<br>show_img(img);</pre><figure name="caf8" id="caf8" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_FVCX6O367r2oJXVhR1r-Sg.png"></figure><pre name="5582" id="5582" class="graf graf--pre graf-after--figure">t_img = md.val_ds.transform(img)<br>pred = learn.predict_array(t_img[<strong class="markup--strong markup--pre-strong">None</strong>])</pre><pre name="bbcb" id="bbcb" class="graf graf--pre graf-after--pre">idxs,dists = get_knn(nn_predwv, pred)<br>show_imgs([open_image(PATH/md.val_ds.fnames[i]) <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> idxs[1:4]],<br>                      3, figsize=(9,3));</pre><figure name="a044" id="a044" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_hQ8r1gcwNfh7KlZGjS-lcQ.png"></figure><p name="8e97" id="8e97" class="graf graf--p graf-after--figure">And here are all the other images of whatever that is. So you can see, this is crazy — we’ve trained a thing on all of ImageNet in an hour, using a custom head that required basically like two lines fo code, and these things run in 300 milliseconds to do these searches.</p><p name="787d" id="787d" class="graf graf--p graf-after--p">Jeremy taught this basic idea last year as well, but it was in Keras, and it was pages and pages of code, and everything took a long time and complicated. And back then, Jeremy said he can’t begin to think all of the stuff you could do with this. He doesn’t think anybody has really thought deeply about this yet, but he thinks it’s fascinating. So go back and read the DeVICE paper because Andrea had a whole bunch of other thoughts and now that it is so easy to do, hopefully people will dig into this now. Jeremy thinks it’s crazy and amazing.</p><p name="ffb1" id="ffb1" class="graf graf--p graf-after--p graf--trailing">Alright, see you next week!</p><hr class="section-divider"><p name="1c54" id="1c54" class="graf graf--p graf--leading graf--trailing">Lessons: <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" class="markup--anchor markup--p-anchor" target="_blank">1</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" class="markup--anchor markup--p-anchor" target="_blank">2</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" class="markup--anchor markup--p-anchor" target="_blank">3</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" class="markup--anchor markup--p-anchor" target="_blank">4</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" class="markup--anchor markup--p-anchor" target="_blank">5</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" class="markup--anchor markup--p-anchor" target="_blank">6</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" class="markup--anchor markup--p-anchor" target="_blank">7</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" class="markup--anchor markup--p-anchor" target="_blank">8</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" class="markup--anchor markup--p-anchor" target="_blank">9</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" class="markup--anchor markup--p-anchor" target="_blank">10</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" class="markup--anchor markup--p-anchor" target="_blank"><strong class="markup--strong markup--p-strong">11</strong></a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" class="markup--anchor markup--p-anchor" target="_blank">12</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" class="markup--anchor markup--p-anchor" target="_blank">13</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" class="markup--anchor markup--p-anchor" target="_blank">14</a></p></body></html>