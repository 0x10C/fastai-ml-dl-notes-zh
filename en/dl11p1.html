
<!-- saved from url=(0064)file:///C:/Users/asus/Desktop/fastai-ml-dl-notes-zh/en/dl11.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></head><body><hr class="section-divider"><h1 name="a794" id="a794" class="graf graf--h3 graf--leading graf--title">Deep Learning 2: Part 2 Lesson&nbsp;11</h1><p name="2560" id="2560" class="graf graf--p graf-after--h3"><em class="markup--em markup--p-em">My personal notes from </em><a href="http://www.fast.ai/" data-href="http://www.fast.ai/" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener noopener noopener nofollow noopener nofollow noopener noopener nofollow noopener" target="_blank"><em class="markup--em markup--p-em">fast.ai course</em></a><em class="markup--em markup--p-em">. These notes will continue to be updated and improved as I continue to review the course to “really” understand it. Much appreciation to </em><a href="https://twitter.com/jeremyphoward" data-href="https://twitter.com/jeremyphoward" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener noopener noopener nofollow noopener nofollow noopener noopener nofollow noopener" target="_blank"><em class="markup--em markup--p-em">Jeremy</em></a><em class="markup--em markup--p-em"> and </em><a href="https://twitter.com/math_rachel" data-href="https://twitter.com/math_rachel" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener noopener noopener nofollow noopener nofollow noopener noopener nofollow noopener" target="_blank"><em class="markup--em markup--p-em">Rachel</em></a><em class="markup--em markup--p-em"> who gave me this opportunity to learn.</em></p><p name="dfbd" id="dfbd" class="graf graf--p graf-after--p graf--trailing">Lessons: <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" class="markup--anchor markup--p-anchor" target="_blank">1</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" class="markup--anchor markup--p-anchor" target="_blank">2</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" class="markup--anchor markup--p-anchor" target="_blank">3</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" class="markup--anchor markup--p-anchor" target="_blank">4</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" class="markup--anchor markup--p-anchor" target="_blank">5</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" class="markup--anchor markup--p-anchor" target="_blank">6</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" class="markup--anchor markup--p-anchor" target="_blank">7</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" class="markup--anchor markup--p-anchor" target="_blank">8</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" class="markup--anchor markup--p-anchor" target="_blank">9</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" class="markup--anchor markup--p-anchor" target="_blank">10</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" class="markup--anchor markup--p-anchor" target="_blank"><strong class="markup--strong markup--p-strong">11</strong></a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" class="markup--anchor markup--p-anchor" target="_blank">12</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" class="markup--anchor markup--p-anchor" target="_blank">13</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" class="markup--anchor markup--p-anchor" target="_blank">14</a></p><hr class="section-divider"><h3 name="0687" id="0687" class="graf graf--h3 graf--leading">Links</h3><p name="a8dc" id="a8dc" class="graf graf--p graf-after--h3"><a href="http://forums.fast.ai/t/part-2-lesson-11-in-class/14699/1" data-href="http://forums.fast.ai/t/part-2-lesson-11-in-class/14699/1" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank"><strong class="markup--strong markup--p-strong">Forum</strong></a><strong class="markup--strong markup--p-strong"> / </strong><a href="https://youtu.be/tY0n9OT5_nA" data-href="https://youtu.be/tY0n9OT5_nA" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong">Video</strong></a></p><h3 name="3522" id="3522" class="graf graf--h3 graf-after--p">Before getting&nbsp;started:</h3><ul class="postList"><li name="b947" id="b947" class="graf graf--li graf-after--h3"><a href="https://sgugger.github.io/the-1cycle-policy.html#the-1cycle-policy" data-href="https://sgugger.github.io/the-1cycle-policy.html#the-1cycle-policy" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">The 1cycle policy</a> by Sylvain Gugger. Based on Leslie Smith’s new paper which takes the previous two key papers (cyclical learning rate and super convergence) and built on them with a number of experiments to show how you can achieve super convergence. Super convergence lets you train models five times faster than the previous stepwise approach (and faster than CLR, although it is less than five times). Super convergence lets you get up to massively high learning rates by somewhere between 1 and 3. The interesting thing about super convergence is that you train at those very high learning rates for quite a large percentage of your epochs and during that time, the loss doesn’t really improve very much. But the trick is it’s doing a lot of searching through the space to find really generalizable areas it seems. Sylvain implemented it in fastai by flushing out the pieces that were missing then confirmed that he actually achieved super convergence on training on CIFAR10. It is currently called <code class="markup--code markup--li-code">use_clr_beta</code> but will be renamed in future. He also added cyclical momentum to fastai library.</li><li name="cc4e" id="cc4e" class="graf graf--li graf-after--li"><a href="https://towardsdatascience.com/how-to-create-data-products-that-are-magical-using-sequence-to-sequence-models-703f86a231f8" data-href="https://towardsdatascience.com/how-to-create-data-products-that-are-magical-using-sequence-to-sequence-models-703f86a231f8" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">How To Create Data Products That Are Magical Using Sequence-to-Sequence Model</a>s by Hamel Husain. He blogged about training a model to summarize GitHub issues. Here is the <a href="http://gh-demo.kubeflow.org/" data-href="http://gh-demo.kubeflow.org/" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">demo</a> Kubeflow team created based on his blog.</li></ul><h3 name="6a51" id="6a51" class="graf graf--h3 graf-after--li">Neural Machine Translation [<a href="https://youtu.be/tY0n9OT5_nA?t=5m36s" data-href="https://youtu.be/tY0n9OT5_nA?t=5m36s" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">5:36</a>]</h3><p name="764a" id="764a" class="graf graf--p graf-after--h3">Let’s build a sequence-to-sequence model! We are going to be working on machine translation. Machine translation is something that’s been around for a long time, but we are going to look at an approach called neural translation which uses neural networks for translation. Neural machine translation appeared a couple years ago and it was not as good as the statistical machine translation approaches that use classic feature engineering and standard NLP approaches like stemming, fiddling around with word frequencies, n-grams, etc. By a year later, it was better than everything else. It is based on a metric called BLEU — we are not going to discuss the metric because it is not a very good metric and it is not interesting, but everybody uses it.</p><figure name="e600" id="e600" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_f0hoBLrTuevFPgAl-lFIfQ.png"></figure><p name="2ea6" id="2ea6" class="graf graf--p graf-after--figure">We are seeing machine translation starting down the path that we saw starting computer vision object classification in 2012 which just surpassed the state-of-the-art and now zipping past it at great rate. It is unlikely that anybody watching this is actually going to build a machine translation model because <a href="https://translate.google.com/" data-href="https://translate.google.com/" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">https://translate.google.com/</a> works quite well. So why are we learning about machine translation? The reason we are learning about machine translation is that the general idea of taking some kind of input like a sentence in French and transforming it into some other kind of output with arbitrary length such as a sentence in English is a really useful thing to do. For example, as we just saw, Hamel took GitHub issues and turn them into summaries. Another example is taking videos and turning them into descriptions, or basically anything where you are spitting out an arbitrary sized output which is very often a sentence. Maybe taking a CT scan and spitting out a radiology report — this is where you can use sequence to sequence learning.</p><h4 name="d052" id="d052" class="graf graf--h4 graf-after--p">Four big wins of Neural Machine Translation [<a href="https://youtu.be/tY0n9OT5_nA?t=8m36s" data-href="https://youtu.be/tY0n9OT5_nA?t=8m36s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">8:36</a>]</h4><figure name="bf3a" id="bf3a" class="graf graf--figure graf-after--h4"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_c2kAArVl9mF_VaeqnXafBw.png"></figure><ul class="postList"><li name="5fa6" id="5fa6" class="graf graf--li graf-after--figure">End-to-end training: No fussing around with heuristics and hacky feature engineering.</li><li name="b2f3" id="b2f3" class="graf graf--li graf-after--li">We are able to build these distributed representations which are shared by lots of concepts within a single network.</li><li name="41a0" id="41a0" class="graf graf--li graf-after--li">We are able to use long term state in the RNN so it uses a lot more context than n-gram type approaches.</li><li name="0c46" id="0c46" class="graf graf--li graf-after--li">In the end, text we are generating uses RNN as well so we can build something that is more fluid.</li></ul><h4 name="5340" id="5340" class="graf graf--h4 graf-after--li">BiLSTMs(+Attn) not just for neural MT&nbsp;[<a href="https://youtu.be/tY0n9OT5_nA?t=9m20s" data-href="https://youtu.be/tY0n9OT5_nA?t=9m20s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">9:20</a>]</h4><figure name="4991" id="4991" class="graf graf--figure graf-after--h4"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_LrzmE8Xi5-mwsUrqzEQA6Q.png"></figure><p name="8fec" id="8fec" class="graf graf--p graf-after--figure">We are going to use bi-directional GRU (basically the same as LSTM) with attention — these general ideas can also be used for lots of other things as you see above.</p><h4 name="9a67" id="9a67" class="graf graf--h4 graf-after--p">Let’s jump into the code&nbsp;[<a href="https://youtu.be/tY0n9OT5_nA?t=9m47s" data-href="https://youtu.be/tY0n9OT5_nA?t=9m47s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">9:47</a>]</h4><p name="9e24" id="9e24" class="graf graf--p graf-after--h4"><a href="https://github.com/fastai/fastai/blob/master/courses/dl2/translate.ipynb" data-href="https://github.com/fastai/fastai/blob/master/courses/dl2/translate.ipynb" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Notebook</a></p><p name="f726" id="f726" class="graf graf--p graf-after--p">We are going to try to translate French into English by following the standard neural network approach:</p><ol class="postList"><li name="711f" id="711f" class="graf graf--li graf-after--p">Data</li><li name="7d68" id="7d68" class="graf graf--li graf-after--li">Architecture</li><li name="b242" id="b242" class="graf graf--li graf-after--li">Loss Function</li></ol><h4 name="e838" id="e838" class="graf graf--h4 graf-after--li">1. Data</h4><p name="3fb2" id="3fb2" class="graf graf--p graf-after--h4">As usual, we need <code class="markup--code markup--p-code">(x, y)</code> pair. In this case, x: French sentence, y: English sentence which you will compare your prediction against. We need lots of these tuples of French sentences with their equivalent English sentence — that is called “parallel corpus” and harder to find than a corpus for a language model. For a language model, we just need text in some language. For any living language, there will be a few gigabytes at least of text floating around the internet for you to grab. For translation, there are some pretty good parallel corpus available for European languages. The European Parliament has every sentence in every European language. Anything that goes to the UN is translated to lots of languages. For French to English, we have particularly nice thing which is pretty much any semi official Canadian website will have a French version and an English version[<a href="https://youtu.be/tY0n9OT5_nA?t=12m13s" data-href="https://youtu.be/tY0n9OT5_nA?t=12m13s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">12:13</a>].</p><h4 name="0b1d" id="0b1d" class="graf graf--h4 graf-after--p">Translation files</h4><pre name="36f6" id="36f6" class="graf graf--pre graf-after--h4"><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">fastai.text</strong> <strong class="markup--strong markup--pre-strong">import</strong> *</pre><p name="c210" id="c210" class="graf graf--p graf-after--pre">French/English parallel texts from <a href="http://www.statmt.org/wmt15/translation-task.html" data-href="http://www.statmt.org/wmt15/translation-task.html" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">http://www.statmt.org/wmt15/translation-task.html</a>&nbsp;. It was created by Chris Callison-Burch, who crawled millions of web pages and then used <em class="markup--em markup--p-em">a set of simple heuristics to transform French URLs onto English URLs (i.e. replacing “fr” with “en” and about 40 other hand-written rules), and assume that these documents are translations of each other</em>.</p><pre name="35d6" id="35d6" class="graf graf--pre graf-after--p">PATH = Path('data/translate')<br>TMP_PATH = PATH/'tmp'<br>TMP_PATH.mkdir(exist_ok=<strong class="markup--strong markup--pre-strong">True</strong>)<br>fname='giga-fren.release2.fixed'<br>en_fname = PATH/f'<strong class="markup--strong markup--pre-strong">{fname}</strong>.en'<br>fr_fname = PATH/f'<strong class="markup--strong markup--pre-strong">{fname}</strong>.fr'</pre><p name="a906" id="a906" class="graf graf--p graf-after--pre">For bounding boxes, all of the interesting stuff was in the loss function, but for neural translation, all of the interesting stuff is going to be int he architecture [<a href="https://youtu.be/tY0n9OT5_nA?t=13m1s" data-href="https://youtu.be/tY0n9OT5_nA?t=13m1s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">13:01</a>]. Let’s zip through this pretty quickly and one of the things Jeremy wants you to think about particularly is what are the relationships or the similarities in terms of the tasks we are doing and how we do it between language modeling vs. neural translation.</p><figure name="160a" id="160a" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_458KhA7uSET5eH3fe4EhDw.png"></figure><p name="8546" id="8546" class="graf graf--p graf-after--figure">The first step is to do the exact same thing we do in a language model which is to take a sentence and chuck it through an RNN[<a href="https://youtu.be/tY0n9OT5_nA?t=13m35s" data-href="https://youtu.be/tY0n9OT5_nA?t=13m35s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">13:35</a>].</p><figure name="6dac" id="6dac" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_ujJzFdJfk2jP0466peyyrQ.png"></figure><p name="a4b6" id="a4b6" class="graf graf--p graf-after--figure">Now with the classification model, we had a decoder which took the RNN output and grabbed three things: <code class="markup--code markup--p-code">maxpool</code> and <code class="markup--code markup--p-code">meanpool</code> over all of the time steps, and the value of the RNN at the last time step, stack all those together and put it through a linear layer [<a href="https://youtu.be/tY0n9OT5_nA?t=14m24s" data-href="https://youtu.be/tY0n9OT5_nA?t=14m24s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">14:24</a>]. Most people do not do that and just use the last time step, so all the things we will be talking about today uses the last time step.</p><p name="6cad" id="6cad" class="graf graf--p graf-after--p">We start out by chucking the input sentence through an RNN and out of it comes some “hidden state” (i.e. some vector that represents the output of an RNN that has encoded the sentence).</p><h4 name="eaf5" id="eaf5" class="graf graf--h4 graf-after--p">Encoder ≈ Backbone&nbsp;[<a href="https://youtu.be/tY0n9OT5_nA?t=15m18s" data-href="https://youtu.be/tY0n9OT5_nA?t=15m18s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">15:18</a>]</h4><p name="c8b4" id="c8b4" class="graf graf--p graf-after--h4">Stephen used the word “encoder”, but we tend to use the word “backbone”. Like when we talked about adding a custom head to an existing model, the existing pre-trained ImageNet model, for example, we say that is our backbone and then we stick on top of it some head that does the task we want. In sequence to sequence learning, they use the word encoder, but it basically is the same thing — it is some piece of a neural network architecture that takes the input and turns it into some representation which we can then stick a few more layers on top to grab something out of it such as we did for the classifier where we stack a linear layer on top of it to turn int into a sentiment. This time though, we have something that’s a little bit harder than just creating sentiment [<a href="https://youtu.be/tY0n9OT5_nA?t=16m12s" data-href="https://youtu.be/tY0n9OT5_nA?t=16m12s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">16:12</a>]. Instead of turning the hidden state into a positive or negative sentiment, we want to turn it into a sequence of tokens where that sequence of token is the German sentence in Stephen’s example.</p><p name="cb0e" id="cb0e" class="graf graf--p graf-after--p">This is sounding more like the language model than the classifier because the language had multiple tokens (for every input word, there was an output word). But the language model was also much easier because the number of tokens in the language model output was the same length as the number of tokens in the language model input. Not only they were the same length, but they exactly matched up (e.g. after word one comes word two, after word two comes word three, and so forth). For translating language, you don’t necessarily know that the word “he” will be translated as the first word in the output (unfortunately, it is in this particular case). Very often, the subject object order will be different or there will be some extra words inserted, or some pronouns we will need to add some gendered article, etc. This is the key issue we are going to have to deal with is the fact that we have an arbitrary length output where the tokens in the output do not correspond to the same order or the specific tokens in the input [<a href="https://youtu.be/tY0n9OT5_nA?t=17m31s" data-href="https://youtu.be/tY0n9OT5_nA?t=17m31s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">17:31</a>]. But the general idea is the same. Here is an RNN to encode the input, turns it into some hidden state, then this is the new thing we are going to learn is generating a sequence output.</p><h4 name="2737" id="2737" class="graf graf--h4 graf-after--p">Sequence output&nbsp;[<a href="https://youtu.be/tY0n9OT5_nA?t=17m47s" data-href="https://youtu.be/tY0n9OT5_nA?t=17m47s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">17:47</a>]</h4><p name="4228" id="4228" class="graf graf--p graf-after--h4">We already know:</p><ul class="postList"><li name="f4c5" id="f4c5" class="graf graf--li graf-after--p">Sequence to class (IMDB classifier)</li><li name="09ce" id="09ce" class="graf graf--li graf-after--li">Sequence to equal length sequence (Language model)</li></ul><p name="dd37" id="dd37" class="graf graf--p graf-after--li">But we do not know yet how to do a general purpose sequence to sequence, so that’s the new thing today. Very little of this will make sense unless you really understand lesson 6 how an RNN works.</p><h4 name="52a2" id="52a2" class="graf graf--h4 graf-after--p">Quick review of <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" class="markup--anchor markup--h4-anchor" target="_blank">Lesson 6</a>&nbsp;[<a href="https://youtu.be/tY0n9OT5_nA?t=18m20s" data-href="https://youtu.be/tY0n9OT5_nA?t=18m20s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">18:20</a>]</h4><p name="ed65" id="ed65" class="graf graf--p graf-after--h4">We learnt that an RNN at its heart is a standard fully connected network. Below is one with 4 layers — takes an input and puts it through four layers, but at the second layer, it concatenates in the second input, third layer concatenated in the third input, but we actually wrote this in Python as just a four layer neural network. There was nothing else we used other than linear layers and ReLUs. We used the same weight matrix every time when an input came in, we used the same matrix every time when we went from one of the hidden states to the next — that is why these arrows are the same color.</p><figure name="8eb7" id="8eb7" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_ZTtw8vtjy-K2CptW_xpLqw.png"></figure><p name="3e36" id="3e36" class="graf graf--p graf-after--figure">We can redraw the above diagram like the below [<a href="https://youtu.be/tY0n9OT5_nA?t=19m29s" data-href="https://youtu.be/tY0n9OT5_nA?t=19m29s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">19:29</a>].</p><figure name="9924" id="9924" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_QE70fqvLLDhq8fq_ctxpZQ.png"></figure><p name="eb44" id="eb44" class="graf graf--p graf-after--figure">Not only did we redraw it but we took the four lines of linear linear linear linear code in PyTorch and we replaced it with a for loop. Remember, we had something that did exactly the same thing as below, but it just had four lines of code saying <code class="markup--code markup--p-code">self.l_in(input)</code> and we replaced it with a for loop because that’s nice to refactor. The refactoring which does not change any of the math, any of the ideas, or any of the outputs is an RNN. It’s turning a bunch of separate lines in the code into a Python for loop.</p><figure name="3122" id="3122" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_ewV_N6jZBjStFNpSCMndxg.png"></figure><p name="4822" id="4822" class="graf graf--p graf-after--figure">We could take the output so that it is not outside the loop and put it inside the loop [<a href="https://youtu.be/tY0n9OT5_nA?t=20m25s" data-href="https://youtu.be/tY0n9OT5_nA?t=20m25s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">20:25</a>]. If we do that, we are now going to generate a separate output for every input. The code above, the hidden state gets replaced each time and we end up just spitting out the final hidden state. But if instead, we had something that said <code class="markup--code markup--p-code">hs.append(h)</code> and returned <code class="markup--code markup--p-code">hs</code> at the end, that would be the picture below.</p><figure name="fa1e" id="fa1e" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_CX45skUFZZO6uHsR8IndzA.png"></figure><p name="b36b" id="b36b" class="graf graf--p graf-after--figure">The main thing to remember is when we say hidden state, we are referring to a vector — technically a vector for each thing in the mini-batch so it’s a matrix, but generally when Jeremy speaks about these things, he ignores the mini-batch piece and treat it for just a single item.</p><figure name="8259" id="8259" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_ch4De-RThVp-fthGpqsaWw.png"></figure><p name="c658" id="c658" class="graf graf--p graf-after--figure">We also learned that you can stack these layers on top of each other [<a href="https://youtu.be/tY0n9OT5_nA?t=21m41s" data-href="https://youtu.be/tY0n9OT5_nA?t=21m41s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">21:41</a>]. So rather than the left RNN (in the diagram above) spitting out output, they could just spit out inputs into a second RNN. If you are thinking at this point “I think I understand this but I am not quite sure” that means you don’t understand this. The only way you know that you actually understand it is to go and write this from scratch in PyTorch or Numpy. If you can’t do that, then you know you don’t understand it and you can go back and re-watch lesson 6 and check out the notebook and copy some of the ideas until you can. It is really important that you can write that from scratch — it’s less than a screen of code. So you want to make sure you can create a 2 layer RNN. Below is what it looks like if you unroll it.</p><figure name="9272" id="9272" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_2GKBK9P_zpUieQF6JFyChw.png"></figure><p name="95cf" id="95cf" class="graf graf--p graf-after--figure">To get to a point that we have (x, y) pairs of sentences, we will start by downloading the dataset [<a href="https://youtu.be/tY0n9OT5_nA?t=22m39s" data-href="https://youtu.be/tY0n9OT5_nA?t=22m39s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">22:39</a>]. Training a translation model takes a long time. Google’s translation model has eight layers of RNN stacked on top of each other. There is no conceptual difference between eight layers and two layers. If you are Google and you have more GPUs or TPUs than you know what to do with, then you are fine doing that. Where else, in our case, it’s pretty likely that the kind of sequence to sequence models we are building are not going to require that level of computation. So to keep things simple [<a href="https://youtu.be/tY0n9OT5_nA?t=23m22s" data-href="https://youtu.be/tY0n9OT5_nA?t=23m22s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">23:22</a>], let’s do a cut-down thing where rather than learning how to translate French into English for any sentence, let’s learn to translate French questions into English questions — specifically questions that start with what/where/which/when. So here is a regex which looks for things that start with “wh” and end with a question mark.</p><pre name="2ba2" id="2ba2" class="graf graf--pre graf-after--p">re_eq = re.compile('^(Wh[^?.!]+\?)')<br>re_fq = re.compile('^([^?.!]+\?)')</pre><pre name="06a3" id="06a3" class="graf graf--pre graf-after--pre">lines = ((re_eq.search(eq), re_fq.search(fq)) <br>         <strong class="markup--strong markup--pre-strong">for</strong> eq, fq <strong class="markup--strong markup--pre-strong">in</strong> zip(open(en_fname, encoding='utf-8'), <br>                           open(fr_fname, encoding='utf-8')))</pre><pre name="9147" id="9147" class="graf graf--pre graf-after--pre">qs = [(e.group(), f.group()) <strong class="markup--strong markup--pre-strong">for</strong> e,f <strong class="markup--strong markup--pre-strong">in</strong> lines <strong class="markup--strong markup--pre-strong">if</strong> e <strong class="markup--strong markup--pre-strong">and</strong> f]</pre><p name="179f" id="179f" class="graf graf--p graf-after--pre">We go through the corpus [<a href="https://youtu.be/tY0n9OT5_nA?t=23m43s" data-href="https://youtu.be/tY0n9OT5_nA?t=23m43s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">23:43</a>], open up each of the two files, each line is one parallel text, zip them together, grab the English question and the French question, and check whether they match the regular expressions.</p><pre name="2aef" id="2aef" class="graf graf--pre graf-after--p">pickle.dump(qs, (PATH/'fr-en-qs.pkl').open('wb'))<br>qs = pickle.load((PATH/'fr-en-qs.pkl').open('rb'))</pre><p name="c656" id="c656" class="graf graf--p graf-after--pre">Dump that out as a pickle so we don’t have to do it again and so now we have 52,000 sentence pairs and here are some examples:</p><pre name="794f" id="794f" class="graf graf--pre graf-after--p">qs[:5], len(qs)</pre><pre name="34dc" id="34dc" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">([('What is light ?', 'Qu’est-ce que la lumière?'),<br>  ('Who are we?', 'Où sommes-nous?'),<br>  ('Where did we come from?', "D'où venons-nous?"),<br>  ('What would we do without it?', 'Que ferions-nous sans elle ?'),<br>  ('What is the absolute location (latitude and longitude) of Badger, Newfoundland and Labrador?',<br>   'Quelle sont les coordonnées (latitude et longitude) de Badger, à Terre-Neuve-etLabrador?')],<br> 52331)</em></pre><p name="3a70" id="3a70" class="graf graf--p graf-after--pre">One nice thing about this is that what/who/where type questions tend to be fairly short [<a href="https://youtu.be/tY0n9OT5_nA?t=24m8s" data-href="https://youtu.be/tY0n9OT5_nA?t=24m8s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">24:08</a>]. But the idea that we could learn from scratch with no previous understanding of the idea of language let alone of English or French that we could create something that can translate one to the other for any arbitrary question with only 50k sentences sounds like a ludicrously difficult thing to ask this to do. So it would be impressive if we can make any progress what so ever. This is very little data to do a very complex exercise.</p><p name="d8ad" id="d8ad" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">qs</code> contains the tuples of French and English [<a href="https://youtu.be/tY0n9OT5_nA?t=24m48s" data-href="https://youtu.be/tY0n9OT5_nA?t=24m48s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">24:48</a>]. You can use this handy idiom to split them apart into a list of English questions and a list of French questions.</p><pre name="2bb5" id="2bb5" class="graf graf--pre graf-after--p">en_qs,fr_qs = zip(*qs)</pre><p name="bd0a" id="bd0a" class="graf graf--p graf-after--pre">Then we tokenize the English questions and tokenize the French questions. So remember that just means splitting them up into separate words or word-like things. By default [<a href="https://youtu.be/tY0n9OT5_nA?t=25m11s" data-href="https://youtu.be/tY0n9OT5_nA?t=25m11s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">25:11</a>], the tokenizer that we have here (remember this is a wrapper around the spaCy tokenizer which is a fantastic tokenizer) assumes English. So to ask for French, you just add an extra parameter <code class="markup--code markup--p-code">'fr'</code>. The first time you do this, you will get an error saying you don’t have the spaCy French model installed so you can run <code class="markup--code markup--p-code">python -m spacy download fr</code> to grab the French model.</p><pre name="5efd" id="5efd" class="graf graf--pre graf-after--p">en_tok = Tokenizer.proc_all_mp(partition_by_cores(en_qs))</pre><pre name="aea8" id="aea8" class="graf graf--pre graf-after--pre">fr_tok = Tokenizer.proc_all_mp(partition_by_cores(fr_qs), 'fr')</pre><p name="d690" id="d690" class="graf graf--p graf-after--pre">It is unlikely that any of you are going to have RAM problems here because this is not particularly big corpus but some of the students were trying to train a new language models during the week and were having RAM problems. If you do, it’s worth knowing what these functions (<code class="markup--code markup--p-code">proc_all_mp</code>) are actually doing. <code class="markup--code markup--p-code">proc_all_mp</code> is processing every sentence across multiple processes [<a href="https://youtu.be/tY0n9OT5_nA?t=25m59s" data-href="https://youtu.be/tY0n9OT5_nA?t=25m59s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">25:59</a>]:</p><figure name="a07d" id="a07d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*3dijGYRXl1Vf9MFLD5AUOA.png" data-width="513" data-height="80" src="../img/1_3dijGYRXl1Vf9MFLD5AUOA.png"></figure><p name="e011" id="e011" class="graf graf--p graf-after--figure">The function above finds out how many CPUs you have, divide it by two (because normally with hyper-threading they don’t actually all work in parallel), then in parallel run this <code class="markup--code markup--p-code">proc_all</code> function. So that is going to spit out a whole separate Python processes for every CPU you have. If you have a lot of cores, that is a lot of Python processes — everyone is going to load all this data in and that can potentially use up all your RAM. So you could replace that with just <code class="markup--code markup--p-code">proc_all</code> rather than <code class="markup--code markup--p-code">proc_all_mp</code> to use less RAM. Or you could just use less cores. At the moment, we are calling <code class="markup--code markup--p-code">partition_by_cores</code> which calls <code class="markup--code markup--p-code">partition</code> on a list and asks to split it into a number of equal length things according to how many CPUs you have. So you could replace that to split into a smaller list and run it on less things.</p><figure name="1c92" id="1c92" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*9_D6dkXM4mR8fPf0E2eLcg.png" data-width="358" data-height="39" src="../img/1_9_D6dkXM4mR8fPf0E2eLcg.png"></figure><p name="ab7b" id="ab7b" class="graf graf--p graf-after--figure">Having tokenized the English and French, you can see how it gets split up [<a href="https://youtu.be/tY0n9OT5_nA?t=28m4s" data-href="https://youtu.be/tY0n9OT5_nA?t=28m4s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">28:04</a>]:</p><pre name="b154" id="b154" class="graf graf--pre graf-after--p">en_tok[0], fr_tok[0]</pre><pre name="8205" id="8205" class="graf graf--pre graf-after--pre">(['what', 'is', 'light', '?'],<br> ['qu’', 'est', '-ce', 'que', 'la', 'lumière', '?'])</pre><p name="7ae1" id="7ae1" class="graf graf--p graf-after--pre">You can see the tokenization for French is quite different looking because French loves their apostrophes and their hyphens. So if you try to use an English tokenizer for a French sentence, you’re going to get a pretty crappy outcome. You don’t need to know heaps of NLP ideas to use deep learning for NLP, but just some basic stuff like use the right tokenizer for your language is important [<a href="https://youtu.be/tY0n9OT5_nA?t=28m23s" data-href="https://youtu.be/tY0n9OT5_nA?t=28m23s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">28:23</a>]. Some of the students this week in our study group have been trying to build language models for Chinese instance which of course doesn’t really have the concept of a tokenizer in the same way, so we’ve been starting to look at <a href="https://github.com/google/sentencepiece" data-href="https://github.com/google/sentencepiece" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">sentence piece</a> which splits things into arbitrary sub-word units and so when Jeremy says tokenize, if you are using a language that doesn’t have spaces in, you should probably be checking out sentence piece or some other similar sub-word unit thing instead. Hopefully in the next week or two, we will be able to report back with some early results of these experiments with Chinese.</p><pre name="0e86" id="0e86" class="graf graf--pre graf-after--p">np.percentile([len(o) <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> en_tok], 90), <br>    np.percentile([len(o) <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> fr_tok], 90)</pre><pre name="2c02" id="2c02" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(23.0, 28.0)</em></pre><pre name="7002" id="7002" class="graf graf--pre graf-after--pre">keep = np.array([len(o)&lt;30 <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> en_tok])</pre><pre name="4773" id="4773" class="graf graf--pre graf-after--pre">en_tok = np.array(en_tok)[keep]<br>fr_tok = np.array(fr_tok)[keep]</pre><pre name="20e4" id="20e4" class="graf graf--pre graf-after--pre">pickle.dump(en_tok, (PATH/'en_tok.pkl').open('wb'))<br>pickle.dump(fr_tok, (PATH/'fr_tok.pkl').open('wb'))</pre><pre name="2cfa" id="2cfa" class="graf graf--pre graf-after--pre">en_tok = pickle.load((PATH/'en_tok.pkl').open('rb'))<br>fr_tok = pickle.load((PATH/'fr_tok.pkl').open('rb'))</pre><p name="6c7d" id="6c7d" class="graf graf--p graf-after--pre">So having tokenized it [<a href="https://youtu.be/tY0n9OT5_nA?t=29m25s" data-href="https://youtu.be/tY0n9OT5_nA?t=29m25s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">29:25</a>], we will save that to disk. Then remember, the next step after we create tokens is to turn them into numbers&nbsp;. To do that, we have two steps — the first is to get a list of all of the words that appear and then we turn every word into the index. If there are more than 40,000 words that appear, then let’s cut it off there so it doesn’t go too crazy. We insert a few extra tokens for beginning of stream (<code class="markup--code markup--p-code">_bos_</code>), padding (<code class="markup--code markup--p-code">_pad_</code>), end of stream (<code class="markup--code markup--p-code">_eos_</code>), and unknown (<code class="markup--code markup--p-code">_unk</code>). So if we try to look up something that wasn’t in the 40,000 most common, then we use a <code class="markup--code markup--p-code">deraultdict</code> to return 3 which is unknown.</p><pre name="b4cc" id="b4cc" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> toks2ids(tok,pre):<br>    freq = Counter(p <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> tok <strong class="markup--strong markup--pre-strong">for</strong> p <strong class="markup--strong markup--pre-strong">in</strong> o)<br>    itos = [o <strong class="markup--strong markup--pre-strong">for</strong> o,c <strong class="markup--strong markup--pre-strong">in</strong> freq.most_common(40000)]<br>    itos.insert(0, '_bos_')<br>    itos.insert(1, '_pad_')<br>    itos.insert(2, '_eos_')<br>    itos.insert(3, '_unk')<br>    stoi = collections.defaultdict(<strong class="markup--strong markup--pre-strong">lambda</strong>: 3, <br>                                   {v:k <strong class="markup--strong markup--pre-strong">for</strong> k,v <strong class="markup--strong markup--pre-strong">in</strong> enumerate(itos)})<br>    ids = np.array([([stoi[o] <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> p] + [2]) <strong class="markup--strong markup--pre-strong">for</strong> p <strong class="markup--strong markup--pre-strong">in</strong> tok])<br>    np.save(TMP_PATH/f'<strong class="markup--strong markup--pre-strong">{pre}</strong>_ids.npy', ids)<br>    pickle.dump(itos, open(TMP_PATH/f'<strong class="markup--strong markup--pre-strong">{pre}</strong>_itos.pkl', 'wb'))<br>    <strong class="markup--strong markup--pre-strong">return</strong> ids,itos,stoi</pre><p name="af75" id="af75" class="graf graf--p graf-after--pre">Now we can go ahead and turn every token into an ID by putting it through the string to integer dictionary (<code class="markup--code markup--p-code">stoi</code>) we just created and then at the end of that let’s add the number 2 which is the end of stream. The code you see here is the code Jeremy writes when he is iterating and experimenting [<a href="https://youtu.be/tY0n9OT5_nA?t=30m25s" data-href="https://youtu.be/tY0n9OT5_nA?t=30m25s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">30:25</a>]. Because 99% of the code he writes while iterating and experimenting turns out to be totally wrong or stupid or embarrassing and you don’t get to see it. But there is not point refactoring that and making it beautiful when he’s writing it so he wanted you to see all the little shortcuts he has. Rather than having some constant for <code class="markup--code markup--p-code">_eos_</code> marker and using that, when he is prototyping he just does the easy stuff. Not so much that he ends up with broken code but he tries to find some middle ground between beautiful code and code that works.</p><p name="fb04" id="fb04" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: Just heard him mention that we divide the number of CPUs by 2 because with hyper-threading, we don’t get a speed-up using all the hyper threaded cores. Is this based on practical experience or is there some underlying reason why we wouldn’t get additional speedup [<a href="https://youtu.be/tY0n9OT5_nA?t=31m18s" data-href="https://youtu.be/tY0n9OT5_nA?t=31m18s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">31:18</a>]? Yes, it’s just practical experience and it’s not all things seemed like this, but I definitely noticed with tokenization — hyper-threading seemed to slow things down a little bit. Also if I use all the cores, often I want to do something else at the same time (like running some interactive notebook) and I don’t have any spare room to do that.</p><p name="e5c2" id="e5c2" class="graf graf--p graf-after--p">Now for our English and French, we can grab a list of IDs <code class="markup--code markup--p-code">en_ids</code> [<a href="https://youtu.be/tY0n9OT5_nA?t=32m1s" data-href="https://youtu.be/tY0n9OT5_nA?t=32m1s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">32:01</a>]. When we do that, of course, we need to make sure that we also store the vocabulary. There is no point having IDs if we don’t know what a number 5 represents, there is no point having a number 5. So that’s our vocabulary <code class="markup--code markup--p-code">en_itos</code> and reverse mapping <code class="markup--code markup--p-code">en_stoi</code> that we can use to convert more corpuses in the future.</p><pre name="1d04" id="1d04" class="graf graf--pre graf-after--p">en_ids,en_itos,en_stoi = toks2ids(en_tok,'en')<br>fr_ids,fr_itos,fr_stoi = toks2ids(fr_tok,'fr')</pre><p name="900a" id="900a" class="graf graf--p graf-after--pre">Just to confirm it’s working, we can go through each ID, convert the int to a string, and spit that out — there we have our sentence back now with an end of stream marker at the end. Our English vocab is 17,000 and our French vocab is 25,000, so that’s not too big and not too complex vocab that we are dealing with.</p><pre name="9976" id="9976" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> load_ids(pre):<br>    ids = np.load(TMP_PATH/f'<strong class="markup--strong markup--pre-strong">{pre}</strong>_ids.npy')<br>    itos = pickle.load(open(TMP_PATH/f'<strong class="markup--strong markup--pre-strong">{pre}</strong>_itos.pkl', 'rb'))<br>    stoi = collections.defaultdict(<strong class="markup--strong markup--pre-strong">lambda</strong>: 3, <br>                                   {v:k <strong class="markup--strong markup--pre-strong">for</strong> k,v <strong class="markup--strong markup--pre-strong">in</strong> enumerate(itos)})<br>    <strong class="markup--strong markup--pre-strong">return</strong> ids,itos,stoi</pre><pre name="dbb5" id="dbb5" class="graf graf--pre graf-after--pre">en_ids,en_itos,en_stoi = load_ids('en')<br>fr_ids,fr_itos,fr_stoi = load_ids('fr')</pre><pre name="da07" id="da07" class="graf graf--pre graf-after--pre">[fr_itos[o] <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> fr_ids[0]], len(en_itos), len(fr_itos)</pre><pre name="1c35" id="1c35" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(['qu’', 'est', '-ce', 'que', 'la', 'lumière', '?', '_eos_'], 17573, 24793)</em></pre><h4 name="c016" id="c016" class="graf graf--h4 graf-after--pre">Word vectors&nbsp;[<a href="https://youtu.be/tY0n9OT5_nA?t=32m53s" data-href="https://youtu.be/tY0n9OT5_nA?t=32m53s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">32:53</a>]</h4><p name="fef2" id="fef2" class="graf graf--p graf-after--h4">We spent a lot of time on the forum during the week discussing how pointless word vectors are and how you should stop getting so excited about them — and now we are going to use them. Why? All the stuff we’ve been learning about using language models and pre-trained proper models rather than pre-trained linear single layers which is what word vectors are, applies equally well to sequence to sequence. But Jeremy and Sebastian are starting to look at that. There is a whole thing for anybody interested in creating some genuinely new highly publishable results, the entire area of sequence to sequence with pre-trained language models has not been touched yet. Jeremy believes it is going to be just as good as classifications. If you work on this and you get to the point where you have something that is looking exciting and you want help publishing it, Jeremy is very happy to help co-author papers. So feel free to reach out when you have some interesting results.</p><p name="7a41" id="7a41" class="graf graf--p graf-after--p">At this stage, we do not have any of that, so we are going to use very little fastai [<a href="https://youtu.be/tY0n9OT5_nA?t=34m14s" data-href="https://youtu.be/tY0n9OT5_nA?t=34m14s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">34:14</a>]. All we have is word vectors — so let’s at least use decent word vectors. Word2vec is very old word word vectors. There are better word vectors now and fast.text is a pretty good source of word vectors. There is hundreds of languages available for them, and your language is likely to be represented.</p><p name="684c" id="684c" class="graf graf--p graf-after--p">fasttext word vectors available from <a href="https://fasttext.cc/docs/en/english-vectors.html" data-href="https://fasttext.cc/docs/en/english-vectors.html" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">https://fasttext.cc/docs/en/english-vectors.html</a></p><p name="617b" id="617b" class="graf graf--p graf-after--p">fasttext Python library is not available in PyPI but here is a handy trick [<a href="https://youtu.be/tY0n9OT5_nA?t=35m3s" data-href="https://youtu.be/tY0n9OT5_nA?t=35m3s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">35:03</a>]. If there is a GitHub repo that has a setup.py and reqirements.txt in it, you can just chuck <code class="markup--code markup--p-code">git+</code> at the start then stick that in your <code class="markup--code markup--p-code">pip install</code> and it works. Hardly anybody seems to know this and if you go to the fasttext repo, they won’t tell you this — they’ll say you have to download it and <code class="markup--code markup--p-code">cd</code> into it and blah but you don’t. You can just run this:</p><pre name="df82" id="df82" class="graf graf--pre graf-after--p"><em class="markup--em markup--pre-em"># ! pip install git+https://github.com/facebookresearch/fastText.git</em></pre><pre name="1e85" id="1e85" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">import</strong> <strong class="markup--strong markup--pre-strong">fastText</strong> <strong class="markup--strong markup--pre-strong">as</strong> <strong class="markup--strong markup--pre-strong">ft</strong></pre><p name="7e13" id="7e13" class="graf graf--p graf-after--pre">To use the fastText library, you’ll need to download <a href="https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md" data-href="https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">fasttext word vectors</a> for your language (download the ‘bin plus text’ ones).</p><pre name="d8b2" id="d8b2" class="graf graf--pre graf-after--p">en_vecs = ft.load_model(str((PATH/'wiki.en.bin')))</pre><pre name="10c6" id="10c6" class="graf graf--pre graf-after--pre">fr_vecs = ft.load_model(str((PATH/'wiki.fr.bin')))</pre><p name="e549" id="e549" class="graf graf--p graf-after--pre">Above are our English and French models. There are a text version and a binary version. The binary version is faster, so we will use that. The text version is also a bit buggy. We are going to convert it into a standard Python dictionary to make it a bit easier to work with [<a href="https://youtu.be/tY0n9OT5_nA?t=35m55s" data-href="https://youtu.be/tY0n9OT5_nA?t=35m55s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">35:55</a>]. This is just going through each word with a dictionary comprehension and save it as a pickle dictionary:</p><pre name="66d1" id="66d1" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> get_vecs(lang, ft_vecs):<br>    vecd = {w:ft_vecs.get_word_vector(w) <br>                <strong class="markup--strong markup--pre-strong">for</strong> w <strong class="markup--strong markup--pre-strong">in</strong> ft_vecs.get_words()}<br>    pickle.dump(vecd, open(PATH/f'wiki.<strong class="markup--strong markup--pre-strong">{lang}</strong>.pkl','wb'))<br>    <strong class="markup--strong markup--pre-strong">return</strong> vecd</pre><pre name="e154" id="e154" class="graf graf--pre graf-after--pre">en_vecd = get_vecs('en', en_vecs)<br>fr_vecd = get_vecs('fr', fr_vecs)</pre><pre name="8971" id="8971" class="graf graf--pre graf-after--pre">en_vecd = pickle.load(open(PATH/'wiki.en.pkl','rb'))<br>fr_vecd = pickle.load(open(PATH/'wiki.fr.pkl','rb'))</pre><pre name="7632" id="7632" class="graf graf--pre graf-after--pre">ft_words = ft_vecs.get_words(include_freq=<strong class="markup--strong markup--pre-strong">True</strong>)<br>ft_word_dict = {k:v <strong class="markup--strong markup--pre-strong">for</strong> k,v <strong class="markup--strong markup--pre-strong">in</strong> zip(*ft_words)}<br>ft_words = sorted(ft_word_dict.keys(), <br>                     key=<strong class="markup--strong markup--pre-strong">lambda</strong> x: ft_word_dict[x])</pre><p name="e671" id="e671" class="graf graf--p graf-after--pre">Now we have our pickle dictionary, we can go ahead and look up a word, for example, a comma [<a href="https://youtu.be/tY0n9OT5_nA?t=36m7s" data-href="https://youtu.be/tY0n9OT5_nA?t=36m7s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">36:07</a>]. That will return a vector. The length of the vector is the dimensionality of this set of word vectors. In this case, we have 300 dimensional English and French word vectors.</p><pre name="955a" id="955a" class="graf graf--pre graf-after--p">dim_en_vec = len(en_vecd[','])<br>dim_fr_vec = len(fr_vecd[','])<br>dim_en_vec,dim_fr_vec</pre><pre name="23a0" id="23a0" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(300, 300)</em></pre><p name="9fa0" id="9fa0" class="graf graf--p graf-after--pre">For reasons you will see in a moment, we also want to find out what the mean and standard deviation of our vectors are. So the mean is about zero and standard deviation is about 0.3.</p><pre name="c268" id="c268" class="graf graf--pre graf-after--p">en_vecs = np.stack(list(en_vecd.values()))<br>en_vecs.mean(),en_vecs.std()</pre><pre name="8d1d" id="8d1d" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(0.0075652334, 0.29283327)</em></pre><h4 name="9944" id="9944" class="graf graf--h4 graf-after--pre">Model data&nbsp;[<a href="https://youtu.be/tY0n9OT5_nA?t=36m48s" data-href="https://youtu.be/tY0n9OT5_nA?t=36m48s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">36:48</a>]</h4><p name="92a6" id="92a6" class="graf graf--p graf-after--h4">Often corpuses have a pretty long tailed distribution of sequence length and it’s the longest sequences that tend to overwhelm how long things take, how much memory is used, etc. So in this case, we are going to grab 99th to 97th percentile of the English and French and truncate them to that amount. Originally Jeremy was using 90 percentiles (hence the variable name):</p><pre name="f946" id="f946" class="graf graf--pre graf-after--p">enlen_90 = int(np.percentile([len(o) <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> en_ids], 99))<br>frlen_90 = int(np.percentile([len(o) <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> fr_ids], 97))<br>enlen_90,frlen_90</pre><pre name="9295" id="9295" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(29, 33)</em></pre><p name="abbc" id="abbc" class="graf graf--p graf-after--pre">We are nearly there [<a href="https://youtu.be/tY0n9OT5_nA?t=37m24s" data-href="https://youtu.be/tY0n9OT5_nA?t=37m24s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">37:24</a>]. We’ve got our tokenized, numerixalized English and French dataset. We’ve got some word vectors. So now we need to get it ready for PyTorch. PyTorch expects a <code class="markup--code markup--p-code">Dataset</code> object and hopefully by now you can say that a Dataset object requires two things — a length (<code class="markup--code markup--p-code">__len__</code>)and an indexer (<code class="markup--code markup--p-code">__getitem__</code>). Jeremy started out writing <code class="markup--code markup--p-code">Seq2SeqDataset</code> which turned out to be just a generic <code class="markup--code markup--p-code">Dataset</code> [<a href="https://youtu.be/tY0n9OT5_nA?t=37m52s" data-href="https://youtu.be/tY0n9OT5_nA?t=37m52s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">37:52</a>].</p><pre name="c9ec" id="c9ec" class="graf graf--pre graf-after--p">en_ids_tr = np.array([o[:enlen_90] <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> en_ids])<br>fr_ids_tr = np.array([o[:frlen_90] <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> fr_ids])</pre><pre name="7fca" id="7fca" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">Seq2SeqDataset</strong>(Dataset):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, x, y): self.x,self.y = x,y<br>    <strong class="markup--strong markup--pre-strong">def</strong> __getitem__(self, idx): <strong class="markup--strong markup--pre-strong">return</strong> A(self.x[idx], self.y[idx])<br>    <strong class="markup--strong markup--pre-strong">def</strong> __len__(self): <strong class="markup--strong markup--pre-strong">return</strong> len(self.x)</pre><ul class="postList"><li name="d8d7" id="d8d7" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">A</code>&nbsp;: Arrays. It will go through each of the thing you pass it, if it is not already a numpy array, it converts into a numpy array and returns back a tuple of all of the things you passed it which are now guaranteed to be numpy arrays [<a href="https://youtu.be/tY0n9OT5_nA?t=38m32s" data-href="https://youtu.be/tY0n9OT5_nA?t=38m32s" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">38:32</a>].</li><li name="b87a" id="b87a" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">V</code>&nbsp;: Variables</li><li name="bffc" id="bffc" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">T</code>&nbsp;: Tensors</li></ul><h4 name="19e1" id="19e1" class="graf graf--h4 graf-after--li">Training set and validation set&nbsp;[<a href="https://youtu.be/tY0n9OT5_nA?t=39m3s" data-href="https://youtu.be/tY0n9OT5_nA?t=39m3s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">39:03</a>]</h4><p name="0c7c" id="0c7c" class="graf graf--p graf-after--h4">Now we need to grab our English and French IDs and get a training set and a validation set. One of the things which is pretty disappointing about a lot of code out there on the internet is that they don’t follow some simple best practices. For example, if you go to PyTorch website, they have an example section for sequence to sequence translation. Their example does not have a separate validation set. Jeremy tried training according to their settings and tested it with a validation set and it turned out that it overfit massively. So this is not just a theoretical problem — the actual PyTorch repo has the actual official sequence to sequence translation example which does not check for overfitting and overfits horribly [<a href="https://youtu.be/tY0n9OT5_nA?t=39m41s" data-href="https://youtu.be/tY0n9OT5_nA?t=39m41s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">39:41</a>]. Also it fails to use mini-batches so it actually fails to utilize any of the efficiency of PyTorch whatsoever. Even if you find code in the official PyTorch repo, don’t assume it’s any good at all. The other thing you’ll notice is that pretty much every other sequence to sequence model Jeremy found in PyTorch anywhere on the internet has clearly copied from that crappy PyTorch repo because all has the same variable names, it has the same problems, it has the same mistakes.</p><p name="04a7" id="04a7" class="graf graf--p graf-after--p">Another example is that nearly every PyTorch convolutional neural network Jeremy found does not use an adaptive pooling layer [<a href="https://youtu.be/tY0n9OT5_nA?t=40m27s" data-href="https://youtu.be/tY0n9OT5_nA?t=40m27s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">40:27</a>]. So in other words, the final layer is always average pool (7,7). They assume that the previous layer is 7 by 7 and if you use any other size input, you get an exception, and therefore nearly everybody Jeremy has spoken who uses PyTorch thinks that there is a fundamental limitation of CNNs that they are tied to the input size and that has not been true since VGG. So every time Jeremy grabs a new model and stick it in the fastai repo, he has to go and search for “pool” and add “adaptive” to the start and replace the 7 with a 1and now it works on any sized object. So just be careful. It’s still early days and believe it or not, even though most of you have only started in the last year your deep learning journey, you know quite a lot more about a lot of the more important practical aspects than the vast majority of people that have publishing and writing stuff in official repos. So you need to have a little more self-confidence than you might expect when it comes to reading other people’s code. If you find yourself thinking “that looks odd”, it’s not necessarily you.</p><p name="eb9f" id="eb9f" class="graf graf--p graf-after--p">If the repo you are looking at doesn’t have a section on it saying here is the test we did where we got the same results as the paper that’s supposed to be implementing, that almost certainly means they haven’t got the same results of the paper they’re implementing, but probably haven’t even checked [<a href="https://youtu.be/tY0n9OT5_nA?t=42m13s" data-href="https://youtu.be/tY0n9OT5_nA?t=42m13s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">42:13</a>]. If you run it, definitely won’t get those results because it’s hard to get things right the first time — it takes Jeremy 12 goes. If they haven’t tested it once, it’s almost certainly won’t work.</p><p name="8a8a" id="8a8a" class="graf graf--p graf-after--p">Here is an easy way to get training and validation sets [<a href="https://youtu.be/tY0n9OT5_nA?t=42m45s" data-href="https://youtu.be/tY0n9OT5_nA?t=42m45s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">42:45</a>]. Grab a bunch of random numbers — one for each row of your data, and see if they are bigger than 0.1 or not. That gets you a list of booleans. Index into your array with that list of booleans to grab a training set, index into that array with the opposite of that list of booleans to get your validation set.</p><pre name="35c2" id="35c2" class="graf graf--pre graf-after--p">np.random.seed(42)<br>trn_keep = np.random.rand(len(en_ids_tr))&gt;0.1<br>en_trn,fr_trn = en_ids_tr[trn_keep],fr_ids_tr[trn_keep]<br>en_val,fr_val = en_ids_tr[~trn_keep],fr_ids_tr[~trn_keep]<br>len(en_trn),len(en_val)</pre><pre name="dc57" id="dc57" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(45219, 5041)</em></pre></body></html>