<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></head><body><pre name="64b2" id="64b2" class="graf graf--pre graf-after--p">itos = pickle.load((LM_PATH/'tmp'/'itos.pkl').open('rb'))<br>stoi = collections.defaultdict(<strong class="markup--strong markup--pre-strong">lambda</strong>:0, {v:k <strong class="markup--strong markup--pre-strong">for</strong> k,v <strong class="markup--strong markup--pre-strong">in</strong> <br>                                          enumerate(itos)})<br>len(itos)</pre><pre name="0e12" id="0e12" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">60002</em></pre><pre name="26ca" id="26ca" class="graf graf--pre graf-after--pre">trn_clas = np.array([[stoi[o] <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> p] <strong class="markup--strong markup--pre-strong">for</strong> p <strong class="markup--strong markup--pre-strong">in</strong> tok_trn])<br>val_clas = np.array([[stoi[o] <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> p] <strong class="markup--strong markup--pre-strong">for</strong> p <strong class="markup--strong markup--pre-strong">in</strong> tok_val])</pre><pre name="221e" id="221e" class="graf graf--pre graf-after--pre">np.save(CLAS_PATH/'tmp'/'trn_ids.npy', trn_clas)<br>np.save(CLAS_PATH/'tmp'/'val_ids.npy', val_clas)</pre><h4 name="bbd3" id="bbd3" class="graf graf--h4 graf-after--pre">Classifier</h4><pre name="e422" id="e422" class="graf graf--pre graf-after--h4">trn_clas = np.load(CLAS_PATH/'tmp'/'trn_ids.npy')<br>val_clas = np.load(CLAS_PATH/'tmp'/'val_ids.npy')</pre><pre name="4e4e" id="4e4e" class="graf graf--pre graf-after--pre">trn_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'trn_labels.npy'))<br>val_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'val_labels.npy'))</pre><p name="83cb" id="83cb" class="graf graf--p graf-after--pre">The construction of the model hyper parameters are the same [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h33m16s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h33m16s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:33:16</a>]. We can change the dropout. Pick a batch size that is as big as you can that doesn’t run out of memory.</p><pre name="75ea" id="75ea" class="graf graf--pre graf-after--p">bptt,em_sz,nh,nl = 70,400,1150,3<br>vs = len(itos)<br>opt_fn = partial(optim.Adam, betas=(0.8, 0.99))<br>bs = 48</pre><pre name="4ac1" id="4ac1" class="graf graf--pre graf-after--pre">min_lbl = trn_labels.min()<br>trn_labels -= min_lbl<br>val_labels -= min_lbl<br>c=int(trn_labels.max())+1</pre><h4 name="c689" id="c689" class="graf graf--h4 graf-after--pre">TextDataset [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h33m37s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h33m37s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:33:37</a>]</h4><p name="1d9f" id="1d9f" class="graf graf--p graf-after--h4">This bit is interesting. There’s fun stuff going on here.</p><pre name="f3a7" id="f3a7" class="graf graf--pre graf-after--p">trn_ds = TextDataset(trn_clas, trn_labels)<br>val_ds = TextDataset(val_clas, val_labels)</pre><p name="bcf5" id="bcf5" class="graf graf--p graf-after--pre">The basic idea here is that for the classifier, we do really want to look at one document. Is this document positive or negative? So we do want to shuffle the documents. But those documents have different lengths and so if we stick them all into one batch (this is a handy thing that fastai does for you) — you can stick things of different lengths into a batch and it will automatically pat them, so you don’t have to worry about that. But if they are wildly different lengths, then you’re going to be wasting a lot of computation times. If there is one thing that’s 2,000 words long and everything else is 50 words long, that means you end up with 2000 wide tensor. That’s pretty annoying. So James Bradbury who is one of Stephen Merity’s colleagues and the guy who came up with torchtext came up with a neat idea which was “let’s sort the dataset by length-ish”. So kind of make it so the first things in the list are, on the whole, shorter than the things at the end, but a little bit random as well.</p><p name="dad1" id="dad1" class="graf graf--p graf-after--p">Here is how Jeremy implemented that [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h35m10s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h35m10s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:35:10</a>]. The first thing we need is a Dataset. So we have a Dataset passing in the documents and their labels. Here is <code class="markup--code markup--p-code">TextDataSet</code> which inherits from <code class="markup--code markup--p-code">Dataset</code> and <code class="markup--code markup--p-code">Dataset</code> from PyTorch is also shown below:</p><figure name="6707" id="6707" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_5X1u6uQ6ywmiDVOa8qzbgg.png"></figure><p name="fd1b" id="fd1b" class="graf graf--p graf-after--figure">Actually <code class="markup--code markup--p-code">Dataset</code> doesn’t do anything at all [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h35m34s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h35m34s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:35:34</a>]. It says you need <code class="markup--code markup--p-code">__getitem__</code> if you don’t have one, you’re going to get an error. Same is true for <code class="markup--code markup--p-code">__len__</code>. So this is an abstract class. To <code class="markup--code markup--p-code">TextDataset</code>, we are going to pass in our <code class="markup--code markup--p-code">x</code> and <code class="markup--code markup--p-code">y</code>, and <code class="markup--code markup--p-code">__getitem__</code> will grab <code class="markup--code markup--p-code">x</code> and <code class="markup--code markup--p-code">y</code>, and return them — it couldn’t be much simpler. Optionally, 1. they could reverse it, 2. stick an end of stream at the end, 3. stick start of stream at the beginning. But we are not doing any of those things, so literally all we are doing is putting <code class="markup--code markup--p-code">x</code> and <code class="markup--code markup--p-code">y</code> and <code class="markup--code markup--p-code">__getitem__</code> returns them as a tuple. The length is however long the <code class="markup--code markup--p-code">x</code> is. That’s all <code class="markup--code markup--p-code">Dataset</code> is — something with a length that you can index.</p><h4 name="8a5a" id="8a5a" class="graf graf--h4 graf-after--p">Turning it to a DataLoader [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h36m27s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h36m27s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:36:27</a>]</h4><pre name="f03a" id="f03a" class="graf graf--pre graf-after--h4">trn_samp = SortishSampler(trn_clas, key=<strong class="markup--strong markup--pre-strong">lambda</strong> x: len(trn_clas[x]), <br>                          bs=bs//2)<br>val_samp = SortSampler(val_clas, key=<strong class="markup--strong markup--pre-strong">lambda</strong> x: len(val_clas[x]))</pre><pre name="34c1" id="34c1" class="graf graf--pre graf-after--pre">trn_dl = DataLoader(trn_ds, bs//2, transpose=<strong class="markup--strong markup--pre-strong">True</strong>, num_workers=1,<br>                    pad_idx=1, sampler=trn_samp)<br>val_dl = DataLoader(val_ds, bs, transpose=<strong class="markup--strong markup--pre-strong">True</strong>, num_workers=1, <br>                    pad_idx=1, sampler=val_samp)<br>md = ModelData(PATH, trn_dl, val_dl)</pre><p name="0bc9" id="0bc9" class="graf graf--p graf-after--pre">To turn it into a DataLoader, you simply pass the Dataset to the DataLoader constructor, and it’s now going to give you a batch of that at a time. Normally you can say shuffle equals true or shuffle equals false, it’ll decide whether to randomize it for you. In this case though, we are actually going to pass in a sampler parameter and sampler is a class we are going to define that tells the data loader how to shuffle.</p><ul class="postList"><li name="6ca8" id="6ca8" class="graf graf--li graf-after--p">For validation set, we are going to define something that actually just sorts. It just deterministically sorts it so that all the shortest documents will be at the start, all the longest documents will be at the end, and that’s going to minimize the amount of padding.</li><li name="7bb6" id="7bb6" class="graf graf--li graf-after--li">For training sampler, we are going to create this thing called sort-ish sampler which also sorts (ish!)</li></ul><figure name="efce" id="efce" class="graf graf--figure graf--layoutOutsetCenter graf-after--li" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_Z_0F0rRH8odcUq8n7bRDVg.png"></figure><p name="6e2b" id="6e2b" class="graf graf--p graf-after--figure">What’s great about PyTorch is that they came up with this idea for an API for their data loader where we can hook in new classes to make it behave in different ways [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h37m27s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h37m27s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:37:27</a>]. SortSampler is something which has a length which is the length of the data source and has an iterator which is simply an iterator which goes through the data source sorted by length (which is passed in as <code class="markup--code markup--p-code">key</code>). For the SortishSampler, it basically does the same thing with a little bit of randomness. It’s just another of those beautiful design things in PyTorch that Jeremy discovered. He could take James Bradbury’s ideas which he had written a whole new set of classes around, and he could just use inbuilt hooks inside PyTorch. You will notice data loader is not actually PyTorch’s data loader — it’s actually fastai’s data loader. But it’s basically almost entirely plagiarized from PyTorch but customized in some ways to make it faster mainly using multi-threading instead of multi-processing.</p><p name="174d" id="174d" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: Does the pre-trained LSTM depth and <code class="markup--code markup--p-code">bptt</code> need to match with the new one we are training [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h39m" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h39m" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:39:00</a>]? No, the <code class="markup--code markup--p-code">bptt</code> doesn’t need to match at all. That’s just like how many things we look at at a time. It has nothing to do with the architecture.</p><p name="7f93" id="7f93" class="graf graf--p graf-after--p">So now we can call that function we just saw before <code class="markup--code markup--p-code">get_rnn_classifier</code> [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h39m16s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h39m16s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:39:16</a>]. It’s going to create exactly the same encoder more or less, and we are going to pass in the same architectural details as before. But this time, with the head we add on, you have a few more things you can do. One is you can add more than one hidden layer. In <code class="markup--code markup--p-code">layers=[em_sz*3, 50, c]</code>:</p><ul class="postList"><li name="dd32" id="dd32" class="graf graf--li graf-after--p"><code class="markup--code markup--li-code">em_sz * 3</code>: this is what the input to my head (i.e. classifier section) is going to be.</li><li name="8d9c" id="8d9c" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">50</code>: this is the output of the first layer</li><li name="2a02" id="2a02" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">c</code>: this is the output of the second layer</li></ul><p name="8a42" id="8a42" class="graf graf--p graf-after--li">And you can add as many as you like. So you can basically create a little multi-layer neural net classifier at the end. Similarly, for <code class="markup--code markup--p-code">drops=[dps[4], 0.1]</code>, these are the dropouts to go after each of these layers.</p><pre name="a36b" id="a36b" class="graf graf--pre graf-after--p"><em class="markup--em markup--pre-em"> # part 1</em><br>dps = np.array([0.4, 0.5, 0.05, 0.3, 0.1])</pre><pre name="a511" id="a511" class="graf graf--pre graf-after--pre">dps = np.array([0.4,0.5,0.05,0.3,0.4])*0.5</pre><pre name="10ee" id="10ee" class="graf graf--pre graf-after--pre">m = get_rnn_classifer(bptt, 20*70, c, vs, emb_sz=em_sz, n_hid=nh, <br>                      n_layers=nl, pad_token=1,<br>                      layers=[em_sz*3, 50, c], drops=[dps[4], 0.1],<br>                      dropouti=dps[0], wdrop=dps[1],        <br>                      dropoute=dps[2], dropouth=dps[3])</pre><pre name="189d" id="189d" class="graf graf--pre graf-after--pre">opt_fn = partial(optim.Adam, betas=(0.7, 0.99))</pre><p name="dad9" id="dad9" class="graf graf--p graf-after--pre">We are going to use RNN_Learner just like before.</p><pre name="4683" id="4683" class="graf graf--pre graf-after--p">learn = RNN_Learner(md, TextModel(to_gpu(m)), opt_fn=opt_fn)<br>learn.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)<br>learn.clip=25.<br>learn.metrics = [accuracy]</pre><p name="2c19" id="2c19" class="graf graf--p graf-after--pre">We are going to use discriminative learning rates for different layers [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h40m20s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h40m20s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:40:20</a>].</p><pre name="68d6" id="68d6" class="graf graf--pre graf-after--p">lr=3e-3<br>lrm = 2.6<br>lrs = np.array([lr/(lrm**4), lr/(lrm**3), lr/(lrm**2), lr/lrm, lr])</pre><pre name="8626" id="8626" class="graf graf--pre graf-after--pre">lrs=np.array([1e-4,1e-4,1e-4,1e-3,1e-2])</pre><p name="0a57" id="0a57" class="graf graf--p graf-after--pre">You can try using weight decay or not. Jeremy has been fiddling around a bit with that to see what happens.</p><pre name="b02e" id="b02e" class="graf graf--pre graf-after--p">wd = 1e-7<br>wd = 0<br>learn.load_encoder('lm2_enc')</pre><p name="03e7" id="03e7" class="graf graf--p graf-after--pre">We start out just training the last layer and we get 92.9% accuracy:</p><pre name="e20a" id="e20a" class="graf graf--pre graf-after--p">learn.freeze_to(-1)</pre><pre name="edf5" id="edf5" class="graf graf--pre graf-after--pre">learn.lr_find(lrs/1000)<br>learn.sched.plot()</pre><pre name="9107" id="9107" class="graf graf--pre graf-after--pre">learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))</pre><pre name="3962" id="3962" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss   accuracy                      <br>    0      0.365457   0.185553   0.928719</em></pre><pre name="6209" id="6209" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[0.18555279, 0.9287188090884525]</em></pre><pre name="9c99" id="9c99" class="graf graf--pre graf-after--pre">learn.save('clas_0')<br>learn.load('clas_0')</pre><p name="bba1" id="bba1" class="graf graf--p graf-after--pre">Then we unfreeze one more layer, get 93.3% accuracy:</p><pre name="10d9" id="10d9" class="graf graf--pre graf-after--p">learn.freeze_to(-2)</pre><pre name="2e70" id="2e70" class="graf graf--pre graf-after--pre">learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))</pre><pre name="f93f" id="f93f" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss   accuracy                      <br>    0      0.340473   0.17319    0.933125</em></pre><pre name="9338" id="9338" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[0.17319041, 0.9331253991245995]</em></pre><pre name="2ab8" id="2ab8" class="graf graf--pre graf-after--pre">learn.save('clas_1')<br>learn.load('clas_1')</pre><pre name="6017" id="6017" class="graf graf--pre graf-after--pre">learn.unfreeze()</pre><pre name="d275" id="d275" class="graf graf--pre graf-after--pre">learn.fit(lrs, 1, wds=wd, cycle_len=14, use_clr=(32,10))</pre><pre name="80e4" id="80e4" class="graf graf--pre graf-after--pre">epoch      trn_loss   val_loss   accuracy                      <br>    0      0.337347   0.186812   0.930782  <br>    1      0.284065   0.318038   0.932062                      <br>    2      0.246721   0.156018   0.941747                      <br>    3      0.252745   0.157223   0.944106                      <br>    4      0.24023    0.159444   0.945393                      <br>    5      0.210046   0.202856   0.942858                      <br>    6      0.212139   0.149009   0.943746                      <br>    7      0.21163    0.186739   0.946553                      <br>    8      0.186233   0.1508     0.945218                      <br>    9      0.176225   0.150472   0.947985                      <br>    10     0.198024   0.146215   0.948345                      <br>    11     0.20324    0.189206   0.948145                      <br>    12     0.165159   0.151402   0.947745                      <br>    13     0.165997   0.146615   0.947905</pre><pre name="e90e" id="e90e" class="graf graf--pre graf-after--pre">[0.14661488, 0.9479046703071374]</pre><pre name="bd91" id="bd91" class="graf graf--pre graf-after--pre">learn.sched.plot_loss()</pre><pre name="c033" id="c033" class="graf graf--pre graf-after--pre">learn.save('clas_2')</pre><p name="af6f" id="af6f" class="graf graf--p graf-after--pre">Then we fine-tune the whole thing [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h40m47s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h40m47s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:40:47</a>]. This was the main attempt before our paper came along at using a pre-trained model:</p><p name="2042" id="2042" class="graf graf--p graf-after--p"><a href="https://arxiv.org/abs/1708.00107" data-href="https://arxiv.org/abs/1708.00107" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Learned in Translation: Contextualized Word Vectors</a></p><p name="1d42" id="1d42" class="graf graf--p graf-after--p">What they did is they used a pre-trained translation model but they didn’t fine tune the whole thing. They just took the activations of the translation model and when they tried IMDb, they got 91.8% — which we beat easily after only fine-tuning one layer. They weren’t state-of-the-art, the state-of-the-art is 94.1% which we beat after fine-tuning the whole thing for 3 epochs and by the end, we are at 94.8% which is obviously a huge difference because in terms of error rate, that’s gone done from 5.9%. A simple little trick is go back to the start of this notebook and reverse the order of all of the documents, and then re-run the whole thing. When you get to the bit that says <code class="markup--code markup--p-code">fwd_wt_103</code>, replace <code class="markup--code markup--p-code">fwd</code> for forward with <code class="markup--code markup--p-code">bwd</code> for backward. That’s a backward English language model that learns to read English backward. So if you redo this whole thing, put all the documents in reverse, and change this to backward, you now have a second classifier which classifies things by positive or negative sentiment based on the reverse document. If you then take the two predictions and take the average of them, you basically have a bi-directional model (which you trained each bit separately)and that gets you to 95.4% accuracy. So we basically lowered it from 5.9% to 4.6%. So this kind of 20% change in the state-of-the-art is almost unheard of. It doesn’t happen very often. So you can see this idea of using transfer learning, it’s ridiculously powerful that every new field thinks their new field is too special and you can’t do it. So it’s a big opportunity for all of us.</p><h4 name="71a0" id="71a0" class="graf graf--h4 graf-after--p">Universal Language Model Fine-tuning for Text Classification [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h44m2s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h44m2s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:44:02</a>]</h4><figure name="688d" id="688d" class="graf graf--figure graf-after--h4"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_XzWZUyxcsTu-ehYucd_vFQ.png"></figure><p name="7751" id="7751" class="graf graf--p graf-after--figure">So we turned this into a paper, and when I say we, I did it with this guy Sebastian Ruder. Now you might remember his name because in lesson 5, I told you that I actually had shared lesson 4 with Sebastian because I think he is an awesome researcher who I thought might like it. I didn’t know him personally at all. Much to my surprise, he actually watched the video. He watched the whole video and said:</p><p name="7caa" id="7caa" class="graf graf--p graf-after--p">Sebastian: “That’s actually quite fantastic! We should turn this into a paper.”</p><p name="ac56" id="ac56" class="graf graf--p graf-after--p">Jeremy: “I don’t write papers. I don’t care about papers and am not interested in papers — that sounds really boring”</p><p name="95e4" id="95e4" class="graf graf--p graf-after--p">Sebastian: “Okay, how about I write the paper for you.”</p><p name="fbf3" id="fbf3" class="graf graf--p graf-after--p">Jeremy: “You can’t really write a paper about this yet because you’d have to do like studies to compare it to other things (they are called ablation studies) to see which bit actually works. There’s no rigor here, I just put in everything that came in my head and chucked it all together and it happened to work”</p><p name="2db0" id="2db0" class="graf graf--p graf-after--p">Sebastian: “Okay, what if I write all the paper and do all your ablation studies, then can we write the paper?”</p><p name="f6a3" id="f6a3" class="graf graf--p graf-after--p">Jeremy: “Well, it’s like a whole library that I haven’t documented and I’m not going to yet and you don’t know how it all works”</p><p name="d987" id="d987" class="graf graf--p graf-after--p">Sebastian: “Okay, if I wrote the paper, and do the ablation studies, and figure out from scratch how the code works without bothering you, then can we write the paper?”</p><p name="a55f" id="a55f" class="graf graf--p graf-after--p">Jeremy: “Um… yeah, if you did all those things, then we can write the paper. Okay!”</p><p name="cc4f" id="cc4f" class="graf graf--p graf-after--p">Then two days later, he comes back and says “okay, I’ve done a draft of the paper.” So, I share this story to say, if you are some student in Ireland and you want to do good work, don’t let anybody stop you. I did not encourage him to say the least. But in the end, he said “I want to do this work, I think it’s going to be good, and I’ll figure it out” and he wrote a fantastic paper. He did the ablation study and he figured out how fastai works, and now we are planning to write another paper together. You’ve got to be a bit careful because sometimes I get messages from random people saying like “I’ve got lots of good ideas, can we have coffee?” — “I don’t want… I can have coffee in my office anytime, thank you”. But it’s very different to say “hey, I took your ideas and I wrote a paper, and I did a bunch of experiments, and I figured out how your code works, and I added documentation to it — should we submit this to a conference?” You see what I mean? There is nothing to stop you doing amazing work and if you do amazing work that helps somebody else, in this case, I’m happy that we have a paper. I don’t particularly care about papers but I think it’s cool that these ideas now have this rigorous study.</p><h4 name="b47f" id="b47f" class="graf graf--h4 graf-after--p">Let me show you what he did [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h47m19s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h47m19s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:47:19</a>]</h4><p name="47af" id="47af" class="graf graf--p graf-after--h4">He took all my code, so I’d already done all the fastai.text and as you have seen, it lets us work with large corpuses. Sebastian is fantastically well-read and he said “here’s a paper that Yann LeCun and some guys just came out with where they tried lots of classification datasets so I’m going to try running your code on all these datasets.” So these are the datasets:</p><figure name="4fe9" id="4fe9" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_NFanphEYzNa9uMV4iSY2bw.png"></figure><p name="b6a8" id="b6a8" class="graf graf--p graf-after--figure">Some of them had many many hundreds of thousands of documents and they were far bigger than I had tried — but I thought it should work.</p><p name="d97a" id="d97a" class="graf graf--p graf-after--p">And he had a few good ideas as we went along and so you should totally make sure you read the paper. He said “well, this thing that you called in the lessons differential learning rates, differential kind of means something else. Maybe we should rename it” so we renamed it. It’s now called discriminative learning rate. So this idea that we had from part one where we use different learning rates for different layers, after doing some literature research, it does seem like that hasn’t been done before so it’s now officially a thing — discriminative learning rates. This is something we learnt in lesson 1 but it now has an equation with Greek and everything [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h48m41s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h48m41s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:48:41</a>]:</p><figure name="ac0d" id="ac0d" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_KeaQyBreXN5QHfKCG-dJ0Q.png"></figure><p name="8140" id="8140" class="graf graf--p graf-after--figure">When you see an equation with Greek and everything, that doesn’t necessarily mean it’s more complex than anything we did in lesson 1 because this one isn’t.</p><p name="81a0" id="81a0" class="graf graf--p graf-after--p">Again, that idea of like unfreezing a layer at a time, also seems to never been done before so it’s now a thing and it’s got the very clever name “gradual unfreezing” [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h48m57s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h48m57s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:48:57</a>].</p><figure name="6d34" id="6d34" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_W3JSe1RPeRaYhMrr-RZoWw.png"></figure><h4 name="53d4" id="53d4" class="graf graf--h4 graf-after--figure">Slanted triangular learning rate [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h49m10s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h49m10s" class="markup--anchor markup--h4-anchor" rel="noopener nofollow" target="_blank">1:49:10</a>]</h4><p name="11e3" id="11e3" class="graf graf--p graf-after--h4">So then, as promised, we will look at slanted triangular learning rates&nbsp;. This actually was not my idea. Leslie Smith, one of my favorite researchers who you all now know about, emailed me a while ago and said “I’m so over cyclical learning rates. I don’t do that anymore. I now do a slightly different version where I have one cycle which goes up quickly at the start, and then slowly down afterwards. I often find it works better.” I’ve tried going back over all of my old datasets and it works better for all of them — every one I tried. So this is what the learning rate look like. You can use it in fastai just by adding <code class="markup--code markup--p-code">use_clr=</code> to your <code class="markup--code markup--p-code">fit</code>. The first number is the ratio between the highest learning rate and the lowest learning rate so the initial learning rate is 1/32 of the peak. The second number is the ratio between the first peak and the last peak. The basic idea is if you are doing a cycle length 10, that you want the first epoch to be the upward bit and the other 9 epochs to be the downward bit, then you would use 10. I find that works pretty well and that was also Leslie’s suggestion is make about 1/10 of it the upward bit and 9/10 the downward bit. Since he told me about it, maybe two days ago, he wrote this amazing paper: <a href="https://arxiv.org/abs/1803.09820" data-href="https://arxiv.org/abs/1803.09820" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">A DISCIPLINED APPROACH TO NEURAL NETWORK HYPER-PARAMETERS</a>. In which, he describes something very slightly different to this again, but the same basic idea. This is a must read paper. It’s got all the kinds of ideas that fastai talks about a lot in great depth and nobody else is talking about this. It’s kind of a slog, unfortunately Leslie had to go away on a trop before he really had time to edit it properly, so it’s a little bit slow reading, but don’t let that stop you. It’s amazing.</p><figure name="19d2" id="19d2" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_ydr4ZUCrsDg71s_C73ggTg.png"></figure><p name="4d28" id="4d28" class="graf graf--p graf-after--figure">The equation on the right is from my paper with Sebastian. Sebastian asked “Jeremy, can you send me the math equation behind that code you wrote?” and I said “no, I just wrote the code. I could not turn it into math” so he figured out the math for it.</p><h4 name="f151" id="f151" class="graf graf--h4 graf-after--p">Concat pooling [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h51m36s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h51m36s" class="markup--anchor markup--h4-anchor" rel="noopener nofollow" target="_blank">1:51:36</a>]</h4><p name="070c" id="070c" class="graf graf--p graf-after--h4">So you might have noticed, the first layer of our classifier was equal to embedding size*3&nbsp;. Why times 3? Times 3 because, and again, this seems to be something which people haven’t done before, so a new idea “concat pooling”. It is that we take the average pooling over the sequence of the activations, the max pooling of the sequence over the activations, and the final set of activations, and just concatenate them all together. This is something which we talked about in part 1 but doesn’t seem to be in the literature before so it’s now called “concat pooling” and it’s now got an equation and everything but this is the entirety of the implementation. So you can go through this paper and see how the fastai code implements each piece.</p><figure name="ae42" id="ae42" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_ilEQlVMIdx3m2WAKzOCjfQ.png"></figure><h4 name="c4b4" id="c4b4" class="graf graf--h4 graf-after--figure">BPT3C [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h52m46s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h52m46s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:52:46</a>]</h4><p name="3fc2" id="3fc2" class="graf graf--p graf-after--h4">One of the kind of interesting pieces is the difference between <code class="markup--code markup--p-code">RNN_Encoder</code> which you’ve already seen and MultiBatchRNN encoder. So what’s the difference there? The key difference is that the normal RNN encoder for the language model, we could just do <code class="markup--code markup--p-code">bptt</code> chunk at a time. But for the classifier, we need to do the whole document. We need to do the whole movie review before we decide if it’s positive or negative. And the whole movie review can easily be 2,000 words long and we can’t fit 2.000 words worth of gradients in my GPU memory for every single one of my weights. So what do we do? So the idea was very simple which is I go through my whole sequence length one batch of <code class="markup--code markup--p-code">bptt</code> at a time. And I call <code class="markup--code markup--p-code">super().forward</code> (in other words, the <code class="markup--code markup--p-code">RNN_Encoder</code>) to grab its outputs, and then I’ve got this maximum sequence length parameter where it says “okay, as long as you are doing no more than that sequence length, then start appending it to my list of outputs.” So in other words, the thing that it sends back to this pooling is only as many activations as we’ve asked it to keep. That way, you can figure out what <code class="markup--code markup--p-code">max_seq</code> can your particular GPU handle. So it’s still using the whole document, but let’s say <code class="markup--code markup--p-code">max_seq</code> is 1,000 words and your longest document length is 2, 000 words. It’s still going through RNN creating states for those first thousand words, but it’s not actually going to store the activations for the backprop of the first thousand. It’s only going to keep the last thousand. So that means that it can’t back-propagate the loss back to any state that was created in the first thousand words — basically that’s now gone. So it’s a really simple piece of code and honestly when I wrote it I didn’t spend much time thinking about it, it seems so obviously the only way this could possibly work. But again, it seems to be a new thing, so we now have backprop through time for text classification. You can see there’s lots of little pieces in this paper.</p><figure name="5a1f" id="5a1f" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_N-GZd5Z6Z3HjbEJnTID43g.png"></figure><h4 name="7110" id="7110" class="graf graf--h4 graf-after--figure">Results [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h55m56s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h55m56s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:55:56</a>]</h4><p name="b323" id="b323" class="graf graf--p graf-after--h4">What was the result? On every single dataset we tried, we got better result than any previous academic paper for text classification. All different types. Honestly, IMDb was the only one I spent any time trying to optimize the model, so most of them, we just did it whatever came out first. So if we actually spent time with it, I think this would be a lot better. The things that these are comparing to, most of them are different on each table because they are customized algorithms on the whole. So this is saying one simple fine-tuning algorithm can beat these really customized algorithms.</p><figure name="227e" id="227e" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_D9ntGwft-g9FgWsuNonGJQ.png"></figure><h4 name="91f9" id="91f9" class="graf graf--h4 graf-after--figure">Ablation studies [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h56m56s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h56m56s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:56:56</a>]</h4><p name="5173" id="5173" class="graf graf--p graf-after--h4">Here is the ablation studies Sebastian did. I was really keen that if you are going to publish a paper, we had to say why it works. So Sebastian went through and tried removing all of those different contributions I mentioned. So what is we don’t use gradual freezing? What if we don’t use discriminative learning rates? What if instead of discrimination rates, we use cosign annealing? What if we don’t do any pre-training with Wikipedia? What if we don’t do any fine tuning? And the really interesting one to me was, what’s the validation error rate on IMDb if we only used a hundred training examples (vs. 200, vs. 500, etc). And you can see, very interestingly, the full version of this approach is nearly as accurate on just a hundred training examples — it’s still very accurate vs. full 20,000 training examples. Where as if you are training from scratch on 100, it’s almost random. It’s what I expected. I’ve said to Sebastian I really think that this is most beneficial when you don’t have much data. This is where fastai is most interested in contributing — small data regimes, small compute regimes, and so forth. So he did these studies to check.</p><figure name="1725" id="1725" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_JsahawCY9ja-kZHTd90lFQ.png"></figure><h3 name="12b5" id="12b5" class="graf graf--h3 graf-after--figure">Tricks to run ablation studies [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h58m32s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h58m32s" class="markup--anchor markup--h3-anchor" rel="noopener nofollow" target="_blank">1:58:32</a>]</h3><h4 name="0528" id="0528" class="graf graf--h4 graf-after--h3">Trick #1:&nbsp;VNC</h4><p name="d186" id="d186" class="graf graf--p graf-after--h4">The first trick is something which I know you’re all going to find really handy. I know you’ve all been annoyed when you are running something in a Jupyter notebook, and you lose your internet connection for long enough that it decides you’ve gone away, and then your session disappears, and you have to start it again from scratch. So what do you do? There is a very simple cool thing called VNC where you can install on your AWS instance or PaperSpace, or whatever:</p><ul class="postList"><li name="ae5a" id="ae5a" class="graf graf--li graf-after--p">X Windows (<code class="markup--code markup--li-code">xorg</code>)</li><li name="628f" id="628f" class="graf graf--li graf-after--li">Lightweight window manager (<code class="markup--code markup--li-code">lxde-core</code>)</li><li name="6f05" id="6f05" class="graf graf--li graf-after--li">VNC server (<code class="markup--code markup--li-code">tightvncserver</code>)</li><li name="53ea" id="53ea" class="graf graf--li graf-after--li">Firefox (<code class="markup--code markup--li-code">firefox</code>)</li><li name="ce58" id="ce58" class="graf graf--li graf-after--li">Terminal (<code class="markup--code markup--li-code">lxterminal</code>)</li><li name="ba38" id="ba38" class="graf graf--li graf-after--li">Some fonts (<code class="markup--code markup--li-code">xfonts-100dpi</code>)</li></ul><p name="27f2" id="27f2" class="graf graf--p graf-after--li">Chuck the lines at the end of your&nbsp;<code class="markup--code markup--p-code">./vnc/xstartup</code> configuration file, and then run this command (<code class="markup--code markup--p-code">tightvncserver&nbsp;:13 -geometry 1200x900</code>):</p><figure name="3bda" id="3bda" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_A6iP79W389q7anG5nyASyg.png"></figure><p name="d8ac" id="d8ac" class="graf graf--p graf-after--figure">It’s now running a server where you can then run the TightVNC Viewer or any VNC viewer on your computer and you point it at your server. But specifically, what you do is you use SSH port forwarding to forward&nbsp;:5913 to localhost:5913:</p><figure name="c7ed" id="c7ed" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*fPDXeYX8HkT_JTuUEIHgSQ.png" data-width="1039" data-height="35" data-action="zoom" data-action-value="1*fPDXeYX8HkT_JTuUEIHgSQ.png" src="../img/1_fPDXeYX8HkT_JTuUEIHgSQ.png"></figure><p name="356e" id="356e" class="graf graf--p graf-after--figure">Then you connect to port 5013 on localhost. It will send it off to port 5913 on your server which is the VNC port (because you said&nbsp;<code class="markup--code markup--p-code">:13</code>) and it will display an X Windows desktop. Then you can click on the Linux start like button and click on Firefox and you now have Firefox. You see here in Firefox, it says localhost because this Firefox is running on my AWS server. So you now run Firefox, you start your thing running, and then you close your VNC viewer remembering that Firefox is displaying on this virtual VNC display, not in a real display, so then later on that day, you log back into VNC viewer and it pops up again. So it’s like a persistent desktop, and it’s shockingly fast. It works really well. There’s lots of different VNC servers and clients, but this one works fine for me.</p><h4 name="e4fa" id="e4fa" class="graf graf--h4 graf-after--p">Trick #2: Google Fire [<a href="https://youtu.be/h5Tz7gZT9Fo?t=2h1m27s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=2h1m27s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">2:01:27</a>]</h4><figure name="5682" id="5682" class="graf graf--figure graf-after--h4"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_03yxHYXeuHZUZbYaqKRs5g.png"></figure><p name="9e32" id="9e32" class="graf graf--p graf-after--figure">Trick #2 is to create Python scripts, and this is what we ended up doing. So I ended up creating a little Python script for Sebastian to kind of say this is the basic steps you need to do, and now you need to create different versions for everything else. And I suggested to him that he tried using this thing called Google Fire. What Google Fire does is, you create a function with tons of parameters, so these are all the things that Sebastian wanted to try doing — different dropout amounts, different learning rates, do I use pre-training or not, do I use CLR or not, do I use discriminative learning rate or not, etc. So you create a function, and then you add something saying:</p><pre name="a1ec" id="a1ec" class="graf graf--pre graf-after--p">if __name__ == '__main__': fire.Fire(train_clas)</pre><p name="93fb" id="93fb" class="graf graf--p graf-after--pre">You do nothing else at all — you don’t have to add any metadata, any docstrings, anything at all, and you then call that script and automatically you now have a command line interface. That’s a super fantastic easy way to run lots of different variations in a terminal. This ends up being easier if you want to do lots of variations than using a notebook because you can just have a bash script that tries all of them and spits them all out.</p><h4 name="aae4" id="aae4" class="graf graf--h4 graf-after--p">Trick #3: IMDb scripts [<a href="https://youtu.be/h5Tz7gZT9Fo?t=2h2m47s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=2h2m47s" class="markup--anchor markup--h4-anchor" rel="noopener nofollow" target="_blank">2:02:47</a>]</h4><p name="5c60" id="5c60" class="graf graf--p graf-after--h4">You’ll find inside the <code class="markup--code markup--p-code">courses/dl2</code>, there’s now something called <code class="markup--code markup--p-code">imdb_scripts</code>, and I put all the scripts Sebastian and I used. Because we needed to tokenize and numericalize every dataset, then train a language model and a classifier for every dataset. And we had to do all of those things in a variety of different ways to compare them, so we had scripts for all those things. You can check out and see all of the scripts that we used.</p><figure name="aae0" id="aae0" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*4wNUZhHpjSgRLj6s6ECddQ.png" data-width="1052" data-height="80" data-action="zoom" data-action-value="1*4wNUZhHpjSgRLj6s6ECddQ.png" src="../img/1_4wNUZhHpjSgRLj6s6ECddQ.png"></figure><figure name="b2e4" id="b2e4" class="graf graf--figure graf--layoutOutsetCenter graf-after--figure" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_SkRiJH47FdHtubjyUeYdLA.png"></figure><h4 name="f084" id="f084" class="graf graf--h4 graf-after--figure">Trick #4: pip install -e [<a href="https://youtu.be/h5Tz7gZT9Fo?t=2h3m32s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=2h3m32s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">2:03:32</a>]</h4><p name="7bc0" id="7bc0" class="graf graf--p graf-after--h4">When you are doing a lot of scripts, you got different code all over the place. Eventually it might get frustrating that you don’t want to symlink your fastai library again and again. But you probably don’t want to pip install it because that version tends to be a little bit old as we move so fast that you want to use the current version in Git. If you say <code class="markup--code markup--p-code">pip install -e&nbsp;.</code> from fastai repo base, it does something quite neat which is basically creates a symlink to the fastai library (i.e. your locally cloned Git repo) inside site-packages directory. Your site-packages directory is your main Python library. So if you do this, you can then access fastai from anywhere but every time you do <code class="markup--code markup--p-code">git pull</code>, you’ve got the most recent version. One downside of this is that it installs any updated versions of packages from pip which can confuse Conda a little bit, so another alternative here is just do symlink the fastai library to your site packages library. That works just as well. You can use fastai from anywhere and it’s quite handy when you want to run scripts that use fastai from different directories on your system.</p><figure name="03b0" id="03b0" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_tg8X-gjGJ6rFAg-aiPIgpQ.png"></figure><h4 name="9510" id="9510" class="graf graf--h4 graf-after--figure">Trick #5: SentencePiece [<a href="https://youtu.be/h5Tz7gZT9Fo?t=2h5m6s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=2h5m6s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">2:05:06</a>]</h4><p name="35cf" id="35cf" class="graf graf--p graf-after--h4">This is something you can try if you like. You don’t have to tokenize. Instead of tokenizing words, you can tokenize what are called sub-word units.For example, “unsupervised” could be tokenized as “un” and “supervised”. “Tokenizer” can be tokenized as [“token”, “izer”]. Then you could do the same thing. The language model that works on sub-word units, a classifier that works on sub-word units, etc. How well does that work? I started playing with it and with not too much playing, I was getting classification results that were nearly as good as using word level tokenization — not quite as good, but nearly as good. I suspect with more careful thinking and playing around, maybe I could have gotten as good or better. But even if I couldn’t, if you create a sub-word-unit wikitext model, then IMDb language model, and then classifier forwards and backwards and then ensemble it with the forwards and backwards word level ones, you should be able to beat us. So here is an approach you may be able to beat our state-of-the-art result.</p><figure name="26e6" id="26e6" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_Ihivmbwld8tPdMracJ-FuQ.png"></figure><p name="b800" id="b800" class="graf graf--p graf-after--figure">Sebastian told me this particular project — Google has a project called sentence peace which actually uses a neural net to figure out the optimal splitting up of words and so you end up with vocabulary of sub-word units. In my playing around, I found that create vocabulary of about 30,000 sub-word units seems to be about optimal. If you are interested, there is something you can try. It is a bit of a pain to install — it’s C++, doesn’t have create error message, but it will work. There is a Python library for it. If anybody tries this, I’m happy to help them get it working. There’s been little, if any, experiments with ensembling sub-word and word level classification, and I do think it should be the best approach.</p><p name="3931" id="3931" class="graf graf--p graf-after--p graf--trailing">Have a great week!</p><hr class="section-divider"><p name="bbc0" id="bbc0" class="graf graf--p graf--leading graf--trailing">Lessons: <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" class="markup--anchor markup--p-anchor" target="_blank">1</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" class="markup--anchor markup--p-anchor" target="_blank">2</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" class="markup--anchor markup--p-anchor" target="_blank">3</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" class="markup--anchor markup--p-anchor" target="_blank">4</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" class="markup--anchor markup--p-anchor" target="_blank">5</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" class="markup--anchor markup--p-anchor" target="_blank">6</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" class="markup--anchor markup--p-anchor" target="_blank">7</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" class="markup--anchor markup--p-anchor" target="_blank">8</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" class="markup--anchor markup--p-anchor" target="_blank">9</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" class="markup--anchor markup--p-anchor" target="_blank"><strong class="markup--strong markup--p-strong">10</strong></a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" class="markup--anchor markup--p-anchor" target="_blank">11</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" class="markup--anchor markup--p-anchor" target="_blank">12</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" class="markup--anchor markup--p-anchor" target="_blank">13</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" class="markup--anchor markup--p-anchor" target="_blank">14</a></p></body></html>