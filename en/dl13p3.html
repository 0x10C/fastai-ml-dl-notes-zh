<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></head><body><pre name="cbab" id="cbab" class="graf graf--pre graf-after--p">m_vgg(VV(img_tfm[<strong class="markup--strong markup--pre-strong">None</strong>]))<br>targ_v = V(sf.features.clone())<br>targ_v.shape</pre><pre name="7b63" id="7b63" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">torch.Size([1, 512, 36, 36])</em></pre><pre name="8383" id="8383" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> actn_loss2(x):<br>    m_vgg(x)<br>    out = V(sf.features)<br>    <strong class="markup--strong markup--pre-strong">return</strong> F.mse_loss(out, targ_v)*1000</pre><p name="30e0" id="30e0" class="graf graf--p graf-after--pre">Now I can go ahead and do exactly the same thing. But now I’m going to use a different loss function <code class="markup--code markup--p-code">actn_loss2</code> (activation loss #2) which doesn’t say <code class="markup--code markup--p-code">out=m_vgg</code>, again, it calls <code class="markup--code markup--p-code">m_vgg</code> to do a forward pass, throws away the results, and and grabs <code class="markup--code markup--p-code">sf.features</code>. So that’s now my 32nd layer activations which I can then do my MSE loss on. You might have noticed, the last loss function and this one are both multiplied by a thousand. Why are they multiplied by a thousand? This was like all the things that were trying to get this lesson to not work correctly. I didn’t used to have a thousand and it wasn’t training. Lunch time today, nothing was working. After days of trying to get this thing to work, and finally just randomly noticed “gosh, the loss functions — the numbers are really low (like 10E-7)” and I thought what if they weren’t so low. So I multiplied them by a thousand and it started working. So why did it not work? Because we are doing single precision floating point, and single precision floating point isn’t that precise. Particularly once you’re getting gradients that are kind of small and then you are multiplying by the learning rate that can be small, and you end up with a small number. If it’s so small, they could get rounded to zero and that’s what was happening and my model wasn’t ready. I’m sure there are better ways than multiplying by a thousand, but whatever. It works fine. It doesn’t matter what you multiply a loss function by because all you care about is its direction and the relative size. Interestingly, this is something similar we do for when we were training ImageNet. We were using half precision floating point because Volta tensor cores require that. And it’s actually a standard practice if you want to get the half precision floating to train, you actually have to multiply the loss function by a scaling factor. We were using 1024 or 512. I think fast.ai is now the first library that has all of the tricks necessary to train in half precision floating point built-in, so if you are lucky enough to have a Volta or you can pay for a AWS P3, if you’ve got a learner object, you can just say <code class="markup--code markup--p-code">learn.half</code>, it’ll now just magically train correctly half precision floating point. It’s built into the model data object as well, and it’s all automatic. Pretty sure no other library does that.</p><pre name="1df6" id="1df6" class="graf graf--pre graf-after--p">n_iter=0<br><strong class="markup--strong markup--pre-strong">while</strong> n_iter &lt;= max_iter: optimizer.step(partial(step,actn_loss2))</pre><pre name="cce4" id="cce4" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">Iteration: n_iter, loss: 0.2112911492586136<br>Iteration: n_iter, loss: 0.0902421623468399<br>Iteration: n_iter, loss: 0.05904778465628624<br>Iteration: n_iter, loss: 0.04517251253128052<br>Iteration: n_iter, loss: 0.03721420466899872<br>Iteration: n_iter, loss: 0.03215853497385979<br>Iteration: n_iter, loss: 0.028526008129119873<br>Iteration: n_iter, loss: 0.025799645110964775<br>Iteration: n_iter, loss: 0.02361033484339714<br>Iteration: n_iter, loss: 0.021835438907146454</em></pre><p name="a7fd" id="a7fd" class="graf graf--p graf-after--pre">This is just doing the same thing on a slightly earlier layer [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h37m35s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h37m35s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:37:35</a>]. And the bird looks more bird-like. Hopefully that makes sense to you that earlier layers are getting closer to the pixels. There are more grid cells, each cell is smaller, smaller receptive field, less complex semantic features. So the earlier we get, the more it’s going to look like a bird.</p><pre name="b62a" id="b62a" class="graf graf--pre graf-after--p">x = val_tfms.denorm(np.rollaxis(to_np(opt_img_v.data),1,4))[0]<br>plt.figure(figsize=(7,7))<br>plt.imshow(x);</pre><figure name="437a" id="437a" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_i2SK83mI6XYD9al6OV4fhw.png"></figure><pre name="49d5" id="49d5" class="graf graf--pre graf-after--figure">sf.close()</pre><p name="48bc" id="48bc" class="graf graf--p graf-after--pre">In fact, the paper has a nice picture of that showing various different layers and zooming into this house [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h38m17s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h38m17s" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">1:38:17</a>]. They are trying to make this house look like The Starry Night picture. And you can see that later on, it’s pretty messy, and earlier on, it looks like the house. So this is just doing what we just did. One of the things I’ve noticed in our study group is anytime I say to somebody to answer a question, anytime I say read the paper there is a thing in the paper that tells you the answer to that question, there’s always this shocked look “read the paper? me?” but seriously the papers have done these experiments and drawn the pictures. There’s all this stuff in the papers. It doesn’t mean you have to read every part of the paper. But at least look at the pictures. So check out Gatys’ paper, it’s got nice pictures. So they’ve done the experiment for us but looks like they didn’t go as deep — they just got some earlier ones.</p><figure name="8a61" id="8a61" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_cqZ5Az70HWX2dUhPnUDAZg.png"></figure><h4 name="9879" id="9879" class="graf graf--h4 graf-after--figure">Style match [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h39m29s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h39m29s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:39:29</a>]</h4><p name="c9f9" id="c9f9" class="graf graf--p graf-after--h4">The next thing we need to do is to create style loss. We’ve already got the loss which is how much like the bird is it. Now we need how like this painting style is it. And we are going to do nearly the same thing. We are going to grab the activations of some layer. Now the problem is, the activations of some layer, let’s say it was a 5x5 layer (of course there are no 5x5 layers, it’s 224x224, but we’ll pretend). So here’re some activations and we could get these activations both per the image we are optimizing and for our Van Gogh painting. Let’s look at our Van Gogh painting. There it is — The Starry Night</p><pre name="7e53" id="7e53" class="graf graf--pre graf-after--p">style_fn = PATH/'style'/'starry_night.jpg'</pre><pre name="3e02" id="3e02" class="graf graf--pre graf-after--pre">style_img = open_image(style_fn)<br>style_img.shape, img.shape</pre><pre name="4b13" id="4b13" class="graf graf--pre graf-after--pre">((1198, 1513, 3), (291, 483, 3))</pre><pre name="bc21" id="bc21" class="graf graf--pre graf-after--pre">plt.imshow(style_img);</pre><figure name="7f85" id="7f85" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_3QN8_RpikQBlk8wwjD9B3w.png"></figure><p name="318e" id="318e" class="graf graf--p graf-after--figure">I downloaded this from Wikipedia and I was wondering what is taking son long to load [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h40m39s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h40m39s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:40:39</a>] — turns out, the Wikipedia version I downloaded was 30,000 by 30,000 pixels. It’s pretty cool that they’ve got this serious gallery quality archive stuff there. I didn’t know it existed. Don’t try to run a neural net on that. Totally killed my Jupyter notebook.</p><p name="8f03" id="8f03" class="graf graf--p graf-after--p">So we can do that for our Van Gogh image and we can do that for our optimized image. Then we can compare the two and we would end up creating an image that has content like the painting but it’s not the painting — that’s not what we want. We want something with the same style but it’s not the painting and doesn’t have the content. So we want to throw away all of the spatial information. We are not trying to create something that has a moon here, stars here, and a church here. We don’t want any of that. So how do we throw away all the special information?</p><figure name="599c" id="599c" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_YVBXuBYYyoalPWcW2avrsg.png"></figure><p name="1132" id="1132" class="graf graf--p graf-after--figure">In this case, there are 19 faces on this — 19 slices. So let’s grab this top slice that’s going to be a 5x5 matrix. Now, let’s flatten it and we’ve got a 25 long vector. In one stroke, we’ve thrown away the bulk of the spacial information by flattening it. Now let’s grab a second slice (i.e. another channel) and do the same thing. So we have channel 1 flattened and channel 2 flattened, and they both have 25 elements. Now, let’s take the dot product which we can do with <code class="markup--code markup--p-code">@</code> in Numpy (Note: <a href="http://forums.fast.ai/t/part-2-lesson-13-wiki/15297/140?u=hiromi" data-href="http://forums.fast.ai/t/part-2-lesson-13-wiki/15297/140?u=hiromi" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">here is Jeremy’s answer to my dot product vs. matrix multiplication question</a>). So the dot product is going to give us one number. What’s that number? What is it telling us? Assuming the activations are somewhere around the middle layer of the VGG network, we might expect some of these activations to be how textured is the brush stroke, and some of them to be like how bright is this area, and some of them to be like is this part of a house or a part of a circular thing, or other parts to be, how dark is this part of the painting. So a dot product is basically a correlation. If this element and and this element are both highly positive or both highly negative, it gives us a big result. Where else, if they are the opposite, it gives a small results. If they are both close to zero, it gives no result. So basically a dot product is a measure of how similar these two things are. So if the activations of channel 1 and channel 2 are similar, then it basically says — Let’s give an example [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h44m28s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h44m28s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:44:28</a>]. Let’s say the first one was how textured are the brushstrokes (C1) and that one there says how diagonally oriented are the brush strokes (C2).</p><figure name="cd0b" id="cd0b" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_ho9iuqmJh3hVXPNeZ9E_Xg.png"></figure><p name="39d0" id="39d0" class="graf graf--p graf-after--figure">If C1 and C2 are both high for a cell (1, 1) at the same time, and same is true for a cell (4, 2), then it’s saying grid cells that would have texture tend to also have diagonal. So dot product would be high when grid cells that have texture also have diagonal, and when they don’t, they don’t (have high dot product). So that’s <code class="markup--code markup--p-code">C1 @ C2</code>. Where else, <code class="markup--code markup--p-code">C1 @ C1</code> is the 2-norm effectively (i.e. the sum of the squares of C1). This is basically saying how many grid cells in the textured channel is active and how active it is. So in other words, <code class="markup--code markup--p-code">C1 @ C1</code> tells us how much textured painting is going on. And <code class="markup--code markup--p-code">C2 @ C2</code> tells us how much diagonal paint stroke is going on. Maybe C3 is “is it bright colors?” so <code class="markup--code markup--p-code">C3 @ C3</code> would be how often do we have bright colored cells.</p><p name="a5eb" id="a5eb" class="graf graf--p graf-after--p">So what we could do then is we could create a 19 by 19 matrix containing every dot product [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h47m17s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h47m17s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:47:17</a>]. And like we discussed, mathematicians have to give everything a name, so this particular matrix where you flatten something out and then do all the dot product is called Gram matrix.</p><figure name="88e9" id="88e9" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_hboObzQV-8h0yiVvqZNvZg.png"></figure><p name="7dcd" id="7dcd" class="graf graf--p graf-after--figure">I’ll tell you a secret [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h48m29s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h48m29s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:48:29</a>]. Most deep learning practitioners either don’t know or don’t remember all these things like what is a Gram matrix if they ever did study at university. They probably forgot it because they had a big night afterwards. And the way it works in practice is you realize “oh, I could create a kind of non-spacial representation of how the channels correlate with each other” and then when I write up the paper, I have to go and ask around and say “does this thing have a name?” and somebody will be like “isn’t that the Gram matrix?” and you go and look it up and it is. So don’t think you have to go study all of math first. Use your intuition and common sense and then you worry about what the math is called later, normally. Sometimes it works the other way, not with me because I can’t do math.</p><p name="2847" id="2847" class="graf graf--p graf-after--p">So this is called the Gram matrix [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h49m22s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h49m22s" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">1:49:22</a>]. And of course, if you are a real mathematician, it’s very important that you say this as if you always knew it was a Gram matrix and you kind of just go oh yes, we just calculate the Gram matrix. So the Gram matrix then is this kind of map — the diagonal is perhaps the most interesting. The diagonal is which channels are the most active and then the off diagonal is which channels tend to appear together. And overall, if two pictures have the same style, then we are expecting that some layer of activations, they will have similar Gram matrices. Because if we found the level of activations that capture a lot of stuff about like paint strokes and colors, then the diagonal alone (in Gram matrices) might even be enough. That’s another interesting homework assignment, if somebody wants to take it, is try doing Gatys’ style transfer not using the Gram matrix but just using the diagonal of the Gram matrix. That would be like a single line of code to change. But I haven’t seen it tried and I don’t know if it would work at all, but it might work fine.</p><p name="a2f9" id="a2f9" class="graf graf--p graf--startsWithDoubleQuote graf-after--p">“Okay, yes Christine, you’ve tried it” [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h50m51s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h50m51s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:50:51</a>]. “I have tried that and it works most of the time except when you have funny pictures where you need two styles to appear in the same spot. So it seems like grass in one half and a crowd in one half, and you need the two styles.” (Christine). Cool, you’re still gonna do your homework, but Christine says she’ll do it for you.</p><pre name="9ce4" id="9ce4" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> scale_match(src, targ):<br>    h,w,_ = img.shape<br>    sh,sw,_ = style_img.shape<br>    rat = max(h/sh,w/sw); rat<br>    res = cv2.resize(style_img, (int(sw*rat), int(sh*rat)))<br>    <strong class="markup--strong markup--pre-strong">return</strong> res[:h,:w]</pre><pre name="5a0b" id="5a0b" class="graf graf--pre graf-after--pre">style = scale_match(img, style_img)</pre><pre name="59ff" id="59ff" class="graf graf--pre graf-after--pre">plt.imshow(style)<br>style.shape, img.shape</pre><pre name="187b" id="187b" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">((291, 483, 3), (291, 483, 3))</em></pre><figure name="c038" id="c038" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_3QDp1KCdg6RkKL8yhkRbDw.png"></figure><p name="37d5" id="37d5" class="graf graf--p graf-after--figure">So here is our painting [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h51m22s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h51m22s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:51:22</a>]. I’ve tried to resize the painting so it’s the same size as my bird picture. So that’s all this is just doing. It doesn’t matter too much which bit I use as long as it’s got lots of the nice style in it.</p><p name="74c6" id="74c6" class="graf graf--p graf-after--p">I grab my optimizer and my random image just like before:</p><pre name="4bf2" id="4bf2" class="graf graf--pre graf-after--p">opt_img_v, optimizer = get_opt()</pre><p name="603c" id="603c" class="graf graf--p graf-after--pre">And this time, I call <code class="markup--code markup--p-code">SaveFeatures</code> for all of my <code class="markup--code markup--p-code">block_ends</code> and that’s going to give me an array of SaveFeatures objects — one for each module that appears the layer before the max pool. Because this time, I want to play around with different activation layer styles, or more specifically I want to let you play around with it. So now I’ve got a whole array of them.</p><pre name="d249" id="d249" class="graf graf--pre graf-after--p">sfs = [SaveFeatures(children(m_vgg)[idx]) <strong class="markup--strong markup--pre-strong">for</strong> idx <strong class="markup--strong markup--pre-strong">in</strong> block_ends]</pre><p name="0db2" id="0db2" class="graf graf--p graf-after--pre"><code class="markup--code markup--p-code">style_img</code> is my Van Gogh painting. So I take my <code class="markup--code markup--p-code">style_img</code>, put it through my transformations to create my transform style image (<code class="markup--code markup--p-code">style_tfm</code>).</p><pre name="c329" id="c329" class="graf graf--pre graf-after--p">style_tfm = val_tfms(style_img)</pre><p name="668c" id="668c" class="graf graf--p graf-after--pre">Turn that into a variable, put it through the forward pass of my VGG module, and now I can go through all of my SaveFeatures objects and grab each set of features. Notice I call <code class="markup--code markup--p-code">clone</code> because later on, if I call my VGG object again, it’s going to replace those contents. I haven’t quite thought about whether this is necessary. If you take it away and it’s not, that’s fine. But I was just being careful. So here is now an array of the activations at every <code class="markup--code markup--p-code">block_end</code> layer. And here, you can see all of those shapes:</p><pre name="3ae5" id="3ae5" class="graf graf--pre graf-after--p">m_vgg(VV(style_tfm[<strong class="markup--strong markup--pre-strong">None</strong>]))<br>targ_styles = [V(o.features.clone()) <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> sfs]<br>[o.shape <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> targ_styles]</pre><pre name="78d1" id="78d1" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[torch.Size([1, 64, 288, 288]),<br> torch.Size([1, 128, 144, 144]),<br> torch.Size([1, 256, 72, 72]),<br> torch.Size([1, 512, 36, 36]),<br> torch.Size([1, 512, 18, 18])]</em></pre><p name="37e9" id="37e9" class="graf graf--p graf-after--pre">And you can see, being able to whip up a list comprehension really quickly, it’s really important in your Jupyter fiddling around [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h53m30s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h53m30s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:53:30</a>]. Because you really want to be able to immediately see here’s my channel (64, 128, 256,&nbsp;…), and grid size halving as we would expect (288, 144, 72…) because all of these appear just before a max pool.</p><p name="dc8c" id="dc8c" class="graf graf--p graf-after--p">So to do a Gram MSE loss, it’s going to be the MSE loss on the Gram matrix of the input vs. the gram matrix of the target. And the Gram matrix is just the matrix multiply of <code class="markup--code markup--p-code">x</code> with <code class="markup--code markup--p-code">x</code> transpose (<code class="markup--code markup--p-code">x.t()</code>) where x is simply equal to my input where I’ve flattened the batch and channel axes all down together. I’ve only got one image, so you can ignore the batch part — it’s basically channel. Then everything else (<code class="markup--code markup--p-code">-1</code>), which in this case is the height and width, is the other dimension because there’s now going to be channel by height and width, and then as we discussed we can them just do the matrix multiply of that by its transpose. And just to normalize it, we’ll divide that by the number of elements (<code class="markup--code markup--p-code">b*c*h*w</code>) — it would actually be more elegant if I had said <code class="markup--code markup--p-code">input.numel</code> (number of elements) that would be the same thing. Again, this gave me tiny numbers so I multiply it by a big number to make it something more sensible. So that’s basically my loss.</p><pre name="4d38" id="4d38" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> gram(input):<br>        b,c,h,w = input.size()<br>        x = input.view(b*c, -1)<br>        <strong class="markup--strong markup--pre-strong">return</strong> torch.mm(x, x.t())/input.numel()*1e6<br><br><strong class="markup--strong markup--pre-strong">def</strong> gram_mse_loss(input, target): <br>        <strong class="markup--strong markup--pre-strong">return</strong> F.mse_loss(gram(input), gram(target))</pre><p name="639f" id="639f" class="graf graf--p graf-after--pre">So now my style loss is to take my image to optimize, throw it through VGG forward pass, grab an array of the features in all of the SaveFeatures objects, and then call my Gram MSE loss on every one of those layers [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h55m13s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h55m13s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:55:13</a>]. And that’s going to give me an array and then I just add them up. Now you could add them up with different weightings, you could add up subsets, or whatever. In this case, I’m just grabbing all of them.</p><pre name="f2b1" id="f2b1" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> style_loss(x):<br>    m_vgg(opt_img_v)<br>    outs = [V(o.features) <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> sfs]<br>    losses = [gram_mse_loss(o, s) <strong class="markup--strong markup--pre-strong">for</strong> o,s <strong class="markup--strong markup--pre-strong">in</strong> zip(outs, targ_styles)]<br>    <strong class="markup--strong markup--pre-strong">return</strong> sum(losses) </pre><p name="c927" id="c927" class="graf graf--p graf-after--pre">Pass that into my optimizer as before:</p><pre name="c09d" id="c09d" class="graf graf--pre graf-after--p">n_iter=0<br><strong class="markup--strong markup--pre-strong">while</strong> n_iter &lt;= max_iter: optimizer.step(partial(step,style_loss))</pre><pre name="6795" id="6795" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">Iteration: n_iter, loss: 230718.453125<br>Iteration: n_iter, loss: 219493.21875<br>Iteration: n_iter, loss: 202618.109375<br>Iteration: n_iter, loss: 481.5616760253906<br>Iteration: n_iter, loss: 147.41177368164062<br>Iteration: n_iter, loss: 80.62625122070312<br>Iteration: n_iter, loss: 49.52326965332031<br>Iteration: n_iter, loss: 32.36254119873047<br>Iteration: n_iter, loss: 21.831811904907227<br>Iteration: n_iter, loss: 15.61091423034668</em></pre><p name="7555" id="7555" class="graf graf--p graf-after--pre">And here we have a random image in the style of Van Gogh which I think is kind of cool.</p><pre name="c1ba" id="c1ba" class="graf graf--pre graf-after--p">x = val_tfms.denorm(np.rollaxis(to_np(opt_img_v.data),1,4))[0]<br>plt.figure(figsize=(7,7))<br>plt.imshow(x);</pre><figure name="11e1" id="11e1" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_Z2UuUEecjCVOR07scQDw1g.png"></figure><p name="08a7" id="08a7" class="graf graf--p graf-after--figure">Again Gatys has done it for us. Here is different layers of random image in the style of Van Gogh. So the first one, as you can see, the activations are simple geometric things — not very interesting at all. The later layers are much more interesting. So we kind of have a suspicion that we probably want to use later layers largely for our style loss if we wanted to look good.</p><figure name="e2d3" id="e2d3" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_p5JFBuMVDA5kw6CYh_fCfQ.png"></figure><figure name="3c55" id="3c55" class="graf graf--figure graf-after--figure"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_BBOkPG0_GV-KNdPhlmLUgA.png"></figure><p name="3962" id="3962" class="graf graf--p graf-after--figure">I added this <code class="markup--code markup--p-code">SaveFeatures.close</code> [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h56m35s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h56m35s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:56:35</a>] which just calls <code class="markup--code markup--p-code">self.hook.remove()</code>. Remember, I stored the hook as <code class="markup--code markup--p-code">self.hook</code> so <code class="markup--code markup--p-code">hook.remove()</code> gets rid of it. It’s a good idea to get rid of it because otherwise you can potentially just keep using memory. So at the end, I just go through each of my SaveFeatures object and close it:</p><pre name="bdad" id="bdad" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">for</strong> sf <strong class="markup--strong markup--pre-strong">in</strong> sfs: sf.close()</pre><h4 name="1b24" id="1b24" class="graf graf--h4 graf-after--pre">Style transfer [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h57m8s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h57m8s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:57:08</a>]</h4><p name="fcf3" id="fcf3" class="graf graf--p graf-after--h4">Style transfer is adding content loss and style loss together with some weight. So there is no much to show.</p><p name="8303" id="8303" class="graf graf--p graf-after--p">Grab my optimizer, grab my image:</p><pre name="86c9" id="86c9" class="graf graf--pre graf-after--p">opt_img_v, optimizer = get_opt()</pre><p name="d3ed" id="d3ed" class="graf graf--p graf-after--pre">And my combined loss is the MSE loss at one particular layer, my style loss at all of my layers, sum up the style losses, add them to the content loss, the content loss I’m scaling. Actually the style loss, I scaled already by 1E6. So they are both scaled exactly the same. Add them together. Again, you could trying weighting the different style losses or you could maybe remove some of them, so this is the simplest possible version.</p><pre name="5a67" id="5a67" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> comb_loss(x):<br>    m_vgg(opt_img_v)<br>    outs = [V(o.features) <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> sfs]<br>    losses = [gram_mse_loss(o, s) <strong class="markup--strong markup--pre-strong">for</strong> o,s <strong class="markup--strong markup--pre-strong">in</strong> zip(outs, targ_styles)]<br>    cnt_loss   = F.mse_loss(outs[3], targ_vs[3])*1000000<br>    style_loss = sum(losses)<br>    <strong class="markup--strong markup--pre-strong">return</strong> cnt_loss + style_loss</pre><p name="0dc9" id="0dc9" class="graf graf--p graf-after--pre">Train that:</p><pre name="99a6" id="99a6" class="graf graf--pre graf-after--p">n_iter=0<br><strong class="markup--strong markup--pre-strong">while</strong> n_iter &lt;= max_iter: optimizer.step(partial(step,comb_loss))</pre><pre name="c66c" id="c66c" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">Iteration: n_iter, loss: 1802.36767578125<br>Iteration: n_iter, loss: 1163.05908203125<br>Iteration: n_iter, loss: 961.6024169921875<br>Iteration: n_iter, loss: 853.079833984375<br>Iteration: n_iter, loss: 784.970458984375<br>Iteration: n_iter, loss: 739.18994140625<br>Iteration: n_iter, loss: 706.310791015625<br>Iteration: n_iter, loss: 681.6689453125<br>Iteration: n_iter, loss: 662.4088134765625<br>Iteration: n_iter, loss: 646.329833984375</em></pre><pre name="3642" id="3642" class="graf graf--pre graf-after--pre">x = val_tfms.denorm(np.rollaxis(to_np(opt_img_v.data),1,4))[0]<br>plt.figure(figsize=(9,9))<br>plt.imshow(x, interpolation='lanczos')<br>plt.axis('off');</pre><figure name="6bce" id="6bce" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_ElVcIGvL7cWUMhoftkw9-g.png"></figure><pre name="adc6" id="adc6" class="graf graf--pre graf-after--figure"><strong class="markup--strong markup--pre-strong">for</strong> sf <strong class="markup--strong markup--pre-strong">in</strong> sfs: sf.close()</pre><p name="1147" id="1147" class="graf graf--p graf-after--pre">And holy crap, it actually looks good. So I think that’s pretty awesome. The main take away here is if you want to solve something with a neural network, all you’ve got to do is set up a loss function and then optimize something. And the loss function is something which a lower number is something that you’re happier with. Because then when you optimize it, it’s going to make that number as low as you can, and it’ll do what you wanted it to do. So here, Gatys came up with the loss function that does a good job of being a smaller number when it looks like the thing we want it to look like, and it looks like the style of the thing we want to be in the style of. That’s all we had to do.</p><p name="96f8" id="96f8" class="graf graf--p graf-after--p">What it actually comes to it [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h59m10s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h59m10s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:59:10</a>], apart from implementing Gram MSE loss which was like 6 lines of code if that, that’s our loss function:</p><figure name="d1ff" id="d1ff" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_si7enkSgWg1k6EKmx-2ijQ.png"></figure><p name="4bf4" id="4bf4" class="graf graf--p graf-after--figure">Pass it to our optimizer, and wait about 5 seconds, and we are done. And remember, we could do a batch of these at a time, so we could wait 5 seconds and 64 of these will be done. So I think that’s really interesting and since this paper came out, it has really inspired a lot of interesting work. To me though, most of the interesting work hasn’t happened yet because to me, the interesting work is the work where you combine human creativity with these kinds of tools. I haven’t seen much in the way of tools that you can download or use where the artist is in control and can kind of do things interactively. It’s interesting talking to the guys at <a href="https://magenta.tensorflow.org/" data-href="https://magenta.tensorflow.org/" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Google Magenta</a> project which is their creative AI project, all of the stuff they are doing with music is specifically about this. It’s building tools that musicians can use to perform in real time. And you’ll see much more of that on the music space thanks to Magenta. If you go to their website, there’s all kinds of things where you can press the buttons to actually change the drum beats, melodies, keys, etc. You can definitely see Adobe or Nvidia is starting to release little prototypes and starting to do this but this kind of creative AI explosion hasn’t happened yet. I think we have pretty much all the technology we need but no one’s put it together into a thing and said “look at the thing I built and look at the stuff that people built with my thing.” So that’s just a huge area of opportunity.</p><p name="db95" id="db95" class="graf graf--p graf-after--p">So the paper that I mentioned at the start of class in passing [<a href="https://youtu.be/xXXiC4YRGrQ?t=2h1m16s" data-href="https://youtu.be/xXXiC4YRGrQ?t=2h1m16s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:01:16</a>] — the one where we can add Captain America’s shield to arbitrary paintings basically used this technique. The trick was though some minor tweaks to make the pasted Captain America shield blend in nicely. But that paper is only a couple of days old, so that would be a really interesting project to try because you can use all this code. It really does leverage this approach. Then you could start by making the content image be like the painting with the shield and then the style image could be the painting without the shield. That would be a good start, and then you could see what specific problems they try to solve in this paper to make it better. But you could have a start on it right now.</p><p name="7167" id="7167" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: Two questions — earlier there were a number of people that expressed interest in your thoughts on Pyro and probabilistic programming [<a href="https://youtu.be/xXXiC4YRGrQ?t=2h2m34s" data-href="https://youtu.be/xXXiC4YRGrQ?t=2h2m34s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:02:34</a>]. So TensorFlow has now got this TensorFlow probability or something. There’s a bunch of probabilistic programming framework out there. I think they are intriguing, but as yet unproven in the sense that I haven’t seen anything done with any probabilistic programming system which hasn’t been done better without them. The basic premise is that it allows you to create more of a model of how you think the world works and then plug in the parameters. So back when I used to work in management consulting 20 years ago, we used to do a lot of stuff where we would use a spreadsheet and then we would have these Monte Carlo simulation plugins — there was one called At Risk(?) and one called Crystal Ball. I don’t know if they still exist decades later. Basically they would let you change a spreadsheet cell to say this is not a specific value but it actually represents a distribution of values with this mean and the standard deviation or it’s got this distribution, and then you would hit a button and the spreadsheet would recalculate a thousand times pulling random numbers from these distributions and show you the distribution of your outcome that might be profit or market share or whatever. We used them all the time back then. Apparently feel that a spreadsheet is a more obvious place to do that kind of work because you can see it all much more naturally, but I don’t know. We’ll see. At this stage, I hope it turns out to be useful because I find it very appealing and it appeals to as I say the kind of work I used to do a lot of. There’s actually whole practices around this stuff they used to call system dynamics which really was built on top of this kind of stuff, but it’s not quite gone anywhere.</p><p name="9acb" id="9acb" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: Then there was a question about pre-training for generic style transfer [<a href="https://youtu.be/xXXiC4YRGrQ?t=2h4m57s" data-href="https://youtu.be/xXXiC4YRGrQ?t=2h4m57s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:04:57</a>]. I don’t think you can pre-train for a generic style, but you can pre-train for a generic photo for a particular style which is where we are going to get to. Although, it may end up being a homework. I haven’t decided yet. But I’m going to do all the pieces.</p><p name="11db" id="11db" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: Please ask him to talk about multi-GPU [<a href="https://youtu.be/xXXiC4YRGrQ?t=2h5m31s" data-href="https://youtu.be/xXXiC4YRGrQ?t=2h5m31s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:05:31</a>]. Oh yeah, I haven’t had a slide about that. We’re about to hit it.</p><p name="bd2a" id="bd2a" class="graf graf--p graf-after--p">Before we do, just another interesting picture from the Gatys’ paper. They’ve got a few more just didn’t fit in my slide but different convolutional layers for the style. Different style to content ratios, and here’s the different images. Obviously this isn’t Van Gogh any more, this is a different combination. So you can see, if you just do all style, you don’t see any image. If you do lots of content, but you use low enough convolutional layer, it looks okay but the back ground is kind of dumb. So you kind of want somewhere in the middle. So you can play around with it and experiment, but also use the paper to help guide you.</p><figure name="162b" id="162b" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_x_UN319I-Ppe3xHnvzqgag.png"></figure><h4 name="983f" id="983f" class="graf graf--h4 graf-after--figure">The Math [<a href="https://youtu.be/xXXiC4YRGrQ?t=2h6m33s" data-href="https://youtu.be/xXXiC4YRGrQ?t=2h6m33s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">2:06:33</a>]</h4><p name="098e" id="098e" class="graf graf--p graf-after--h4">Actually, I think I might work on the math now and we’ll talk about multi GPU and super resolution next week because this is from the paper and one of the things I really do want you to do after we talk about a paper is to read the paper and then ask questions on the forum anything that’s not clear. But there’s a key part of this paper which I wanted to talk about and discuss how to interpret it. So the paper says, we’re going to be given an input image <em class="markup--em markup--p-em">x</em> and this little thing means normally it means it’s a vector, Rachel, but this one is a matrix. I guess it could mean either. I don’t know. Normally small letter bold means vector or a small letter with an arrow on top means vector. And normally big letter means matrix or small letter with two arrows on top means matrix. In this case, our image is a matrix. We are going to basically treat it as a vector, so maybe we’re just getting ahead of ourselves.</p><figure name="d655" id="d655" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_kU-HMZL4kI2So7WV5xow6g.png"></figure><p name="c5d7" id="c5d7" class="graf graf--p graf-after--figure">So we’ve got an input image <em class="markup--em markup--p-em">x</em> and it can be encoded in a particular layer of the CNN by the filter responses (i.e. activations). Filter responses are activations. Hopefully, that’s something you all understand. That’s basically what a CNN does is it produces layers of activations. A layer has a bunch of filters which produce a number of channels. This year says that layer number L has capital N<em class="markup--em markup--p-em">l</em> filters. Again, this capital does not mean matrix. So I don’t know, math notation is so inconsistent. So capital Nl distinct filters at layer L which means it has also that many feature maps. So make sure you can see this letter Nl is the same as this letter. So you’ve got to be very careful to read the letters and recognize it’s like snap, that’s the same letter as that. So obviously, Nl filters create create Nl feature maps or channels, each one of size M<em class="markup--em markup--p-em">l</em> (okay, I can see this is where the unrolling is happening). So this is like M[<em class="markup--em markup--p-em">l</em>] in numpy notation. It’s the <em class="markup--em markup--p-em">l</em>th layer. So M for the <em class="markup--em markup--p-em">l</em>th layer. The size is height times width — so we flattened it out. So the responses in a layer l can be stored in a matrix F (and now the <em class="markup--em markup--p-em">l</em> goes at the top for some reason). So this is not f^<em class="markup--em markup--p-em">l</em>, it’s just another indexing. We are just moving it around for fun. This thing here where we say it’s an element of R — this is a special R meaning the real numbers N times M (this is saying that the dimensions of this is N by M). So this is really important, you don’t move on. It’s just like with PyTorch, making sure that you understand the rank and size of your dimensions first, same with math. These are the bits where you stop and think why is it N by M? N is a number of filters, M is height by width. So do you remember that thing when we did&nbsp;<code class="markup--code markup--p-code">.view(b*c, -1)</code>? Here that is. So try to map the code to the math. So F is <code class="markup--code markup--p-code">x</code>:</p><figure name="0765" id="0765" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_uZYTy9gDHiXBhjRhbtbabg.png"></figure><p name="60af" id="60af" class="graf graf--p graf-after--figure">If I was nicer to you, I would have used the same letters as the paper. But I was too busy getting this darn thing working to do that carefully. So you can go back and rename it as capital F.</p><p name="db83" id="db83" class="graf graf--p graf-after--p">So this is why we moved the L to the top is because we’re now going to have some more indexing. Where else in Numpy or PyTorch, we index things by square brackets and then lots of things with commas between. The approach in math is to surround your letter by little letters all around it — just throw them up there everywhere. So here, F<em class="markup--em markup--p-em">l</em> is the <em class="markup--em markup--p-em">l</em>th layer of F and then <em class="markup--em markup--p-em">ij</em> is the activation of the <em class="markup--em markup--p-em">i</em>th filter at position <em class="markup--em markup--p-em">j</em> of layer <em class="markup--em markup--p-em">l</em>. So position <em class="markup--em markup--p-em">j</em> is up to size M which is up to size height by width. This is the kind of thing that would be easy to get confused. Often you’d see an <em class="markup--em markup--p-em">ij</em> and assume that’s indexing into a position of an image like height by width, but it’s totally not, is it? It’s indexing into channel by flattened image. It even tells you — it’s the <em class="markup--em markup--p-em">i</em>th filter/channel in the <em class="markup--em markup--p-em">j</em>th position in the flattened out image in layer <em class="markup--em markup--p-em">l</em>. So you’re not gonna be able to get any further in the paper unless you understand what F is. That’s why these are the bits where you stop and make sure you’re comfortable.</p><p name="7d8a" id="7d8a" class="graf graf--p graf-after--p">So now, the content loss, I’m not going to spend much time on but basically we are going to just check out the values of the activations vs. the predictions squared [<a href="https://youtu.be/xXXiC4YRGrQ?t=2h12m3s" data-href="https://youtu.be/xXXiC4YRGrQ?t=2h12m3s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:12:03</a>]. So there’s our content loss. The style loss will be much the same thing, but using the Gram matrix G:</p><figure name="16cd" id="16cd" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_v6S37SK4jm1o-aJXUFysAw.png"></figure><p name="8e99" id="8e99" class="graf graf--p graf-after--figure">I really wanted to show you this one. I think it’s super. Sometimes I really like things you can do in math notation, and they’re things that you can also generally do in J and APL which is this kind of this implicit loop going on here. What this is saying is there’s a whole bunch of values of <em class="markup--em markup--p-em">i</em> and a whole bunch of values of <em class="markup--em markup--p-em">j</em>, and I’m going to define G for all of them. And there’s whole bunch of values of <em class="markup--em markup--p-em">l</em> as well, and I’m going to define G for all of those as well. So for all of my G at every <em class="markup--em markup--p-em">l</em> of every <em class="markup--em markup--p-em">i</em> at every <em class="markup--em markup--p-em">j</em>, it’s going to be equal to something. And you can see that something has an <em class="markup--em markup--p-em">i</em> and a <em class="markup--em markup--p-em">j</em> and a <em class="markup--em markup--p-em">l</em>, matching G, and it also has a <em class="markup--em markup--p-em">k</em> and that’s part of the sum. So what’s going on here? Well, it’s saying that my Gram matrix in layer <em class="markup--em markup--p-em">l</em> for the <em class="markup--em markup--p-em">i</em>th position in one axis and the <em class="markup--em markup--p-em">j</em>th position in another axis is equal to my F matrix (so my flattened out matrix) for the <em class="markup--em markup--p-em">i</em>th channel in that layer vs. the <em class="markup--em markup--p-em">j</em>th channel in the same layer, then I’m going to sum over. We are going to take the <em class="markup--em markup--p-em">k</em>th position and multiply them together and then add them all up. So that’s exactly what we just did before when we calculated our Gram matrix. So this, there’s a lot going on because of some, to me, very neat notation — which is there are three implicit loops all going on at the same time, plus one explicit loop in the sum, then they all work together to create this Gram matrix for every layer. So let’s go back and see if you can match this. Sl all that’s happening all at once which is pretty great.</p><p name="50b4" id="50b4" class="graf graf--p graf-after--p graf--trailing">That’s it. So next week, we’re going to be looking at a very similar approach, basically doing style transfer all over again but in a way where we actually going to train a neural network to do it for us rather than having to do the optimization. We’ll also see that you can do the same thing to do super resolution. And we are also going to go back and revisit some of the SSD stuff as well as doing some segmentation. So if you’ve forgotten SSD, might be worth doing a little bit of revision this week. Alright, thanks everybody. See you next week.</p><hr class="section-divider"><p name="bec5" id="bec5" class="graf graf--p graf--leading graf--trailing">Lessons: <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" class="markup--anchor markup--p-anchor" target="_blank">1</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" class="markup--anchor markup--p-anchor" target="_blank">2</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" class="markup--anchor markup--p-anchor" target="_blank">3</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" class="markup--anchor markup--p-anchor" target="_blank">4</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" class="markup--anchor markup--p-anchor" target="_blank">5</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" class="markup--anchor markup--p-anchor" target="_blank">6</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" class="markup--anchor markup--p-anchor" target="_blank">7</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" class="markup--anchor markup--p-anchor" target="_blank">8</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" class="markup--anchor markup--p-anchor" target="_blank">9</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" class="markup--anchor markup--p-anchor" target="_blank">10</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" class="markup--anchor markup--p-anchor" target="_blank">11</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" class="markup--anchor markup--p-anchor" target="_blank">12</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" class="markup--anchor markup--p-anchor" target="_blank"><strong class="markup--strong markup--p-strong">13</strong></a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" class="markup--anchor markup--p-anchor" target="_blank">14</a></p></body></html>