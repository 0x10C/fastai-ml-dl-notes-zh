<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></head><body><p name="2699" id="2699" class="graf graf--p graf-after--p">This stuff matters [<a href="https://youtu.be/xXXiC4YRGrQ?t=41m41s" data-href="https://youtu.be/xXXiC4YRGrQ?t=41m41s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">41:41</a>]. It matters in ways more than just awkward translations or black people’s photos not being classified correctly. Maybe there’s some wins too as well — like horrifying surveillance everywhere and maybe won’t work on black people. “Or it’ll be even worse because it’s horrifying surveillance and it’s flat-out racist and wrong” (Rachel). But let’s go deeper. For all we say about human failings, there is a long history of civilization and societies creating layers of human judgement which avoid, hopefully, the most horrible things happening. And sometimes companies which love technology think “let’s throw away humans and replace them with technology” like Facebook did. A couple years ago, Facebook literally got rid of their human editors, and this was in the news at the time. And they were replaced with algorithms. So now as algorithms put all the stuff on your news feed and human editors were out of the loop. What happened next?</p><figure name="90db" id="90db" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_VkIbRF2g5fsRvgfopPRDZQ.png"></figure><p name="00b5" id="00b5" class="graf graf--p graf-after--figure">Many things happened next. One of which was a massive horrifying genocide in Myanmar. Babies getting torn out of their mothers arms and thrown into fires. Mass rape, murder, and an entire people exiled from their homeland.</p><figure name="e7d8" id="e7d8" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_6-Uu8ezBnUol5cYw4Q11lA.png"></figure><p name="1abd" id="1abd" class="graf graf--p graf-after--figure">Okay, I’m not gonna say that was because Facebook did this, but what I will say is that when the leaders of this horrifying project are interviewed, they regularly talk about how everything they learnt about the disgusting animal behaviors of Rohingyas that need to be thrown off the earth, they learnt from Facebook. Because the algorithms just want to feed you more stuff that gets you clicking. If you get told these people that don’t look like you and you don’t know the bad people and here’s lots of stories about bad people and then you start clicking on them and then they feed you more of those things. Next thing you know, you have this extraordinary cycle. People have been studying this, so for example, we’ve been told a few times people click on our fast.ai videos and then the next thing recommended to them is like conspiracy theory videos from Alex Jones, and then continues from there. Because humans click on things that shock us, surprise us, and horrify us. At so many levels, this decision has had extraordinary consequences which we’re only beginning to understand. Again, this is not to say this particular consequence is because of this one thing, but to say it’s entirely unrelated would be clearly ignoring all of the evidence and information that we have.</p><h4 name="c15c" id="c15c" class="graf graf--h4 graf-after--p">Unintended consequences [<a href="https://youtu.be/xXXiC4YRGrQ?t=45m4s" data-href="https://youtu.be/xXXiC4YRGrQ?t=45m4s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">45:04</a>]</h4><figure name="fe24" id="fe24" class="graf graf--figure graf-after--h4"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_8stxAKlNajQqn4Q6mt9HpQ.png"></figure><p name="b958" id="b958" class="graf graf--p graf-after--figure">The key takeaway is to think what are you building and how could it be used. Lots and lots of effort now being put into face detection including in our course. We’ve been spending a lot of time thinking about how to recognize stuff and where it is. There’s lots of good reasons to want to be good at that for improving crop yields in agriculture, for improving diagnostic and treatment planning in medicine, for improving your LEGO sorting robot system, etc. But it’s also being widely used in surveillance, propaganda, and disinformation. Again, the question is what do I do about that? I don’t exactly know. But it’s definitely at least important to be thinking about it, talking about it.</p><h4 name="c669" id="c669" class="graf graf--h4 graf-after--p">Runaway feedback loops&nbsp;[<a href="https://youtu.be/xXXiC4YRGrQ?t=46m10s" data-href="https://youtu.be/xXXiC4YRGrQ?t=46m10s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">46:10</a>]</h4><figure name="9306" id="9306" class="graf graf--figure graf-after--h4"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_MQa0eNjEl__LOn8pc0YmDw.png"></figure><p name="d7d6" id="d7d6" class="graf graf--p graf-after--figure">Sometimes you can do really good things. For example, meetup.com did something which I would put in the category of really good thing which is they recognized early a potential problem which is that more men are tending to go to their meet ups. And that was causing their collaborative filtering systems, which you are familiar building now to recommend more technical content to men. And that was causing more men to go to more technical content which was causing the recommendation system to suggest more technical content to men. This kind of runaway feedback loop is extremely common when we interface the algorithm and the human together. So what did Meetup do? They intentionally made the decision to recommend more technical content to women, not because highfalutin idea about how the world should be, but just because that makes sense. Runaway feedback loop was a bug — there are women that want to go to tech meetups, but when you turn up for a tech meet up and it’s all men and you don’t go, then it recommends more to men and so on and so forth. So Meetup made a really strong product management decision here which was to not do what the algorithm said to do. Unfortunately this is rare. Most of these runaway feedback loops, for example, in predictive policing where algorithms tell policemen where to go which very often is more black neighborhoods which end up crawling with more policemen which leads to more arrests which is assisting to tell more policemen to go to more black neighborhoods and so forth.</p><h4 name="62ad" id="62ad" class="graf graf--h4 graf-after--p">Bias in AI&nbsp;[<a href="https://youtu.be/xXXiC4YRGrQ?t=48m9s" data-href="https://youtu.be/xXXiC4YRGrQ?t=48m9s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">48:09</a>]</h4><figure name="b15b" id="b15b" class="graf graf--figure graf-after--h4"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_Bd_fR4tfFYj5fBQYgum35A.png"></figure><p name="7617" id="7617" class="graf graf--p graf-after--figure">This problem of algorithmic bias is now very wide spread and as algorithms become more and more widely used for specific policy decisions, judicial decisions, day-to-day decisions about who to give what offer to, this just keeps becoming a bigger problem. Some of them are really things that the people involved in the product management decision should have seen at the very start, didn’t make sense, and unreasonable under any definition of the term. For example, this stuff Abe Gong pointed out — these were questions that were used for both pretrial so who was required to post bail, so these are people that haven’t even been convicted, as well as for sentencing and for who gets parole. This was upheld by the Wisconsin Supreme Court last year despite all the flaws. So whether you have to stay in jail because you can’t pay the bail and how long your sentence is for, and how long you stay in jail for depends on what your father did, whether your parents stayed married, who your friends are, and where you live. Now turns out these algorithms are actually terribly terribly bad so some recent analysis showed that they are basically worse than chance. But even if the company’s building them were confident on these were statistically accurate correlations, does anybody imagine there’s a world where it makes sense to decide what happens to you based on what your dad did?</p><p name="3c69" id="3c69" class="graf graf--p graf-after--p">A lot of this stuff at the basic level is obviously unreasonable and a lot of it just fails in these ways that you can see empirically that these kind of runaway feedback loops must have happened and these over generalizations must have happened. For example, these are the cross tabs that anybody working in any field using these algorithm should be preparing. So prediction of likelihood of reoffending for black vs. white defendants, we can just calculate this very simply. Of the people that were labeled high-risk but didn’t reoffend — they were 23.5% white but about twice that African American. Where else, those that were labeled lower risk but did reoffend was half the white people and only 28% of the African American. This is the kind of stuff where at least if you are taking the technologies we’ve been talking about and putting the production in any way, building an API for other people, providing training for people, or whatever — then at least make sure that what you are doing can be tracked in a way that people know what’s going on so at least they are informed. I think it’s a mistake in my opinion to assume that people are evil and trying to break society. I think I would prefer to start with an assumption of if people are doing dumb stuff, it’s because they don’t know better. So at least make sure they have this information. I find very few ML practitioners thinking about what is the information they should be presenting in their interface. Then often I’ll talk to data scientists who will say “oh, the stuff I’m working on doesn’t have a societal impact.” Really? A number of people who think that what they are doing is entirely pointless? Come on. People are paying you to do it for a reason. It’s going to impact people in some way. So think about what that is.</p><h4 name="86d1" id="86d1" class="graf graf--h4 graf-after--p">Responsibility in hiring&nbsp;[<a href="https://youtu.be/xXXiC4YRGrQ?t=52m46s" data-href="https://youtu.be/xXXiC4YRGrQ?t=52m46s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">52:46</a>]</h4><figure name="45dd" id="45dd" class="graf graf--figure graf-after--h4"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_7V8grUptQO556VPQ4Dw8Sw.png"></figure><p name="28d8" id="28d8" class="graf graf--p graf-after--figure">The other thing I know is a lot of people involved here are hiring people and if you are hiring people, I guess you are all very familiar with the fast.ai philosophy now which is the basic premise that, and I thin it comes back to this idea that I don’t think people on the whole are evil, I think they need to be informed and have tools. So we are trying to give as many people the tools as possible that they need and particularly we are trying to put those tools in the hands of a more diverse range of people. So if you are involved in hiring decisions, perhaps you can keep this kind of philosophy in mind as well. If you are not just hiring a wider range of people, but also promoting a wider range of people, and providing appropriate career management for a wider range of people, apart from anything else, your company will do better. It actually turns out that more diverse teams are more creative and tend to solve problems more quickly and better than less diverse teams, but also you might avoid these kind of awful screw-ups which, at one level, are bad for the world and another level if you ever get found out, they can destroy your company.</p><h4 name="d834" id="d834" class="graf graf--h4 graf-after--p">IBM &amp; “Death’s Calculator” [<a href="https://youtu.be/xXXiC4YRGrQ?t=54m8s" data-href="https://youtu.be/xXXiC4YRGrQ?t=54m8s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">54:08</a>]</h4><figure name="aa4f" id="aa4f" class="graf graf--figure graf-after--h4"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_sOrJFBPiJYdUsxzythGk8w.png"></figure><p name="4c4c" id="4c4c" class="graf graf--p graf-after--figure">Also they can destroy you or at least make you look pretty bad in history. A couple of examples, one is going right back to the second world war. IBM provided all of the infrastructure necessary to track the Holocaust. These are the forms they used and they had different code — Jews were 8, Gypsies were 12, death in the gas chambers was 6, and they all went on these punch cards. You can go and look at these punch cards in museums now and this has actually been reviewed by a Swiss judge who said that IBM’s technical assistance facilitated the task of the Nazis and the commission their crimes against humanity. It is interesting to read back the history from these times to see what was going through the minds of people at IBM at that time. What was clearly going through the minds was the opportunity to show technical superiority, the opportunity to test out their new systems, and of course the extraordinary amount of money that they were making. When you do something which at some point down the line turns out to be a problem, even if you were told to do it, that can turn out to be a problem for you personally. For example, you all remember the diesel emission scandal in VW. Who is the one guy that went to jail? It was the engineer just doing his job. If all of this stuff about actually not messing up the world isn’t enough to convince you, it can mess up your life too. If you do something that turns out to cause problems even though somebody told you to do it, you can absolutely be held criminally responsible. Aleksandr Kogan was the guy that handed over the Cambridge Analytica data. He is a Cambridge academic. Now a very famous Cambridge academic the world over for doing his part to destroy the foundations of democracy. This is not how we want to go down in history.</p><figure name="1621" id="1621" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_qXLN21dyZdaaYfxXuCTWhg.png"></figure><p name="80ea" id="80ea" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Question:</strong> In one of your tweets, you said dropout is patented [<a href="https://youtu.be/xXXiC4YRGrQ?t=56m50s" data-href="https://youtu.be/xXXiC4YRGrQ?t=56m50s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">56:50</a>]. I think this is about WaveNet patent from Google. What does it mean? Can you please share more insight on this subject? Does it mean that we will have to pay to use dropout in the future? One of the patent holders is Geoffrey Hinton. So what? Isn’t that great? Invention is all about patents, blah blah. My answer is no. Patents have gone wildly crazy. The amount of things that are patentable that we talk about every week would be dozens. It’s so easy to come up with a little tweak and then if you turn that into a patent to stop everybody from using that little tweak for the next 14 years and you end up with a situation we have now where everything is patented in 50 different ways. Then you get these patent trolls who have made a very good business out of buying lots of crappy little patents and then suing anybody who accidentally turned out did that thing like putting rounded corners on buttons. So what does it mean for us that a lot of stuff is patented in deep learning? I don’t know.</p><p name="8891" id="8891" class="graf graf--p graf-after--p">One of the main people doing this is Google and people from Google who replied to this patent tend to assume that Google doing it because they want to have it defensively so if somebody sues them, they can say don’t sue us we’ll sue you back because we have all these patents. The problem is that as far as I know, they haven’t signed what’s called a defensive patent pledge so basically you can sign a legally binding document that says our patent portfolio will only be used in defense and not offense. Even if you believe all the management of Google would never turn into a patent troll, you’ve got to remember that management changes. To give you a specific example I know, the somewhat recent CFO of Google has a much more aggressive stance towards the PNL, I don’t know, maybe she might decide that they should start monetizing their patents or maybe the group that made that patent might get spun off and then sold to another company that might end up in private equity hands and decide to monetize the patents or whatever. So I think it’s a problem. There has been a big shift legally recently away from software patents actually having any legal standing, so it’s possible that these will all end up thrown out of court but the reality is that anything but a big company is unlikely to have the financial ability to defend themselves against one of these huge patent trolls.</p><p name="6e75" id="6e75" class="graf graf--p graf-after--p">You can’t avoid using patented stuff if you write code. I wouldn’t be surprised if most lines of code you write have patents on them. Actually funnily enough, the best thing to do is not to study the patents because if you do and you infringe knowingly then the penalties are worse. So the best thing to do is to put your hands in your ear, sing a song, and get back to work. So the thing about dropouts patented, forget I said that. You don’t know that. You skipped that bit.</p><h3 name="5e3d" id="5e3d" class="graf graf--h3 graf-after--p">Style Transfer [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h1m28s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h1m28s" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">1:01:28</a>]</h3><p name="4f6c" id="4f6c" class="graf graf--p graf-after--h3"><a href="https://github.com/fastai/fastai/blob/master/courses/dl2/style-transfer.ipynb" data-href="https://github.com/fastai/fastai/blob/master/courses/dl2/style-transfer.ipynb" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Notebook</a></p><figure name="1bf8" id="1bf8" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_GPdF7Xu7mAiUAYEDbT-SHA.png"><figcaption class="imageCaption"><a href="https://arxiv.org/abs/1508.06576" data-href="https://arxiv.org/abs/1508.06576" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">https://arxiv.org/abs/1508.06576</a></figcaption></figure><p name="1fb9" id="1fb9" class="graf graf--p graf-after--figure">This is super fun — artistic style. We are going a bit retro here because this is actually the original artistic style paper and there’s been a lot of updates to it and a lot of different approaches and I actually think in many ways the original is the best. We are going to look at some of the newer approaches as well, but I actually think the original is a terrific way to do it even with everything that’s gone since. Let’s jump to the code.</p><pre name="3013" id="3013" class="graf graf--pre graf-after--p">%matplotlib inline<br>%reload_ext autoreload<br>%autoreload 2</pre><pre name="a0c0" id="a0c0" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">fastai.conv_learner</strong> <strong class="markup--strong markup--pre-strong">import</strong> *<br><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">pathlib</strong> <strong class="markup--strong markup--pre-strong">import</strong> Path<br><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">scipy</strong> <strong class="markup--strong markup--pre-strong">import</strong> ndimage<br>torch.cuda.set_device(3)<br><br>torch.backends.cudnn.benchmark=<strong class="markup--strong markup--pre-strong">True</strong></pre><pre name="0911" id="0911" class="graf graf--pre graf-after--pre">PATH = Path('data/imagenet')<br>PATH_TRN = PATH/'train'</pre><pre name="69b2" id="69b2" class="graf graf--pre graf-after--pre">m_vgg = to_gpu(vgg16(<strong class="markup--strong markup--pre-strong">True</strong>)).eval()<br>set_trainable(m_vgg, <strong class="markup--strong markup--pre-strong">False</strong>)</pre><p name="0740" id="0740" class="graf graf--p graf-after--pre">The idea here is that we want to take a photo of a bird, and we want to create a painting that looks like Van Gogh painted the picture of the bird. Quite a bit of the stuff that I’m doing, by the way, uses an ImageNet. You don’t have to download the whole of ImageNet for any of the things I’m doing. There is an ImageNet sample in <a href="http://files.fast.ai/data/" data-href="http://files.fast.ai/data/" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">files.fast.ai/data</a> which has a couple of gig which should be plenty good enough for everything we are doing. If you want to get really great result, you can grab ImageNet. You can download it from <a href="https://www.kaggle.com/c/imagenet-object-localization-challenge/data" data-href="https://www.kaggle.com/c/imagenet-object-localization-challenge/data" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Kaggle</a>. The localization competition actually contains all of the classification data as well. If you’ve got room, it’s good to have a copy of ImageNet because it comes in handy all the time.</p><pre name="ad91" id="ad91" class="graf graf--pre graf-after--p">img_fn = PATH_TRN/'n01558993'/'n01558993_9684.JPEG'<br>img = open_image(img_fn)<br>plt.imshow(img);</pre><p name="3cf8" id="3cf8" class="graf graf--p graf-after--pre">So I just grabbed the bird out of my ImageNet folder and there is my bird:</p><figure name="6a00" id="6a00" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_eZb9GpF1VGMMIukO2AE91w.png"></figure><pre name="141b" id="141b" class="graf graf--pre graf-after--figure">sz=288</pre><pre name="d73f" id="d73f" class="graf graf--pre graf-after--pre">trn_tfms,val_tfms = tfms_from_model(vgg16, sz)<br>img_tfm = val_tfms(img)<br>img_tfm.shape</pre><pre name="d3d4" id="d3d4" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(3, 288, 288)</em></pre><pre name="e3cd" id="e3cd" class="graf graf--pre graf-after--pre">opt_img = np.random.uniform(0, 1, size=img.shape).astype(np.float32)<br>plt.imshow(opt_img);</pre><p name="6185" id="6185" class="graf graf--p graf-after--pre">What I’m going to do is I’m going to start with this picture:</p><figure name="4182" id="4182" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_jJzsPvZ9uHAtHqu9mZ_skA.png"></figure><p name="49b8" id="49b8" class="graf graf--p graf-after--figure">And I’m going to try to make it more and more like a picture of the bird painted by Van Gogh. The way I do that is actually very simple. You’re all familiar with it [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h3m44s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h3m44s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:03:44</a>]. We will create a loss function which we will call <em class="markup--em markup--p-em">f</em>. The loss function is going to take as input a picture and spit out as output a value. The value will be lower if the image looks more like the bird photo painted by Van Gogh. Having written that loss function, we will then use the PyTorch gradient and optimizers. Gradient times the learning rate, and and we are not going to update any weights, we are going to update the pixels of the input image to make it a little bit more like a picture which would be a bird painted by Van Gogh. And we will stick it through the loss function again to get more gradients, and do it again and again. That’s it. So it’s identical to how we solve every problem. You know I’m a one-trick pony, right? This is my only trick. Create a loss function, use it to get some gradients, multiply it by learning rates to update something, always before, we’ve updated weights in a model but today, we are not going to do that. They’re going to update the pixels in the input. But it’s no different at all. We are just taking the gradient with respect to the input rather than respect to the weights. That’s it. So we are nearly done.</p><figure name="03bf" id="03bf" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_6sYVxXfPJU86MMBBKib7Rw.png"></figure><p name="2104" id="2104" class="graf graf--p graf-after--figure">Let’s do a couple more things [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h5m49s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h5m49s" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">1:05:49</a>]. Let’s mention here that there’s going to be two more inputs to our loss function One is the picture of the bird. The second is an artwork by Van Gogh. By having those as inputs as well, that means we’ll be able to rerun the function later to make it look like a bird painted by Monet or a jumbo jet painted by Van Gogh, etc. Those are going to be the three inputs. Initially, as we discussed, our input here is some random noise. We start with some random noise, use the loss function, get the gradients, make it a little bit more like a bird painted by Van Gogh, and so forth.</p><p name="b297" id="b297" class="graf graf--p graf-after--p">So the only outstanding question which I guess we can talk about briefly is how we calculate how much our image looks like this bird painted by Van Gogh [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h7m9s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h7m9s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:07:09</a>]. Let’s split it into two parts:</p><p name="4f27" id="4f27" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Content Loss</strong>: Returns a value that’s lower if it looks more like the bird (not just any bird, the specific bird that we have coming in).</p><p name="8dca" id="8dca" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Style Loss</strong>: Returns a lower number if the image is more like V.G.’s style</p><figure name="95b3" id="95b3" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_MetpfEESntmYRQ5Z6e5rQw.png"></figure><p name="20b1" id="20b1" class="graf graf--p graf-after--figure">There is one way to do the content loss which is very simple — we could look at the pixel of the output, compare them to the pixel of the bird, and do a mean squared error, and add them up. So if we did that, I ran this for a while. Eventually our image would turn into an image of the bird. You should try it. You should try this as an exercise. Try to use the optimizer in PyTorch to start with a random image and turn it into another image by using mean squared error pixel loss. Not terribly exciting but that would be step one.</p><p name="9cc2" id="9cc2" class="graf graf--p graf-after--p">The problem is, even if we already had our style loss function working beautifully and then presumably, what we are going to do is we are going to add these two together, and then one of them, we’ll multiply by some lambda to adjust how much style versus how much content. Assuming we had a style loss and we picked some sensible lambda, if we used pixel wise content loss then anything that makes it look more like Van Gogh and less like the exact photo, the exact background, the exact contrast, lighting, everything will increase the content loss — which is not what we want. We want it to look like the bird but not in the same way. It is still going to have the same two eyes in the same place and be the same kind of shape and so forth, but not the same representation. So what we are going to do is, this is going to shock you, we are going to use a neural network! We are going to use the VGG neural network because that’s what I used last year and I didn’t have time to see if other things worked so you can try that yourself during the week.</p><p name="673f" id="673f" class="graf graf--p graf-after--p">The VGG network is something which takes in an input and sticks it through a number of layers, and I’m going to treat these as just the convolutional layers there’s obviously ReLU there and if it’s a VGG with batch norm, which most are today, then it’s also got batch norm. There’s some max pooling and so forth but that’s fine. What we could do is, we could take one of these convolutional activations and then rather than comparing the pixels of this bird, we could instead compare the VGG layer 5 activations of this (bird painted by V.G.) to the VGG layer 5 activations of our original bird (or layer 6, or layer 7, etc). So why might that be more interesting? Well for one thing, it wouldn’t be the same bird. It wouldn’t be exactly the same because we are not checking the pixels. We are checking some later set of activations. So what are those later sets of activations contain? Assuming it’s after some max pooling, they contain a smaller grid — so it’s less specific about where things are. And rather than containing pixel color values, they are more like semantic things like is this kind of an eyeball, is this kind of furry, is this kind of bright, or is this kind of reflective, or laying flat, or whatever. So we would hope that there’s some level of semantic features through those layers where if we get a picture that matches those activations, then any picture that matches those activations looks like the bird but it’s not the same representation of the bird. So that’s what we are going to do. That’s what our content loss is going to be. People generally call this a <strong class="markup--strong markup--p-strong">perceptual loss</strong> because it’s really important in deep learning that you always create a new name for every obvious thing you do. If you compare two activations together, you are doing a perceptual loss. That’s it. Our content loss is going to be a perceptual loss. Then we will do the style loss later.</p><p name="aa9a" id="aa9a" class="graf graf--p graf-after--p">Let’s start by trying to create a bird that initially is random noise and we are going to use perceptual loss to create something that is bird-like but it’s not the particular bird [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h13m13s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h13m13s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:13:13</a>]. We are going to start with 288 by 288. Because we are going to do one bird, there is going to be no GPU memory problems. I was actually disappointed that I realized that I picked a rather small input image. It would be fun to try this with something much bigger to create a really grand scale piece. The other thing to remember is if you are productionizing this, you could do a whole batch at a time. People sometimes complain about this approach (Gatys is the lead author) the Gatys’ style transfer approaches being slow, and I don’t agree it’s slow. It takes a few seconds and you can do a whole batch in a few seconds.</p><figure name="f635" id="f635" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_eZb9GpF1VGMMIukO2AE91w.png"></figure><pre name="5017" id="5017" class="graf graf--pre graf-after--figure">sz=288</pre><p name="03c0" id="03c0" class="graf graf--p graf-after--pre">So we are going to stick it through some transforms for VGG16 model as per usual [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h14m12s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h14m12s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:14:12</a>]. Remember, the transform class has dunder call method (<code class="markup--code markup--p-code">__call__</code>) so we can treat it as if it’s a function. If you pass an image into that, then we get the transformed image. Try not to treat the fast.ai and PyTorch infrastructure as a black box because it’s all designed to be really easy to use in a decoupled way. So this idea of that transforms are just “callables” (i.e. things that you can do with parentheses) comes from PyTorch and we totally plagiarized the idea. So with torch.vision or with fast.ai, your transforms are just callables. And the whole pipelines of transforms is just a callable.</p><pre name="7e9c" id="7e9c" class="graf graf--pre graf-after--p">trn_tfms,val_tfms = tfms_from_model(vgg16, sz)<br>img_tfm = val_tfms(img)<br>img_tfm.shape</pre><pre name="df6d" id="df6d" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(3, 288, 288)</em></pre><p name="d8c1" id="d8c1" class="graf graf--p graf-after--pre">Now we have something of 3 by 288 by 288 because PyTorch likes the channel to be first [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h15m5s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h15m5s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:15:05</a>]. As you can see, it’s been turned into a square for us, it’s been normalized to (0, 1), all that normal stuff.</p><p name="a691" id="a691" class="graf graf--p graf-after--p">Now we are creating a random image.</p><pre name="3a8a" id="3a8a" class="graf graf--pre graf-after--p">opt_img = np.random.uniform(0, 1, size=img.shape).astype(np.float32)<br>plt.imshow(opt_img);</pre><figure name="c7c4" id="c7c4" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_jJzsPvZ9uHAtHqu9mZ_skA.png"></figure><p name="acda" id="acda" class="graf graf--p graf-after--figure">Here is something I discovered. Trying to turn this into a picture of anything is actually really hard. I found it very difficult to actually get an optimizer to get reasonable gradients that went anywhere. And just as I thought I was going to run out of time for this class and really embarrass myself, I realized the key issue is that pictures don’t look like this. They have more smoothness, so I turned this into the following by blurring it a little bit:</p><pre name="bb0c" id="bb0c" class="graf graf--pre graf-after--p">opt_img = scipy.ndimage.filters.median_filter(opt_img, [8,8,1])<br>plt.imshow(opt_img);</pre><figure name="dbf0" id="dbf0" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_84Vk7fPct3lIUwXWFZhWRQ.png"></figure><p name="e63a" id="e63a" class="graf graf--p graf-after--figure">I used a median filter — basically it is like a median pooling, effectively. As soon as I change it to this, it immediately started training really well. A number of little tweaks you have to do to get these things to work is kind of insane, but here is a little tweak.</p><p name="cc96" id="cc96" class="graf graf--p graf-after--p">So we start with a random image which is at least somewhat smooth [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h16m21s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h16m21s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:16:21</a>]. I found that my bird image had a mean of pixels that was about half of this, so I divided it by 2 just trying to make it a little bit easier for it to match (I don’t know if it matters). Turn that into a variable because this image, remember, we are going to be modifying those pixels with an optimization algorithm, so anything that’s involved in the loss function needs to be a variable. And specifically, it requires a gradient because we are actually updating the image.</p><pre name="e9eb" id="e9eb" class="graf graf--pre graf-after--p">opt_img = val_tfms(opt_img)/2<br>opt_img_v = V(opt_img[<strong class="markup--strong markup--pre-strong">None</strong>], requires_grad=<strong class="markup--strong markup--pre-strong">True</strong>)<br>opt_img_v.shape</pre><pre name="7ab1" id="7ab1" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">torch.Size([1, 3, 288, 288])</em></pre><p name="ec11" id="ec11" class="graf graf--p graf-after--pre">So we now have a mini batch of 1, 3 channels, 288 by 288 random noise.</p><pre name="c255" id="c255" class="graf graf--pre graf-after--p">m_vgg = nn.Sequential(*children(m_vgg)[:37])</pre><p name="94cc" id="94cc" class="graf graf--p graf-after--pre">We are going to use, for no particular reason, the 37th layer of VGG. If you print out the VGG network (you can just type in <code class="markup--code markup--p-code">m_vgg</code> and prints it out), you’ll see that this is mid to late stage layer. So we can just grab the first 37 layers and turn it into a sequential model. So now we have a subset of VGG that will spit out some mid layer activations, and that’s what the model is going to be. So we can take our actual bird image and we want to create a mini batch of one. Remember, if you slice in Numpy with <code class="markup--code markup--p-code">None</code>, also known as <code class="markup--code markup--p-code">np.newaxis</code>, it introduces a new unit axis in that point. Here, I want to create an axis of size 1 to say this is a mini batch of size one. So slicing with None just like I did here (<code class="markup--code markup--p-code u-paddingRight0 u-marginRight0">opt_img_v = V(opt_img[<strong class="markup--strong markup--p-strong">None</strong>], requires_grad=<strong class="markup--strong markup--p-strong">True</strong>)</code>) to get one unit axis at the front. Then we turn that into a variable and this one doesn’t need to be updated, so we use <code class="markup--code markup--p-code">VV</code> to say you don’t need gradients for this guy. So that is going to give us our target activations.</p><ul class="postList"><li name="9341" id="9341" class="graf graf--li graf-after--p">We’ve taken our bird image</li><li name="e909" id="e909" class="graf graf--li graf-after--li">Turned it into a variable</li><li name="013d" id="013d" class="graf graf--li graf-after--li">Stuck it through our model to grab the 37th layer activations which is our target. We want our content loss to be this set of activations.</li><li name="0517" id="0517" class="graf graf--li graf-after--li">We are going to create an optimizer (we will go back to the details of this in a moment)</li><li name="afb1" id="afb1" class="graf graf--li graf-after--li">We are going to step a bunch of times</li><li name="a132" id="a132" class="graf graf--li graf-after--li">Zero the gradients</li><li name="a328" id="a328" class="graf graf--li graf-after--li">Call some loss function</li><li name="8bbf" id="8bbf" class="graf graf--li graf-after--li">Loss.backward()</li></ul><p name="fd4d" id="fd4d" class="graf graf--p graf-after--li">That’s the high level version. I’m going to come back to the details in a moment, but the key thing is that the loss function we are passing in that randomly generated image — the variable of optimization image. So we pass that to our loss function and it’s going to update this using the loss function, and the loss function is the mean squared error loss comparing our current optimization image passed through our VGG to get the intermediate activations and comparing it to our target activations. We run that bunch of times and we’ll print it out. And we have our bird but not the representation of it.</p><pre name="8cd0" id="8cd0" class="graf graf--pre graf-after--p">targ_t = m_vgg(VV(img_tfm[<strong class="markup--strong markup--pre-strong">None</strong>]))<br>targ_v = V(targ_t)<br>targ_t.shape</pre><pre name="0238" id="0238" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">torch.Size([1, 512, 18, 18])</em></pre><pre name="42fe" id="42fe" class="graf graf--pre graf-after--pre">max_iter = 1000<br>show_iter = 100<br>optimizer = optim.LBFGS([opt_img_v], lr=0.5)</pre><h4 name="03ea" id="03ea" class="graf graf--h4 graf-after--pre">Broyden–Fletcher–Goldfarb–Shanno (BFGS) [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h20m18s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h20m18s" class="markup--anchor markup--h4-anchor" rel="noopener nofollow" target="_blank">1:20:18</a>]</h4><p name="e7a5" id="e7a5" class="graf graf--p graf-after--h4">A couple of new details here. One is a weird optimizer (<code class="markup--code markup--p-code">optim.LBFGS</code>). Anybody who’s done certain parts of math and computer science courses comes into deep learning discovers we use all this stuff like Adam and the SGD and always assume that nobody in the field knows the first thing about computer science and immediately says “any of you guys tried using BFGS?” There’s basically a long history of a totally different kind of algorithm for optimization that we don’t use to train neural networks. And of course the answer is actually the people who have spent decades studying neural networks do know a thing or two about computer science and it turns out these techniques on the whole don’t work very well. But it’s actually going to work well for this, and it’s a good opportunity to talk about an interesting algorithm for those of you that haven’t studied this type of optimization algorithm at school. BFGS (initials of four different people) and the L stands for limited memory. It is an optimizer so as an optimizer, that means that there’s some loss function and it’s going to use some gradients (not all optimizers use gradients but all the ones we use do) to find a direction to go and try to make the loss function go lower and lower by adjusting some parameters. It’s just an optimizer. But it’s an interesting kind of optimizer because it does a bit more work than the ones we’re used to on each step. Specifically, the way it works is it starts the same way that we are used to which is we just pick somewhere to get started and in this case, we’ve picked a random image as you saw. As per usual, we calculate the gradient. But we then don’t just take a step but we actually do is as well as finding the gradient, we also try to find the second derivative. The second derivative says how fast does the gradient change.</p><p name="62b2" id="62b2" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Gradient</strong>: how fast the function change</p><p name="c639" id="c639" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The second derivative</strong>: how fast the gradient change</p><p name="c222" id="c222" class="graf graf--p graf-after--p">In other words, how curvy is it? The basic idea is that if you know that it’s not very curvy, then you can probably jump farther. But if it’s very curvy then you probably don’t want to jump as far. So in higher dimensions, the gradient is called the Jacobian and the second derivative is called the Hessian. You’ll see those words all the time, but that’s all they mean. Again, mathematicians have to invent your words for everything as well. They are just like deep learning researchers — maybe a bit more snooty. With BFGS, we are going to try and calculate the second derivative and then we are going to use that to figure out what direction to go and how far to go — so it’s less of a wild jump into the unknown.</p><p name="8379" id="8379" class="graf graf--p graf-after--p">Now the problem is that actually calculating the Hessian (the second derivative) is almost certainly not a good idea[<a href="https://youtu.be/xXXiC4YRGrQ?t=1h24m15s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h24m15s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:24:15</a>]. Because in each possible direction that you are going to head, for each direction that you’re measuring the gradient in, you also have to calculate the Hessian in every direction. It gets ridiculously big. So rather than actually calculating it, we take a few steps and we basically look at how much the gradient is changing as we do each step, and we approximate the Hessian using that little function. Again, this seems like a really obvious thing to do but nobody thought of it until someone did surprisingly a long time later. Keeping track of every single step you take takes a lot of memory, so duh, don’t keep track of every step you take — just keep the last ten or twenty. And the second bit there, that’s the L to the LBFGS. So a limited-memory BFGS means keep the last 10 or 20 gradients, use that to approximate the amount of curvature, and then use the curvature in gradient to estimate what direction to travel and how far. That’s normally not a good idea in deep learning for a number of reasons. It’s obviously more work to do than than Adam or SGD update, and it also uses more memory — memory is much more of a big issue when you’ve got a GPU to store it on and hundreds of millions of weights. But more importantly, the mini-batch is super bumpy so figuring out curvature to decide exactly how far to travel is kind of polishing turds as we say (yeah, Australian and English expression — you get the idea). Interestingly, actually using the second derivative information, it turns out, is like a magnet for saddle points. So there’s some interesting theoretical results that basically say it actually sends you towards nasty flat areas of the function if you use second derivative information. So normally not a good idea.</p><pre name="e3fb" id="e3fb" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> actn_loss(x): <strong class="markup--strong markup--pre-strong">return</strong> F.mse_loss(m_vgg(x), targ_v)*1000</pre><pre name="b18b" id="b18b" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> step(loss_fn):<br>    <strong class="markup--strong markup--pre-strong">global</strong> n_iter<br>    optimizer.zero_grad()<br>    loss = loss_fn(opt_img_v)<br>    loss.backward()<br>    n_iter+=1<br>    <strong class="markup--strong markup--pre-strong">if</strong> n_iter%show_iter==0: <br>        print(f'Iteration: n_iter, loss: <strong class="markup--strong markup--pre-strong">{loss.data[0]}</strong>')<br>    <strong class="markup--strong markup--pre-strong">return</strong> loss</pre><p name="4a66" id="4a66" class="graf graf--p graf-after--pre">But in this case [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h26m40s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h26m40s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:26:40</a>], we are not optimizing weights, we are optimizing pixels so all the rules change and actually turns out BFGS does make sense. Because it does more work each time, it’s a different kind of optimizer, the API is a little bit different in PyTorch. As you can see here, when you say <code class="markup--code markup--p-code">optimizer.step</code>, you actually pass in the loss function. So our loss function is to call <code class="markup--code markup--p-code">step</code> with a particular loss function which is our activation loss (<code class="markup--code markup--p-code">actn_loss</code>). And inside the loop, you don’t say step, step, step. But rather it looks like this. So it’s a little bit different and you’re welcome to try and rewrite this to use SGD, it’ll still work. It’ll just take a bit longer — I haven’t tried it with SGD yet and I’d be interested to know how much longer it takes.</p><pre name="b82e" id="b82e" class="graf graf--pre graf-after--p">n_iter=0<br><strong class="markup--strong markup--pre-strong">while</strong> n_iter &lt;= max_iter: optimizer.step(partial(step,actn_loss))</pre><pre name="5d11" id="5d11" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">Iteration: n_iter, loss: 0.8466196656227112<br>Iteration: n_iter, loss: 0.34066855907440186<br>Iteration: n_iter, loss: 0.21001280844211578<br>Iteration: n_iter, loss: 0.15562333166599274<br>Iteration: n_iter, loss: 0.12673595547676086<br>Iteration: n_iter, loss: 0.10863320529460907<br>Iteration: n_iter, loss: 0.0966048613190651<br>Iteration: n_iter, loss: 0.08812198787927628<br>Iteration: n_iter, loss: 0.08170554041862488<br>Iteration: n_iter, loss: 0.07657770067453384</em></pre><p name="b4fa" id="b4fa" class="graf graf--p graf-after--pre">So you can see the loss function going down [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h27m38s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h27m38s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:27:38</a>]. The mean squared error between the activations at layer 37 of our VGG model for our optimized image vs. the target activations, remember the target activations were the VGG applied to our bird. Make sense? So we’ve now got a content loss. Now, one thing I’ll say about this content loss is we don’t know which layer is going to work the best. So it would be nice if we were able to experiment a little bit more. And the way it is here is annoying:</p><figure name="18c3" id="18c3" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_KTfbdTPG-pZ95vEOLrJa9Q.png"></figure><p name="c12c" id="c12c" class="graf graf--p graf-after--figure">Maybe we even want to use multiple layers. So rather than lopping off all of the layers after the one we want, wouldn’t it be nice if we could somehow grab the activations of a few layers as it calculates. Now, we already know one way to do that back when we did SSD, we actually wrote our own network which had a number of outputs. Remember? The different convolutional layers, we spat out a different <code class="markup--code markup--p-code">oconv</code> thing? But I don’t really want to go and add that to the torch.vision ResNet model especially not if later on, I want to try torch.vision VGG model, and then I want to try NASNet-A model, I don’t want to go into all of them and change their outputs. Beside which, I’d like to easily be able to turn certain activations on and off on demand. So we briefly touched before this idea that PyTorch has these fantastic things called hooks. You can have forward hooks that let you plug anything you like into the forward pass of a calculation or a backward hook that lets you plug anything you like into the backward pass. So we are going to create the world’s simplest forward hook.</p><pre name="d744" id="d744" class="graf graf--pre graf-after--p">x = val_tfms.denorm(np.rollaxis(to_np(opt_img_v.data),1,4))[0]<br>plt.figure(figsize=(7,7))<br>plt.imshow(x);</pre><figure name="a43b" id="a43b" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_CzZ-KObFhqarMxnV5lD-IQ.png"></figure><h3 name="f018" id="f018" class="graf graf--h3 graf-after--figure">Forward hook [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h29m42s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h29m42s" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">1:29:42</a>]</h3><p name="29c7" id="29c7" class="graf graf--p graf-after--h3">This is one of these things that almost nobody knows about so almost any code you find on the internet that implements style transfer will have all kind of horrible hacks rather than using forward hooks. But forward hook is really easy.</p><p name="ee12" id="ee12" class="graf graf--p graf-after--p">To create a forward hook, you just create a class. The class has to have something called <code class="markup--code markup--p-code">hook_fn</code>. And your hook function is going to receive the <code class="markup--code markup--p-code">module</code> that you’ve hooked, the <code class="markup--code markup--p-code">input</code> for the forward pass, and the <code class="markup--code markup--p-code">output</code> then you do whatever you’d like. So what I’m going to do is I’m just going to store the output of this module in some attribute. That’s it. So <code class="markup--code markup--p-code">hook_fn</code> can actually be called anything you like, but “hook function” seems to be the standard because, as you can see, what happens in the constructor is I store inside some attribute the result of <code class="markup--code markup--p-code">m.register_forward_hook</code> (<code class="markup--code markup--p-code">m</code> is going to be the layer that I’m going to hook) and pass in the function that you want to be called when the module’s forward method is called. When its forward method is called, it will call <code class="markup--code markup--p-code">self.hook_fn</code> which will store the output in an attribute called <code class="markup--code markup--p-code">features</code>.</p><pre name="bbf0" id="bbf0" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">SaveFeatures</strong>():<br>    features=<strong class="markup--strong markup--pre-strong">None</strong><br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, m): <br>        self.hook = m.register_forward_hook(self.hook_fn)<br>    <strong class="markup--strong markup--pre-strong">def</strong> hook_fn(self, module, input, output): self.features = output<br>    <strong class="markup--strong markup--pre-strong">def</strong> close(self): self.hook.remove()</pre><p name="66f3" id="66f3" class="graf graf--p graf-after--pre">So now what we can do is we can create a VGG as before. And let’s set it to not trainable so we don’t waste time and memory calculating gradients for it. And let’s go through and find all the max pool layers. So let’s go through all of the children of this module and if it’s a max pool layer, let’s spit out index minus 1 — so that’s going to give me the layer before the max pool. In general, the layer before a max pool or stride 2 conv is a very layer. It’s the most complete representation we have at that grid cell size because the very next layer is changing the grid. So that seems to me like a good place to grab the content loss from. The best most semantic, most interesting content we have at that grid size. So that’s why I’m going to pick those indexes.</p><pre name="da84" id="da84" class="graf graf--pre graf-after--p">m_vgg = to_gpu(vgg16(<strong class="markup--strong markup--pre-strong">True</strong>)).eval()<br>set_trainable(m_vgg, <strong class="markup--strong markup--pre-strong">False</strong>)</pre><p name="75a6" id="75a6" class="graf graf--p graf-after--pre">These are the indexes of the last layer before each max pool in VGG [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h32m30s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h32m30s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:32:30</a>].</p><pre name="f9b8" id="f9b8" class="graf graf--pre graf-after--p">block_ends = [i-1 <strong class="markup--strong markup--pre-strong">for</strong> i,o <strong class="markup--strong markup--pre-strong">in</strong> enumerate(children(m_vgg))<br>              <strong class="markup--strong markup--pre-strong">if</strong> isinstance(o,nn.MaxPool2d)]<br>block_ends</pre><pre name="a7c6" id="a7c6" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[5, 12, 22, 32, 42]</em></pre><p name="fa5c" id="fa5c" class="graf graf--p graf-after--pre">I’m going to grab <code class="markup--code markup--p-code">32</code> — no particular reason, just try something else. So I’m going to say <code class="markup--code markup--p-code">block_ends[3]</code> (i.e. 32). <code class="markup--code markup--p-code">children(m_vgg)[block_ends[3]]</code> will give me the 32nd layer of VGG as a module.</p><pre name="d83d" id="d83d" class="graf graf--pre graf-after--p">sf = SaveFeatures(children(m_vgg)[block_ends[3]])</pre><p name="cefe" id="cefe" class="graf graf--p graf-after--pre">Then if I call the <code class="markup--code markup--p-code">SaveFeatures</code> constructor, it’s going to go:</p><p name="8aef" id="8aef" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">self.hook = {32nd layer of VGG}.register_forward_hook(self.hook_fn)</code></p><p name="e770" id="e770" class="graf graf--p graf-after--p">Now, every time I do a forward pass on this VGG model, it’s going to store the 32nd layer’s output inside <code class="markup--code markup--p-code">sf.features</code>.</p><pre name="1392" id="1392" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> get_opt():<br>    opt_img = np.random.uniform(0, 1, <br>                                size=img.shape).astype(np.float32)<br>    opt_img = scipy.ndimage.filters.median_filter(opt_img, [8,8,1])<br>    opt_img_v = V(val_tfms(opt_img/2)[<strong class="markup--strong markup--pre-strong">None</strong>], requires_grad=<strong class="markup--strong markup--pre-strong">True</strong>)<br>    <strong class="markup--strong markup--pre-strong">return</strong> opt_img_v, optim.LBFGS([opt_img_v])</pre><pre name="8725" id="8725" class="graf graf--pre graf-after--pre">opt_img_v, optimizer = get_opt()</pre><p name="0fdc" id="0fdc" class="graf graf--p graf-after--pre">See here [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h33m33s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h33m33s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:33:33</a>], I’m calling my VGG network, but I’m not storing it anywhere. I’m not saying <code class="markup--code markup--p-code u-paddingRight0 u-marginRight0">activations = m_vgg(VV(img_tfm[<strong class="markup--strong markup--p-strong">None</strong>]))</code>. I’m calling it, throwing away the answer, and then grabbing the features we stored in our <code class="markup--code markup--p-code">SaveFeatures</code> object.</p><p name="2411" id="2411" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">m_vgg()</code> — this is how you do a forward path in PyTorch. You don’t say <code class="markup--code markup--p-code">m_vgg.forward()</code>, you just use it as a callable. Using as a callable on an <code class="markup--code markup--p-code">nn.module</code> automatically calls <code class="markup--code markup--p-code">forward</code>. That’s how PyTorch modules work.</p><p name="8e8b" id="8e8b" class="graf graf--p graf-after--p">So we call it as a callable, that ends up calling our forward hook, that forward hook stores the activations in <code class="markup--code markup--p-code">sf.features</code>, and so now we have our target variable — just like before but in a much more flexible way.</p><p name="d422" id="d422" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">get_opt</code> contains the same 4 lines of code we had earlier [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h34m34s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h34m34s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:34:34</a>]. It is just giving me my random image to optimize and an optimizer to optimize that image.</p><pre name="cbab" id="cbab" class="graf graf--pre graf-after--p">m_vgg(VV(img_tfm[<strong class="markup--strong markup--pre-strong">None</strong>]))<br>targ_v = V(sf.features.clone())<br>targ_v.shape</pre><pre name="7b63" id="7b63" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">torch.Size([1, 512, 36, 36])</em></pre><pre name="8383" id="8383" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> actn_loss2(x):<br>    m_vgg(x)<br>    out = V(sf.features)<br>    <strong class="markup--strong markup--pre-strong">return</strong> F.mse_loss(out, targ_v)*1000</pre><p name="30e0" id="30e0" class="graf graf--p graf-after--pre">Now I can go ahead and do exactly the same thing. But now I’m going to use a different loss function <code class="markup--code markup--p-code">actn_loss2</code> (activation loss #2) which doesn’t say <code class="markup--code markup--p-code">out=m_vgg</code>, again, it calls <code class="markup--code markup--p-code">m_vgg</code> to do a forward pass, throws away the results, and and grabs <code class="markup--code markup--p-code">sf.features</code>. So that’s now my 32nd layer activations which I can then do my MSE loss on. You might have noticed, the last loss function and this one are both multiplied by a thousand. Why are they multiplied by a thousand? This was like all the things that were trying to get this lesson to not work correctly. I didn’t used to have a thousand and it wasn’t training. Lunch time today, nothing was working. After days of trying to get this thing to work, and finally just randomly noticed “gosh, the loss functions — the numbers are really low (like 10E-7)” and I thought what if they weren’t so low. So I multiplied them by a thousand and it started working. So why did it not work? Because we are doing single precision floating point, and single precision floating point isn’t that precise. Particularly once you’re getting gradients that are kind of small and then you are multiplying by the learning rate that can be small, and you end up with a small number. If it’s so small, they could get rounded to zero and that’s what was happening and my model wasn’t ready. I’m sure there are better ways than multiplying by a thousand, but whatever. It works fine. It doesn’t matter what you multiply a loss function by because all you care about is its direction and the relative size. Interestingly, this is something similar we do for when we were training ImageNet. We were using half precision floating point because Volta tensor cores require that. And it’s actually a standard practice if you want to get the half precision floating to train, you actually have to multiply the loss function by a scaling factor. We were using 1024 or 512. I think fast.ai is now the first library that has all of the tricks necessary to train in half precision floating point built-in, so if you are lucky enough to have a Volta or you can pay for a AWS P3, if you’ve got a learner object, you can just say <code class="markup--code markup--p-code">learn.half</code>, it’ll now just magically train correctly half precision floating point. It’s built into the model data object as well, and it’s all automatic. Pretty sure no other library does that.</p><pre name="1df6" id="1df6" class="graf graf--pre graf-after--p">n_iter=0<br><strong class="markup--strong markup--pre-strong">while</strong> n_iter &lt;= max_iter: optimizer.step(partial(step,actn_loss2))</pre><pre name="cce4" id="cce4" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">Iteration: n_iter, loss: 0.2112911492586136<br>Iteration: n_iter, loss: 0.0902421623468399<br>Iteration: n_iter, loss: 0.05904778465628624<br>Iteration: n_iter, loss: 0.04517251253128052<br>Iteration: n_iter, loss: 0.03721420466899872<br>Iteration: n_iter, loss: 0.03215853497385979<br>Iteration: n_iter, loss: 0.028526008129119873<br>Iteration: n_iter, loss: 0.025799645110964775<br>Iteration: n_iter, loss: 0.02361033484339714<br>Iteration: n_iter, loss: 0.021835438907146454</em></pre><p name="a7fd" id="a7fd" class="graf graf--p graf-after--pre">This is just doing the same thing on a slightly earlier layer [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h37m35s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h37m35s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:37:35</a>]. And the bird looks more bird-like. Hopefully that makes sense to you that earlier layers are getting closer to the pixels. There are more grid cells, each cell is smaller, smaller receptive field, less complex semantic features. So the earlier we get, the more it’s going to look like a bird.</p><pre name="b62a" id="b62a" class="graf graf--pre graf-after--p">x = val_tfms.denorm(np.rollaxis(to_np(opt_img_v.data),1,4))[0]<br>plt.figure(figsize=(7,7))<br>plt.imshow(x);</pre><figure name="437a" id="437a" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_i2SK83mI6XYD9al6OV4fhw.png"></figure><pre name="49d5" id="49d5" class="graf graf--pre graf-after--figure">sf.close()</pre><p name="48bc" id="48bc" class="graf graf--p graf-after--pre">In fact, the paper has a nice picture of that showing various different layers and zooming into this house [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h38m17s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h38m17s" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">1:38:17</a>]. They are trying to make this house look like The Starry Night picture. And you can see that later on, it’s pretty messy, and earlier on, it looks like the house. So this is just doing what we just did. One of the things I’ve noticed in our study group is anytime I say to somebody to answer a question, anytime I say read the paper there is a thing in the paper that tells you the answer to that question, there’s always this shocked look “read the paper? me?” but seriously the papers have done these experiments and drawn the pictures. There’s all this stuff in the papers. It doesn’t mean you have to read every part of the paper. But at least look at the pictures. So check out Gatys’ paper, it’s got nice pictures. So they’ve done the experiment for us but looks like they didn’t go as deep — they just got some earlier ones.</p><figure name="8a61" id="8a61" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_cqZ5Az70HWX2dUhPnUDAZg.png"></figure><h4 name="9879" id="9879" class="graf graf--h4 graf-after--figure">Style match [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h39m29s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h39m29s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:39:29</a>]</h4><p name="c9f9" id="c9f9" class="graf graf--p graf-after--h4">The next thing we need to do is to create style loss. We’ve already got the loss which is how much like the bird is it. Now we need how like this painting style is it. And we are going to do nearly the same thing. We are going to grab the activations of some layer. Now the problem is, the activations of some layer, let’s say it was a 5x5 layer (of course there are no 5x5 layers, it’s 224x224, but we’ll pretend). So here’re some activations and we could get these activations both per the image we are optimizing and for our Van Gogh painting. Let’s look at our Van Gogh painting. There it is — The Starry Night</p><pre name="7e53" id="7e53" class="graf graf--pre graf-after--p">style_fn = PATH/'style'/'starry_night.jpg'</pre><pre name="3e02" id="3e02" class="graf graf--pre graf-after--pre">style_img = open_image(style_fn)<br>style_img.shape, img.shape</pre><pre name="4b13" id="4b13" class="graf graf--pre graf-after--pre">((1198, 1513, 3), (291, 483, 3))</pre><pre name="bc21" id="bc21" class="graf graf--pre graf-after--pre">plt.imshow(style_img);</pre><figure name="7f85" id="7f85" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_3QN8_RpikQBlk8wwjD9B3w.png"></figure><p name="318e" id="318e" class="graf graf--p graf-after--figure">I downloaded this from Wikipedia and I was wondering what is taking son long to load [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h40m39s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h40m39s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:40:39</a>] — turns out, the Wikipedia version I downloaded was 30,000 by 30,000 pixels. It’s pretty cool that they’ve got this serious gallery quality archive stuff there. I didn’t know it existed. Don’t try to run a neural net on that. Totally killed my Jupyter notebook.</p><p name="8f03" id="8f03" class="graf graf--p graf-after--p">So we can do that for our Van Gogh image and we can do that for our optimized image. Then we can compare the two and we would end up creating an image that has content like the painting but it’s not the painting — that’s not what we want. We want something with the same style but it’s not the painting and doesn’t have the content. So we want to throw away all of the spatial information. We are not trying to create something that has a moon here, stars here, and a church here. We don’t want any of that. So how do we throw away all the special information?</p><figure name="599c" id="599c" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_YVBXuBYYyoalPWcW2avrsg.png"></figure><p name="1132" id="1132" class="graf graf--p graf-after--figure">In this case, there are 19 faces on this — 19 slices. So let’s grab this top slice that’s going to be a 5x5 matrix. Now, let’s flatten it and we’ve got a 25 long vector. In one stroke, we’ve thrown away the bulk of the spacial information by flattening it. Now let’s grab a second slice (i.e. another channel) and do the same thing. So we have channel 1 flattened and channel 2 flattened, and they both have 25 elements. Now, let’s take the dot product which we can do with <code class="markup--code markup--p-code">@</code> in Numpy (Note: <a href="http://forums.fast.ai/t/part-2-lesson-13-wiki/15297/140?u=hiromi" data-href="http://forums.fast.ai/t/part-2-lesson-13-wiki/15297/140?u=hiromi" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">here is Jeremy’s answer to my dot product vs. matrix multiplication question</a>). So the dot product is going to give us one number. What’s that number? What is it telling us? Assuming the activations are somewhere around the middle layer of the VGG network, we might expect some of these activations to be how textured is the brush stroke, and some of them to be like how bright is this area, and some of them to be like is this part of a house or a part of a circular thing, or other parts to be, how dark is this part of the painting. So a dot product is basically a correlation. If this element and and this element are both highly positive or both highly negative, it gives us a big result. Where else, if they are the opposite, it gives a small results. If they are both close to zero, it gives no result. So basically a dot product is a measure of how similar these two things are. So if the activations of channel 1 and channel 2 are similar, then it basically says — Let’s give an example [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h44m28s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h44m28s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:44:28</a>]. Let’s say the first one was how textured are the brushstrokes (C1) and that one there says how diagonally oriented are the brush strokes (C2).</p><figure name="cd0b" id="cd0b" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_ho9iuqmJh3hVXPNeZ9E_Xg.png"></figure><p name="39d0" id="39d0" class="graf graf--p graf-after--figure">If C1 and C2 are both high for a cell (1, 1) at the same time, and same is true for a cell (4, 2), then it’s saying grid cells that would have texture tend to also have diagonal. So dot product would be high when grid cells that have texture also have diagonal, and when they don’t, they don’t (have high dot product). So that’s <code class="markup--code markup--p-code">C1 @ C2</code>. Where else, <code class="markup--code markup--p-code">C1 @ C1</code> is the 2-norm effectively (i.e. the sum of the squares of C1). This is basically saying how many grid cells in the textured channel is active and how active it is. So in other words, <code class="markup--code markup--p-code">C1 @ C1</code> tells us how much textured painting is going on. And <code class="markup--code markup--p-code">C2 @ C2</code> tells us how much diagonal paint stroke is going on. Maybe C3 is “is it bright colors?” so <code class="markup--code markup--p-code">C3 @ C3</code> would be how often do we have bright colored cells.</p><p name="a5eb" id="a5eb" class="graf graf--p graf-after--p">So what we could do then is we could create a 19 by 19 matrix containing every dot product [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h47m17s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h47m17s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:47:17</a>]. And like we discussed, mathematicians have to give everything a name, so this particular matrix where you flatten something out and then do all the dot product is called Gram matrix.</p><figure name="88e9" id="88e9" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_hboObzQV-8h0yiVvqZNvZg.png"></figure><p name="7dcd" id="7dcd" class="graf graf--p graf-after--figure">I’ll tell you a secret [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h48m29s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h48m29s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:48:29</a>]. Most deep learning practitioners either don’t know or don’t remember all these things like what is a Gram matrix if they ever did study at university. They probably forgot it because they had a big night afterwards. And the way it works in practice is you realize “oh, I could create a kind of non-spacial representation of how the channels correlate with each other” and then when I write up the paper, I have to go and ask around and say “does this thing have a name?” and somebody will be like “isn’t that the Gram matrix?” and you go and look it up and it is. So don’t think you have to go study all of math first. Use your intuition and common sense and then you worry about what the math is called later, normally. Sometimes it works the other way, not with me because I can’t do math.</p><p name="2847" id="2847" class="graf graf--p graf-after--p">So this is called the Gram matrix [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h49m22s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h49m22s" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">1:49:22</a>]. And of course, if you are a real mathematician, it’s very important that you say this as if you always knew it was a Gram matrix and you kind of just go oh yes, we just calculate the Gram matrix. So the Gram matrix then is this kind of map — the diagonal is perhaps the most interesting. The diagonal is which channels are the most active and then the off diagonal is which channels tend to appear together. And overall, if two pictures have the same style, then we are expecting that some layer of activations, they will have similar Gram matrices. Because if we found the level of activations that capture a lot of stuff about like paint strokes and colors, then the diagonal alone (in Gram matrices) might even be enough. That’s another interesting homework assignment, if somebody wants to take it, is try doing Gatys’ style transfer not using the Gram matrix but just using the diagonal of the Gram matrix. That would be like a single line of code to change. But I haven’t seen it tried and I don’t know if it would work at all, but it might work fine.</p><p name="a2f9" id="a2f9" class="graf graf--p graf--startsWithDoubleQuote graf-after--p">“Okay, yes Christine, you’ve tried it” [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h50m51s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h50m51s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:50:51</a>]. “I have tried that and it works most of the time except when you have funny pictures where you need two styles to appear in the same spot. So it seems like grass in one half and a crowd in one half, and you need the two styles.” (Christine). Cool, you’re still gonna do your homework, but Christine says she’ll do it for you.</p><pre name="9ce4" id="9ce4" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> scale_match(src, targ):<br>    h,w,_ = img.shape<br>    sh,sw,_ = style_img.shape<br>    rat = max(h/sh,w/sw); rat<br>    res = cv2.resize(style_img, (int(sw*rat), int(sh*rat)))<br>    <strong class="markup--strong markup--pre-strong">return</strong> res[:h,:w]</pre><pre name="5a0b" id="5a0b" class="graf graf--pre graf-after--pre">style = scale_match(img, style_img)</pre><pre name="59ff" id="59ff" class="graf graf--pre graf-after--pre">plt.imshow(style)<br>style.shape, img.shape</pre><pre name="187b" id="187b" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">((291, 483, 3), (291, 483, 3))</em></pre><figure name="c038" id="c038" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_3QDp1KCdg6RkKL8yhkRbDw.png"></figure><p name="37d5" id="37d5" class="graf graf--p graf-after--figure">So here is our painting [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h51m22s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h51m22s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:51:22</a>]. I’ve tried to resize the painting so it’s the same size as my bird picture. So that’s all this is just doing. It doesn’t matter too much which bit I use as long as it’s got lots of the nice style in it.</p><p name="74c6" id="74c6" class="graf graf--p graf-after--p">I grab my optimizer and my random image just like before:</p><pre name="4bf2" id="4bf2" class="graf graf--pre graf-after--p">opt_img_v, optimizer = get_opt()</pre><p name="603c" id="603c" class="graf graf--p graf-after--pre">And this time, I call <code class="markup--code markup--p-code">SaveFeatures</code> for all of my <code class="markup--code markup--p-code">block_ends</code> and that’s going to give me an array of SaveFeatures objects — one for each module that appears the layer before the max pool. Because this time, I want to play around with different activation layer styles, or more specifically I want to let you play around with it. So now I’ve got a whole array of them.</p><pre name="d249" id="d249" class="graf graf--pre graf-after--p">sfs = [SaveFeatures(children(m_vgg)[idx]) <strong class="markup--strong markup--pre-strong">for</strong> idx <strong class="markup--strong markup--pre-strong">in</strong> block_ends]</pre><p name="0db2" id="0db2" class="graf graf--p graf-after--pre"><code class="markup--code markup--p-code">style_img</code> is my Van Gogh painting. So I take my <code class="markup--code markup--p-code">style_img</code>, put it through my transformations to create my transform style image (<code class="markup--code markup--p-code">style_tfm</code>).</p><pre name="c329" id="c329" class="graf graf--pre graf-after--p">style_tfm = val_tfms(style_img)</pre><p name="668c" id="668c" class="graf graf--p graf-after--pre">Turn that into a variable, put it through the forward pass of my VGG module, and now I can go through all of my SaveFeatures objects and grab each set of features. Notice I call <code class="markup--code markup--p-code">clone</code> because later on, if I call my VGG object again, it’s going to replace those contents. I haven’t quite thought about whether this is necessary. If you take it away and it’s not, that’s fine. But I was just being careful. So here is now an array of the activations at every <code class="markup--code markup--p-code">block_end</code> layer. And here, you can see all of those shapes:</p><pre name="3ae5" id="3ae5" class="graf graf--pre graf-after--p">m_vgg(VV(style_tfm[<strong class="markup--strong markup--pre-strong">None</strong>]))<br>targ_styles = [V(o.features.clone()) <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> sfs]<br>[o.shape <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> targ_styles]</pre><pre name="78d1" id="78d1" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[torch.Size([1, 64, 288, 288]),<br> torch.Size([1, 128, 144, 144]),<br> torch.Size([1, 256, 72, 72]),<br> torch.Size([1, 512, 36, 36]),<br> torch.Size([1, 512, 18, 18])]</em></pre><p name="37e9" id="37e9" class="graf graf--p graf-after--pre">And you can see, being able to whip up a list comprehension really quickly, it’s really important in your Jupyter fiddling around [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h53m30s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h53m30s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:53:30</a>]. Because you really want to be able to immediately see here’s my channel (64, 128, 256,&nbsp;…), and grid size halving as we would expect (288, 144, 72…) because all of these appear just before a max pool.</p><p name="dc8c" id="dc8c" class="graf graf--p graf-after--p">So to do a Gram MSE loss, it’s going to be the MSE loss on the Gram matrix of the input vs. the gram matrix of the target. And the Gram matrix is just the matrix multiply of <code class="markup--code markup--p-code">x</code> with <code class="markup--code markup--p-code">x</code> transpose (<code class="markup--code markup--p-code">x.t()</code>) where x is simply equal to my input where I’ve flattened the batch and channel axes all down together. I’ve only got one image, so you can ignore the batch part — it’s basically channel. Then everything else (<code class="markup--code markup--p-code">-1</code>), which in this case is the height and width, is the other dimension because there’s now going to be channel by height and width, and then as we discussed we can them just do the matrix multiply of that by its transpose. And just to normalize it, we’ll divide that by the number of elements (<code class="markup--code markup--p-code">b*c*h*w</code>) — it would actually be more elegant if I had said <code class="markup--code markup--p-code">input.numel</code> (number of elements) that would be the same thing. Again, this gave me tiny numbers so I multiply it by a big number to make it something more sensible. So that’s basically my loss.</p><pre name="4d38" id="4d38" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> gram(input):<br>        b,c,h,w = input.size()<br>        x = input.view(b*c, -1)<br>        <strong class="markup--strong markup--pre-strong">return</strong> torch.mm(x, x.t())/input.numel()*1e6<br><br><strong class="markup--strong markup--pre-strong">def</strong> gram_mse_loss(input, target): <br>        <strong class="markup--strong markup--pre-strong">return</strong> F.mse_loss(gram(input), gram(target))</pre><p name="639f" id="639f" class="graf graf--p graf-after--pre">So now my style loss is to take my image to optimize, throw it through VGG forward pass, grab an array of the features in all of the SaveFeatures objects, and then call my Gram MSE loss on every one of those layers [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h55m13s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h55m13s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:55:13</a>]. And that’s going to give me an array and then I just add them up. Now you could add them up with different weightings, you could add up subsets, or whatever. In this case, I’m just grabbing all of them.</p><pre name="f2b1" id="f2b1" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> style_loss(x):<br>    m_vgg(opt_img_v)<br>    outs = [V(o.features) <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> sfs]<br>    losses = [gram_mse_loss(o, s) <strong class="markup--strong markup--pre-strong">for</strong> o,s <strong class="markup--strong markup--pre-strong">in</strong> zip(outs, targ_styles)]<br>    <strong class="markup--strong markup--pre-strong">return</strong> sum(losses) </pre><p name="c927" id="c927" class="graf graf--p graf-after--pre">Pass that into my optimizer as before:</p><pre name="c09d" id="c09d" class="graf graf--pre graf-after--p">n_iter=0<br><strong class="markup--strong markup--pre-strong">while</strong> n_iter &lt;= max_iter: optimizer.step(partial(step,style_loss))</pre><pre name="6795" id="6795" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">Iteration: n_iter, loss: 230718.453125<br>Iteration: n_iter, loss: 219493.21875<br>Iteration: n_iter, loss: 202618.109375<br>Iteration: n_iter, loss: 481.5616760253906<br>Iteration: n_iter, loss: 147.41177368164062<br>Iteration: n_iter, loss: 80.62625122070312<br>Iteration: n_iter, loss: 49.52326965332031<br>Iteration: n_iter, loss: 32.36254119873047<br>Iteration: n_iter, loss: 21.831811904907227<br>Iteration: n_iter, loss: 15.61091423034668</em></pre><p name="7555" id="7555" class="graf graf--p graf-after--pre">And here we have a random image in the style of Van Gogh which I think is kind of cool.</p><pre name="c1ba" id="c1ba" class="graf graf--pre graf-after--p">x = val_tfms.denorm(np.rollaxis(to_np(opt_img_v.data),1,4))[0]<br>plt.figure(figsize=(7,7))<br>plt.imshow(x);</pre><figure name="11e1" id="11e1" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_Z2UuUEecjCVOR07scQDw1g.png"></figure><p name="08a7" id="08a7" class="graf graf--p graf-after--figure">Again Gatys has done it for us. Here is different layers of random image in the style of Van Gogh. So the first one, as you can see, the activations are simple geometric things — not very interesting at all. The later layers are much more interesting. So we kind of have a suspicion that we probably want to use later layers largely for our style loss if we wanted to look good.</p><figure name="e2d3" id="e2d3" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_p5JFBuMVDA5kw6CYh_fCfQ.png"></figure><figure name="3c55" id="3c55" class="graf graf--figure graf-after--figure"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_BBOkPG0_GV-KNdPhlmLUgA.png"></figure><p name="3962" id="3962" class="graf graf--p graf-after--figure">I added this <code class="markup--code markup--p-code">SaveFeatures.close</code> [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h56m35s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h56m35s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:56:35</a>] which just calls <code class="markup--code markup--p-code">self.hook.remove()</code>. Remember, I stored the hook as <code class="markup--code markup--p-code">self.hook</code> so <code class="markup--code markup--p-code">hook.remove()</code> gets rid of it. It’s a good idea to get rid of it because otherwise you can potentially just keep using memory. So at the end, I just go through each of my SaveFeatures object and close it:</p><pre name="bdad" id="bdad" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">for</strong> sf <strong class="markup--strong markup--pre-strong">in</strong> sfs: sf.close()</pre><h4 name="1b24" id="1b24" class="graf graf--h4 graf-after--pre">Style transfer [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h57m8s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h57m8s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:57:08</a>]</h4><p name="fcf3" id="fcf3" class="graf graf--p graf-after--h4">Style transfer is adding content loss and style loss together with some weight. So there is no much to show.</p><p name="8303" id="8303" class="graf graf--p graf-after--p">Grab my optimizer, grab my image:</p><pre name="86c9" id="86c9" class="graf graf--pre graf-after--p">opt_img_v, optimizer = get_opt()</pre><p name="d3ed" id="d3ed" class="graf graf--p graf-after--pre">And my combined loss is the MSE loss at one particular layer, my style loss at all of my layers, sum up the style losses, add them to the content loss, the content loss I’m scaling. Actually the style loss, I scaled already by 1E6. So they are both scaled exactly the same. Add them together. Again, you could trying weighting the different style losses or you could maybe remove some of them, so this is the simplest possible version.</p><pre name="5a67" id="5a67" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> comb_loss(x):<br>    m_vgg(opt_img_v)<br>    outs = [V(o.features) <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> sfs]<br>    losses = [gram_mse_loss(o, s) <strong class="markup--strong markup--pre-strong">for</strong> o,s <strong class="markup--strong markup--pre-strong">in</strong> zip(outs, targ_styles)]<br>    cnt_loss   = F.mse_loss(outs[3], targ_vs[3])*1000000<br>    style_loss = sum(losses)<br>    <strong class="markup--strong markup--pre-strong">return</strong> cnt_loss + style_loss</pre><p name="0dc9" id="0dc9" class="graf graf--p graf-after--pre">Train that:</p><pre name="99a6" id="99a6" class="graf graf--pre graf-after--p">n_iter=0<br><strong class="markup--strong markup--pre-strong">while</strong> n_iter &lt;= max_iter: optimizer.step(partial(step,comb_loss))</pre><pre name="c66c" id="c66c" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">Iteration: n_iter, loss: 1802.36767578125<br>Iteration: n_iter, loss: 1163.05908203125<br>Iteration: n_iter, loss: 961.6024169921875<br>Iteration: n_iter, loss: 853.079833984375<br>Iteration: n_iter, loss: 784.970458984375<br>Iteration: n_iter, loss: 739.18994140625<br>Iteration: n_iter, loss: 706.310791015625<br>Iteration: n_iter, loss: 681.6689453125<br>Iteration: n_iter, loss: 662.4088134765625<br>Iteration: n_iter, loss: 646.329833984375</em></pre><pre name="3642" id="3642" class="graf graf--pre graf-after--pre">x = val_tfms.denorm(np.rollaxis(to_np(opt_img_v.data),1,4))[0]<br>plt.figure(figsize=(9,9))<br>plt.imshow(x, interpolation='lanczos')<br>plt.axis('off');</pre><figure name="6bce" id="6bce" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_ElVcIGvL7cWUMhoftkw9-g.png"></figure><pre name="adc6" id="adc6" class="graf graf--pre graf-after--figure"><strong class="markup--strong markup--pre-strong">for</strong> sf <strong class="markup--strong markup--pre-strong">in</strong> sfs: sf.close()</pre><p name="1147" id="1147" class="graf graf--p graf-after--pre">And holy crap, it actually looks good. So I think that’s pretty awesome. The main take away here is if you want to solve something with a neural network, all you’ve got to do is set up a loss function and then optimize something. And the loss function is something which a lower number is something that you’re happier with. Because then when you optimize it, it’s going to make that number as low as you can, and it’ll do what you wanted it to do. So here, Gatys came up with the loss function that does a good job of being a smaller number when it looks like the thing we want it to look like, and it looks like the style of the thing we want to be in the style of. That’s all we had to do.</p><p name="96f8" id="96f8" class="graf graf--p graf-after--p">What it actually comes to it [<a href="https://youtu.be/xXXiC4YRGrQ?t=1h59m10s" data-href="https://youtu.be/xXXiC4YRGrQ?t=1h59m10s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:59:10</a>], apart from implementing Gram MSE loss which was like 6 lines of code if that, that’s our loss function:</p><figure name="d1ff" id="d1ff" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_si7enkSgWg1k6EKmx-2ijQ.png"></figure><p name="4bf4" id="4bf4" class="graf graf--p graf-after--figure">Pass it to our optimizer, and wait about 5 seconds, and we are done. And remember, we could do a batch of these at a time, so we could wait 5 seconds and 64 of these will be done. So I think that’s really interesting and since this paper came out, it has really inspired a lot of interesting work. To me though, most of the interesting work hasn’t happened yet because to me, the interesting work is the work where you combine human creativity with these kinds of tools. I haven’t seen much in the way of tools that you can download or use where the artist is in control and can kind of do things interactively. It’s interesting talking to the guys at <a href="https://magenta.tensorflow.org/" data-href="https://magenta.tensorflow.org/" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Google Magenta</a> project which is their creative AI project, all of the stuff they are doing with music is specifically about this. It’s building tools that musicians can use to perform in real time. And you’ll see much more of that on the music space thanks to Magenta. If you go to their website, there’s all kinds of things where you can press the buttons to actually change the drum beats, melodies, keys, etc. You can definitely see Adobe or Nvidia is starting to release little prototypes and starting to do this but this kind of creative AI explosion hasn’t happened yet. I think we have pretty much all the technology we need but no one’s put it together into a thing and said “look at the thing I built and look at the stuff that people built with my thing.” So that’s just a huge area of opportunity.</p><p name="db95" id="db95" class="graf graf--p graf-after--p">So the paper that I mentioned at the start of class in passing [<a href="https://youtu.be/xXXiC4YRGrQ?t=2h1m16s" data-href="https://youtu.be/xXXiC4YRGrQ?t=2h1m16s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:01:16</a>] — the one where we can add Captain America’s shield to arbitrary paintings basically used this technique. The trick was though some minor tweaks to make the pasted Captain America shield blend in nicely. But that paper is only a couple of days old, so that would be a really interesting project to try because you can use all this code. It really does leverage this approach. Then you could start by making the content image be like the painting with the shield and then the style image could be the painting without the shield. That would be a good start, and then you could see what specific problems they try to solve in this paper to make it better. But you could have a start on it right now.</p><p name="7167" id="7167" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: Two questions — earlier there were a number of people that expressed interest in your thoughts on Pyro and probabilistic programming [<a href="https://youtu.be/xXXiC4YRGrQ?t=2h2m34s" data-href="https://youtu.be/xXXiC4YRGrQ?t=2h2m34s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:02:34</a>]. So TensorFlow has now got this TensorFlow probability or something. There’s a bunch of probabilistic programming framework out there. I think they are intriguing, but as yet unproven in the sense that I haven’t seen anything done with any probabilistic programming system which hasn’t been done better without them. The basic premise is that it allows you to create more of a model of how you think the world works and then plug in the parameters. So back when I used to work in management consulting 20 years ago, we used to do a lot of stuff where we would use a spreadsheet and then we would have these Monte Carlo simulation plugins — there was one called At Risk(?) and one called Crystal Ball. I don’t know if they still exist decades later. Basically they would let you change a spreadsheet cell to say this is not a specific value but it actually represents a distribution of values with this mean and the standard deviation or it’s got this distribution, and then you would hit a button and the spreadsheet would recalculate a thousand times pulling random numbers from these distributions and show you the distribution of your outcome that might be profit or market share or whatever. We used them all the time back then. Apparently feel that a spreadsheet is a more obvious place to do that kind of work because you can see it all much more naturally, but I don’t know. We’ll see. At this stage, I hope it turns out to be useful because I find it very appealing and it appeals to as I say the kind of work I used to do a lot of. There’s actually whole practices around this stuff they used to call system dynamics which really was built on top of this kind of stuff, but it’s not quite gone anywhere.</p><p name="9acb" id="9acb" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: Then there was a question about pre-training for generic style transfer [<a href="https://youtu.be/xXXiC4YRGrQ?t=2h4m57s" data-href="https://youtu.be/xXXiC4YRGrQ?t=2h4m57s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:04:57</a>]. I don’t think you can pre-train for a generic style, but you can pre-train for a generic photo for a particular style which is where we are going to get to. Although, it may end up being a homework. I haven’t decided yet. But I’m going to do all the pieces.</p><p name="11db" id="11db" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: Please ask him to talk about multi-GPU [<a href="https://youtu.be/xXXiC4YRGrQ?t=2h5m31s" data-href="https://youtu.be/xXXiC4YRGrQ?t=2h5m31s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:05:31</a>]. Oh yeah, I haven’t had a slide about that. We’re about to hit it.</p><p name="bd2a" id="bd2a" class="graf graf--p graf-after--p">Before we do, just another interesting picture from the Gatys’ paper. They’ve got a few more just didn’t fit in my slide but different convolutional layers for the style. Different style to content ratios, and here’s the different images. Obviously this isn’t Van Gogh any more, this is a different combination. So you can see, if you just do all style, you don’t see any image. If you do lots of content, but you use low enough convolutional layer, it looks okay but the back ground is kind of dumb. So you kind of want somewhere in the middle. So you can play around with it and experiment, but also use the paper to help guide you.</p><figure name="162b" id="162b" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_x_UN319I-Ppe3xHnvzqgag.png"></figure><h4 name="983f" id="983f" class="graf graf--h4 graf-after--figure">The Math [<a href="https://youtu.be/xXXiC4YRGrQ?t=2h6m33s" data-href="https://youtu.be/xXXiC4YRGrQ?t=2h6m33s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">2:06:33</a>]</h4><p name="098e" id="098e" class="graf graf--p graf-after--h4">Actually, I think I might work on the math now and we’ll talk about multi GPU and super resolution next week because this is from the paper and one of the things I really do want you to do after we talk about a paper is to read the paper and then ask questions on the forum anything that’s not clear. But there’s a key part of this paper which I wanted to talk about and discuss how to interpret it. So the paper says, we’re going to be given an input image <em class="markup--em markup--p-em">x</em> and this little thing means normally it means it’s a vector, Rachel, but this one is a matrix. I guess it could mean either. I don’t know. Normally small letter bold means vector or a small letter with an arrow on top means vector. And normally big letter means matrix or small letter with two arrows on top means matrix. In this case, our image is a matrix. We are going to basically treat it as a vector, so maybe we’re just getting ahead of ourselves.</p><figure name="d655" id="d655" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_kU-HMZL4kI2So7WV5xow6g.png"></figure><p name="c5d7" id="c5d7" class="graf graf--p graf-after--figure">So we’ve got an input image <em class="markup--em markup--p-em">x</em> and it can be encoded in a particular layer of the CNN by the filter responses (i.e. activations). Filter responses are activations. Hopefully, that’s something you all understand. That’s basically what a CNN does is it produces layers of activations. A layer has a bunch of filters which produce a number of channels. This year says that layer number L has capital N<em class="markup--em markup--p-em">l</em> filters. Again, this capital does not mean matrix. So I don’t know, math notation is so inconsistent. So capital Nl distinct filters at layer L which means it has also that many feature maps. So make sure you can see this letter Nl is the same as this letter. So you’ve got to be very careful to read the letters and recognize it’s like snap, that’s the same letter as that. So obviously, Nl filters create create Nl feature maps or channels, each one of size M<em class="markup--em markup--p-em">l</em> (okay, I can see this is where the unrolling is happening). So this is like M[<em class="markup--em markup--p-em">l</em>] in numpy notation. It’s the <em class="markup--em markup--p-em">l</em>th layer. So M for the <em class="markup--em markup--p-em">l</em>th layer. The size is height times width — so we flattened it out. So the responses in a layer l can be stored in a matrix F (and now the <em class="markup--em markup--p-em">l</em> goes at the top for some reason). So this is not f^<em class="markup--em markup--p-em">l</em>, it’s just another indexing. We are just moving it around for fun. This thing here where we say it’s an element of R — this is a special R meaning the real numbers N times M (this is saying that the dimensions of this is N by M). So this is really important, you don’t move on. It’s just like with PyTorch, making sure that you understand the rank and size of your dimensions first, same with math. These are the bits where you stop and think why is it N by M? N is a number of filters, M is height by width. So do you remember that thing when we did&nbsp;<code class="markup--code markup--p-code">.view(b*c, -1)</code>? Here that is. So try to map the code to the math. So F is <code class="markup--code markup--p-code">x</code>:</p><figure name="0765" id="0765" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_uZYTy9gDHiXBhjRhbtbabg.png"></figure><p name="60af" id="60af" class="graf graf--p graf-after--figure">If I was nicer to you, I would have used the same letters as the paper. But I was too busy getting this darn thing working to do that carefully. So you can go back and rename it as capital F.</p><p name="db83" id="db83" class="graf graf--p graf-after--p">So this is why we moved the L to the top is because we’re now going to have some more indexing. Where else in Numpy or PyTorch, we index things by square brackets and then lots of things with commas between. The approach in math is to surround your letter by little letters all around it — just throw them up there everywhere. So here, F<em class="markup--em markup--p-em">l</em> is the <em class="markup--em markup--p-em">l</em>th layer of F and then <em class="markup--em markup--p-em">ij</em> is the activation of the <em class="markup--em markup--p-em">i</em>th filter at position <em class="markup--em markup--p-em">j</em> of layer <em class="markup--em markup--p-em">l</em>. So position <em class="markup--em markup--p-em">j</em> is up to size M which is up to size height by width. This is the kind of thing that would be easy to get confused. Often you’d see an <em class="markup--em markup--p-em">ij</em> and assume that’s indexing into a position of an image like height by width, but it’s totally not, is it? It’s indexing into channel by flattened image. It even tells you — it’s the <em class="markup--em markup--p-em">i</em>th filter/channel in the <em class="markup--em markup--p-em">j</em>th position in the flattened out image in layer <em class="markup--em markup--p-em">l</em>. So you’re not gonna be able to get any further in the paper unless you understand what F is. That’s why these are the bits where you stop and make sure you’re comfortable.</p><p name="7d8a" id="7d8a" class="graf graf--p graf-after--p">So now, the content loss, I’m not going to spend much time on but basically we are going to just check out the values of the activations vs. the predictions squared [<a href="https://youtu.be/xXXiC4YRGrQ?t=2h12m3s" data-href="https://youtu.be/xXXiC4YRGrQ?t=2h12m3s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:12:03</a>]. So there’s our content loss. The style loss will be much the same thing, but using the Gram matrix G:</p><figure name="16cd" id="16cd" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_v6S37SK4jm1o-aJXUFysAw.png"></figure><p name="8e99" id="8e99" class="graf graf--p graf-after--figure">I really wanted to show you this one. I think it’s super. Sometimes I really like things you can do in math notation, and they’re things that you can also generally do in J and APL which is this kind of this implicit loop going on here. What this is saying is there’s a whole bunch of values of <em class="markup--em markup--p-em">i</em> and a whole bunch of values of <em class="markup--em markup--p-em">j</em>, and I’m going to define G for all of them. And there’s whole bunch of values of <em class="markup--em markup--p-em">l</em> as well, and I’m going to define G for all of those as well. So for all of my G at every <em class="markup--em markup--p-em">l</em> of every <em class="markup--em markup--p-em">i</em> at every <em class="markup--em markup--p-em">j</em>, it’s going to be equal to something. And you can see that something has an <em class="markup--em markup--p-em">i</em> and a <em class="markup--em markup--p-em">j</em> and a <em class="markup--em markup--p-em">l</em>, matching G, and it also has a <em class="markup--em markup--p-em">k</em> and that’s part of the sum. So what’s going on here? Well, it’s saying that my Gram matrix in layer <em class="markup--em markup--p-em">l</em> for the <em class="markup--em markup--p-em">i</em>th position in one axis and the <em class="markup--em markup--p-em">j</em>th position in another axis is equal to my F matrix (so my flattened out matrix) for the <em class="markup--em markup--p-em">i</em>th channel in that layer vs. the <em class="markup--em markup--p-em">j</em>th channel in the same layer, then I’m going to sum over. We are going to take the <em class="markup--em markup--p-em">k</em>th position and multiply them together and then add them all up. So that’s exactly what we just did before when we calculated our Gram matrix. So this, there’s a lot going on because of some, to me, very neat notation — which is there are three implicit loops all going on at the same time, plus one explicit loop in the sum, then they all work together to create this Gram matrix for every layer. So let’s go back and see if you can match this. Sl all that’s happening all at once which is pretty great.</p><p name="50b4" id="50b4" class="graf graf--p graf-after--p graf--trailing">That’s it. So next week, we’re going to be looking at a very similar approach, basically doing style transfer all over again but in a way where we actually going to train a neural network to do it for us rather than having to do the optimization. We’ll also see that you can do the same thing to do super resolution. And we are also going to go back and revisit some of the SSD stuff as well as doing some segmentation. So if you’ve forgotten SSD, might be worth doing a little bit of revision this week. Alright, thanks everybody. See you next week.</p><hr class="section-divider"><p name="bec5" id="bec5" class="graf graf--p graf--leading graf--trailing">Lessons: <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" class="markup--anchor markup--p-anchor" target="_blank">1</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" class="markup--anchor markup--p-anchor" target="_blank">2</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" class="markup--anchor markup--p-anchor" target="_blank">3</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" class="markup--anchor markup--p-anchor" target="_blank">4</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" class="markup--anchor markup--p-anchor" target="_blank">5</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" class="markup--anchor markup--p-anchor" target="_blank">6</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" class="markup--anchor markup--p-anchor" target="_blank">7</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" class="markup--anchor markup--p-anchor" target="_blank">8</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" class="markup--anchor markup--p-anchor" target="_blank">9</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" class="markup--anchor markup--p-anchor" target="_blank">10</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" class="markup--anchor markup--p-anchor" target="_blank">11</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" class="markup--anchor markup--p-anchor" target="_blank">12</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" class="markup--anchor markup--p-anchor" target="_blank"><strong class="markup--strong markup--p-strong">13</strong></a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" class="markup--anchor markup--p-anchor" target="_blank">14</a></p></body></html>