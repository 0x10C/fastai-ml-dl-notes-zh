
<!-- saved from url=(0064)file:///C:/Users/asus/Desktop/fastai-ml-dl-notes-zh/en/dl12.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></head><body><hr class="section-divider"><h1 name="5fe8" id="5fe8" class="graf graf--h3 graf--leading graf--title">Deep Learning 2: Part 2 Lesson&nbsp;12</h1><p name="5e74" id="5e74" class="graf graf--p graf-after--h3"><em class="markup--em markup--p-em">My personal notes from </em><a href="http://www.fast.ai/" data-href="http://www.fast.ai/" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener noopener noopener nofollow noopener nofollow noopener noopener nofollow noopener nofollow noopener" target="_blank"><em class="markup--em markup--p-em">fast.ai course</em></a><em class="markup--em markup--p-em">. These notes will continue to be updated and improved as I continue to review the course to “really” understand it. Much appreciation to </em><a href="https://twitter.com/jeremyphoward" data-href="https://twitter.com/jeremyphoward" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener noopener noopener nofollow noopener nofollow noopener noopener nofollow noopener nofollow noopener" target="_blank"><em class="markup--em markup--p-em">Jeremy</em></a><em class="markup--em markup--p-em"> and </em><a href="https://twitter.com/math_rachel" data-href="https://twitter.com/math_rachel" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener noopener noopener nofollow noopener nofollow noopener noopener nofollow noopener nofollow noopener" target="_blank"><em class="markup--em markup--p-em">Rachel</em></a><em class="markup--em markup--p-em"> who gave me this opportunity to learn.</em></p><p name="5a20" id="5a20" class="graf graf--p graf-after--p graf--trailing">Lessons: <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" class="markup--anchor markup--p-anchor" target="_blank">1</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" class="markup--anchor markup--p-anchor" target="_blank">2</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" class="markup--anchor markup--p-anchor" target="_blank">3</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" class="markup--anchor markup--p-anchor" target="_blank">4</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" class="markup--anchor markup--p-anchor" target="_blank">5</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" class="markup--anchor markup--p-anchor" target="_blank">6</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" class="markup--anchor markup--p-anchor" target="_blank">7</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" class="markup--anchor markup--p-anchor" target="_blank">8</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" class="markup--anchor markup--p-anchor" target="_blank">9</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" class="markup--anchor markup--p-anchor" target="_blank">10</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" class="markup--anchor markup--p-anchor" target="_blank">11</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" class="markup--anchor markup--p-anchor" target="_blank"><strong class="markup--strong markup--p-strong">12</strong></a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" class="markup--anchor markup--p-anchor" target="_blank">13</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" class="markup--anchor markup--p-anchor" target="_blank">14</a></p><hr class="section-divider"><figure name="8b44" id="8b44" class="graf graf--figure graf--leading"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_iFOmwPIB-BHiM7G4ttDb9w.png"></figure><h3 name="de02" id="de02" class="graf graf--h3 graf-after--figure">Generative Adversarial Networks&nbsp;(GANs)</h3><p name="59cb" id="59cb" class="graf graf--p graf-after--h3"><a href="https://youtu.be/ondivPiwQho" data-href="https://youtu.be/ondivPiwQho" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Video</a> / <a href="http://forums.fast.ai/t/part-2-lesson-12-in-class/15023" data-href="http://forums.fast.ai/t/part-2-lesson-12-in-class/15023" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">Forum</a></p><p name="fc88" id="fc88" class="graf graf--p graf-after--p">Very hot technology but definitely deserving to be in the cutting edge deep learning part of the course because they are not quite proven to be necessarily useful for anything but they are nearly there and will definitely get there. We are going to focus on the things where they are definitely going to be useful in practice and there is a number of areas where they may turn out to be useful but we don’t know yet. So I think the area that they are definitely going to be useful in practice is the kind of thing you see on the left of the slide — which is for example turning drawing into rendered pictures. This comes from <a href="https://arxiv.org/abs/1804.04732" data-href="https://arxiv.org/abs/1804.04732" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">a paper that just came out 2 days ago</a>, so there’s a very active research going on right now.</p><p name="c131" id="c131" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">From the last lecture [</strong><a href="https://youtu.be/ondivPiwQho?t=1m4s" data-href="https://youtu.be/ondivPiwQho?t=1m4s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong">1:04</strong></a><strong class="markup--strong markup--p-strong">]: </strong>One of our diversity fellows Christine Payne has a master’s in medicine from Stanford and so she had an interest in thinking what it would look like if we built a language model of medicine. One of the things we briefly touched on back in lesson 4 but didn’t really talk much about last time is this idea that you can actually seed a generative language model which means you’ve trained a language model on some corpus and then you are going to generate some text from that language model. You can start off by feeding it a few words to say “here is the first few words to create the hidden state in the language model and generate from there please. Christine did something clever which was to seed it with a question and repeat the question three times and let it generate from there. She fed a language model lots of different medical texts and fed in questions as you see below:</p><figure name="546e" id="546e" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_v6gjjQ9Eu_yyJnj5qJoMyA.png"></figure><p name="2c31" id="2c31" class="graf graf--p graf-after--figure">What Jeremy found interesting about this is it’s pretty close to being a believable answer to the question for people without master’s in medicine. But it has no bearing on reality whatsoever. He thinks it is an interesting kind of ethical and user experience quandary. Jeremy is involved in a company called doc.ai that’s trying to doing a number of things but in the end provide an app for doctors and patients which can help create a conversational user interface around helping them with their medical issues. He’s been continually saying to the software engineers on that team please don’t try to create a generative model using LSTM or something because they are going to be really good at creating bad advice that sounds impressive — kind of like political pundits or tenured professor who can say bullcrap with great authority. So he thought it was really interesting experiment. If you’ve done some interesting experiments, share them in the forum, blog, Twitter. Let people know about it and get noticed by awesome people.</p><h4 name="7f35" id="7f35" class="graf graf--h4 graf-after--p">CIFAR10 [<a href="https://youtu.be/ondivPiwQho?t=5m26s" data-href="https://youtu.be/ondivPiwQho?t=5m26s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">5:26</a>]</h4><p name="3970" id="3970" class="graf graf--p graf-after--h4">Let’s talk about CIFAR10 and the reason is that we are going to be looking at some more bare-bones PyTorch stuff today to build these generative adversarial models. There is no fastai support to speak up at all for GANs at the moment — there will be soon enough but currently there isn’t so we are going to be building a lot of models from scratch. It’s been a while since we’ve done much serious model building. We looked at CIFAR10 in the part 1 of the course and we built something which was getting about 85% accuracy and took a couple hours to train. Interestingly, there is a competition going on now to see who can actually train CIFAR10 the fastest (<a href="https://dawn.cs.stanford.edu/benchmark/#cifar10-train-time" data-href="https://dawn.cs.stanford.edu/benchmark/#cifar10-train-time" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">DAWN</a>), and the goal is to get it to train to 94% accuracy. It would be interesting to see if we can build an architecture that can get to 94% accuracy because that is a lot better than our previous attempt. Hopefully in doing so we will learn something about creating good architectures, that will be then useful for looking at GANs today. Also it is useful because Jeremy has been looking much more deeply into the last few years’ papers about different kinds of CNN architectures and realizes that a lot of the insights in those papers are not being widely leveraged and clearly not widely understood. So he wants to show you what happens if we can leverage so me of that understanding.</p><h4 name="2121" id="2121" class="graf graf--h4 graf-after--p"><a href="https://github.com/fastai/fastai/blob/master/courses/dl2/cifar10-darknet.ipynb" data-href="https://github.com/fastai/fastai/blob/master/courses/dl2/cifar10-darknet.ipynb" class="markup--anchor markup--h4-anchor" rel="noopener nofollow" target="_blank">cifar10-darknet.ipynb</a> [<a href="https://youtu.be/ondivPiwQho?t=7m17s" data-href="https://youtu.be/ondivPiwQho?t=7m17s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">7:17</a>]</h4><p name="2948" id="2948" class="graf graf--p graf-after--h4">The notebook is called <a href="https://pjreddie.com/darknet/" data-href="https://pjreddie.com/darknet/" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">darknet</a> because the particular architecture we are going to look at is very close to the darknet architecture. But you will see in the process that the darknet architecture as in not the whole YOLO v3 end-to-end thing but just the part of it that they pre-trained on ImageNet to do classification. It’s almost like the most generic simple architecture you could come up with, so it’s a really great starting point for experiments. So we will call it “darknet” but it’s not quite that and you can fiddle around with it to create things that definitely aren’t darknet. It’s really just the basis of nearly any modern ResNet based architecture.</p><p name="252d" id="252d" class="graf graf--p graf-after--p">CIFAR10 is a fairly small dataset [<a href="https://youtu.be/ondivPiwQho?t=8m6s" data-href="https://youtu.be/ondivPiwQho?t=8m6s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">8:06</a>]. The images are only 32 by 32 in size, and it’s a great dataset to work with because:</p><ul class="postList"><li name="7993" id="7993" class="graf graf--li graf-after--p">You can train it relatively quickly unlike ImageNet</li><li name="892c" id="892c" class="graf graf--li graf-after--li">A relatively small amount of data</li><li name="5091" id="5091" class="graf graf--li graf-after--li">Actually quite hard to recognize the images because 32 by 32 is too small to easily see what’s going on.</li></ul><p name="756e" id="756e" class="graf graf--p graf-after--li">It is an under-appreciated dataset because it’s old. Who wants to work with small old dataset when they could use their entire server room to process something much bigger. But it’s is a really great dataset to focus on.</p><p name="fec6" id="fec6" class="graf graf--p graf-after--p">Go ahead and import our usual stuff and we are going to try and build a network from scratch to train this with[<a href="https://youtu.be/ondivPiwQho?t=8m58s" data-href="https://youtu.be/ondivPiwQho?t=8m58s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">8:58</a>].</p><pre name="5dd5" id="5dd5" class="graf graf--pre graf-after--p">%matplotlib inline<br>%reload_ext autoreload<br>%autoreload 2</pre><pre name="144a" id="144a" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">fastai.conv_learner</strong> <strong class="markup--strong markup--pre-strong">import</strong> *<br>PATH = Path("data/cifar10/")<br>os.makedirs(PATH,exist_ok=<strong class="markup--strong markup--pre-strong">True</strong>)</pre><p name="1161" id="1161" class="graf graf--p graf-after--pre">A really good exercise for anybody who is not 100% confident with their broadcasting and PyTorch basic skill is figure out how Jeremy came up with these <code class="markup--code markup--p-code">stats</code> numbers. These numbers are the averages and standard deviations for each channel in CIFAR10. Try and make sure you can recreate those numbers and see if you can do it with no more than a couple of lines of code (no loops!).</p><p name="e0f3" id="e0f3" class="graf graf--p graf-after--p">Because these are fairly small, we can use a larger batch size than usual and the size of these images is 32 [<a href="https://youtu.be/ondivPiwQho?t=9m46s" data-href="https://youtu.be/ondivPiwQho?t=9m46s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">9:46</a>].</p><pre name="e265" id="e265" class="graf graf--pre graf-after--p">classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', <br>           'horse', 'ship', 'truck')<br>stats = (np.array([ 0.4914 ,  0.48216,  0.44653]), <br>         np.array([ 0.24703,  0.24349,  0.26159]))<br><br>num_workers = num_cpus()//2<br>bs=256<br>sz=32</pre><p name="e717" id="e717" class="graf graf--p graf-after--pre">Transformations [<a href="https://youtu.be/ondivPiwQho?t=9m57s" data-href="https://youtu.be/ondivPiwQho?t=9m57s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">9:57</a>], normally we have this standard set of side_on transformations we use for photos of normal objects. We are not going to use that here because these images are so small that trying to rotate a 32 by 32 image a bit is going to introduce a lot of blocky distortions. So the standard transformations that people tend to use is a random horizontal flip and then we add 4 pixels (size divided by 8) of padding on each side. One thing which works really well is by default fastai does not add black padding which many other libraries do. Fastai takes the last 4 pixels of the existing photo and flip it and reflect it, and we find that we get much better results by using reflection padding by default. Now that we have 40 by 40 image, this set of transforms in training will randomly pick a 32 by 32 crops, so we get a little bit of variation but not heaps. Wo we can use the normal <code class="markup--code markup--p-code">from_paths</code> to grab our data.</p><pre name="b669" id="b669" class="graf graf--pre graf-after--p">tfms = tfms_from_stats(stats, sz, aug_tfms=[RandomFlip()], <br>                       pad=sz//8)<br>data = ImageClassifierData.from_paths(PATH, val_name='test', <br>                                      tfms=tfms, bs=bs)</pre><p name="84f9" id="84f9" class="graf graf--p graf-after--pre">Now we need an architecture and we are going to create one which fits in one screen [<a href="https://youtu.be/ondivPiwQho?t=11m7s" data-href="https://youtu.be/ondivPiwQho?t=11m7s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">11:07</a>]. This is from scratch. We are using predefined <code class="markup--code markup--p-code">Conv2d</code>, <code class="markup--code markup--p-code">BatchNorm2d</code>, <code class="markup--code markup--p-code">LeakyReLU</code> modules but we are not using any blocks or anything. The entire thing is in one screen so if you are ever wondering can I understand a modern good quality architecture, absolutely! Let’s study this one.</p><pre name="7ad7" id="7ad7" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> conv_layer(ni, nf, ks=3, stride=1):<br>    <strong class="markup--strong markup--pre-strong">return</strong> nn.Sequential(<br>        nn.Conv2d(ni, nf, kernel_size=ks, bias=<strong class="markup--strong markup--pre-strong">False</strong>, stride=stride,<br>                  padding=ks//2),<br>        nn.BatchNorm2d(nf, momentum=0.01),<br>        nn.LeakyReLU(negative_slope=0.1, inplace=<strong class="markup--strong markup--pre-strong">True</strong>))</pre><pre name="f326" id="f326" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">ResLayer</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, ni):<br>        super().__init__()<br>        self.conv1=conv_layer(ni, ni//2, ks=1)<br>        self.conv2=conv_layer(ni//2, ni, ks=3)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x): <strong class="markup--strong markup--pre-strong">return</strong> x.add_(self.conv2(self.conv1(x)))</pre><pre name="b4fa" id="b4fa" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">Darknet</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> make_group_layer(self, ch_in, num_blocks, stride=1):<br>        <strong class="markup--strong markup--pre-strong">return</strong> [conv_layer(ch_in, ch_in*2,stride=stride)<br>               ] + [(ResLayer(ch_in*2)) <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(num_blocks)]<br><br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, num_blocks, num_classes, nf=32):<br>        super().__init__()<br>        layers = [conv_layer(3, nf, ks=3, stride=1)]<br>        <strong class="markup--strong markup--pre-strong">for</strong> i,nb <strong class="markup--strong markup--pre-strong">in</strong> enumerate(num_blocks):<br>            layers += self.make_group_layer(nf, nb, stride=2-(i==1))<br>            nf *= 2<br>        layers += [nn.AdaptiveAvgPool2d(1), Flatten(), <br>                   nn.Linear(nf, num_classes)]<br>        self.layers = nn.Sequential(*layers)<br>    <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x): <strong class="markup--strong markup--pre-strong">return</strong> self.layers(x)</pre><p name="1af3" id="1af3" class="graf graf--p graf-after--pre">The basic starting point with an architecture is to say it’s a stacked bunch of layers and generally speaking there is going to be some kind of hierarchy of layers [<a href="https://youtu.be/ondivPiwQho?t=11m51s" data-href="https://youtu.be/ondivPiwQho?t=11m51s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">11:51</a>]. At the very bottom level, there is things like a convolutional layer and a batch norm layer, but any time you have a convolution, you are probably going to have some standard sequence. Normally it’s going to be:</p><ol class="postList"><li name="1578" id="1578" class="graf graf--li graf-after--p">conv</li><li name="6725" id="6725" class="graf graf--li graf-after--li">batch norm</li><li name="3cae" id="3cae" class="graf graf--li graf-after--li">a nonlinear activation (e.g. ReLU)</li></ol><p name="8864" id="8864" class="graf graf--p graf-after--li">We will start by determining what our basic unit is going to be and define it in a function (<code class="markup--code markup--p-code">conv_layer</code>) so we don’t have to worry about trying to keep everything consistent and it will make everything a lot simpler.</p><p name="e7db" id="e7db" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Leaky Relu</strong> [<a href="https://youtu.be/ondivPiwQho?t=12m43s" data-href="https://youtu.be/ondivPiwQho?t=12m43s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">12:43</a>]:</p><figure name="c423" id="c423" class="graf graf--figure graf--layoutOutsetCenter graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_p1xIcvOk2F-EWWDgTTfW7Q.png"></figure><p name="0c77" id="0c77" class="graf graf--p graf-after--figure">The gradient of Leaky ReLU (where <em class="markup--em markup--p-em">x</em> &lt; 0) varies but something about 0.1 or 0.01 is common. The idea behind it is that when you are in the negative zone, you don’t end up with a zero gradient which makes it very hard to update it. In practice, people have found Leaky ReLU more useful on smaller datasets and less useful in big datasets. But it is interesting that for the <a href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" data-href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">YOLO v3</a> paper, they used Leaky ReLU and got great performance from it. It rarely makes things worse and it often makes things better. So it’s probably not bad if you need to create your own architecture to make that your default go-to is to use Leaky ReLU.</p><p name="eeaf" id="eeaf" class="graf graf--p graf-after--p">You’ll notice that we don’t define PyTorch module in <code class="markup--code markup--p-code">conv_layer</code>, we just do <code class="markup--code markup--p-code">nn.Sequential</code> [<a href="https://youtu.be/ondivPiwQho?t=14m7s" data-href="https://youtu.be/ondivPiwQho?t=14m7s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">14:07</a>]. This is something if you read other people’s PyTorch code, it’s really underutilized. People tend to write everything as a PyTorch module with <code class="markup--code markup--p-code">__init__</code> and <code class="markup--code markup--p-code">forward</code>, but if the thing you want is just a sequence of things one after the other, it’s much more concise and easy to understand to make it a <code class="markup--code markup--p-code">Sequential</code>.</p><p name="2e5f" id="2e5f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Residual block</strong> [<a href="https://youtu.be/ondivPiwQho?t=14m40s" data-href="https://youtu.be/ondivPiwQho?t=14m40s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">14:40</a>]: As mentioned before that there is generally a number of hierarchies of units in most modern networks, and we know now that the next level in this unit hierarchy for ResNet is the ResBlock or residual block (see <code class="markup--code markup--p-code">ResLayer</code>). Back when we last did CIFAR10, we oversimplified this (cheated a little bit). We had <code class="markup--code markup--p-code">x</code> coming in and we put that through a <code class="markup--code markup--p-code">conv</code>, then we added it back up to <code class="markup--code markup--p-code">x</code> to go out. In the real ResBlock, there are two of them. When we say “conv” we are using it as a shortcut for our <code class="markup--code markup--p-code">conv_layer</code> (conv, batch norm, ReLU).</p><figure name="7287" id="7287" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_unH5bhpWH7HfLCG8WozNfA.png"></figure><p name="ff4a" id="ff4a" class="graf graf--p graf-after--figure">One interesting insight here is the number of channels in these convolutions [<a href="https://youtu.be/ondivPiwQho?t=16m47s" data-href="https://youtu.be/ondivPiwQho?t=16m47s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">16:47</a>]. We have some <code class="markup--code markup--p-code">ni</code> coming in (some number of input channels/filters). The way the darknet folks set things up is they make every one of these Res layers spit out the same number of channels that came in, and Jeremy liked that and that’s why he used it in <code class="markup--code markup--p-code">ResLayer</code> because it makes life simpler. The first conv halves the number of channels, and then second conv doubles it again. So you have this funneling effect where 64 channels coming in, squished down with a first conv down to 32 channels, and then taken back up again to 64 channels coming out.</p><p name="730e" id="730e" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question:</strong> Why is <code class="markup--code markup--p-code">inplace=True</code> in the <code class="markup--code markup--p-code">LeakyReLU </code>[<a href="https://youtu.be/ondivPiwQho?t=17m54s" data-href="https://youtu.be/ondivPiwQho?t=17m54s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">17:54</a>]? Thanks for asking! A lot of people forget this or don’t know about it, but this is a really important memory technique. If you think about it, this <code class="markup--code markup--p-code">conv_layer</code>, it’s the lowest level thing, so pretty much everything in our ResNet once it’s all put together is going to be many <code class="markup--code markup--p-code">conv_layer</code>’s. If you do not have <code class="markup--code markup--p-code">inplace=True</code>, it’s going to create a whole separate piece of memory for the output of the ReLU so it’s going to allocate a whole bunch of memory that is totally unnecessary. Another example is that the original <code class="markup--code markup--p-code">forward</code> in <code class="markup--code markup--p-code">ResLayer</code> looked like:</p><pre name="b54a" id="b54a" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> forward(self, x): <strong class="markup--strong markup--pre-strong">return</strong> x + self.conv2(self.conv1(x))</pre><p name="13cf" id="13cf" class="graf graf--p graf-after--pre">Hopefully some of you might remember that in PyTorch pretty much every every function has an underscore suffix version which tells it to do it in-place. <code class="markup--code markup--p-code">+</code> is equivalent to <code class="markup--code markup--p-code">add</code> and in-place version of <code class="markup--code markup--p-code">add</code> is <code class="markup--code markup--p-code">add_</code> so this will reduce memory usage:</p><pre name="9e76" id="9e76" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> forward(self, x): <strong class="markup--strong markup--pre-strong">return</strong> x.add_(self.conv2(self.conv1(x)))</pre><p name="a52c" id="a52c" class="graf graf--p graf-after--pre">These are really handy little tricks. Jeremy forgot the <code class="markup--code markup--p-code">inplace=True</code> at first but he was having to decrease the batch size to much lower amounts and it was driving him crazy — then he realized that that was missing. You can also do that with dropout if you have dropout. Here are what to look out for:</p><ul class="postList"><li name="4c6f" id="4c6f" class="graf graf--li graf-after--p">Dropout</li><li name="83da" id="83da" class="graf graf--li graf-after--li">All the activation functions</li><li name="587c" id="587c" class="graf graf--li graf-after--li">Any arithmetic operation</li></ul><p name="a6ae" id="a6ae" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">Question</strong>: In ResNet, why is bias usually set to False in conv_layer [<a href="https://youtu.be/ondivPiwQho?t=19m53s" data-href="https://youtu.be/ondivPiwQho?t=19m53s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">19:53</a>]? Immediately after the <code class="markup--code markup--p-code">Conv</code>, there is a <code class="markup--code markup--p-code">BatchNorm</code>. Remember, <code class="markup--code markup--p-code">BatchNorm</code> has 2 learnable parameters for each activation — the thing you multiply by and the thing you add. If we had bias in <code class="markup--code markup--p-code">Conv</code> and then add another thing in <code class="markup--code markup--p-code">BatchNorm</code>, we would be adding two things which is totally pointless — that’s two weights where one would do. So if you have a BatchNorm after a <code class="markup--code markup--p-code">Conv</code>, you can either tell <code class="markup--code markup--p-code">BatchNorm</code> not to include the add bit or easier is to tell <code class="markup--code markup--p-code">Conv</code> not to include the bias. There is no particular harm, but again, it’s going to take more memory because that is more gradients that it has to keep track of, so best to avoid.</p><p name="d926" id="d926" class="graf graf--p graf-after--p">Also another little trick is, most people’s <code class="markup--code markup--p-code">conv_layer</code>’s have padding as a parameter [<a href="https://youtu.be/ondivPiwQho?t=21m11s" data-href="https://youtu.be/ondivPiwQho?t=21m11s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">21:11</a>]. But generally speaking, you should be able to calculate the padding easily enough. If you have a kernel size of 3, then obviously that is going to overlap by one unit on each side, so we want padding of 1. Or else, if it’s kernel size of 1, then we don’t need any padding. So in general, padding of kernel size “integer divided” by 2 is what you need. There’re some tweaks sometimes but in this case, this works perfectly well. Again, trying to simplify my code by having the computer calculate stuff for me rather than me having to do it myself.</p><figure name="0768" id="0768" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_Pc3_ut-tOnPm5FLdYqRrOA.png"></figure><p name="8cc8" id="8cc8" class="graf graf--p graf-after--figure">Another thing with the two <code class="markup--code markup--p-code">conv_layer</code>’s [<a href="https://youtu.be/ondivPiwQho?t=22m14s" data-href="https://youtu.be/ondivPiwQho?t=22m14s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">22:14</a>]: We had this idea of bottleneck (reducing the channels and then increase them again), there is also what kernel size to use. The first one has 1 by 1 <code class="markup--code markup--p-code">Conv</code>. What actually happen in 1 by 1 conv? If we have 4 by 4 grid with 32 filters/channels and we will do 1 by 1 conv, the kernel for the conv looks like the one in the middle. When we talk about the kernel size, we never mention the last piece — but let’s say it’s 1 by 1 by 32 because that’s the part of the filters in and filters out. The kernel gets placed on the first cell in yellow and we get a dot product these 32 deep bits which gives us our first output. We then move it to the second cell and get the second output. So there will be bunch of dot products for each point in the grid. It is allowing us to change the dimensionality in whatever way we want in the channel dimension. We are creating <code class="markup--code markup--p-code">ni//2</code> filters and we will have <code class="markup--code markup--p-code">ni//2</code> dot products which are basically different weighted averages of the input channels. With very little computation, it lets us add this additional step of calculations and nonlinearities. It is a cool trick to take advantage of these 1 by 1 convs, creating this bottleneck, and then pulling it out again with 3 by 3 convs — which will take advantage of the 2D nature of the input properly. Or else, 1 by 1 conv doesn’t take advantage of that at all.</p><figure name="1cc4" id="1cc4" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_-lUndrOFGp_FdJ27T_Px-g.png"></figure><p name="ebbf" id="ebbf" class="graf graf--p graf-after--figure">These two lines of code, there is not much in it, but it’s a really great test of your understanding and intuition about what is going on [<a href="https://youtu.be/ondivPiwQho?t=25m17s" data-href="https://youtu.be/ondivPiwQho?t=25m17s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">25:17</a>] — why does it work? why do the tensor ranks line up? why do the dimensions all line up nicely? why is it a good idea? what is it really doing? It’s a really good thing to fiddle around with. Maybe create some small ones in Jupyter Notebook, run them yourself, see what inputs and outputs come in and out. Really get a feel for that. Once you’ve done so, you can then play around with different things.</p><p name="03c3" id="03c3" class="graf graf--p graf-after--p">One of the really unappreciated papers is this one [<a href="https://youtu.be/ondivPiwQho?t=26m9s" data-href="https://youtu.be/ondivPiwQho?t=26m9s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">26:09</a>] — <a href="https://arxiv.org/abs/1605.07146" data-href="https://arxiv.org/abs/1605.07146" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">Wide Residual Networks</a>. It’s really quite simple paper but what they do is they fiddle around with these two lines of code:</p><ul class="postList"><li name="2943" id="2943" class="graf graf--li graf-after--p">What is we did <code class="markup--code markup--li-code">ni*2</code> instead of <code class="markup--code markup--li-code">ni//2</code>?</li><li name="3c43" id="3c43" class="graf graf--li graf-after--li">What if we added <code class="markup--code markup--li-code">conv3</code>?</li></ul><p name="db09" id="db09" class="graf graf--p graf-after--li">They come up with this kind of simple notation for defining what the two lines of code can look like and they show lots of experiments. What they show is that this approach of a bottlenecking of decreasing the number of channels which is almost universal in ResNet is probably not a good idea. In fact, from the experiments, definitely not a good idea. Because what happens is it lets you create really deep networks. The guys who created ResNet got particularly famous for creating 1001 layer network. But the thing about 1001 layers is you can’t calculate layer 2 until you are finished layer 1. You can’t calculate layer 3 until you finish calculating layer 2. So it’s sequential. GPUs don’t like sequential. So what they showed is that if you have less layers but with more calculations per layer — so one easy way to do that would be to remove <code class="markup--code markup--p-code">//2</code>, no other changes:</p><figure name="5d7c" id="5d7c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*89Seymgfa5Bdx1_EXBW_lA.png" data-width="405" data-height="54" src="../img/1_89Seymgfa5Bdx1_EXBW_lA.png"></figure><p name="a309" id="a309" class="graf graf--p graf-after--figure">Try this at home. Try running CIFAR and see what happens. Even multiply by 2 or fiddle around. That lets your GPU do more work and it’s very interesting because the vast majority of papers that talk about performance of different architectures never actually time how long it takes to run a batch through it. They say “this one takes X number of floating-point operations per batch” but they never actually bother to run it like a proper experimentalists and find out whether it’s faster or slower. A lot of the architectures that are really famous now turn out to be slow as molasses and take crap loads of memory and just totally useless because the researchers never actually bothered to see whether they are fast and to actually see whether they fit in RAM with normal batch sizes. So Wide ResNet paper is unusual in that it actually times how long it takes as does the YOLO v3 paper which made the same insight. They might have missed the Wide ResNet paper because the YOLO v3 paper came to a lot of the same conclusions but Jeremy is not sure they sited the Wide ResNet paper so they might not be aware that all that work has been done. It’s great to see people are actually timing things and noticing what actually makes sense.</p><p name="9847" id="9847" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: What is your opinion on SELU (scaled exponential linear units)? [<a href="https://youtu.be/ondivPiwQho?t=29m44s" data-href="https://youtu.be/ondivPiwQho?t=29m44s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">29:44</a>] SELU is largely for fully connected layers which allows you to get rid of batch norm and the basic idea is that if you use this different activation function, it’s self normalizing. Self normalizing means it will always remain at a unit standard deviation and zero mean and therefore you don’t need batch norm. It hasn’t really gone anywhere and the reason is because it’s incredibly finicky — you have to use a very specific initialization otherwise it doesn’t start with exactly the right standard deviation and mean. Very hard to use it with things like embeddings, if you do then you have to use a particular kind of embedding initialization which doesn’t make sense for embeddings. And you do all this work, very hard to get it right, and if you do finally get it right, what’s the point? Well, you’ve managed to get rid of some batch norm layers which weren’t really hurting you anyway. It’s interesting because the SELU paper — the main reason people noticed it was because it was created by the inventor of LSTM and also it had a huge mathematical appendix. So people thought “lots of maths from a famous guy — it must be great!” but in practice, Jeremy doesn’t see anybody using it to get any state-of-the-art results or win any competitions.</p><p name="519a" id="519a" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">Darknet.make_group_layer</code> contains a bunch of <code class="markup--code markup--p-code">ResLayer</code> [<a href="https://youtu.be/ondivPiwQho?t=31m28s" data-href="https://youtu.be/ondivPiwQho?t=31m28s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">31:28</a>]. <code class="markup--code markup--p-code">group_layer</code> is going to have some number of channels/filters coming in. We will double the number of channels coming in by just using the standard <code class="markup--code markup--p-code">conv_layer</code>. Optionally, we will halve the grid size by using a stride of 2. Then we are going to do a whole bunch of ResLayers — we can pick how many (2, 3, 8 etc) because remember ResLayers do not change the grid size and they don’t change the number of channels, so you can add as many as you like without causing any problems. This is going to use more computation and more RAM but there is no reason other than that you can’t add as many as you like. <code class="markup--code markup--p-code">group_layer</code>, therefore, is going to end up doubling the number of channels because the initial convolution doubles the number of channels and depending on what we pass in a <code class="markup--code markup--p-code">stride</code>, it may also halve the grid size if we put <code class="markup--code markup--p-code">stride=2</code>. And then we can do a whole bunch of Res block computations as many as we like.</p><p name="08c1" id="08c1" class="graf graf--p graf-after--p">To define our <code class="markup--code markup--p-code">Darknet</code>, we are going to pass in something that looks like this [<a href="https://youtu.be/ondivPiwQho?t=33m13s" data-href="https://youtu.be/ondivPiwQho?t=33m13s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">33:13</a>]:</p><pre name="93ab" id="93ab" class="graf graf--pre graf-after--p">m = Darknet([1, 2, 4, 6, 3], num_classes=10, nf=32)<br>m = nn.DataParallel(m, [1,2,3])</pre><p name="5349" id="5349" class="graf graf--p graf-after--pre">What this says is create five group layers: the first one will contain 1 extra ResLayer, the second will contain 2, then 4, 6, 3 and we want to start with 32 filters. The first one of ResLayers will contain 32 filters, and there’ll just be one extra ResLayer. The second one, it’s going to double the number of filters because that’s what we do each time we have a new group layer. So the second one will have 64, and then 128, 256, 512 and that’ll be it. Nearly all of the network is going to be those bunches of layers and remember, every one of those group layers also has one convolution at the start. So then all we have is before that all happens, we are going to have one convolutional layer at the very start, and at the very end we are going to do our standard adaptive average pooling, flatten, and a linear layer to create the number of classes out at the end. To summarize [<a href="https://youtu.be/ondivPiwQho?t=34m44s" data-href="https://youtu.be/ondivPiwQho?t=34m44s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">34:44</a>], one convolution at one end, adaptive pooling and one linear layer at the other end, and in the middle, these group layers each one consisting of a convolutional layer followed by <code class="markup--code markup--p-code">n</code> number of ResLayers.</p><p name="bd60" id="bd60" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Adaptive average pooling</strong> [<a href="https://youtu.be/ondivPiwQho?t=35m2s" data-href="https://youtu.be/ondivPiwQho?t=35m2s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">35:02</a>]: Jeremy’s mentioned this a few times, but he’s yet to see any code out there, any example, anything anywhere, that uses adaptive average pooling. Every one he’s seen writes it like <code class="markup--code markup--p-code">nn.AvgPool2d(n)</code> where <code class="markup--code markup--p-code">n</code> is a particular number — this means that it’s now tied to a particular image size which definitely isn’t what you want. So most people are still under the impression that a specific architecture is tied to a specific size. That’s a huge problem when people think that because it really limits their ability to use smaller sizes to kick-start their modeling or to use smaller size for doing experiments.</p><p name="4ab5" id="4ab5" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Sequential</strong> [<a href="https://youtu.be/ondivPiwQho?t=35m53s" data-href="https://youtu.be/ondivPiwQho?t=35m53s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">35:53</a>]: A nice way to create architectures is to start out by creating a list, in this case this is this is a list with just one <code class="markup--code markup--p-code">conv_layer</code> in, and <code class="markup--code markup--p-code">make_group_layer</code> returns another list. Then we can append that list to the previous list with <code class="markup--code markup--p-code">+=</code> and do the same for another list containing <code class="markup--code markup--p-code">AdaptiveAvnPool2d</code>. Finally we will call <code class="markup--code markup--p-code">nn.Sequential</code> of all those layers. Now the <code class="markup--code markup--p-code">forward</code> is just <code class="markup--code markup--p-code">self.layers(x)</code>.</p><figure name="c089" id="c089" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_nr69J3I7lNPlsblmrLt15A.png"></figure><p name="8c55" id="8c55" class="graf graf--p graf-after--figure">This is a nice picture of how to make your architectures as simple as possible. There are a lot you can fiddle around with. You can parameterize the divider of <code class="markup--code markup--p-code">ni</code> to make it a number that you pass in to pass in different numbers- maybe do times 2 instead. You can also pass in things that change the kernel size, or change the number of convolutional layers. Jeremy has a version of this which he is going to run for you which implements all of the different parameters that were in the Wide ResNet paper, so he could fiddle around to see what worked well.</p><figure name="7c19" id="7c19" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_mR3BupmhN_XGo34Qvm58Sg.png"></figure><pre name="6e90" id="6e90" class="graf graf--pre graf-after--figure">lr = 1.3<br>learn = ConvLearner.from_model_data(m, data)<br>learn.crit = nn.CrossEntropyLoss()<br>learn.metrics = [accuracy]<br>wd=1e-4</pre><pre name="a8cc" id="a8cc" class="graf graf--pre graf-after--pre">%time learn.fit(lr, 1, wds=wd, cycle_len=30, use_clr_beta=(20, 20, <br>                0.95, 0.85))</pre><p name="aa0a" id="aa0a" class="graf graf--p graf-after--pre">Once we’ve got that, we can use <code class="markup--code markup--p-code">ConvLearner.from_model_data</code> to take our PyTorch module and a model data object, and turn them into a learner [<a href="https://youtu.be/ondivPiwQho?t=37m8s" data-href="https://youtu.be/ondivPiwQho?t=37m8s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">37:08</a>]. Give it a criterion, add a metrics if we like, and then we can fit and away we go.</p><p name="873c" id="873c" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: Could you please explain adaptive average pooling? How does setting to 1 work [<a href="https://youtu.be/ondivPiwQho?t=37m25s" data-href="https://youtu.be/ondivPiwQho?t=37m25s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">37:25</a>]? Sure. Normally when we are doing average pooling, let’s say we have 4x4 and we did <code class="markup--code markup--p-code">avgpool((2, 2))</code> [<a href="https://youtu.be/ondivPiwQho?t=40m35s" data-href="https://youtu.be/ondivPiwQho?t=40m35s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">40:35</a>]. That creates 2x2 area (blue in the below) and takes the average of those four. If we pass in <code class="markup--code markup--p-code">stride=1</code>, the next one is 2x2 shown in green and take the average. So this is what a normal 2x2 average pooling would be. If we didn’t have any padding, that would spit out 3x3. If we wanted 4x4, we can add padding.</p><figure name="c8e4" id="c8e4" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_vTPZGULUC12lQplYtkGZuQ.png"></figure><p name="2b3a" id="2b3a" class="graf graf--p graf-after--figure">What if we wanted 1x1? Then we could say <code class="markup--code markup--p-code">avgpool((4,4), stride=1)</code> that would do 4x4 in yellow and average the whole lot which results in 1x1. But that’s just one way to do it. Rather than saying the size of the pooling filter, why don’t we instead say “I don’t care what the size of the input grid is. I always want one by one”. That’s where you say <code class="markup--code markup--p-code">adap_avgpool(1)</code>. In this case, you don’t say what’s the size of the pooling filter, you instead say what the size of the output we want. We want something that’s one by one. If you put a single integer <code class="markup--code markup--p-code">n</code>, it assumes you mean <code class="markup--code markup--p-code">n</code> by <code class="markup--code markup--p-code">n</code>. In this case, adaptive average pooling 1 with a 4x4 grid coming in is the same as average pooling (4, 4). If it was 7x7 grid coming in, it would be the same as average pooling (7, 7). It is the same operation, it’s just expressing it in a way that regardless of the input, we want something of that sized output.</p><p name="0d17" id="0d17" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">DAWNBench</strong> [<a href="https://youtu.be/ondivPiwQho?t=37m43s" data-href="https://youtu.be/ondivPiwQho?t=37m43s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">37:43</a>]: Let’s see how we go with our simple network against these state-of-the-art results. Jeremy has the command ready to go. We’ve taken all that stuff and put it into a simple Python script, and he modified some of the parameters he mentioned to create something he called <code class="markup--code markup--p-code">wrn_22</code> network which doesn’t officially exist but it has a bunch of changes to the parameters we talked about based on Jeremy’s experiments. It has bunch of cool stuff like:</p><ul class="postList"><li name="29f3" id="29f3" class="graf graf--li graf-after--p">Leslie Smith’s one cycle</li><li name="c368" id="c368" class="graf graf--li graf-after--li">Half-precision floating-point implementation</li></ul><figure name="3e8f" id="3e8f" class="graf graf--figure graf-after--li"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_OhRBmhkrMWXgMpDHKZGJJQ.png"></figure><p name="4305" id="4305" class="graf graf--p graf-after--figure">This is going to run on AWS p3 which has 8 GPUs and Volta architecture GPUs which have special support for half-precision floating-point. Fastai is the first library to actually integrate the Volta optimized half-precision floating-point into the library, so you can just do <code class="markup--code markup--p-code">learn.half()</code> and get that support automatically. And it’s also the first to integrate one cycle.</p><p name="ca20" id="ca20" class="graf graf--p graf-after--p">What this actually does is it’s using PyTorch’s multi-GPU support [<a href="https://youtu.be/ondivPiwQho?t=39m35s" data-href="https://youtu.be/ondivPiwQho?t=39m35s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">39:35</a>]. Since there are eight GPUs, it is actually going to fire off eight separate Python processors and each one is going to train on a little bit and then at the end it’s going to pass the gradient updates back to the master process that is going to integrate them all together. So you will see lots of progress bars pop up together.</p><figure name="73a2" id="73a2" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_7JkxOliX34lgAkzwGbl1tA.png"></figure><p name="ca5b" id="ca5b" class="graf graf--p graf-after--figure">You can see it’s training three or four seconds when you do it this way. Where else, when Jeremy was training earlier, he was getting 30 seconds per epoch. So doing it this way, we can train things ~10 times faster which is pretty cool.</p><p name="6399" id="6399" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Checking on the status</strong> [<a href="https://youtu.be/ondivPiwQho?t=43m19s" data-href="https://youtu.be/ondivPiwQho?t=43m19s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">43:19</a>]:</p><figure name="5215" id="5215" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_fgY5v-w-44eIBkkS4fPqEA.png"></figure><p name="2d4c" id="2d4c" class="graf graf--p graf-after--figure">It’s done! We got to 94% and it took 3 minutes and 11 seconds. Previous state-of-the-art was 1 hour 7 minutes. Was it worth fiddling around with those parameters and learning a little bit about how these architectures actually work and not just using what came out of the box? Well, holy crap. We just used a publicly available instance (we used a spot instance so it costs us $8 per hour — for 3 minutes 40 cents) to train this from scratch 20 times faster than anybody has ever done it before. So that is one of the craziest state-of-the-art result. We’ve seen many but this one just blew it out of the water. This is partly thanks to fiddling around with those parameters of the architecture, mainly frankly about using Leslie Smith’s one cycle. Reminder of what it is doing [<a href="https://youtu.be/ondivPiwQho?t=44m35s" data-href="https://youtu.be/ondivPiwQho?t=44m35s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">44:35</a>], for learning rate, it creates upward path that is equally long as the downward path so it’s true triangular cyclical learning rate (CLR). As per usual, you can pick the ratio of x and y (i.e. starting LR / peak LR). In</p><figure name="e742" id="e742" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_5lQZ0Jln6Cn29rd_9Bvzfw.png"></figure><p name="4a57" id="4a57" class="graf graf--p graf-after--figure">In this case, we picked 50 for the ratio. So we started out with much smaller learning rate. Then it has this cool idea where you get to say what percentage of your epochs is spent going from the bottom of the triangle all the way down pretty much to zero — that is the second number. So 15% of the batches are spent going from the bottom of our triangle even further.</p><figure name="4595" id="4595" class="graf graf--figure graf--layoutOutsetCenter graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_E0gxTQ5sf4XSceo9pWKxWQ.png"></figure><p name="c7e4" id="c7e4" class="graf graf--p graf-after--figure">That is not the only thing one cycle does, we also have momentum. Momentum goes from&nbsp;.95 to&nbsp;.85. In other words, when learning rate is really low, we use a lot of momentum and when the learning rate is really high, we use very little momentum which makes a lot of sense but until Leslie Smith showed this in the paper, Jeremy has never seen anybody do it before. It’s a really cool trick. You can now use that by using <code class="markup--code markup--p-code">use-clr-beta</code> parameter in fastai (<a href="http://forums.fast.ai/t/using-use-clr-beta-and-new-plotting-tools/14702" data-href="http://forums.fast.ai/t/using-use-clr-beta-and-new-plotting-tools/14702" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">forum post by Sylvain</a>) and you should be able to replicate the state-of-the-art result. You can use it on your own computer or your paper space, the only thing you won’t get is the multi-GPU piece, but that makes it a bit easier to train anyway.</p><p name="e44a" id="e44a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: <code class="markup--code markup--p-code">make_group_layer</code> contains stride equals 2, so this means stride is one for layer one and two for everything else. What is the logic behind it? Usually the strides I have seen are odd [<a href="https://youtu.be/ondivPiwQho?t=46m52s" data-href="https://youtu.be/ondivPiwQho?t=46m52s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">46:52</a>]. Strides are either one or two. I think you are thinking of kernel sizes. So stride=2 means that I jump two across which means that you halve your grid size. So I think you might have got confused between stride and kernel size there. If you have a stride of one, the grid size does not change. If you have a stride of two, then it does. In this case, because this is CIFAR10, 32 by 32 is small and we don’t get to halve the grid size very often because pretty quickly we are going to run out of cells. So that is why the first layer has a stride of one so we don’t decrease the grid size straight away. It is kind of a nice way of doing it because that’s why we have a low number at first <code class="markup--code markup--p-code">Darknet([1, 2, 4, 6, 3],&nbsp;…)</code>&nbsp;. We can start out with not too much computation on the big grid, and then we can gradually doing more and more computation as the grids get smaller and smaller because the smaller grid the computation will take less time</p><h3 name="3d3c" id="3d3c" class="graf graf--h3 graf-after--p">Generative Adversarial Networks (GAN)&nbsp;[<a href="https://youtu.be/ondivPiwQho?t=48m49s" data-href="https://youtu.be/ondivPiwQho?t=48m49s" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">48:49</a>]</h3><ul class="postList"><li name="c8bf" id="c8bf" class="graf graf--li graf-after--h3"><a href="https://arxiv.org/abs/1701.07875" data-href="https://arxiv.org/abs/1701.07875" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">Wasserstein GAN</a></li><li name="36df" id="36df" class="graf graf--li graf-after--li"><a href="https://arxiv.org/abs/1511.06434" data-href="https://arxiv.org/abs/1511.06434" class="markup--anchor markup--li-anchor" rel="noopener nofollow" target="_blank">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</a></li></ul><p name="ef3f" id="ef3f" class="graf graf--p graf-after--li">We are going to talk about generative adversarial networks also known as GANs and specifically we are going to focus on Wasserstein GAN paper which included Soumith Chintala who went on to create PyTorch. Wasserstein GAN (WGAN) was heavily influenced by the deep convolutional generative adversarial network paper which also Soumith was involved with. It is a really interesting paper to read. A lot of it looks like this:</p><figure name="c46b" id="c46b" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_9zXXZvCNC8_eF_V9LJIolw.png"></figure><p name="9660" id="9660" class="graf graf--p graf-after--figure">The good news is you can skip those bits because there is also a bit that looks like this:</p><figure name="fe33" id="fe33" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_T90I-RKpUzV7yyo_TOqowQ.png"></figure><p name="6909" id="6909" class="graf graf--p graf-after--figure">A lot of papers have a theoretical section which seems to be there entirely to get past the reviewer’s need for theory. That’s not true with WGAN paper. The theory bit is actually interesting — you don’t need to know it to use it, but if you want to learn about some cool ideas and see the thinking behind why this particular algorithm, it’s absolutely fascinating. Before this paper came out, Jeremy knew nobody who studied the math it’s based on, so everybody had to learn the math. The paper does a pretty good job of laying out all the pieces (you have to do a bunch of reading yourself). So if you are interested in digging into the deeper math behind some paper to see what it’s like to study it, I would pick this one because at the end of that theory section, you’ll come away saying “I can see now why they made this algorithm the way it is.”</p><p name="1b8b" id="1b8b" class="graf graf--p graf-after--p">The basic idea of GAN is it’s a generative model[<a href="https://youtu.be/ondivPiwQho?t=51m23s" data-href="https://youtu.be/ondivPiwQho?t=51m23s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">51:23</a>]. It is something that is going to create sentences, create images, or generate something. It is going to try and create thing which is very hard to tell the difference between generated stuff and real stuff. So generative model could be used to face-swap a video — a very controversial thing of deep fakes and fake pornography happening at the moment. It could be used to fake somebody’s voice. It could be used to fake the answer to a medical question — but in that case, it’s not really a fake, it could be a generative answer to a medical question that is actually a good answer so you are generating language. You could generate a caption to an image, for example. So generative models have lots of interesting applications. But generally speaking, they need to be good enough that for example if you are using it to automatically create a new scene for Carrie Fisher in the next Star Wars movie and she is not around to play that part anymore, you want to try and generate an image of her that looks the same then it has to fool the Star Wars audience into thinking “okay, that doesn’t look like some weird Carrie Fisher — that looks like the real Carrie Fisher. Or if you are trying to generate an answer to a medical question, you want to generate English that reads nicely and clearly, and sounds authoritative and meaningful. The idea of generative adversarial network is we are going to create not just a generative model to create the generated image, but a second model that’s going to try to pick which ones are real and which ones are generated (we will call them “fake”). So we have a generator that is going to create our fake content and a discriminator that’s going to try to get good at recognizing which ones are real and which ones are fake. So there are going to be two models and they are going to be adversarial meaning the generator is going to try to keep getting better at fooling the discriminator into thinking that fake is real, and the discriminator is going to try to keep getting better at discriminating between the real and the fake. So they are going to go head to head. It is basically as easy as Jeremy just described [<a href="https://youtu.be/ondivPiwQho?t=54m14s" data-href="https://youtu.be/ondivPiwQho?t=54m14s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">54:14</a>]:</p><ul class="postList"><li name="23ff" id="23ff" class="graf graf--li graf-after--p">We are going to build two models in PyTorch</li><li name="65c6" id="65c6" class="graf graf--li graf-after--li">We are going to create a training loop that first of all says the loss function for the discriminator is “can you tell the difference between real and fake, then update the weights of that.</li><li name="cacc" id="cacc" class="graf graf--li graf-after--li">We are going to create a loss function for the generator which is “can you generate something which fools the discriminator and update the weights from that loss.</li><li name="758d" id="758d" class="graf graf--li graf-after--li">And we are going to loop through that a few times and see what happens.</li></ul><h4 name="789c" id="789c" class="graf graf--h4 graf-after--li">Looking at the code&nbsp;[<a href="https://youtu.be/ondivPiwQho?t=54m52s" data-href="https://youtu.be/ondivPiwQho?t=54m52s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">54:52</a>]</h4><p name="bb0e" id="bb0e" class="graf graf--p graf-after--h4"><a href="https://github.com/fastai/fastai/blob/master/courses/dl2/wgan.ipynb" data-href="https://github.com/fastai/fastai/blob/master/courses/dl2/wgan.ipynb" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Notebook</a></p><p name="360e" id="360e" class="graf graf--p graf-after--p">There is a lot of different things you can do with GANS. We are going to do something that is kind of boring but easy to understand and it’s kind of cool that it’s even possible which is we are going to generate some pictures from nothing. We are just going to get it to draw some pictures. Specifically, we are going to get it to draw pictures of bedrooms. Hopefully you get a chance to play around with this during the week with your own datasets. If you pick a dataset that’s very varied like ImageNet and then get a GAN to try and create ImageNet pictures, it tends not to do so well because it’s not clear enough what you want a picture of. So it’s better to give it, for example, there is a dataset called <a href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html" data-href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">CelebA</a> which is pictures of celebrities’ faces that works great with GANs. You create really clear celebrity faces that don’t actually exist. The bedroom dataset is also a good one — pictures of the same kind of thing.</p><p name="91e1" id="91e1" class="graf graf--p graf-after--p">There is something called LSUN scene classification dataset [<a href="https://youtu.be/ondivPiwQho?t=55m55s" data-href="https://youtu.be/ondivPiwQho?t=55m55s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">55:55</a>].</p><pre name="ffca" id="ffca" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">fastai.conv_learner</strong> <strong class="markup--strong markup--pre-strong">import</strong> *<br><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">fastai.dataset</strong> <strong class="markup--strong markup--pre-strong">import</strong> *<br><strong class="markup--strong markup--pre-strong">import</strong> <strong class="markup--strong markup--pre-strong">gzip</strong></pre><p name="79a8" id="79a8" class="graf graf--p graf-after--pre">Download the LSUN scene classification dataset bedroom category, unzip it, and convert it to jpg files (the scripts folder is here in the <code class="markup--code markup--p-code">dl2</code> folder):</p><pre name="3d13" id="3d13" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">curl 'http://lsun.cs.princeton.edu/htbin/download.cgi?tag=latest&amp;category=bedroom&amp;set=train' -o bedroom.zip</code></pre><pre name="66d9" id="66d9" class="graf graf--pre graf-after--pre"><code class="markup--code markup--pre-code">unzip bedroom.zip</code></pre><pre name="abab" id="abab" class="graf graf--pre graf-after--pre"><code class="markup--code markup--pre-code">pip install lmdb</code></pre><pre name="7e42" id="7e42" class="graf graf--pre graf-after--pre"><code class="markup--code markup--pre-code">python lsun-data.py {PATH}/bedroom_train_lmdb --out_dir {PATH}/bedroom</code></pre><p name="18eb" id="18eb" class="graf graf--p graf-after--pre">This isn't tested on Windows - if it doesn't work, you could use a Linux box to convert the files, then copy them over. Alternatively, you can download <a href="https://www.kaggle.com/jhoward/lsun_bedroom" data-href="https://www.kaggle.com/jhoward/lsun_bedroom" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">this 20% sample</a> from Kaggle datasets.</p><pre name="5f16" id="5f16" class="graf graf--pre graf-after--p">PATH = Path('data/lsun/')<br>IMG_PATH = PATH/'bedroom'<br>CSV_PATH = PATH/'files.csv'<br>TMP_PATH = PATH/'tmp'<br>TMP_PATH.mkdir(exist_ok=<strong class="markup--strong markup--pre-strong">True</strong>)</pre><p name="2b54" id="2b54" class="graf graf--p graf-after--pre">In this case, it is much easier to go the CSV route when it comes to handling our data. So we generate a CSV with the list of files that we want, and a fake label “0” because we don’t really have labels for these at all. One CSV file contains everything in that bedroom dataset, and another one contains random 10%. It is nice to do that because then we can most of the time use the sample when we are experimenting because there is well over a million files even just reading in the list takes a while.</p><pre name="d722" id="d722" class="graf graf--pre graf-after--p">files = PATH.glob('bedroom/**/*.jpg')<br><br><strong class="markup--strong markup--pre-strong">with</strong> CSV_PATH.open('w') <strong class="markup--strong markup--pre-strong">as</strong> fo:<br>    <strong class="markup--strong markup--pre-strong">for</strong> f <strong class="markup--strong markup--pre-strong">in</strong> files: fo.write(f'{f.relative_to(IMG_PATH)},0<strong class="markup--strong markup--pre-strong">\n</strong>')</pre><pre name="7159" id="7159" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em"># Optional - sampling a subset of files</em><br>CSV_PATH = PATH/'files_sample.csv'</pre><pre name="71df" id="71df" class="graf graf--pre graf-after--pre">files = PATH.glob('bedroom/**/*.jpg')<br><br><strong class="markup--strong markup--pre-strong">with</strong> CSV_PATH.open('w') <strong class="markup--strong markup--pre-strong">as</strong> fo:<br>    <strong class="markup--strong markup--pre-strong">for</strong> f <strong class="markup--strong markup--pre-strong">in</strong> files:<br>        <strong class="markup--strong markup--pre-strong">if</strong> random.random()&lt;0.1: <br>            fo.write(f'{f.relative_to(IMG_PATH)},0<strong class="markup--strong markup--pre-strong">\n</strong>')</pre><p name="ca9d" id="ca9d" class="graf graf--p graf-after--pre">This will look pretty familiar [<a href="https://youtu.be/ondivPiwQho?t=57m10s" data-href="https://youtu.be/ondivPiwQho?t=57m10s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">57:10</a>]. This is before Jeremy realized that sequential models are much better. So if you compare this to the previous conv block with a sequential model, there is a lot more lines of code here — but it does the same thing of conv, ReLU, batch norm.</p><pre name="1bee" id="1bee" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">ConvBlock</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, ni, no, ks, stride, bn=<strong class="markup--strong markup--pre-strong">True</strong>, pad=<strong class="markup--strong markup--pre-strong">None</strong>):<br>        super().__init__()<br>        <strong class="markup--strong markup--pre-strong">if</strong> pad <strong class="markup--strong markup--pre-strong">is</strong> <strong class="markup--strong markup--pre-strong">None</strong>: pad = ks//2//stride<br>        self.conv = nn.Conv2d(ni, no, ks, stride, padding=pad, <br>                              bias=<strong class="markup--strong markup--pre-strong">False</strong>)<br>        self.bn = nn.BatchNorm2d(no) <strong class="markup--strong markup--pre-strong">if</strong> bn <strong class="markup--strong markup--pre-strong">else</strong> <strong class="markup--strong markup--pre-strong">None</strong><br>        self.relu = nn.LeakyReLU(0.2, inplace=<strong class="markup--strong markup--pre-strong">True</strong>)<br>    <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x):<br>        x = self.relu(self.conv(x))<br>        <strong class="markup--strong markup--pre-strong">return</strong> self.bn(x) <strong class="markup--strong markup--pre-strong">if</strong> self.bn <strong class="markup--strong markup--pre-strong">else</strong> x</pre><p name="3471" id="3471" class="graf graf--p graf-after--pre">The first thing we are going to do is to build a discriminator [<a href="https://youtu.be/ondivPiwQho?t=57m47s" data-href="https://youtu.be/ondivPiwQho?t=57m47s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">57:47</a>]. A discriminator is going to receive an image as an input, and it’s going to spit out a number. The number is meant to be lower if it thinks this image is real. Of course “what does it do for a lower number” thing does not appear in the architecture, that will be in the loss function. So all we have to do is to create something that takes an image and spits out a number. A lot of this code is borrowed from the original authors of this paper, so some of the naming scheme is different to what we are used to. But it looks similar to what we had before. We start out with a convolution (conv, ReLU, batch norm). Then we have a bunch of extra conv layers — this is not going to use a residual so it looks very similar to before a bunch of extra layers but these are going to be conv layers rather than res layers. At the end, we need to append enough stride 2 conv layers that we decrease the grid size down to no bigger than 4x4. So it’s going to keep using stride 2, divide the size by 2, and repeat till our grid size is no bigger than 4. This is quite a nice way of creating as many layers as you need in a network to handle arbitrary sized images and turn them into a fixed known grid size.</p><p name="b0b3" id="b0b3" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: Does GAN need a lot more data than say dogs vs. cats or NLP? Or is it comparable [<a href="https://youtu.be/ondivPiwQho?t=59m48s" data-href="https://youtu.be/ondivPiwQho?t=59m48s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">59:48</a>]? Honestly, I am kind of embarrassed to say I am not an expert practitioner in GANs. The stuff I teach in part one is things I am happy to say I know the best way to do these things and so I can show you state-of-the-art results like we just did with CIFAR10 with the help of some of the students. I am not there at all with GANs so I am not quite sure how much you need. In general, it seems it needs quite a lot but remember the only reason we didn’t need too much in dogs and cats is because we had a pre-trained model and could we leverage pre-trained GAN models and fine tune them, probably. I don’t think anybody has done it as far as I know. That could be really interesting thing for people to think about and experiment with. Maybe people have done it and there is some literature there we haven’t come across. I’m somewhat familiar with the main pieces of literature in GANs but I don’t know all of it, so maybe I’ve missed something about transfer learning in GANs. But that would be the trick to not needing too much data.</p><p name="19a7" id="19a7" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: So the huge speed-up a combination of one cycle learning rate and momentum annealing plus the eight GPU parallel training in the half precision? Is that only possible to do the half precision calculation with consumer GPU? Another question, why is the calculation 8 times faster from single to half precision, while from double the single is only 2 times faster [<a href="https://youtu.be/ondivPiwQho?t=1h1m9s" data-href="https://youtu.be/ondivPiwQho?t=1h1m9s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:01:09</a>]? Okay, so the CIFAR10 result, it’s not 8 times faster from single to half. It’s about 2 or 3 times as fast from single to half. NVIDIA claims about the flops performance of the tensor cores, academically correct, but in practice meaningless because it really depends on what calls you need for what piece — so about 2 or 3x improvement for half. So the half precision helps a bit, the extra GPUs helps a bit, the one cycle helps an enormous amount, then another key piece was the playing around with the parameters that I told you about. So reading the wide ResNet paper carefully, identifying the kinds of things that they found there, and then writing a version of the architecture you just saw that made it really easy for us to fiddle around with parameters, staying up all night trying every possible combination of different kernel sizes, numbers of kernels, number of layer groups, size of layer groups. And remember, we did a bottleneck but actually we tended to focus instead on widening so we increase the size and then decrease it because it takes better advantage of the GPU. So all those things combined together, I’d say the one cycle was perhaps the most critical but every one of those resulted in a big speed-up. That’s why we were able to get this 30x improvement over the state-of-the-art CIFAR10. We have some ideas for other things — after this DAWN bench finishes, maybe we’ll try and go even further to see if we can beat one minute one day. That’ll be fun.</p><pre name="0f03" id="0f03" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">DCGAN_D</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, isize, nc, ndf, n_extra_layers=0):<br>        super().__init__()<br>        <strong class="markup--strong markup--pre-strong">assert</strong> isize % 16 == 0, "isize has to be a multiple of 16"<br><br>        self.initial = ConvBlock(nc, ndf, 4, 2, bn=<strong class="markup--strong markup--pre-strong">False</strong>)<br>        csize,cndf = isize/2,ndf<br>        self.extra = nn.Sequential(*[ConvBlock(cndf, cndf, 3, 1)<br>                                    <strong class="markup--strong markup--pre-strong">for</strong> t <strong class="markup--strong markup--pre-strong">in</strong> range(n_extra_layers)])<br><br>        pyr_layers = []<br>        <strong class="markup--strong markup--pre-strong">while</strong> csize &gt; 4:<br>            pyr_layers.append(ConvBlock(cndf, cndf*2, 4, 2))<br>            cndf *= 2; csize /= 2<br>        self.pyramid = nn.Sequential(*pyr_layers)<br>        <br>        self.final = nn.Conv2d(cndf, 1, 4, padding=0, bias=<strong class="markup--strong markup--pre-strong">False</strong>)<br><br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, input):<br>        x = self.initial(input)<br>        x = self.extra(x)<br>        x = self.pyramid(x)<br>        <strong class="markup--strong markup--pre-strong">return</strong> self.final(x).mean(0).view(1)</pre><p name="1cc5" id="1cc5" class="graf graf--p graf-after--pre">So here is our discriminator [<a href="https://youtu.be/ondivPiwQho?t=1h3m37s" data-href="https://youtu.be/ondivPiwQho?t=1h3m37s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:03:37</a>].The important thing to remember about an architecture is it doesn’t do anything rather than have some input tensor size and rank, and some output tensor size and rank. As you see the last conv has one channel. This is different from what we are used to because normally our last thing is a linear block. But our last layer here is a conv block. It only has one channel but it has a grid size of something around 4x4 (no more than 4x4). So we are going to spit out (let’s say it’s 4x4), 4 by 4 by 1 tensor. What we then do is we then take the mean of that. So it goes from 4x4x1 to a scalar. This is kind of like the ultimate adaptive average pooling because we have something with just one channel and we take the mean. So this is a bit different — normally we first do average pooling and then we put it through a fully connected layer to get our one thing out. But this is getting one channel out and then taking the mean of that. Jeremy suspects that it would work better if we did the normal way, but he hasn’t tried it yet and he doesn’t really have a good enough intuition to know whether he is missing something — but it will be an interesting experiment to try if somebody wants to stick an adaptive average pooling layer and a fully connected layer afterwards with a single output.</p><p name="e32b" id="e32b" class="graf graf--p graf-after--p">So that’s a discriminator. Let’s assume we already have a generator — somebody says “okay, here is a generator which generates bedrooms. I want you to build a model that can figure out which ones are real and which ones aren’t”. We are going to take the dataset and label bunch of images which are fake bedrooms from the generator, and a bunch of images of real bedrooms from LSUN dataset to stick a 1 or a 0 on each one. Then we’ll try to get the discriminator to tell the difference. So that is going to be simple enough. But we haven’t been given a generator. We need to build one. We haven’t talked about the loss function yet — we are going to assume that there’s some loss function that does this thing.</p><h4 name="e226" id="e226" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Generator</strong> [<a href="https://youtu.be/ondivPiwQho?t=1h6m15s" data-href="https://youtu.be/ondivPiwQho?t=1h6m15s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:06:15</a>]</h4><p name="5ef8" id="5ef8" class="graf graf--p graf-after--h4">A generator is also an architecture which doesn’t do anything by itself until we have a loss function and data. But what are the ranks and sizes of the tensors? The input to the generator is going to be a vector of random numbers. In the paper, they call that the “prior.” How big? We don’t know. The idea is that a different bunch of random numbers will generate a different bedroom. So our generator has to take as input a vector, stick it through sequential models, and turn it into a rank 4 tensor (rank 3 without the batch dimension) — height by width by 3. So in the final step, <code class="markup--code markup--p-code">nc</code> (number of channel) is going to have to end up being 3 because it’s going to create a 3 channel image of some size.</p><pre name="f01e" id="f01e" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">DeconvBlock</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, ni, no, ks, stride, pad, bn=<strong class="markup--strong markup--pre-strong">True</strong>):<br>        super().__init__()<br>        self.conv = nn.ConvTranspose2d(ni, no, ks, stride, <br>                         padding=pad, bias=<strong class="markup--strong markup--pre-strong">False</strong>)<br>        self.bn = nn.BatchNorm2d(no)<br>        self.relu = nn.ReLU(inplace=<strong class="markup--strong markup--pre-strong">True</strong>)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x):<br>        x = self.relu(self.conv(x))<br>        <strong class="markup--strong markup--pre-strong">return</strong> self.bn(x) <strong class="markup--strong markup--pre-strong">if</strong> self.bn <strong class="markup--strong markup--pre-strong">else</strong> x</pre><pre name="a0d5" id="a0d5" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">DCGAN_G</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, isize, nz, nc, ngf, n_extra_layers=0):<br>        super().__init__()<br>        <strong class="markup--strong markup--pre-strong">assert</strong> isize % 16 == 0, "isize has to be a multiple of 16"<br><br>        cngf, tisize = ngf//2, 4<br>        <strong class="markup--strong markup--pre-strong">while</strong> tisize!=isize: cngf*=2; tisize*=2<br>        layers = [DeconvBlock(nz, cngf, 4, 1, 0)]<br><br>        csize, cndf = 4, cngf<br>        <strong class="markup--strong markup--pre-strong">while</strong> csize &lt; isize//2:<br>            layers.append(DeconvBlock(cngf, cngf//2, 4, 2, 1))<br>            cngf //= 2; csize *= 2<br><br>        layers += [DeconvBlock(cngf, cngf, 3, 1, 1) <br>                       <strong class="markup--strong markup--pre-strong">for</strong> t <strong class="markup--strong markup--pre-strong">in</strong> range(n_extra_layers)]<br>        layers.append(nn.ConvTranspose2d(cngf, nc, 4, 2, 1,<br>                                            bias=<strong class="markup--strong markup--pre-strong">False</strong>))<br>        self.features = nn.Sequential(*layers)<br><br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, input): <strong class="markup--strong markup--pre-strong">return</strong> F.tanh(self.features(input))</pre><p name="ddfc" id="ddfc" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">Question</strong>: In ConvBlock, is there a reason why batch norm comes after ReLU (i.e. <code class="markup--code markup--p-code">self.bn(self.relu(…))</code>) [<a href="https://youtu.be/ondivPiwQho?t=1h7m50s" data-href="https://youtu.be/ondivPiwQho?t=1h7m50s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:07:50</a>]? I would normally expect to go ReLU then batch norm [<a href="https://youtu.be/ondivPiwQho?t=1h8m23s" data-href="https://youtu.be/ondivPiwQho?t=1h8m23s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:08:23</a>] that this is actually the order that makes sense to Jeremy. The order we had in the darknet was what they used in the darknet paper, so everybody seems to have a different order of these things. In fact, most people for CIFAR10 have a different order again which is batch norm → ReLU → conv which is a quirky way of thinking about it, but it turns out that often for residual blocks that works better. That is called a “pre-activation ResNet.” There is a few blog posts out there where people have experimented with different order of those things and it seems to depend a lot on what specific dataset it is and what you are doing with — although the difference in performance is small enough that you won’t care unless it’s for a competition.</p><h4 name="cf8f" id="cf8f" class="graf graf--h4 graf-after--p">Deconvolution [<a href="https://youtu.be/ondivPiwQho?t=1h9m36s" data-href="https://youtu.be/ondivPiwQho?t=1h9m36s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:09:36</a>]</h4><p name="dd33" id="dd33" class="graf graf--p graf-after--h4">So the generator needs to start with a vector and end up with a rank 3 tensor. We don’t really know how to do that yet. We need to use something called a “deconvolution” and PyTorch calls it transposed convolution — same thing, different name. Deconvolution is something which rather than decreasing the grid size, it increases the grid size. So as with all things, it’s easiest to see in an Excel spreadsheet.</p><p name="6fd4" id="6fd4" class="graf graf--p graf-after--p">Here is a convolution. We start, let’s say, with a 4 by 4 grid cell with a single channel. Let’s put it through a 3 by 3 kernel with a single output filter. So we have a single channel in, a single filter kernel, so if we don’t add any padding, we are going to end up with 2 by 2. Remember, the convolution is just the sum of the product of the kernel and the appropriate grid cell [<a href="https://youtu.be/ondivPiwQho?t=1h11m9s" data-href="https://youtu.be/ondivPiwQho?t=1h11m9s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:11:09</a>]. So there is our standard 3 by 3 conv one channel one filter.</p><figure name="0bb4" id="0bb4" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_FqkDO90rEDwa_CgxTAlyIQ.png"></figure><p name="7d0c" id="7d0c" class="graf graf--p graf-after--figure">So the idea now is we want to go the opposite direction [<a href="https://youtu.be/ondivPiwQho?t=1h11m25s" data-href="https://youtu.be/ondivPiwQho?t=1h11m25s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:11:25</a>]. We want to start with our 2 by 2 and we want to create a 4 by 4. Specifically we want to create the same 4 by 4 that we started with. And we want to do that by using a convolution. How would we do that?</p><p name="02d3" id="02d3" class="graf graf--p graf-after--p">If we have a 3 by 3 convolution, then if we want to create a 4 by 4 output, we are going to need to create this much padding:</p><figure name="8bc7" id="8bc7" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_flOxFmF21kUyLpPDJ6kr-w.png"></figure><p name="e028" id="e028" class="graf graf--p graf-after--figure">Because with this much padding, we are going to end up with 4 by 4. So let’s say our convolutional filter was just a bunch of zeros then we can calculate our error for each cell just by taking this subtraction:</p><figure name="2438" id="2438" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_HKcU-wgdLPgxd5kJfEkmlg.png"></figure><p name="41b9" id="41b9" class="graf graf--p graf-after--figure">Then we can get the sum of absolute values (L1 loss) by summing up the absolute values of those errors:</p><figure name="a468" id="a468" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_mjLTOFUXneXGeER4hKj4Kw.png"></figure><p name="9ace" id="9ace" class="graf graf--p graf-after--figure">So now we could use optimization, in Excel it’s called “solver” to do a gradient descent. So we will set the Total cell equal to minimum and we’ll try and reduce our loss by changing our filter. You can see it’s come up with a filter such that Result is almost like Data. It’s not perfect, and in general, you can’t assume that a deconvolution can exactly create the same exact thing you want because there is just not enough. Because there is 9 things in the filter and 16 things in the result. But it’s made a pretty good attempt. So this is what a deconvolution looks like — a stride 1, 3x3 deconvolution on a 2x2 grid cell input.</p><figure name="e1fa" id="e1fa" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_QzJe8qhpZl6hfKAB0Zw-vQ.png"></figure><p name="802f" id="802f" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Question</strong>: How difficult is it to create a discriminator to identify fake news vs. real news [<a href="https://youtu.be/ondivPiwQho?t=1h13m43s" data-href="https://youtu.be/ondivPiwQho?t=1h13m43s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:13:43</a>]? You don’t need anything special — that’s just a classifier. So you would just use the NLP classifier from previous class and lesson 4. In that case, there is no generative piece, so you just need a dataset that says these are the things that we believe are fake news and these are the things we consider to be real news and it should actually work very well. To the best of our knowledge, if you try it you should get as good a result as anybody else has got — whether it’s good enough to be useful in practice, Jeremy doesn’t know. The best thing you could do at this stage would be to generate a kind of a triage that says these things look pretty sketchy based on how they are written and then some human could go in and fact check them. NLP classifier and RNN can’t fact-check things but it could recognize that these are written in that kind of highly popularized style which often fake news is written in so maybe these ones are worth paying attention to. That would probably be the best you could hope for without drawing on some kind of external data sources. But it’s important to remember the discriminator is basically just a classifier and you don’t need any special techniques beyond what we’ve already learned to do NLP classification.</p><h4 name="6b71" id="6b71" class="graf graf--h4 graf-after--p">ConvTranspose2d [<a href="https://youtu.be/ondivPiwQho?t=1h16m" data-href="https://youtu.be/ondivPiwQho?t=1h16m" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:16:00</a>]</h4><p name="5f8e" id="5f8e" class="graf graf--p graf-after--h4">To do deconvolution in PyTorch, just say:</p><p name="e693" id="e693" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">nn.ConvTranspose2d(ni, no, ks, stride, padding=pad, bias=False)</code></p><ul class="postList"><li name="1c41" id="1c41" class="graf graf--li graf-after--p"><code class="markup--code markup--li-code">ni</code>&nbsp;: number of input channels</li><li name="6445" id="6445" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">no</code>: number of ourput channels</li><li name="81f1" id="81f1" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">ks</code>: kernel size</li></ul><p name="6719" id="6719" class="graf graf--p graf-after--li">The reason it’s called a ConvTranspose is because it turns out that this is the same as the calculation of the gradient of convolution. That’s why they call it that.</p><p name="e42c" id="e42c" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Visualizing</strong> [<a href="https://youtu.be/ondivPiwQho?t=1h16m33s" data-href="https://youtu.be/ondivPiwQho?t=1h16m33s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:16:33</a>]</p><figure name="df2f" id="df2f" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_GZz25GtnzqaYy5MV5iQPmA.png"><figcaption class="imageCaption"><a href="http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html" data-href="http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html</a></figcaption></figure><p name="5e46" id="5e46" class="graf graf--p graf-after--figure">One on the left is what we just saw of doing a 2x2 deconvolution. If there is a stride 2, then you don’t just have padding around the outside, but you actually have to put padding in the middle as well. They are not actually quite implemented this way because this is slow to do. In practice, you’ll implement them in a different way but it all happens behind the scene, so you don’t have to worry about it. We’ve talked about this <a href="http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html" data-href="http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">convolution arithmetic tutorial</a> before and if you are still not comfortable with convolutions and in order to get comfortable with deconvolutions, this is a great site to go to. If you want to see the paper, it is <a href="https://arxiv.org/abs/1603.07285" data-href="https://arxiv.org/abs/1603.07285" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">A guide to convolution arithmetic for deep learning</a>.</p><p name="c5dd" id="c5dd" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">DeconvBlock</code> looks identical to a <code class="markup--code markup--p-code">ConvBlock</code> except it has the word <code class="markup--code markup--p-code">Transpose</code> [<a href="https://youtu.be/ondivPiwQho?t=1h17m49s" data-href="https://youtu.be/ondivPiwQho?t=1h17m49s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:17:49</a>]. We just go conv → relu → batch norm as before, and it has input filters and output filters. The only difference is taht stride 2 means that the grid size will double rather than half.</p><figure name="9c6c" id="9c6c" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_vUpDoEX5vPs6y3auKiCFsQ.png"></figure><p name="80e0" id="80e0" class="graf graf--p graf-after--figure">Question: Both <code class="markup--code markup--p-code">nn.ConvTranspose2d</code> and <code class="markup--code markup--p-code">nn.Upsample</code> seem to do the same thing, i.e. expand grid-size (height and width) from previous layer. Can we say <code class="markup--code markup--p-code">nn.ConvTranspose2d</code> is always better than <code class="markup--code markup--p-code">nn.Upsample</code>, since <code class="markup--code markup--p-code">nn.Upsample</code> is merely resize and fill unknowns by zero’s or interpolation [<a href="https://youtu.be/ondivPiwQho?t=1h18m10s" data-href="https://youtu.be/ondivPiwQho?t=1h18m10s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:18:10</a>]? No, you can’t. There is a fantastic interactive paper on distill.pub called <a href="https://distill.pub/2016/deconv-checkerboard/" data-href="https://distill.pub/2016/deconv-checkerboard/" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">Deconvolution and Checkerboard Artifacts</a> which points out that what we are doing right now is extremely suboptimal but the good news is everybody else does it.</p><figure name="c74c" id="c74c" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_-EmXZ1cNtZEO-2SwEYG6bA.png"></figure><p name="64b4" id="64b4" class="graf graf--p graf-after--figure">Have a look here, could you see these checkerboard artifacts? These are all from actual papers and basically they noticed every one of these papers with generative models have these checkerboard artifacts and what they realized is it’s because when you have a stride 2 convolution of size three kernel, they overlap. So some grid cells gets twice as much activation,</p><figure name="09a6" id="09a6" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_rafmdyh7EfqCsptcOppq1w.png"></figure><p name="d420" id="d420" class="graf graf--p graf-after--figure">So even if you start with random weights, you end up with a checkerboard artifacts. So deeper you get, the worse it gets. Their advice is less direct than it ought to be, Jeremy found that for most generative models, upsampling is better. If you <code class="markup--code markup--p-code">nn.Upsample</code>, it’s basically doing the opposite of pooling — it says let’s replace this one grid cell with four (2x2). There is a number of ways to upsample — one is just to copy it all across to those four, and other is to use bilinear or bicubic interpolation. There are various techniques to try and create a smooth upsampled version and you can choose any of them in PyTorch. If you do a 2 x 2 upsample and then regular stride one 3 x 3 convolution, that is another way of doing the same kind of thing as a ConvTranspose — it’s doubling the grid size and doing some convolutional arithmetic on it. For generative models, it pretty much always works better. In that distil.pub publication, they indicate that maybe that’s a good approach but they don’t just come out and say just do this whereas Jeremy would just say just do this. Having said that, for GANS, he hasn’t had that much success with it yet and he thinks it probably requires some tweaking to get it to work, The issue is that in the early stages, it doesn’t create enough noise. He had a version where he tried to do it with an upsample and you could kind of see that the noise didn’t look very noisy. Next week when we look at style transfer and super-resolution, you will see <code class="markup--code markup--p-code">nn.Upsample</code> really comes into its own.</p><p name="d93d" id="d93d" class="graf graf--p graf-after--p">The generator, we can now start with the vector [<a href="https://youtu.be/ondivPiwQho?t=1h22m04s" data-href="https://youtu.be/ondivPiwQho?t=1h22m04s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:22:04</a>]. We can decide and say okay let’s not think of it as a vector but actually it’s 1x1 grid cell, and then we can turn it into a 4x4 then 8x8 and so forth. That is why we have to make sure it’s a suitable multiple so that we can create something of the right size. As you can see, it’s doing the exact opposite as before. It’s making the cell size bigger and bigger by 2 at a time as long as it can until it gets to half the size that we want, and then finally we add <code class="markup--code markup--p-code">n</code> more on at the end with stride 1. Then we add one more ConvTranspose to finally get to the size that we wanted and we are done. Finally we put that through a <code class="markup--code markup--p-code">tanh</code> and that will force us to be in the zero to one range because of course we don’t want to spit out arbitrary size pixel values. So we have a generator architecture which spits out an image of some given size with the correct number of channels with values between zero and one.</p><figure name="b092" id="b092" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_sNvYsoGpBl6vzCdcjWkH1Q.png"></figure><p name="c744" id="c744" class="graf graf--p graf-after--figure">At this point, we can now create our model data object [<a href="https://youtu.be/ondivPiwQho?t=1h23m38s" data-href="https://youtu.be/ondivPiwQho?t=1h23m38s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:23:38</a>]. These things take a while to train, so we made it 128 by 128 (just a convenient way to make it a little bit faster). So that is going to be the size of the input, but then we are going to use transformation to turn it into 64 by 64.</p><p name="bcb7" id="bcb7" class="graf graf--p graf-after--p">There’s been more recent advances which have attempted to really increase this up to high resolution sizes but they still tend to require either a batch size of 1 or lots and lots of GPUs [<a href="https://youtu.be/ondivPiwQho?t=1h24m5s" data-href="https://youtu.be/ondivPiwQho?t=1h24m5s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:24:05</a>]. So we are trying to do things that we can do with a single consumer GPU. Here is an example of one of the 64 by 64 bedrooms.</p><pre name="41d7" id="41d7" class="graf graf--pre graf-after--p">bs,sz,nz = 64,64,100</pre><pre name="c418" id="c418" class="graf graf--pre graf-after--pre">tfms = tfms_from_stats(inception_stats, sz)<br>md = ImageClassifierData.from_csv(PATH, 'bedroom', CSV_PATH, <br>         tfms=tfms, bs=128, skip_header=<strong class="markup--strong markup--pre-strong">False</strong>, continuous=<strong class="markup--strong markup--pre-strong">True</strong>)</pre><pre name="933f" id="933f" class="graf graf--pre graf-after--pre">md = md.resize(128)</pre><pre name="e838" id="e838" class="graf graf--pre graf-after--pre">x,_ = next(iter(md.val_dl))</pre><pre name="2cfc" id="2cfc" class="graf graf--pre graf-after--pre">plt.imshow(md.trn_ds.denorm(x)[0]);</pre><figure name="981e" id="981e" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_FIBPb5I8EloAjg7mvtRXaQ.png"></figure><h4 name="153c" id="153c" class="graf graf--h4 graf-after--figure">Putting them all together [<a href="https://youtu.be/ondivPiwQho?t=1h24m30s" data-href="https://youtu.be/ondivPiwQho?t=1h24m30s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:24:30</a>]</h4><p name="c565" id="c565" class="graf graf--p graf-after--h4">We are going to do pretty much everything manually so let’s go ahead and create our two models — our generator and discriminator and as you can see they are DCGAN, so in other words, they are the same modules that appeared in <a href="https://arxiv.org/abs/1511.06434" data-href="https://arxiv.org/abs/1511.06434" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">this paper</a>. It is well worth going back and looking at the DCGAN paper to see what these architectures are because it’s assumed that when you read the Wasserstein GAN paper that you already know that.</p><pre name="11df" id="11df" class="graf graf--pre graf-after--p">netG = DCGAN_G(sz, nz, 3, 64, 1).cuda()<br>netD = DCGAN_D(sz, 3, 64, 1).cuda()</pre><p name="6890" id="6890" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">Question</strong>: Shouldn’t we use a sigmoid if we want values between 0 and 1 [<a href="https://youtu.be/ondivPiwQho?t=1h25m6s" data-href="https://youtu.be/ondivPiwQho?t=1h25m6s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:25:06</a>]? As usual, our images have been normalized to have a range from -1 to 1, so their pixel values don’t go between 0 and 1 anymore. This is why we want values going from -1 to 1 otherwise we wouldn’t give a correct input for the discriminator (via <a href="http://forums.fast.ai/t/part-2-lesson-12-wiki/15023/140" data-href="http://forums.fast.ai/t/part-2-lesson-12-wiki/15023/140" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">this post</a>).</p><p name="2a92" id="2a92" class="graf graf--p graf-after--p">So we have a generator and a discriminator, and we need a function that returns a “prior” vector (i.e. a bunch of noise)[<a href="https://youtu.be/ondivPiwQho?t=1h25m49s" data-href="https://youtu.be/ondivPiwQho?t=1h25m49s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:25:49</a>]. We do that by creating a bunch of zeros. <code class="markup--code markup--p-code">nz</code> is the size of <code class="markup--code markup--p-code">z</code> — very often in our code, if you see a mysterious letter, it’s because that’s the letter they used in the paper. Here, <code class="markup--code markup--p-code">z</code> is the size of our noise vector. We then use normal distribution to generate random numbers between 0 and 1. And that needs to be a variable because it’s going to be participating in the gradient updates.</p><pre name="ffc4" id="ffc4" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> create_noise(b): <br>   <strong class="markup--strong markup--pre-strong">return</strong> V(torch.zeros(b, nz, 1, 1).normal_(0, 1))</pre><pre name="9c2d" id="9c2d" class="graf graf--pre graf-after--pre">preds = netG(create_noise(4))<br>pred_ims = md.trn_ds.denorm(preds)<br><br>fig, axes = plt.subplots(2, 2, figsize=(6, 6))<br><strong class="markup--strong markup--pre-strong">for</strong> i,ax <strong class="markup--strong markup--pre-strong">in</strong> enumerate(axes.flat): ax.imshow(pred_ims[i])</pre><figure name="bd78" id="bd78" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_4nHm3LLiShNb0pSS3dCuCw.png"></figure><p name="f958" id="f958" class="graf graf--p graf-after--figure">So here is an example of creating some noise and resulting four different pieces of noise.</p><pre name="d7b4" id="d7b4" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> gallery(x, nc=3):<br>    n,h,w,c = x.shape<br>    nr = n//nc<br>    <strong class="markup--strong markup--pre-strong">assert</strong> n == nr*nc<br>    <strong class="markup--strong markup--pre-strong">return</strong> (x.reshape(nr, nc, h, w, c)<br>              .swapaxes(1,2)<br>              .reshape(h*nr, w*nc, c))</pre><p name="880f" id="880f" class="graf graf--p graf-after--pre">We need an optimizer in order to update our gradients [<a href="https://youtu.be/ondivPiwQho?t=1h26m41s" data-href="https://youtu.be/ondivPiwQho?t=1h26m41s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:26:41</a>]. In the Wasserstein GAN paper, they told us to use RMSProp:</p><figure name="5e2b" id="5e2b" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_5o4cwLlNjQfgrNVgLrsVlg.png"></figure><p name="d1a0" id="d1a0" class="graf graf--p graf-after--figure">We can easily do that in PyTorch:</p><pre name="8d3d" id="8d3d" class="graf graf--pre graf-after--p">optimizerD = optim.RMSprop(netD.parameters(), lr = 1e-4)<br>optimizerG = optim.RMSprop(netG.parameters(), lr = 1e-4)</pre><p name="e8a8" id="e8a8" class="graf graf--p graf-after--pre">In the paper, they suggested a learning rate of 0.00005 (<code class="markup--code markup--p-code">5e-5</code>), we found <code class="markup--code markup--p-code">1e-4</code> seem to work, so we made it a little bit bigger.</p><p name="76b3" id="76b3" class="graf graf--p graf-after--p">Now we need a training loop [<a href="https://youtu.be/ondivPiwQho?t=1h27m14s" data-href="https://youtu.be/ondivPiwQho?t=1h27m14s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:27:14</a>]:</p><figure name="214c" id="214c" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_VROXSgyt6HWaJiMMY6ogFQ.png"><figcaption class="imageCaption">For easier&nbsp;reading</figcaption></figure><p name="23ec" id="23ec" class="graf graf--p graf-after--figure">A training loop will go through some number of epochs that we get to pick (so that’s going to be a parameter). Remember, when you do everything manually, you’ve got to remember all the manual steps to do:</p><ol class="postList"><li name="de06" id="de06" class="graf graf--li graf-after--p">You have to set your modules into training mode when you are training them and into evaluation mode when you are evaluating because in training mode batch norm updates happen and dropout happens, in evaluation mode, those two things gets turned off.</li><li name="9f6d" id="9f6d" class="graf graf--li graf-after--li">We are going to grab an iterator from our training data loader</li><li name="db40" id="db40" class="graf graf--li graf-after--li">We are going to see how many steps we have to go through and then we will use <code class="markup--code markup--li-code">tqdm</code> to give us a progress bar, and we are going to go through that many steps.</li></ol><p name="8bb6" id="8bb6" class="graf graf--p graf-after--li">The first step of the algorithm in the paper is to update the discriminator (in the paper, they call discriminator a “critic” and <code class="markup--code markup--p-code">w</code> is the weights of the critic). So the first step is to train our critic a little bit, and then we are going to train our generator a little bit, and we will go back to the top of the loop. The inner <code class="markup--code markup--p-code">for</code> loop in the paper correspond to the second <code class="markup--code markup--p-code">while</code> loop in our code.</p><p name="a506" id="a506" class="graf graf--p graf-after--p">What we are going to do now is we have a generator that is random at the moment [<a href="https://youtu.be/ondivPiwQho?t=1h29m6s" data-href="https://youtu.be/ondivPiwQho?t=1h29m6s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:29:06</a>]. So our generator will generate something that looks like the noise. First of all, we need to teach our discriminator to tell the difference between the noise and a bedroom — which shouldn’t be too hard you would hope. So we just do it in the usual way but there is a few little tweaks:</p><ol class="postList"><li name="e712" id="e712" class="graf graf--li graf-after--p">We are going to grab a mini batch of real bedroom photos so we can just grab the next batch from our iterator, turn it into a variable.</li><li name="496d" id="496d" class="graf graf--li graf-after--li">Then we are going to calculate the loss for that — so this is going to be how much the discriminator thinks this looks fake (“does the real one look fake?”).</li><li name="7d54" id="7d54" class="graf graf--li graf-after--li">Then we are going to create some fake images and to do that we will create some random noise, and we will stick it through our generator which at this stage is just a bunch of random weights. That will create a mini batch of fake images.</li><li name="2218" id="2218" class="graf graf--li graf-after--li">Then we will put that through the same discriminator module as before to get the loss for that (“how fake does the fake one look?”). Remember, when you do everything manually, you have to zero the gradients (<code class="markup--code markup--li-code">netD.zero_grad()</code>) in your loop. If you have forgotten about that, go back to the part 1 lesson where we do everything from scratch.</li><li name="1238" id="1238" class="graf graf--li graf-after--li">Finally, the total discriminator loss is equal to the real loss minus the fake loss.</li></ol><p name="d220" id="d220" class="graf graf--p graf-after--li">So you can see that here [<a href="https://youtu.be/ondivPiwQho?t=1h30m58s" data-href="https://youtu.be/ondivPiwQho?t=1h30m58s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:30:58</a>]:</p><figure name="00e3" id="00e3" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*atls5DInIbp5wHZz8szQ1A.png" data-width="528" data-height="32" src="../img/1_atls5DInIbp5wHZz8szQ1A.png"></figure><p name="a7c2" id="a7c2" class="graf graf--p graf-after--figure">They don’t talk about the loss, they actually just talk about one of the gradient updates.</p><figure name="83a0" id="83a0" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_9nGWityXFzNdgOxN15flRA.png"></figure><p name="4ab6" id="4ab6" class="graf graf--p graf-after--figure">In PyTorch, we don’t have to worry about getting the gradients, we can just specify the loss and call <code class="markup--code markup--p-code">loss.backward()</code> then discriminator’s <code class="markup--code markup--p-code">optimizer.step()</code>[<a href="https://youtu.be/ondivPiwQho?t=1h34m27s" data-href="https://youtu.be/ondivPiwQho?t=1h34m27s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:34:27</a>]. There is one key step which is that we have to keep all of our weights which are the parameters in PyTorch module in the small range of -0.01 and 0.01. Why? Because the mathematical assumptions that make this algorithm work only apply in a small ball. It is interesting to understand the math of why that is the case, but it’s very specific to this one paper and understanding it won’t help you understand any other paper, so only study it if you are interested. It is nicely explained and Jeremy thinks it’s fun but it won’t be information that you will reuse elsewhere unless you get super into GANs. He also mentioned that after the came out and improved Wasserstein GAN came out that said there are better ways to ensure that your weight space is in this tight ball which was to penalize gradients that are too high, so nowadays there are slightly different ways to do this. But this line of code is the key contribution and it is what makes it Wasserstein GAN:</p><pre name="4628" id="4628" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">for</strong> p <strong class="markup--strong markup--pre-strong">in</strong> netD.parameters(): p.data.clamp_(-0.01, 0.01)</pre><p name="63d0" id="63d0" class="graf graf--p graf-after--pre">At the end of this, we have a discriminator that can recognize real bedrooms and our totally random crappy generated images [<a href="https://youtu.be/ondivPiwQho?t=1h36m20s" data-href="https://youtu.be/ondivPiwQho?t=1h36m20s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:36:20</a>]. Let’s now try and create some better images. So now set trainable discriminator to false, set trainable generator to true, zero out the gradients of the generator. Our loss again is <code class="markup--code markup--p-code">fw</code> (discriminator) of the generator applied to some more random noise. So it’s exactly the same as before where we did generator on the noise and then pass that to a discriminator, but this time, the thing that’s trainable is the generator, not the discriminator. In other words, in the pseudo code, the thing they update is Ɵ which is the generator’s parameters. So it takes noise, generate some images, try and figure out if they are fake or real, and use that to get gradients with respect to the generator, as opposed to earlier we got them with respect to the discriminator, and use that to update our weights with RMSProp with an alpha learning rate [<a href="https://youtu.be/ondivPiwQho?t=1h38m21s" data-href="https://youtu.be/ondivPiwQho?t=1h38m21s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:38:21</a>].</p><pre name="fb18" id="fb18" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> train(niter, first=<strong class="markup--strong markup--pre-strong">True</strong>):<br>    gen_iterations = 0<br>    <strong class="markup--strong markup--pre-strong">for</strong> epoch <strong class="markup--strong markup--pre-strong">in</strong> trange(niter):<br>        netD.train(); netG.train()<br>        data_iter = iter(md.trn_dl)<br>        i,n = 0,len(md.trn_dl)<br>        <strong class="markup--strong markup--pre-strong">with</strong> tqdm(total=n) <strong class="markup--strong markup--pre-strong">as</strong> pbar:<br>            <strong class="markup--strong markup--pre-strong">while</strong> i &lt; n:<br>                set_trainable(netD, <strong class="markup--strong markup--pre-strong">True</strong>)<br>                set_trainable(netG, <strong class="markup--strong markup--pre-strong">False</strong>)<br>                d_iters = 100 <strong class="markup--strong markup--pre-strong">if</strong> (first <strong class="markup--strong markup--pre-strong">and</strong> (gen_iterations &lt; 25) <br>                              <strong class="markup--strong markup--pre-strong">or</strong> (gen_iterations % 500 == 0)) <strong class="markup--strong markup--pre-strong">else</strong> 5<br>                j = 0<br>                <strong class="markup--strong markup--pre-strong">while</strong> (j &lt; d_iters) <strong class="markup--strong markup--pre-strong">and</strong> (i &lt; n):<br>                    j += 1; i += 1<br>                    <strong class="markup--strong markup--pre-strong">for</strong> p <strong class="markup--strong markup--pre-strong">in</strong> netD.parameters(): <br>                        p.data.clamp_(-0.01, 0.01)<br>                    real = V(next(data_iter)[0])<br>                    real_loss = netD(real)<br>                    fake = netG(create_noise(real.size(0)))<br>                    fake_loss = netD(V(fake.data))<br>                    netD.zero_grad()<br>                    lossD = real_loss-fake_loss<br>                    lossD.backward()<br>                    optimizerD.step()<br>                    pbar.update()<br><br>                set_trainable(netD, <strong class="markup--strong markup--pre-strong">False</strong>)<br>                set_trainable(netG, <strong class="markup--strong markup--pre-strong">True</strong>)<br>                netG.zero_grad()<br>                lossG = netD(netG(create_noise(bs))).mean(0).view(1)<br>                lossG.backward()<br>                optimizerG.step()<br>                gen_iterations += 1<br>            <br>        print(f'Loss_D {to_np(lossD)}; Loss_G {to_np(lossG)}; '<br>              f'D_real {to_np(real_loss)}; Loss_D_fake<br>              {to_np(fake_loss)}')</pre><p name="d689" id="d689" class="graf graf--p graf-after--pre">You’ll see that it’s unfair that the discriminator is getting trained <em class="markup--em markup--p-em">ncritic</em> times (<code class="markup--code markup--p-code">d_iters</code> in above code) which they set to 5 for every time we train the generator once. And the paper talks a bit about this but the basic idea is there is no point making the generator better if the discriminator doesn’t know how to discriminate yet. So that’s why we have the second while loop. And here is that 5:</p><pre name="e3d8" id="e3d8" class="graf graf--pre graf-after--p">d_iters = 100 <strong class="markup--strong markup--pre-strong">if</strong> (first <strong class="markup--strong markup--pre-strong">and</strong> (gen_iterations &lt; 25) <br>                              <strong class="markup--strong markup--pre-strong">or</strong> (gen_iterations % 500 == 0)) <strong class="markup--strong markup--pre-strong">else</strong> 5</pre><p name="586b" id="586b" class="graf graf--p graf-after--pre">Actually something which was added in the later paper or maybe supplementary material is the idea that from time to time and a bunch of times at the start, you should do more steps at the discriminator to make sure that the discriminator is capable.</p><pre name="c2f1" id="c2f1" class="graf graf--pre graf-after--p">torch.backends.cudnn.benchmark=<strong class="markup--strong markup--pre-strong">True</strong></pre><p name="56e9" id="56e9" class="graf graf--p graf-after--pre">Let’s train that for one epoch:</p><pre name="6c49" id="6c49" class="graf graf--pre graf-after--p">train(1, <strong class="markup--strong markup--pre-strong">False</strong>)</pre><pre name="72c2" id="72c2" class="graf graf--pre graf-after--pre">0%|          | 0/1 [00:00&lt;?, ?it/s]<br>100%|██████████| 18957/18957 [19:48&lt;00:00, 10.74it/s]<br>Loss_D [-0.67574]; Loss_G [0.08612]; D_real [-0.1782]; Loss_D_fake [0.49754]<br>100%|██████████| 1/1 [19:49&lt;00:00, 1189.02s/it]</pre><p name="40b8" id="40b8" class="graf graf--p graf-after--pre">Then let’s create some noise so we can generate some examples.</p><pre name="4975" id="4975" class="graf graf--pre graf-after--p">fixed_noise = create_noise(bs)</pre><p name="fbed" id="fbed" class="graf graf--p graf-after--pre">But before that, reduce the learning rate by 10 and do one more pass:</p><pre name="6b40" id="6b40" class="graf graf--pre graf-after--p">set_trainable(netD, <strong class="markup--strong markup--pre-strong">True</strong>)<br>set_trainable(netG, <strong class="markup--strong markup--pre-strong">True</strong>)<br>optimizerD = optim.RMSprop(netD.parameters(), lr = 1e-5)<br>optimizerG = optim.RMSprop(netG.parameters(), lr = 1e-5)</pre><pre name="9992" id="9992" class="graf graf--pre graf-after--pre">train(1, <strong class="markup--strong markup--pre-strong">False</strong>)</pre><pre name="00f0" id="00f0" class="graf graf--pre graf-after--pre">0%|          | 0/1 [00:00&lt;?, ?it/s]<br>100%|██████████| 18957/18957 [23:31&lt;00:00, 13.43it/s]<br>Loss_D [-1.01657]; Loss_G [0.51333]; D_real [-0.50913]; Loss_D_fake [0.50744]<br>100%|██████████| 1/1 [23:31&lt;00:00, 1411.84s/it]</pre><p name="fd3f" id="fd3f" class="graf graf--p graf-after--pre">Then let’s use the noise to pass it to our generator, then put it through our denormalization to turn it back into something we can see, and then plot it:</p><pre name="3ece" id="3ece" class="graf graf--pre graf-after--p">netD.eval(); netG.eval();<br>fake = netG(fixed_noise).data.cpu()<br>faked = np.clip(md.trn_ds.denorm(fake),0,1)<br><br>plt.figure(figsize=(9,9))<br>plt.imshow(gallery(faked, 8));</pre><figure name="1f9a" id="1f9a" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_b8XHbkL7E3tREt_T2mXFqQ.png"></figure><p name="e6e1" id="e6e1" class="graf graf--p graf-after--figure">And we have some bedrooms. These are not real bedrooms, and some of them don’t look particularly like bedrooms, but some of them look a lot like bedrooms, so that’s the idea. That’s GAN. The best way to think about GAN is it is like an underlying technology that you will probably never use like this, but you will use in lots of interesting ways. For example, we are going to use it to create a cycle GAN.</p><p name="a340" id="a340" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: Is there any reason for using RMSProp specifically as the optimizer as opposed to Adam etc. [<a href="https://youtu.be/ondivPiwQho?t=1h41m38s" data-href="https://youtu.be/ondivPiwQho?t=1h41m38s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:41:38</a>]? I don’t remember it being explicitly discussed in the paper. I don’t know if it’s just experimental or the theoretical reason. Have a look in the paper and see what it says.</p><p name="5b72" id="5b72" class="graf graf--p graf-after--p"><a href="http://forums.fast.ai/t/part-2-lesson-12-wiki/15023/211" data-href="http://forums.fast.ai/t/part-2-lesson-12-wiki/15023/211" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">From the forum</a></p><blockquote name="5f5d" id="5f5d" class="graf graf--blockquote graf-after--p">From experimenting I figured that Adam and WGANs not just work worse — it causes to completely fail to train meaningful generator.</blockquote><blockquote name="80d2" id="80d2" class="graf graf--blockquote graf-after--blockquote">from WGAN paper:</blockquote><blockquote name="3df9" id="3df9" class="graf graf--blockquote graf-after--blockquote"><em class="markup--em markup--blockquote-em">Finally, as a negative result, we report that WGAN training becomes unstable at times when one uses a momentum based optimizer such as Adam [8] (with β1&gt;0) on the critic, or when one uses high learning rates. Since the loss for the critic is nonstationary, momentum based methods seemed to perform worse. We identified momentum as a potential cause because, as the loss blew up and samples got worse, the cosine between the Adam step and the gradient usually turned negative. The only places where this cosine was negative was in these situations of instability. We therefore switched to RMSProp [21] which is known to perform well even on very nonstationary problems</em></blockquote><p name="3111" id="3111" class="graf graf--p graf-after--blockquote"><strong class="markup--strong markup--p-strong">Question</strong>: Which could be a reasonable way of detecting overfitting while training? Or of evaluating the performance of one of these GAN models once we are done training? In other words, how does the notion of train/val/test sets translate to GANs [<a href="https://youtu.be/ondivPiwQho?t=1h41m57s" data-href="https://youtu.be/ondivPiwQho?t=1h41m57s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:41:57</a>]? That is an awesome question, and there’s a lot of people who make jokes about how GANs is the one field where you don’t need a test set and people take advantage of that by making stuff up and saying it looks great. There are some famous problems with GANs, one of them is called Mode Collapse. Mode collapse happens where you look at your bedrooms and it turns out that there’s only three kinds of bedrooms that every possible noise vector maps to. You look at your gallery and it turns out they are all just the same thing or just three different things. Mode collapse is easy to see if you collapse down to a small number of modes, like 3 or 4. But what if you have a mode collapse down to 10,000 modes? So there are only 10,000 possible bedrooms that all of your noise vectors collapse to. You wouldn’t be able to see in the gallery view we just saw because it’s unlikely you would have two identical bedrooms out of 10,000. Or what if every one of these bedrooms is basically a direct copy of one of the input — it basically memorized some input. Could that be happening? And the truth is, most papers don’t do a good job or sometimes any job of checking those things. So the question of how do we evaluate GANS and even the point of maybe we should actually evaluate GANs properly is something that is not widely enough understood even now. Some people are trying to really push. Ian Goodfellow was the first author on the most famous deep learning book and is the inventor of GANs and he’s been sending continuous stream of tweets reminding people about the importance of testing GANs properly. If you see a paper that claims exceptional GAN results, then this is definitely something to look at. Have they talked about mode collapse? Have they talked about memorization? And so forth.</p><p name="1811" id="1811" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: Can GANs be used for data augmentation [<a href="https://youtu.be/ondivPiwQho?t=1h45m33s" data-href="https://youtu.be/ondivPiwQho?t=1h45m33s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:45:33</a>]? Yeah, absolutely you can use GAN for data augmentation. Should you? I don’t know. There are some papers that try to do semi-supervised learning with GANs. I haven’t found any that are particularly compelling showing state-of-the-art results on really interesting datasets that have been widely studied. I’m a little skeptical and the reason I’m a little skeptical is because in my experience, if you train a model with synthetic data, the neural net will become fantastically good at recognizing the specific problems of your synthetic data and that’ll end up what it’s learning from. There are lots of other ways of doing semi-supervised models which do work well. There are some places that can work. For example, you might remember Otavio Good created that fantastic visualization in part 1 of the zooming conv net where it showed letter going through MNIST, he, at least at that time, was the number one in autonomous remote control car competitions, and he trained his model using synthetically augmented data where he basically took real videos of a car driving around the circuit and added fake people and fake other cars. I think that worked well because A. he is kind of a genius and B. because I think he had a well defined little subset that he had to work in. But in general, it’s really really hard to use synthetic data. I’ve tried using synthetic data and models for decades now (obviously not GANs because they’re pretty new) but in general it’s very hard to do. Very interesting research question.</p><h3 name="520c" id="520c" class="graf graf--h3 graf-after--p">Cycle GAN [<a href="https://youtu.be/ondivPiwQho?t=1h41m8s" data-href="https://youtu.be/ondivPiwQho?t=1h41m8s" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">1:41:08</a>]</h3><p name="f1dd" id="f1dd" class="graf graf--p graf-after--h3"><a href="https://arxiv.org/abs/1703.10593" data-href="https://arxiv.org/abs/1703.10593" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Paper</a> / <a href="https://github.com/fastai/fastai/blob/master/courses/dl2/cyclegan.ipynb" data-href="https://github.com/fastai/fastai/blob/master/courses/dl2/cyclegan.ipynb" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Notebook</a></p><p name="a6c8" id="a6c8" class="graf graf--p graf-after--p">We are going to use cycle GAN to turn horses into zebras. You can also use it to turn Monet prints into photos or to turn photos of Yosemite in summer into winter.</p><figure name="239a" id="239a" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_dWd0lVTbnu80UZM641gCbw.gif"></figure><p name="6582" id="6582" class="graf graf--p graf-after--figure">This is going to be really straight forward because it’s just a neural net [<a href="https://youtu.be/ondivPiwQho?t=1h44m46s" data-href="https://youtu.be/ondivPiwQho?t=1h44m46s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:44:46</a>]. All we are going to do is we are going to create an input containing lots of zebra photos and with each one we’ll pair it with an equivalent horse photo and we’ll just train a neural net that goes from one to the other. Or you could do the same thing for every Monet painting — create a dataset containing the photo of the place&nbsp;…oh wait, that’s not possible because the places that Monet painted aren’t there anymore and there aren’t exact zebra versions of horses&nbsp;…how the heck is this going to work? This seems to break everything we know about what neural nets can do and how they do them.</p><p name="e271" id="e271" class="graf graf--p graf-after--p">So somehow these folks at Berkeley cerated a model that can turn a horse into a zebra despite not having any photos. Unless they went out there and painted horses and took before-and-after shots but I believe they didn’t [<a href="https://youtu.be/ondivPiwQho?t=1h47m51s" data-href="https://youtu.be/ondivPiwQho?t=1h47m51s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:47:51</a>]. So how the heck did they do this? It’s kind of genius.</p><p name="e0f6" id="e0f6" class="graf graf--p graf-after--p">The person I know who is doing the most interesting practice of cycle GAN right now is one of our students Helena Sarin <a href="https://twitter.com/glagolista" data-href="https://twitter.com/glagolista" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong">@</strong>glagolista</a>. She is the only artist I know of who is a cycle GAN artist.</p><figure name="c22d" id="c22d" class="graf graf--figure graf--layoutOutsetRow is-partialWidth graf-after--p" style="width: 49.105%;"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_y0xHbQJvxcwUsx7EEK4nHQ.jpeg"></figure><figure name="a429" id="a429" class="graf graf--figure graf--layoutOutsetRowContinue is-partialWidth graf-after--figure" style="width: 50.895%;"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_QZWqdoLXR1TjgeWDivTlnA.jpeg"></figure><figure name="8521" id="8521" class="graf graf--figure graf--layoutOutsetRow is-partialWidth graf-after--figure" style="width: 41.493%;"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_JIF1OaO04wxkWIP_7b14uA.jpeg"></figure><figure name="be01" id="be01" class="graf graf--figure graf--layoutOutsetRowContinue is-partialWidth graf-after--figure" style="width: 58.507%;"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_xn7L_rsu2J6Py2Mjq_q1LA.jpeg"></figure><p name="4467" id="4467" class="graf graf--p graf-after--figure">Here are some more of her amazing works and I think it’s really interesting. I mentioned at the start of this class that GANs are in the category of stuff that is not there yet, but it’s nearly there. And in this case, there is at least one person in the world who is creating beautiful and extraordinary artworks using GANs (specifically cycle GANs). At least a dozen people I know of who are just doing interesting creative work with neural nets more generally. And the field of creative AI is going to expand dramatically.</p><figure name="9c03" id="9c03" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_oqSRuiHT8Z9pWl0Zq9_Sjw.png"></figure><p name="349f" id="349f" class="graf graf--p graf-after--figure">Here is the basic trick [<a href="https://youtu.be/ondivPiwQho?t=1h50m11s" data-href="https://youtu.be/ondivPiwQho?t=1h50m11s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:50:11</a>]. This is from the cycle GAN paper. We are going to have two images (assuming we are doing this with images). The key thing is they are not paired images, so we don’t have a dataset of horses and the equivalent zebras. We have bunch of horses, and bunch of zebras. Grab one horse <em class="markup--em markup--p-em">X</em>, grab one zebra <em class="markup--em markup--p-em">Y</em>. We are going to train a generator (what they call here a “mapping function”) that turns horse into zebra. We’ll call that mapping function <em class="markup--em markup--p-em">G</em> and we’ll create one mapping function (a.k.a. generator) that turns a zebra into a horse and we will call that <em class="markup--em markup--p-em">F. </em>We will create a discriminator just like we did before which is going to get as good as possible at recognizing real from fake horses so that will be <em class="markup--em markup--p-em">Dx. </em>Another discriminator which is going to be as good as possible at recognizing real from fake zebras, we will call that <em class="markup--em markup--p-em">Dy</em>. That is our starting point.</p><p name="146d" id="146d" class="graf graf--p graf-after--p">The key thing to making this work [<a href="https://youtu.be/ondivPiwQho?t=1h51m27s" data-href="https://youtu.be/ondivPiwQho?t=1h51m27s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:51:27</a>]— so we are generating a loss function here (<em class="markup--em markup--p-em">Dx</em> and <em class="markup--em markup--p-em">Dy</em>). We are going to create something called <strong class="markup--strong markup--p-strong">cycle-consistency loss</strong> which says after you turn your horse into a zebra with your generator, and check whether or not I can recognize that it’s a real. We turn our horse into a zebra and then going to try and turn that zebra back into the same horse that we started with. Then we are going to have another function that is going to check whether this horse which are generated knowing nothing about <em class="markup--em markup--p-em">x</em> — generated entirely from this zebra <em class="markup--em markup--p-em">Y </em>is similar to the original horse or not. So the idea would be if your generated zebra doesn’t look anything like your original horse, you’ve got no chance of turning it back into the original horse. So a loss which compares <em class="markup--em markup--p-em">x-hat</em> to <em class="markup--em markup--p-em">x</em> is going to be really bad unless you can go into <em class="markup--em markup--p-em">Y</em> and back out again and you’re probably going to be able to do that if you’re able to create a zebra that looks like the original horse so that you know what the original horse looked like. And vice versa — take your zebra, turn it into a fake horse, and check that you can recognize that and then try and turn it back into the original zebra and check that it looks like the original.</p><p name="ed14" id="ed14" class="graf graf--p graf-after--p">So notice <em class="markup--em markup--p-em">F</em> (zebra to horse) and <em class="markup--em markup--p-em">G</em> (horse to zebra) are doing two things [<a href="https://youtu.be/ondivPiwQho?t=1h53m9s" data-href="https://youtu.be/ondivPiwQho?t=1h53m9s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:53:09</a>]. They are both turning the original horse into the zebra, and then turning the zebra back into the original horse. So there are only two generators. There isn’t a separate generator for the reverse mapping. You have to use the same generator that was used for the original mapping. So this is the cycle-consistency loss. I think this is genius. The idea that this is a thing that could even be possible. Honestly when this came out, it just never occurred to me as a thing that I could even try and solve. It seems so obviously impossible and then the idea that you can solve it like this — I just think it’s so darn smart.</p><p name="2d94" id="2d94" class="graf graf--p graf-after--p">It’s good to look at the equations in this paper because they are good examples — they are written pretty simply and it’s not like some of the Wasserstein GAN paper which is lots of theoretical proofs and whatever else [<a href="https://youtu.be/ondivPiwQho?t=1h54m5s" data-href="https://youtu.be/ondivPiwQho?t=1h54m5s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:54:05</a>]. In this case, they are just equations that lay out what’s going on. You really want to get to a point where you can read them and understand them.</p><figure name="96ba" id="96ba" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_Mygxs_TWrjycbanbH5aUeQ.png"></figure><p name="e152" id="e152" class="graf graf--p graf-after--figure">So we’ve got a horse <em class="markup--em markup--p-em">X</em> and a zebra <em class="markup--em markup--p-em">Y</em>[<a href="https://youtu.be/ondivPiwQho?t=1h54m34s" data-href="https://youtu.be/ondivPiwQho?t=1h54m34s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:54:34</a>]. For some mapping function <em class="markup--em markup--p-em">G</em> which is our horse to zebra mapping function then there is a GAN loss which is a bit we are already familiar with it says we have a horse, a zebra, a fake zebra recognizer, and a horse-zebra generator. The loss is what we saw before — it’s our ability to draw one zebra out of our zebras and recognize whether it is real or fake. Then take a horse and turn it into a zebra and recognize whether that’s real or fake. You then do one minus the other (in this case, they have a log in there but the log is not terribly important). So this is the thing we just saw. That is why we did Wasserstein GAN first. This is just a standard GAN loss in math form.</p><p name="8979" id="8979" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: All of this sounds awfully like translating in one language to another then back to the original. Have GANs or any equivalent been tried in translation [<a href="https://youtu.be/ondivPiwQho?t=1h55m54s" data-href="https://youtu.be/ondivPiwQho?t=1h55m54s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:55:54</a>]? <a href="https://arxiv.org/abs/1711.00043" data-href="https://arxiv.org/abs/1711.00043" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">Paper from the forum</a>. Back up to what I do know — normally with translation you require this kind of paired input (i.e. parallel text — “this is the French translation of this English sentence”). There has been a couple of recent papers that show the ability to create good quality translation models without paired data. I haven’t implemented them and I don’t understand anything I haven’t implemented, but they may well be doing the same basic idea. We’ll look at it during the week and get back to you.</p><p name="3009" id="3009" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Cycle-consistency loss</strong> [<a href="https://youtu.be/ondivPiwQho?t=1h57m14s" data-href="https://youtu.be/ondivPiwQho?t=1h57m14s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:57:14</a>]: So we’ve got a GAN loss and the next piece is the cycle-consistency loss. So the basic idea here is that we start with our horse, use our zebra generator on that to create a zebra, use our horse generator on that to create a horse and compare that to the original horse. This double lines with the 1 is the L1 loss — sum of the absolute value of differences [<a href="https://youtu.be/ondivPiwQho?t=1h57m35s" data-href="https://youtu.be/ondivPiwQho?t=1h57m35s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:57:35</a>]. Where else if this was 2, it would be the L2 loss so the 2-norm which would be the sum of the squared differences.</p><figure name="c900" id="c900" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_0wq511kW9eRhBMWS94G0Bw.png"></figure><p name="193d" id="193d" class="graf graf--p graf-after--figure">We now know this squiggle idea which is from our horses grab a horse. This is what we mean by sample from a distribution. There’s all kinds of distributions but most commonly in these papers we’re using an empirical distribution, in other words we’ve got some rows of data, grab a row. So here, it is saying grab something from the data and we are going to call that thing <em class="markup--em markup--p-em">x</em>. To recapture:</p><ol class="postList"><li name="df84" id="df84" class="graf graf--li graf-after--p">From our horse pictures, grab a horse</li><li name="c679" id="c679" class="graf graf--li graf-after--li">Turn it into a zebra</li><li name="8318" id="8318" class="graf graf--li graf-after--li">Turn it back into a horse</li><li name="8543" id="8543" class="graf graf--li graf-after--li">Compare it to the original and sum of the absolute values</li><li name="c6fa" id="c6fa" class="graf graf--li graf-after--li">Do it for zebra to horse as well</li><li name="44c8" id="44c8" class="graf graf--li graf-after--li">And add the two together</li></ol><p name="6f8b" id="6f8b" class="graf graf--p graf-after--li">That is our cycle-consistency loss.</p><p name="0993" id="0993" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Full objective</strong> [<a href="https://youtu.be/ondivPiwQho?t=1h58m54s" data-href="https://youtu.be/ondivPiwQho?t=1h58m54s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:58:54</a>]</p><figure name="3bfd" id="3bfd" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_84eYJ5eck_7r3zVJzrzGzA.png"></figure><p name="9647" id="9647" class="graf graf--p graf-after--figure">Now we get our loss function and the whole loss function depends on:</p><ul class="postList"><li name="836d" id="836d" class="graf graf--li graf-after--p">our horse generator</li><li name="5bd6" id="5bd6" class="graf graf--li graf-after--li">a zebra generator</li><li name="eb3b" id="eb3b" class="graf graf--li graf-after--li">our horse recognizer</li><li name="31fd" id="31fd" class="graf graf--li graf-after--li">our zebra recognizer (a.k.a. discriminator)</li></ul><p name="0b41" id="0b41" class="graf graf--p graf-after--li">We are going to add up&nbsp;:</p><ul class="postList"><li name="c459" id="c459" class="graf graf--li graf-after--p">the GAN loss for recognizing horses</li><li name="b5ea" id="b5ea" class="graf graf--li graf-after--li">GAN loss for recognizing zebras</li><li name="d133" id="d133" class="graf graf--li graf-after--li">the cycle-consistency loss for our two generators</li></ul><p name="2933" id="2933" class="graf graf--p graf-after--li">We have a lambda here which hopefully we are kind of used to this idea now that is when you have two different kinds of loss, you chuck in a parameter there you can multiply them by so they are about the same scale [<a href="https://youtu.be/ondivPiwQho?t=1h59m23s" data-href="https://youtu.be/ondivPiwQho?t=1h59m23s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:59:23</a>]. We did a similar thing with our bounding box loss compared to our classifier loss when we did the localization.</p><p name="fa0c" id="fa0c" class="graf graf--p graf-after--p">Then for this loss function, we are going to try to maximize the capability of the discriminators to discriminate, whilst minimizing that for the generators. So the generators and the discriminators are going to be facing off against each other. When you see this <em class="markup--em markup--p-em">min max </em>thing in papers, it basically means this idea that in your training loop, one thing is trying to make something better, the other is trying to make something worse, and there’re lots of ways to do it but most commonly, you’ll alternate between the two. You will often see this just referred to in math papers as min-max. So when you see min-max, you should immediately think <strong class="markup--strong markup--p-strong">adversarial training</strong>.</p><h4 name="3df1" id="3df1" class="graf graf--h4 graf-after--p">Implementing cycle GAN [<a href="https://youtu.be/ondivPiwQho?t=2h41s" data-href="https://youtu.be/ondivPiwQho?t=2h41s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">2:00:41</a>]</h4><p name="a096" id="a096" class="graf graf--p graf-after--h4">Let’s look at the code. We are going to do something almost unheard of which is I started looking at somebody else’s code and I was not so disgusted that I threw the whole thing away and did it myself. I actually said I quite like this, I like it enough I’m going to show it to my students. <a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix" data-href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">This</a> is where the code came from, and this is one of the people that created the original code for cycle GANs and they created a PyTorch version. I had to clean it up a little bit but it’s actually pretty darn good. The cool thing about this is that you are now going to get to see almost all the bits of fast.ai or all the relevant bits of fast.ai written in a different way by somebody else. So you’re going to get to see how they do datasets, data loaders, models, training loops, and so forth.</p><p name="56cc" id="56cc" class="graf graf--p graf-after--p">You’ll find there is a <code class="markup--code markup--p-code">cgan</code> directory [<a href="https://youtu.be/ondivPiwQho?t=2h2m12s" data-href="https://youtu.be/ondivPiwQho?t=2h2m12s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:02:12</a>] which is basically nearly the original with some cleanups which I hope to submit as a PR sometime&nbsp;. It was written in a way that unfortunately made it a bit over connected to how they were using it as a script, so I cleaned it up a little bit so I could use it as a module. But other than that, it’s pretty similar.</p><pre name="e22b" id="e22b" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">fastai.conv_learner</strong> <strong class="markup--strong markup--pre-strong">import</strong> *<br><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">fastai.dataset</strong> <strong class="markup--strong markup--pre-strong">import</strong> *<br><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">cgan.options.train_options</strong> <strong class="markup--strong markup--pre-strong">import</strong> *</pre><p name="778a" id="778a" class="graf graf--p graf-after--pre">So <code class="markup--code markup--p-code">cgan</code> is their code copied from their github repo with some minor changes. The way <code class="markup--code markup--p-code">cgan</code> mini library has been set up is that the configuration options, they are assuming, are being passed into like a script. So they have <code class="markup--code markup--p-code">TrainOptions().parse</code> method and I’m basically passing in an array of script options (where’s my data, how many threads, do I want to dropout, how many iterations, what am I going to call this model, which GPU do I want run it on). That gives us an <code class="markup--code markup--p-code">opt</code> object which you can see what it contains. You’ll see that it contains some things we didn’t mention that is because it has defaults for everything else that we didn’t mention.</p><pre name="41b7" id="41b7" class="graf graf--pre graf-after--p">opt = TrainOptions().parse(['--dataroot',    <br>   '/data0/datasets/cyclegan/horse2zebra', '--nThreads', '8', <br>   '--no_dropout', '--niter', '100', '--niter_decay', '100', <br>   '--name', 'nodrop', '--gpu_ids', '2'])</pre><p name="60aa" id="60aa" class="graf graf--p graf-after--pre">So rather than using fast.ai stuff, we are going to largely use cgan stuff.</p><pre name="98b8" id="98b8" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">cgan.data.data_loader</strong> <strong class="markup--strong markup--pre-strong">import</strong> CreateDataLoader<br><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">cgan.models.models</strong> <strong class="markup--strong markup--pre-strong">import</strong> create_model</pre><p name="dc65" id="dc65" class="graf graf--p graf-after--pre">The first thing we are going to need is a data loader. So this is also a great opportunity for you again to practice your ability to navigate through code with your editor or IDE of choice. We are going to start with <code class="markup--code markup--p-code">CreateDataLoader</code>. You should be able to go find symbol or in vim tag to jump straight to <code class="markup--code markup--p-code">CreateDataLoader</code> and we can see that’s creating a <code class="markup--code markup--p-code">CustomDatasetDataLoader</code>. Then we can see <code class="markup--code markup--p-code">CustomDatasetDataLoader</code> is a <code class="markup--code markup--p-code">BaseDataLoader</code>. We can see that it’s going to use a standard PyTorch DataLoader, so that’s good. We know if you are going to use a standard PyTorch DataLoader, you have pass it a dataset, and we know that a dataset is something that contains a length and an indexer so presumably when we look at <code class="markup--code markup--p-code">CreateDataset</code> it’s going to do that.</p><p name="bc6d" id="bc6d" class="graf graf--p graf-after--p">Here is <code class="markup--code markup--p-code">CreateDataset</code> and this library does more than just cycle GAN — it handles both aligned and unaligned image pairs [<a href="https://youtu.be/ondivPiwQho?t=2h4m46s" data-href="https://youtu.be/ondivPiwQho?t=2h4m46s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:04:46</a>]. We know that our image pairs are unaligned so we are going to <code class="markup--code markup--p-code">UnalignedDataset</code>.</p><figure name="e786" id="e786" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_wDbxkFlSWbEnC9QDtymlZA.png"></figure><p name="fb25" id="fb25" class="graf graf--p graf-after--figure">As expected, it has <code class="markup--code markup--p-code">__getitem__</code> and <code class="markup--code markup--p-code">__len__</code>. For length, A and B are our horses and zebras, we got two sets, so whichever one is longer is the length of the <code class="markup--code markup--p-code">DataLoader</code>. <code class="markup--code markup--p-code">__getitem__</code> is going to:</p><ul class="postList"><li name="42ce" id="42ce" class="graf graf--li graf-after--p">Randomly grab something from each of our two horses and zebras</li><li name="053d" id="053d" class="graf graf--li graf-after--li">Open them up with pillow (PIL)</li><li name="c9ec" id="c9ec" class="graf graf--li graf-after--li">Run them through some transformations</li><li name="d1f0" id="d1f0" class="graf graf--li graf-after--li">Then we could either be turning horses into zebras or zebras into horses, so there’s some direction</li><li name="8fab" id="8fab" class="graf graf--li graf-after--li">Return our horse, zebra, a path to the horse, and a path of zebra</li></ul><p name="702d" id="702d" class="graf graf--p graf-after--li">Hopefully you can kind of see that this is looking pretty similar to the kind of things fast.ai does. Fast.ai obviously does quite a lot more when it comes to transforms and performance, but remember, this is research code for this one thing and it’s pretty cool that they did all this work.</p><figure name="c412" id="c412" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_zWN8sgzWry6qu7R9FS0Ydw.png"></figure><pre name="35db" id="35db" class="graf graf--pre graf-after--figure">data_loader = CreateDataLoader(opt)<br>dataset = data_loader.load_data()<br>dataset_size = len(data_loader)<br>dataset_size</pre><pre name="caf1" id="caf1" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">1334</em></pre><p name="4826" id="4826" class="graf graf--p graf-after--pre">We’ve got a data loader so we can go and load our data into it [<a href="https://youtu.be/ondivPiwQho?t=2h6m17s" data-href="https://youtu.be/ondivPiwQho?t=2h6m17s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:06:17</a>]. That will tell us how many mini-batches are in it (that’s the length of the data loader in PyTorch).</p><p name="c169" id="c169" class="graf graf--p graf-after--p">Next step is to create a model. Same idea, we’ve got different kind of models and we’re going to be doing a cycle GAN.</p><figure name="a2bb" id="a2bb" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_TmC6TtfaP2xRyS9KK1ryjA.png"></figure><p name="15e0" id="15e0" class="graf graf--p graf-after--figure">Here is our <code class="markup--code markup--p-code">CycleGANModel</code>. There is quite a lot of stuff in <code class="markup--code markup--p-code">CycleGANModel</code>, so let’s go through and find out what’s going to be used. At this stage, we’ve just called initializer so when we initialize it, it’s going to go through and define two generators which is not surprising a generator for our horses and a generator for zebras. There is some way for it to generate a pool of fake data and then we’re going to grab our GAN loss, and as we talked about our cycle-consistency loss is an L1 loss. They are going to use Adam, so obviously for cycle GANS they found Adam works pretty well. Then we are going to have an optimizer for our horse discriminator, an optimizer for our zebra discriminator, and an optimizer for our generator. The optimizer for the generator is going to contain the parameters both for the horse generator and the zebra generator all in one place.</p><figure name="1f75" id="1f75" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_eDn2CkHKsIDaAz1M5WnWBg.png"></figure><p name="7e31" id="7e31" class="graf graf--p graf-after--figure">So the initializer is going to set up all of the different networks and loss functions we need and they are going to be stored inside this <code class="markup--code markup--p-code">model</code> [<a href="https://youtu.be/ondivPiwQho?t=2h8m14s" data-href="https://youtu.be/ondivPiwQho?t=2h8m14s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:08:14</a>].</p><pre name="b9f2" id="b9f2" class="graf graf--pre graf-after--p">model = create_model(opt)</pre><p name="80f0" id="80f0" class="graf graf--p graf-after--pre">It then prints out and shows us exactly the PyTorch model we have. It’s interesting to see that they are using ResNets and so you can see the ResNets look pretty familiar, so we have conv, batch norm, Relu. <code class="markup--code markup--p-code">InstanceNorm</code> is just the same as batch norm basically but it applies to one image at a time and the difference isn’t particularly important. And you can see they are doing reflection padding just like we are. You can kind of see when you try to build everything from scratch like this, it is a lot of work and you can forget the nice little things that fast.ai does automatically for you. You have to do all of them by hand and only you end up with a subset of them. So over time, hopefully soon, we’ll get all of this GAN stuff into fast.ai and it’ll be nice and easy.</p><figure name="c2b9" id="c2b9" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_YTCDe7-xeLelfeQNiKiq4A.png"></figure><p name="01f6" id="01f6" class="graf graf--p graf-after--figure">We’ve got our model and remember the model contains the loss functions, generators, discriminators, all in one convenient place [<a href="https://youtu.be/ondivPiwQho?t=2h9m32s" data-href="https://youtu.be/ondivPiwQho?t=2h9m32s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:09:32</a>]. I’ve gone ahead and copied and pasted and slightly refactored the training loop from their code so that we can run it inside the notebook. So this one should look a lot familiar. A loop to go through each epoch and a loop to go through the data. Before we did this, we set up <code class="markup--code markup--p-code">dataset</code>. This is actually not a PyTorch dataset, I think this is what they used slightly confusingly to talk about their combined what we would call a model data object — all the data that they need. Loop through that with <code class="markup--code markup--p-code">tqdm</code> to get a progress bar, and so now we can go through and see what happens in the model.</p><pre name="d096" id="d096" class="graf graf--pre graf-after--p">total_steps = 0<br><br><strong class="markup--strong markup--pre-strong">for</strong> epoch <strong class="markup--strong markup--pre-strong">in</strong> range(opt.epoch_count, opt.niter + opt.niter_decay+1):<br>    epoch_start_time = time.time()<br>    iter_data_time = time.time()<br>    epoch_iter = 0<br><br>    <strong class="markup--strong markup--pre-strong">for</strong> i, data <strong class="markup--strong markup--pre-strong">in</strong> tqdm(enumerate(dataset)):<br>        iter_start_time = time.time()<br>        <strong class="markup--strong markup--pre-strong">if</strong> total_steps % opt.print_freq == 0: <br>             t_data = iter_start_time - iter_data_time<br>        total_steps += opt.batchSize<br>        epoch_iter += opt.batchSize<br>        model.set_input(data)<br>        model.optimize_parameters()<br><br>        <strong class="markup--strong markup--pre-strong">if</strong> total_steps % opt.display_freq == 0:<br>            save_result = total_steps % opt.update_html_freq == 0<br><br>        <strong class="markup--strong markup--pre-strong">if</strong> total_steps % opt.print_freq == 0:<br>            errors = model.get_current_errors()<br>            t = (time.time() - iter_start_time) / opt.batchSize<br><br>        <strong class="markup--strong markup--pre-strong">if</strong> total_steps % opt.save_latest_freq == 0:<br>            print('saving the latest model(epoch <strong class="markup--strong markup--pre-strong">%d</strong>,total_steps <strong class="markup--strong markup--pre-strong">%d</strong>)'<br>                    % (epoch, total_steps))<br>            model.save('latest')<br><br>        iter_data_time = time.time()<br>    <strong class="markup--strong markup--pre-strong">if</strong> epoch % opt.save_epoch_freq == 0:<br>        print('saving the model at the end of epoch <strong class="markup--strong markup--pre-strong">%d</strong>, iters <strong class="markup--strong markup--pre-strong">%d</strong>' <br>               % (epoch, total_steps))<br>        model.save('latest')<br>        model.save(epoch)<br><br>    print('End of epoch <strong class="markup--strong markup--pre-strong">%d</strong> / <strong class="markup--strong markup--pre-strong">%d</strong> <strong class="markup--strong markup--pre-strong">\t</strong> Time Taken: <strong class="markup--strong markup--pre-strong">%d</strong> sec' %<br>          (epoch, opt.niter + opt.niter_decay, time.time() <br>          - epoch_start_time))<br>    model.update_learning_rate()</pre><p name="ad20" id="ad20" class="graf graf--p graf-after--pre"><code class="markup--code markup--p-code">set_input</code> [<a href="https://youtu.be/ondivPiwQho?t=2h10m32s" data-href="https://youtu.be/ondivPiwQho?t=2h10m32s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:10:32</a>]: It’s a different approach to what we do in fast.ai. This is kind of neat, it’s quite specific to cycle GANs but basically internally inside this model is this idea that we are going to go into our data and grab the appropriate one. We are either going horse to zebra or zebra to horse, depending on which way we go, <code class="markup--code markup--p-code">A</code> is either horse or zebra, and vice versa. If necessary put it on the appropriate GPU, then grab the appropriate paths. So the model now has a mini-batch of horses and a mini-batch of zebras.</p><figure name="2853" id="2853" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1__s9OBHq4z1OBiR9SJORySw.png"></figure><p name="7fb5" id="7fb5" class="graf graf--p graf-after--figure">Now we optimize the parameters [<a href="https://youtu.be/ondivPiwQho?t=2h11m19s" data-href="https://youtu.be/ondivPiwQho?t=2h11m19s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:11:19</a>]. It’s kind of nice to see it like this. You can see each step. First of all, try to optimize the generators, then try to optimize the horse discriminators, then try to optimize the zebra discriminator. <code class="markup--code markup--p-code">zero_grad()</code> is a part of PyTorch, as well as <code class="markup--code markup--p-code">step()</code>. So the interesting bit is the actual thing that does the back propagation on the generator.</p><figure name="bdce" id="bdce" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_CXawhHC0Mc9pgBFBIWg22Q.png"></figure><p name="5e55" id="5e55" class="graf graf--p graf-after--figure">Here it is [<a href="https://youtu.be/ondivPiwQho?t=2h12m4s" data-href="https://youtu.be/ondivPiwQho?t=2h12m4s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:12:04</a>]. Let’s jump to the key pieces. There’s all the formula that we just saw in the paper. Let’s take a horse and generate a zebra. Let’s now use the discriminator to see if we can tell whether it’s fake or not (<code class="markup--code markup--p-code">pred_fake</code>). Then let’s pop that into our loss function which we set up earlier to get a GAN loss based on that prediction. Let’s do the same thing going the opposite direction using the opposite discriminator then put that through the loss function again. Then let’s do the cycle consistency loss. Again, we take our fake which we created and try and turn it back again into the original. Let’s use the cycle consistency loss function we created earlier to compare it to the real original. And here is that lambda — so there’s some weight that we used and that would set up actually we just use the default that they suggested in their options. Then do the same for the opposite direction and then add them all together. We then do the backward step. That’s it.</p><figure name="73c7" id="73c7" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_q-ir1SHyywXmO5EkTDVq1w.png"></figure><p name="cb04" id="cb04" class="graf graf--p graf-after--figure">So we can do the same thing for the first discriminator [<a href="https://youtu.be/ondivPiwQho?t=2h13m50s" data-href="https://youtu.be/ondivPiwQho?t=2h13m50s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:13:50</a>]. Since basically all the work has been done now, there’s much less to do here. There that is. We won’t step all through it but it’s basically the same basic stuff that we’ve already seen.</p><figure name="e538" id="e538" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_PPZdNJDrTHrrQLVRjzucgg.png"></figure><p name="a2fb" id="a2fb" class="graf graf--p graf-after--figure">So <code class="markup--code markup--p-code">optimize_parameters()</code> is calculating the losses and doing the optimizer step. From time to time, save and print out some results. Then from time to time, update the learning rate so they’ve got some learning rate annealing built in here as well. Kind of like fast.ai, they’ve got this idea of schedulers which you can then use to update your learning rates.</p><figure name="b657" id="b657" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Xrc3Dxs8hKV7pWHQBfZSsQ.png" data-width="439" data-height="96" src="../img/1_Xrc3Dxs8hKV7pWHQBfZSsQ.png"></figure><p name="059e" id="059e" class="graf graf--p graf-after--figure">For those of you are interested in better understanding deep learning APIs, contributing more to fast.ai, or creating your own version of some of this stuff in some different back-end, it’s cool to look at a second API that covers some subset of some of the similar things to get a sense for how they are solving some of these problems and what the similarities/differences are.</p><pre name="b222" id="b222" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> show_img(im, ax=<strong class="markup--strong markup--pre-strong">None</strong>, figsize=<strong class="markup--strong markup--pre-strong">None</strong>):<br>    <strong class="markup--strong markup--pre-strong">if</strong> <strong class="markup--strong markup--pre-strong">not</strong> ax: fig,ax = plt.subplots(figsize=figsize)<br>    ax.imshow(im)<br>    ax.get_xaxis().set_visible(<strong class="markup--strong markup--pre-strong">False</strong>)<br>    ax.get_yaxis().set_visible(<strong class="markup--strong markup--pre-strong">False</strong>)<br>    <strong class="markup--strong markup--pre-strong">return</strong> ax</pre><pre name="d679" id="d679" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> get_one(data):<br>    model.set_input(data)<br>    model.test()<br>    <strong class="markup--strong markup--pre-strong">return</strong> list(model.get_current_visuals().values())</pre><pre name="9ec8" id="9ec8" class="graf graf--pre graf-after--pre">model.save(201)</pre><pre name="e8e0" id="e8e0" class="graf graf--pre graf-after--pre">test_ims = []<br><strong class="markup--strong markup--pre-strong">for</strong> i,o <strong class="markup--strong markup--pre-strong">in</strong> enumerate(dataset):<br>    <strong class="markup--strong markup--pre-strong">if</strong> i&gt;10: <strong class="markup--strong markup--pre-strong">break</strong><br>    test_ims.append(get_one(o))</pre><pre name="fa28" id="fa28" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> show_grid(ims):<br>    fig,axes = plt.subplots(2,3,figsize=(9,6))<br>    <strong class="markup--strong markup--pre-strong">for</strong> i,ax <strong class="markup--strong markup--pre-strong">in</strong> enumerate(axes.flat): show_img(ims[i], ax);<br>    fig.tight_layout()</pre><pre name="8260" id="8260" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(8): show_grid(test_ims[i])</pre><p name="7294" id="7294" class="graf graf--p graf-after--pre">We train that for a little while and then we can just grab a few examples and here we have them [<a href="https://youtu.be/ondivPiwQho?t=2h15m29s" data-href="https://youtu.be/ondivPiwQho?t=2h15m29s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:15:29</a>]. Here are horses, zebras, and back again as horses.</p><figure name="e03c" id="e03c" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_CcsmcW4TlZvn7eywxQPHUQ.png"></figure><figure name="7327" id="7327" class="graf graf--figure graf-after--figure"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_uMqqilXzEmTMXf8x5ry0CQ.png"></figure><figure name="a1e0" id="a1e0" class="graf graf--figure graf-after--figure"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1__9FdL_2vB30MCQ1V8fU-qw.png"></figure><figure name="ead1" id="ead1" class="graf graf--figure graf-after--figure"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_SanvgXWJHOoucKANA6A36A.png"></figure><figure name="fea0" id="fea0" class="graf graf--figure graf-after--figure"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_TcjrwAtTdYLkV1x5kCqdxA.png"></figure><figure name="06e9" id="06e9" class="graf graf--figure graf-after--figure"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_Rlpp3gVTYSaKknsAq4qOig.png"></figure><figure name="506e" id="506e" class="graf graf--figure graf-after--figure"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_xxPYAgd8hRxgvGv2mBQ2Vg.png"></figure><figure name="0884" id="0884" class="graf graf--figure graf-after--figure"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_MxQzw0SwBT_iYfbyd4BD_w.png"></figure><p name="c730" id="c730" class="graf graf--p graf-after--figure">It took me like 24 hours to train it even that far so it’s kind of slow [<a href="https://youtu.be/ondivPiwQho?t=2h16m39s" data-href="https://youtu.be/ondivPiwQho?t=2h16m39s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:16:39</a>]. I know Helena is constantly complaining on Twitter about how long these things take. I don’t know how she’s so productive with them.</p><pre name="b869" id="b869" class="graf graf--pre graf-after--p">#! wget https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip</pre><p name="cae9" id="cae9" class="graf graf--p graf-after--pre">I will mention one more thing that just came out yesterday [<a href="https://youtu.be/ondivPiwQho?t=2h16m54s" data-href="https://youtu.be/ondivPiwQho?t=2h16m54s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:16:54</a>]:</p><p name="44c4" id="44c4" class="graf graf--p graf-after--p"><a href="https://arxiv.org/abs/1804.04732" data-href="https://arxiv.org/abs/1804.04732" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">Multimodal Unsupervised Image-to-Image Translation</a></p><p name="a7be" id="a7be" class="graf graf--p graf-after--p">There is now a multi-modal image to image translation of unpaired. So you can basically now create different cats for instance from this dog.</p><figure name="5b2b" id="5b2b" class="graf graf--figure graf--iframe graf-after--p"><iframe data-width="854" data-height="480" width="700" height="393" data-src="/media/6af412979eb92c5c8e7ee3439d5c31b3?postId=215dfbf04a94" data-media-id="6af412979eb92c5c8e7ee3439d5c31b3" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Fi.ytimg.com%2Fvi%2Fab64TWzWn40%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="../img/saved_resource.html"></iframe><iframe data-width="854" data-height="480" width="700" height="393" src="../img/6af412979eb92c5c8e7ee3439d5c31b3.html" data-media-id="6af412979eb92c5c8e7ee3439d5c31b3" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Fi.ytimg.com%2Fvi%2Fab64TWzWn40%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen="" frameborder="0"></iframe></figure><p name="8f99" id="8f99" class="graf graf--p graf-after--figure graf--trailing">This is basically not just creating one example of the output that you want, but creating multiple ones. This came out yesterday or the day before. I think it’s pretty amazing. So you can kind of see how this technology is developing and I think there’s so many opportunities to maybe do this with music, speech, writing, or to create kind of tools for artists.</p><hr class="section-divider"><p name="18d1" id="18d1" class="graf graf--p graf--leading graf--trailing">Lessons: <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" class="markup--anchor markup--p-anchor" target="_blank">1</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" class="markup--anchor markup--p-anchor" target="_blank">2</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" class="markup--anchor markup--p-anchor" target="_blank">3</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" class="markup--anchor markup--p-anchor" target="_blank">4</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" class="markup--anchor markup--p-anchor" target="_blank">5</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" class="markup--anchor markup--p-anchor" target="_blank">6</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" class="markup--anchor markup--p-anchor" target="_blank">7</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" class="markup--anchor markup--p-anchor" target="_blank">8</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" class="markup--anchor markup--p-anchor" target="_blank">9</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" class="markup--anchor markup--p-anchor" target="_blank">10</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" class="markup--anchor markup--p-anchor" target="_blank">11</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" class="markup--anchor markup--p-anchor" target="_blank"><strong class="markup--strong markup--p-strong">12</strong></a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" class="markup--anchor markup--p-anchor" target="_blank">13</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" class="markup--anchor markup--p-anchor" target="_blank">14</a></p></body></html>