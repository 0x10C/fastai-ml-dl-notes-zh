
<!-- saved from url=(0063)file:///C:/Users/asus/Desktop/fastai-ml-dl-notes-zh/en/dl9.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></head><body><hr class="section-divider"><h1 name="2be5" id="2be5" class="graf graf--h3 graf--leading graf--title">Deep Learning 2: Part 2 Lesson&nbsp;9</h1><p name="2560" id="2560" class="graf graf--p graf-after--h3"><em class="markup--em markup--p-em">My personal notes from </em><a href="http://www.fast.ai/" data-href="http://www.fast.ai/" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener noopener noopener nofollow noopener nofollow noopener" target="_blank"><em class="markup--em markup--p-em">fast.ai course</em></a><em class="markup--em markup--p-em">. These notes will continue to be updated and improved as I continue to review the course to “really” understand it. Much appreciation to </em><a href="https://twitter.com/jeremyphoward" data-href="https://twitter.com/jeremyphoward" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener noopener noopener nofollow noopener nofollow noopener" target="_blank"><em class="markup--em markup--p-em">Jeremy</em></a><em class="markup--em markup--p-em"> and </em><a href="https://twitter.com/math_rachel" data-href="https://twitter.com/math_rachel" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener noopener noopener nofollow noopener nofollow noopener" target="_blank"><em class="markup--em markup--p-em">Rachel</em></a><em class="markup--em markup--p-em"> who gave me this opportunity to learn.</em></p><p name="929c" id="929c" class="graf graf--p graf-after--p graf--trailing">Lessons: <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" class="markup--anchor markup--p-anchor" target="_blank">1</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" class="markup--anchor markup--p-anchor" target="_blank">2</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" class="markup--anchor markup--p-anchor" target="_blank">3</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" class="markup--anchor markup--p-anchor" target="_blank">4</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" class="markup--anchor markup--p-anchor" target="_blank">5</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" class="markup--anchor markup--p-anchor" target="_blank">6</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" class="markup--anchor markup--p-anchor" target="_blank">7</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" class="markup--anchor markup--p-anchor" target="_blank">8</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" class="markup--anchor markup--p-anchor" target="_blank"><strong class="markup--strong markup--p-strong">9</strong></a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" class="markup--anchor markup--p-anchor" target="_blank">10</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" class="markup--anchor markup--p-anchor" target="_blank">11</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" class="markup--anchor markup--p-anchor" target="_blank">12</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" class="markup--anchor markup--p-anchor" target="_blank">13</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" class="markup--anchor markup--p-anchor" target="_blank">14</a></p><hr class="section-divider"><h3 name="b0c6" id="b0c6" class="graf graf--h3 graf--leading">Links</h3><p name="32c6" id="32c6" class="graf graf--p graf-after--h3"><a href="http://forums.fast.ai/t/part-2-lesson-9-in-class/14028/1" data-href="http://forums.fast.ai/t/part-2-lesson-9-in-class/14028/1" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank"><strong class="markup--strong markup--p-strong">Forum</strong></a><strong class="markup--strong markup--p-strong"> / </strong><a href="https://youtu.be/0frKXR-2PBY" data-href="https://youtu.be/0frKXR-2PBY" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong">Video</strong></a></p><h3 name="8cda" id="8cda" class="graf graf--h3 graf-after--p">Review</h3><h4 name="3167" id="3167" class="graf graf--h4 graf-after--h3">From Last&nbsp;week:</h4><ul class="postList"><li name="13b8" id="13b8" class="graf graf--li graf-after--h4">Pathlib; JSON</li><li name="8f79" id="8f79" class="graf graf--li graf-after--li">Dictionary comprehensions</li><li name="c475" id="c475" class="graf graf--li graf-after--li">Defaultdict</li><li name="5c7b" id="5c7b" class="graf graf--li graf-after--li">How to jump around fastai source</li><li name="2b0e" id="2b0e" class="graf graf--li graf-after--li">matplotlib OO API</li><li name="a56b" id="a56b" class="graf graf--li graf-after--li">Lambda functions</li><li name="35f7" id="35f7" class="graf graf--li graf-after--li">Bounding box coordinates</li><li name="474a" id="474a" class="graf graf--li graf-after--li">Custom head; bounding box regression</li></ul><figure name="da49" id="da49" class="graf graf--figure graf-after--li"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_2nxK3zuKRnDCu_3qVhSMnw.png"></figure><figure name="67d3" id="67d3" class="graf graf--figure graf-after--figure"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_9G88jQ42l5RdwFi2Yr_h_Q.png"></figure><h4 name="6247" id="6247" class="graf graf--h4 graf-after--figure">From Part&nbsp;1:</h4><ul class="postList"><li name="5e8a" id="5e8a" class="graf graf--li graf-after--h4">How to view model inputs from a DataLoader</li><li name="648e" id="648e" class="graf graf--li graf-after--li">How to view model outputs</li></ul><figure name="1296" id="1296" class="graf graf--figure graf-after--li"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_E3Z5vKnp6ZkfuLR83979RA.png"></figure><h3 name="5492" id="5492" class="graf graf--h3 graf-after--figure">Data Augmentation and Bounding Box&nbsp;[<a href="https://youtu.be/0frKXR-2PBY?t=2m58s" data-href="https://youtu.be/0frKXR-2PBY?t=2m58s" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">2:58</a>]</h3><p name="e3ff" id="e3ff" class="graf graf--p graf-after--h3"><a href="https://github.com/fastai/fastai/blob/master/courses/dl2/pascal.ipynb" data-href="https://github.com/fastai/fastai/blob/master/courses/dl2/pascal.ipynb" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Notebook</a></p><p name="be1e" id="be1e" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Awkward rough edges of fastai:</strong><br>A <em class="markup--em markup--p-em">classifier</em> is anything with dependent variable is categorical or binomial. As opposed to <em class="markup--em markup--p-em">regression</em> which is anything with dependent variable is continuous. Naming is a little confusing but will be sorted out in future. Here, <code class="markup--code markup--p-code">continuous</code> is <code class="markup--code markup--p-code">True</code> because our dependent variable is the coordinates of bounding box — hence this is actually a regressor data.</p><pre name="67ba" id="67ba" class="graf graf--pre graf-after--p">tfms = tfms_from_model(f_model, sz, crop_type=CropType.NO, <br>                       aug_tfms=augs)<br>md = Image<strong class="markup--strong markup--pre-strong">Classifier</strong>Data.from_csv(PATH, JPEGS, BB_CSV, tfms=tfms,<br>                                  <strong class="markup--strong markup--pre-strong">continuous=True</strong>, bs=4)</pre><h4 name="f768" id="f768" class="graf graf--h4 graf-after--pre">Let’s create some data augmentation [<a href="https://youtu.be/0frKXR-2PBY?t=4m40s" data-href="https://youtu.be/0frKXR-2PBY?t=4m40s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">4:40</a>]</h4><pre name="92de" id="92de" class="graf graf--pre graf-after--h4">augs = [RandomFlip(), <br>        RandomRotate(30),<br>        RandomLighting(0.1,0.1)]</pre><p name="8b2e" id="8b2e" class="graf graf--p graf-after--pre">Normally, we use these shortcuts Jeremy created for us, but they are simply lists of random augmentations. But you can easily create your own (most if not all of them start with “Random”).</p><figure name="2e67" id="2e67" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*lAIQHKT0GbjY0fRZKmpFaA.png" data-width="524" data-height="52" src="../img/1_lAIQHKT0GbjY0fRZKmpFaA.png"></figure><pre name="544c" id="544c" class="graf graf--pre graf-after--figure">tfms = tfms_from_model(f_model, sz, crop_type=CropType.NO,<br>                       aug_tfms=augs)<br>md = ImageClassifierData.from_csv(PATH, JPEGS, BB_CSV, tfms=tfms,<br>                       continuous=True, bs=4)</pre><pre name="aac4" id="aac4" class="graf graf--pre graf-after--pre">idx=3<br>fig,axes = plt.subplots(3,3, figsize=(9,9))<br>for i,ax in enumerate(axes.flat):<br>    x,y=next(iter(md.aug_dl))<br>    ima=md.val_ds.denorm(to_np(x))[idx]<br>    b = bb_hw(to_np(y[idx]))<br>    print(b)<br>    show_img(ima, ax=ax)<br>    draw_rect(ax, b)</pre><pre name="97e9" id="97e9" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[ 115.   63.  240.  311.]<br>[ 115.   63.  240.  311.]<br>[ 115.   63.  240.  311.]<br>[ 115.   63.  240.  311.]<br>[ 115.   63.  240.  311.]<br>[ 115.   63.  240.  311.]<br>[ 115.   63.  240.  311.]<br>[ 115.   63.  240.  311.]<br>[ 115.   63.  240.  311.]</em></pre><figure name="cd34" id="cd34" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_QMa_SUUVOypZHKaAuXDkSw.png"></figure><p name="f3bd" id="f3bd" class="graf graf--p graf-after--figure">As you can see, the image gets rotated and lighting varies, but bounding box is <em class="markup--em markup--p-em">not moving</em> and is <em class="markup--em markup--p-em">in a wrong spot</em> [<a href="https://youtu.be/0frKXR-2PBY?t=6m17s" data-href="https://youtu.be/0frKXR-2PBY?t=6m17s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">6:17</a>]. This is the problem with data augmentations when your dependent variable is pixel values or in some way connected to the independent variable — they need to be augmented together. As you can see in the bounding box coordinates <code class="markup--code markup--p-code">[ 115. 63. 240. 311.]</code>&nbsp;, our image is 224 by 224 — so it is neither scaled nor cropped. The dependent variable needs to go through all the geometric transformation as the independent variables.</p><p name="586a" id="586a" class="graf graf--p graf-after--p">To do this [<a href="https://youtu.be/0frKXR-2PBY?t=7m10s" data-href="https://youtu.be/0frKXR-2PBY?t=7m10s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">7:10</a>], every transformation has an optional <code class="markup--code markup--p-code">tfm_y</code> parameter:</p><pre name="8852" id="8852" class="graf graf--pre graf-after--p">augs = [RandomFlip(tfm_y=TfmType.COORD),<br>        RandomRotate(30, tfm_y=TfmType.COORD),<br>        RandomLighting(0.1,0.1, tfm_y=TfmType.COORD)]</pre><pre name="161c" id="161c" class="graf graf--pre graf-after--pre">tfms = tfms_from_model(f_model, sz, crop_type=CropType.NO,<br>                       tfm_y=TfmType.COORD, aug_tfms=augs)<br>md = ImageClassifierData.from_csv(PATH, JPEGS, BB_CSV, tfms=tfms, <br>                       continuous=True, bs=4)</pre><p name="e0d7" id="e0d7" class="graf graf--p graf-after--pre"><code class="markup--code markup--p-code">TrmType.COORD</code> indicates that the <em class="markup--em markup--p-em">y</em> value represents coordinate. This needs to be added to all the augmentations as well as <code class="markup--code markup--p-code">tfms_from_model</code> which is responsible for cropping, zooming, resizing, padding, etc.</p><pre name="4f8e" id="4f8e" class="graf graf--pre graf-after--p">idx=3<br>fig,axes = plt.subplots(3,3, figsize=(9,9))<br>for i,ax in enumerate(axes.flat):<br>    x,y=next(iter(md.aug_dl))<br>    ima=md.val_ds.denorm(to_np(x))[idx]<br>    b = bb_hw(to_np(y[idx]))<br>    print(b)<br>    show_img(ima, ax=ax)<br>    draw_rect(ax, b)</pre><pre name="e2fc" id="e2fc" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[  48.   34.  112.  188.]<br>[  65.   36.  107.  185.]<br>[  49.   27.  131.  195.]<br>[  24.   18.  147.  204.]<br>[  61.   34.  113.  188.]<br>[  55.   31.  121.  191.]<br>[  52.   19.  144.  203.]<br>[   7.    0.  193.  222.]<br>[  52.   38.  105.  182.]</em></pre><figure name="f82b" id="f82b" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1__ge-RyZpEIQ5fiSvo207rA.png"></figure><p name="dfa4" id="dfa4" class="graf graf--p graf-after--figure">Now, the bounding box moves with the image and is in the right spot. You may notice that sometimes it looks odd like the middle on in the bottom row. This is the constraint of the information we have. If the object occupied the corners of the original bounding box, your new bounding box needs to be bigger after the image rotates. So you must <strong class="markup--strong markup--p-strong">be careful of not doing too higher rotations with bounding boxes</strong> because there is not enough information for them to stay accurate. <span class="markup--quote markup--p-quote is-other" name="anon_ae3d5e1d2e6d" data-creator-ids="anon">If we were doing polygons or segmentations, we would not have this problem.</span></p><figure name="8d9f" id="8d9f" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_4V4sjFZxn-y2cU9tCJPEUw.png"><figcaption class="imageCaption">This why the box gets&nbsp;bigger</figcaption></figure><pre name="123b" id="123b" class="graf graf--pre graf-after--figure">tfm_y = TfmType.COORD<br>augs = [RandomFlip(tfm_y=tfm_y),<br>        RandomRotate(<strong class="markup--strong markup--pre-strong">3</strong>, <strong class="markup--strong markup--pre-strong">p=0.5</strong>, tfm_y=tfm_y),<br>        RandomLighting(0.05,0.05, tfm_y=tfm_y)]</pre><pre name="b051" id="b051" class="graf graf--pre graf-after--pre">tfms = tfms_from_model(f_model, sz, crop_type=CropType.NO, <br>                 tfm_y=tfm_y, aug_tfms=augs)<br>md = ImageClassifierData.from_csv(PATH, JPEGS, BB_CSV, tfms=tfms, <br>                 continuous=True)</pre><p name="76dc" id="76dc" class="graf graf--p graf-after--pre">So here, we do maximum of 3 degree rotation to avoid this problem [<a href="https://youtu.be/0frKXR-2PBY?t=9m14s" data-href="https://youtu.be/0frKXR-2PBY?t=9m14s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">9:14</a>]. It also only rotates half of the time (<code class="markup--code markup--p-code">p=0.5</code>).</p><h4 name="58b8" id="58b8" class="graf graf--h4 graf-after--p">custom_head [<a href="https://youtu.be/0frKXR-2PBY?t=9m34s" data-href="https://youtu.be/0frKXR-2PBY?t=9m34s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">9:34</a>]</h4><p name="e869" id="e869" class="graf graf--p graf-after--h4"><code class="markup--code markup--p-code">learn.summary()</code> will run a small batch of data through a model and prints out the size of tensors at every layer. As you can see, right before the <code class="markup--code markup--p-code">Flatten</code> layer, the tensor has the shape of 512 by 7 by 7. So if it were a rank 1 tensor (i.e. a single vector) its length will be 25088 (512 * 7 * 7)and that is why our custom header’s input size is 25088. Output size is 4 since it is the bounding box coordinates.</p><pre name="1987" id="1987" class="graf graf--pre graf-after--p">head_reg4 = nn.Sequential(Flatten(), nn.Linear(25088,4))<br>learn = ConvLearner.pretrained(f_model, md, custom_head=head_reg4)<br>learn.opt_fn = optim.Adam<br>learn.crit = nn.L1Loss()</pre><figure name="6c46" id="6c46" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_o9NFGVz1ua60kOpIafe5Hg.png"></figure><h4 name="f7f3" id="f7f3" class="graf graf--h4 graf-after--figure">Single object detection [<a href="https://youtu.be/0frKXR-2PBY?t=10m35s" data-href="https://youtu.be/0frKXR-2PBY?t=10m35s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">10:35</a>]</h4><p name="80a2" id="80a2" class="graf graf--p graf-after--h4">Let’s combine the two to create something that can classify and localize the largest object in each image.</p><p name="8a53" id="8a53" class="graf graf--p graf-after--p">There are 3 things that we need to do to train a neural network:</p><ol class="postList"><li name="bb44" id="bb44" class="graf graf--li graf-after--p">Data</li><li name="b33c" id="b33c" class="graf graf--li graf-after--li">Architecture</li><li name="62fa" id="62fa" class="graf graf--li graf-after--li">Loss Function</li></ol><h4 name="a6c0" id="a6c0" class="graf graf--h4 graf-after--li">1. Providing Data</h4><p name="e159" id="e159" class="graf graf--p graf-after--h4">We need a <code class="markup--code markup--p-code">ModelData</code> object whose independent variable is the images, and dependent variable is a tuple of bounding box coordinates and class label. There are several ways to do this, but here is a particularly lazy and convinient way Jeremy came up with is to create two <code class="markup--code markup--p-code">ModelData</code> objects representing the two different dependent variables we want (one with bounding boxes coordinates, one with classes).</p><pre name="b667" id="b667" class="graf graf--pre graf-after--p">f_model=resnet34<br>sz=224<br>bs=64</pre><pre name="65cf" id="65cf" class="graf graf--pre graf-after--pre">val_idxs = get_cv_idxs(len(trn_fns))<br>tfms = tfms_from_model(f_model, sz, crop_type=CropType.NO, <br>                       tfm_y=TfmType.COORD, aug_tfms=augs)</pre><pre name="3de6" id="3de6" class="graf graf--pre graf-after--pre">md = ImageClassifierData.from_csv(PATH, JPEGS, <strong class="markup--strong markup--pre-strong">BB_CSV</strong>, tfms=tfms, <br>                       continuous=True, val_idxs=val_idxs)</pre><pre name="e9fc" id="e9fc" class="graf graf--pre graf-after--pre">md2 = ImageClassifierData.from_csv(PATH, JPEGS, <strong class="markup--strong markup--pre-strong">CSV</strong>,<br>                       tfms=tfms_from_model(f_model, sz))</pre><p name="18c8" id="18c8" class="graf graf--p graf-after--pre">A dataset can be anything with <code class="markup--code markup--p-code">__len__</code> and <code class="markup--code markup--p-code">__getitem__</code>. Here's a dataset that adds a 2nd label to an existing dataset:</p><pre name="d395" id="d395" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">ConcatLblDataset</strong>(Dataset):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, ds, y2): self.ds,self.y2 = ds,y2<br>    <strong class="markup--strong markup--pre-strong">def</strong> __len__(self): <strong class="markup--strong markup--pre-strong">return</strong> len(self.ds)<br>    <br>    <strong class="markup--strong markup--pre-strong">def</strong> __getitem__(self, i):<br>        x,y = self.ds[i]<br>        <strong class="markup--strong markup--pre-strong">return</strong> (x, (y,self.y2[i]))</pre><ul class="postList"><li name="646c" id="646c" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">ds</code>&nbsp;: contains both independent and dependent variables</li><li name="8dc3" id="8dc3" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">y2</code>&nbsp;: contains the additional dependent variables</li><li name="c9d3" id="c9d3" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">(x, (y,self.y2[i]))</code>&nbsp;: <code class="markup--code markup--li-code">__getitem___</code> returns an independent variable and the combination of two dependent variables.</li></ul><p name="8552" id="8552" class="graf graf--p graf-after--li">We’ll use it to add the classes to the bounding boxes labels.</p><pre name="36e4" id="36e4" class="graf graf--pre graf-after--p">trn_ds2 = ConcatLblDataset(md.trn_ds, md2.trn_y)<br>val_ds2 = ConcatLblDataset(md.val_ds, md2.val_y)</pre><p name="a83d" id="a83d" class="graf graf--p graf-after--pre">Here is an example dependent variable:</p><pre name="e61c" id="e61c" class="graf graf--pre graf-after--p">val_ds2[0][1]</pre><pre name="bb40" id="bb40" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(array([   0.,   49.,  205.,  180.], dtype=float32), 14)</em></pre><p name="3916" id="3916" class="graf graf--p graf-after--pre">We can replace the dataloaders’ datasets with these new ones.</p><pre name="abd7" id="abd7" class="graf graf--pre graf-after--p">md.trn_dl.dataset = trn_ds2<br>md.val_dl.dataset = val_ds2</pre><p name="e63c" id="e63c" class="graf graf--p graf-after--pre">We have to <code class="markup--code markup--p-code">denorm</code>alize the images from the dataloader before they can be plotted.</p><pre name="bb3f" id="bb3f" class="graf graf--pre graf-after--p">x,y = next(iter(md.val_dl))<br>idx = 3<br>ima = md.val_ds.ds.denorm(to_np(x))[idx]<br>b = bb_hw(to_np(y[0][idx])); b</pre><pre name="7b26" id="7b26" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">array([  52.,   38.,  106.,  184.], dtype=float32)</em></pre><pre name="31ab" id="31ab" class="graf graf--pre graf-after--pre">ax = show_img(ima)<br>draw_rect(ax, b)<br>draw_text(ax, b[:2], md2.classes[y[1][idx]])</pre><figure name="442b" id="442b" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_6QqfOpqgyRogEiTCU8WZgQ.png"></figure><h4 name="2513" id="2513" class="graf graf--h4 graf-after--figure">2. Choosing Architecture [<a href="https://youtu.be/0frKXR-2PBY?t=13m54s" data-href="https://youtu.be/0frKXR-2PBY?t=13m54s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">13:54</a>]</h4><p name="18d6" id="18d6" class="graf graf--p graf-after--h4">The architecture will be the same as the one we used for the classifier and bounding box regression, but we will just combine them. In other words, if we have <code class="markup--code markup--p-code">c</code> classes, then the number of activations we need in the final layer is 4 plus <code class="markup--code markup--p-code">c</code>. 4 for bounding box coordinates and <code class="markup--code markup--p-code">c</code> probabilities (one per class).</p><p name="f053" id="f053" class="graf graf--p graf-after--p">We’ll use an extra linear layer this time, plus some dropout, to help us train a more flexible model. In general, we want our custom head to be capable of solving the problem on its own if the pre-trained backbone it is connected to is appropriate. So in this case, we are trying to do quite a bit — classifier and bounding box regression, so just the single linear layer does not seem enough. If you were wondering why there is no <code class="markup--code markup--p-code">BatchNorm1d</code> after the first <code class="markup--code markup--p-code">ReLU</code>&nbsp;, ResNet backbone already has <code class="markup--code markup--p-code">BatchNorm1d</code> as its final layer.</p><pre name="eb1b" id="eb1b" class="graf graf--pre graf-after--p">head_reg4 = nn.Sequential(<br>    Flatten(),<br>    nn.ReLU(),<br>    nn.Dropout(0.5),<br>    nn.Linear(25088,256),<br>    nn.ReLU(),<br>    nn.BatchNorm1d(256),<br>    nn.Dropout(0.5),<br>    nn.Linear(256,<strong class="markup--strong markup--pre-strong">4+len(cats)</strong>),<br>)<br>models = ConvnetBuilder(f_model, 0, 0, 0, custom_head=head_reg4)<br><br>learn = ConvLearner(md, models)<br>learn.opt_fn = optim.Adam</pre><h4 name="f6e9" id="f6e9" class="graf graf--h4 graf-after--pre">3. Loss Function&nbsp;[<a href="https://youtu.be/0frKXR-2PBY?t=15m46s" data-href="https://youtu.be/0frKXR-2PBY?t=15m46s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">15:46</a>]</h4><p name="95d7" id="95d7" class="graf graf--p graf-after--h4">The loss function needs to look at these <code class="markup--code markup--p-code">4 + len(cats)</code> activations and decide if they are good — whether these numbers accurately reflect the position and class of the largest object in the image. We know how to do this. For the first 4 activations, we will use L1Loss just like we did before (L1Loss is like a Mean Squared Error — instead of sum of squared errors, it uses sum of absolute values). For rest of the activations, we can use cross entropy loss.</p><pre name="7b18" id="7b18" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> detn_loss(input, target):<br>    bb_t,c_t = target<br>    bb_i,c_i = input[:, :4], input[:, 4:]<br>    bb_i = F.sigmoid(bb_i)*224<br><em class="markup--em markup--pre-em">    # I looked at these quantities separately first then picked a <br>    # multiplier to make them approximately equal</em><br>    <strong class="markup--strong markup--pre-strong">return</strong> F.l1_loss(bb_i, bb_t) + F.cross_entropy(c_i, c_t)*20</pre><pre name="16e0" id="16e0" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> detn_l1(input, target):<br>    bb_t,_ = target<br>    bb_i = input[:, :4]<br>    bb_i = F.sigmoid(bb_i)*224<br>    <strong class="markup--strong markup--pre-strong">return</strong> F.l1_loss(V(bb_i),V(bb_t)).data</pre><pre name="f829" id="f829" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> detn_acc(input, target):<br>    _,c_t = target<br>    c_i = input[:, 4:]<br>    <strong class="markup--strong markup--pre-strong">return</strong> accuracy(c_i, c_t)</pre><pre name="4eaa" id="4eaa" class="graf graf--pre graf-after--pre">learn.crit = detn_loss<br>learn.metrics = [detn_acc, detn_l1]</pre><ul class="postList"><li name="f337" id="f337" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">input</code>&nbsp;: activations</li><li name="5cb2" id="5cb2" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">target</code>&nbsp;: ground truth</li><li name="9b1e" id="9b1e" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">bb_t,c_t = target</code>&nbsp;: Our custom dataset returns a tuple containing bounding box coordinates and classes. This assignment will destructure them.</li><li name="8867" id="8867" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">bb_i,c_i = input[:,&nbsp;:4], input[:, 4:]</code>&nbsp;: the first&nbsp;<code class="markup--code markup--li-code">:</code> is for the batch dimension.</li><li name="e6c0" id="e6c0" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">b_i = F.sigmoid(bb_i)*224</code>&nbsp;: we know our image is 224 by 224. <code class="markup--code markup--li-code">Sigmoid</code> will force it to be between 0 and 1, and multiply it by 224 to help our neural net to be in the range of what it has to be.</li></ul><p name="ab55" id="ab55" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">Question:</strong> As a general rule, is it better to put BatchNorm before or after ReLU [<a href="https://youtu.be/0frKXR-2PBY?t=18m2s" data-href="https://youtu.be/0frKXR-2PBY?t=18m2s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">18:02</a>]? Jeremy would suggest to put it after a ReLU because BathNorm is meant to move towards zero-mean one-standard deviation. So if you put ReLU right after it, you are truncating it at zero so there is no way to create negative numbers. But if you put ReLU then BatchNorm, it does have that ability and gives slightly better results. Having said that, it is not too big of a deal either way. You see during this part of the course, most of the time, Jeremy does ReLU then BatchNorm but sometimes does the opposite when he wants to be consistent with the paper.</p><p name="f68d" id="f68d" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: What is the intuition behind using dropout after a BatchNorm? Doesn’t BatchNorm already do a good job of regularizing [<a href="https://youtu.be/0frKXR-2PBY?t=19m12s" data-href="https://youtu.be/0frKXR-2PBY?t=19m12s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">19:12</a>]? BatchNorm does an okay job of regularizing but if you think back to part 1 when we discussed a list of things we do to avoid overfitting and adding BatchNorm is one of them as is data augmentation. But it’s perfectly possible that you’ll still be overfitting. One nice thing about dropout is that is it has a parameter to say how much to drop out. Parameters are great specifically parameters that decide how much to regularize because it lets you build a nice big over parameterized model and then decide on how much to regularize it. Jeremy tends to always put in a drop out starting with <code class="markup--code markup--p-code">p=0</code> and then as he adds regularization, he can just change the dropout parameter without worrying about if he saved a model he want to be able to load it back, but if he had dropout layers in one but no in another, it will not load anymore. So this way, it stays consistent.</p><p name="c531" id="c531" class="graf graf--p graf-after--p">Now we have out inputs and targets, we can calculate the L1 loss and add the cross entropy [<a href="https://youtu.be/0frKXR-2PBY?t=20m39s" data-href="https://youtu.be/0frKXR-2PBY?t=20m39s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">20:39</a>]:</p><p name="95f1" id="95f1" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">F.l1_loss(bb_i, bb_t) + F.cross_entropy(c_i, c_t)*20</code></p><p name="d04f" id="d04f" class="graf graf--p graf-after--p">This is our loss function. Cross entropy and L1 loss may be of wildly different scales — in which case in the loss function, the larger one is going to dominate. In this case, Jeremy printed out the values and found out that if we multiply cross entropy by 20 that makes them about the same scale.</p><pre name="eb4a" id="eb4a" class="graf graf--pre graf-after--p">lr=1e-2<br>learn.fit(lr, 1, cycle_len=3, use_clr=(32,5))</pre><pre name="70ea" id="70ea" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss   detn_acc   detn_l1       <br>    0      72.036466  45.186367  0.802133   32.647586 <br>    1      51.037587  36.34964   0.828425   25.389733     <br>    2      41.4235    35.292709  0.835637   24.343577</em></pre><pre name="0564" id="0564" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[35.292709, 0.83563701808452606, 24.343576669692993]</em></pre><p name="6180" id="6180" class="graf graf--p graf-after--pre">It is nice to print out information as you train, so we grabbed L1 loss and added it as metrics.</p><pre name="06fa" id="06fa" class="graf graf--pre graf-after--p">learn.save('reg1_0')<br>learn.freeze_to(-2)<br>lrs = np.array([lr/100, lr/10, lr])<br>learn.fit(lrs/5, 1, cycle_len=5, use_clr=(32,10))</pre><pre name="11fb" id="11fb" class="graf graf--pre graf-after--pre">epoch      trn_loss   val_loss   detn_acc   detn_l1       <br>    0      34.448113  35.972973  0.801683   22.918499 <br>    1      28.889909  33.010857  0.830379   21.689888     <br>    2      24.237017  30.977512  0.81881    20.817996     <br>    3      21.132993  30.60677   0.83143    20.138552     <br>    4      18.622983  30.54178   0.825571   19.832196</pre><pre name="0865" id="0865" class="graf graf--pre graf-after--pre">[30.54178, 0.82557091116905212, 19.832195997238159]</pre><pre name="c0fb" id="c0fb" class="graf graf--pre graf-after--pre">learn.unfreeze()<br>learn.fit(lrs/10, 1, cycle_len=10, use_clr=(32,10))</pre><pre name="6c87" id="6c87" class="graf graf--pre graf-after--pre">epoch      trn_loss   val_loss   detn_acc   detn_l1       <br>    0      15.957164  31.111507  0.811448   19.970753 <br>    1      15.955259  32.597153  0.81235    20.111022     <br>    2      15.648723  32.231941  0.804087   19.522853     <br>    3      14.876172  30.93821   0.815805   19.226574     <br>    4      14.113872  31.03952   0.808594   19.155093     <br>    5      13.293885  29.736671  0.826022   18.761728     <br>    6      12.562566  30.000023  0.827524   18.82006      <br>    7      11.885125  30.28841   0.82512    18.904158     <br>    8      11.498326  30.070133  0.819712   18.635296     <br>    9      11.015841  30.213772  0.815805   18.551489</pre><pre name="0591" id="0591" class="graf graf--pre graf-after--pre">[30.213772, 0.81580528616905212, 18.551488876342773]</pre><p name="2c47" id="2c47" class="graf graf--p graf-after--pre">A detection accuracy is in the low 80’s which is the same as what it was before. This is not surprising because ResNet was designed to do classification so we wouldn’t expect to be able to improve things in such a simple way. It certainly wasn’t designed to do bounding box regression. It was explicitly actually designed in such a way to not care about geometry — it takes the last 7 by 7 grid of activations and averages them all together throwing away all the information about where everything came from.</p><p name="684d" id="684d" class="graf graf--p graf-after--p">Interestingly, when we do accuracy (classification) and bounding box at the same time, the L1 seems a little bit better than when we just do bounding box regression [<a href="https://youtu.be/0frKXR-2PBY?t=22m46s" data-href="https://youtu.be/0frKXR-2PBY?t=22m46s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">22:46</a>]. If that is counterintuitive to you, then this would be one of the main things to think about after this lesson since it is a really important idea. The idea is this — figuring out what the main object in an image is, is kind of the hard part. Then figuring out exactly where the bounding box is and what class it is is the easy part in a way. So when you have a single network that’s both saying what is the object and where is the object, it’s going to share all the computation about finding the object. And all that shared computation is very efficient. When we back propagate the errors in the class and in the place, that’s all the information that is going to help the computation around finding the biggest object. So anytime you have multiple tasks which share some concept of what those tasks would need to do to complete their work, it is very likely they should share at least some layers of the network together. Later today, we will look at a model where most of the layers are shared except for the last one.</p><p name="acb4" id="acb4" class="graf graf--p graf-after--p">Here are the result [<a href="https://youtu.be/0frKXR-2PBY?t=24m34s" data-href="https://youtu.be/0frKXR-2PBY?t=24m34s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">24:34</a>]. As before, it does a good job when there is single major object in the image.</p><figure name="3d36" id="3d36" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_g4JAJgAcDNDikhgwtLTcwQ.png"></figure><h3 name="8223" id="8223" class="graf graf--h3 graf-after--figure">Multi label classification [<a href="https://youtu.be/0frKXR-2PBY?t=25m29s" data-href="https://youtu.be/0frKXR-2PBY?t=25m29s" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">25:29</a>]</h3><p name="cf6b" id="cf6b" class="graf graf--p graf-after--h3"><a href="https://github.com/fastai/fastai/blob/master/courses/dl2/pascal-multi.ipynb" data-href="https://github.com/fastai/fastai/blob/master/courses/dl2/pascal-multi.ipynb" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Notebook</a></p><p name="5908" id="5908" class="graf graf--p graf-after--p">We want to keep building models that are slightly more complex than the last model so that if something stops working, we know exactly where it broke. Here are functions from the previous notebook:</p><pre name="52fd" id="52fd" class="graf graf--pre graf-after--p">%matplotlib inline<br>%reload_ext autoreload<br>%autoreload 2</pre><pre name="9b4a" id="9b4a" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">fastai.conv_learner</strong> <strong class="markup--strong markup--pre-strong">import</strong> *<br><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">fastai.dataset</strong> <strong class="markup--strong markup--pre-strong">import</strong> *<br><br><strong class="markup--strong markup--pre-strong">import</strong> <strong class="markup--strong markup--pre-strong">json</strong>, <strong class="markup--strong markup--pre-strong">pdb</strong><br><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">PIL</strong> <strong class="markup--strong markup--pre-strong">import</strong> ImageDraw, ImageFont<br><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">matplotlib</strong> <strong class="markup--strong markup--pre-strong">import</strong> patches, patheffects<br>torch.backends.cudnn.benchmark=<strong class="markup--strong markup--pre-strong">True</strong></pre><h4 name="197a" id="197a" class="graf graf--h4 graf-after--pre">Setup</h4><pre name="1c51" id="1c51" class="graf graf--pre graf-after--h4">PATH = Path('data/pascal')<br>trn_j = json.load((PATH / 'pascal_train2007.json').open())<br>IMAGES,ANNOTATIONS,CATEGORIES = ['images', 'annotations', <br>                                 'categories']<br>FILE_NAME,ID,IMG_ID,CAT_ID,BBOX = 'file_name','id','image_id', <br>                                  'category_id','bbox'<br><br>cats = dict((o[ID], o['name']) <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> trn_j[CATEGORIES])<br>trn_fns = dict((o[ID], o[FILE_NAME]) <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> trn_j[IMAGES])<br>trn_ids = [o[ID] <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> trn_j[IMAGES]]<br><br>JPEGS = 'VOCdevkit/VOC2007/JPEGImages'<br>IMG_PATH = PATH/JPEGS</pre><pre name="5736" id="5736" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> get_trn_anno():<br>    trn_anno = collections.defaultdict(<strong class="markup--strong markup--pre-strong">lambda</strong>:[])<br>    <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> trn_j[ANNOTATIONS]:<br>        <strong class="markup--strong markup--pre-strong">if</strong> <strong class="markup--strong markup--pre-strong">not</strong> o['ignore']:<br>            bb = o[BBOX]<br>            bb = np.array([bb[1], bb[0], bb[3]+bb[1]-1, <br>                           bb[2]+bb[0]-1])<br>            trn_anno[o[IMG_ID]].append((bb,o[CAT_ID]))<br>    <strong class="markup--strong markup--pre-strong">return</strong> trn_anno<br><br>trn_anno = get_trn_anno()</pre><pre name="d612" id="d612" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> show_img(im, figsize=<strong class="markup--strong markup--pre-strong">None</strong>, ax=<strong class="markup--strong markup--pre-strong">None</strong>):<br>    <strong class="markup--strong markup--pre-strong">if</strong> <strong class="markup--strong markup--pre-strong">not</strong> ax: fig,ax = plt.subplots(figsize=figsize)<br>    ax.imshow(im)<br>    ax.set_xticks(np.linspace(0, 224, 8))<br>    ax.set_yticks(np.linspace(0, 224, 8))<br>    ax.grid()<br>    ax.set_yticklabels([])<br>    ax.set_xticklabels([])<br>    <strong class="markup--strong markup--pre-strong">return</strong> ax<br><br><strong class="markup--strong markup--pre-strong">def</strong> draw_outline(o, lw):<br>    o.set_path_effects([patheffects.Stroke(<br>        linewidth=lw, foreground='black'), patheffects.Normal()])<br><br><strong class="markup--strong markup--pre-strong">def</strong> draw_rect(ax, b, color='white'):<br>    patch = ax.add_patch(patches.Rectangle(b[:2], *b[-2:], <br>                         fill=<strong class="markup--strong markup--pre-strong">False</strong>, edgecolor=color, lw=2))<br>    draw_outline(patch, 4)<br><br><strong class="markup--strong markup--pre-strong">def</strong> draw_text(ax, xy, txt, sz=14, color='white'):<br>    text = ax.text(*xy, txt,<br>        verticalalignment='top', color=color, fontsize=sz, <br>        weight='bold')<br>    draw_outline(text, 1)</pre><pre name="9675" id="9675" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> bb_hw(a): <strong class="markup--strong markup--pre-strong">return</strong> np.array([a[1],a[0],a[3]-a[1],a[2]-a[0]])<br><br><strong class="markup--strong markup--pre-strong">def</strong> draw_im(im, ann):<br>    ax = show_img(im, figsize=(16,8))<br>    <strong class="markup--strong markup--pre-strong">for</strong> b,c <strong class="markup--strong markup--pre-strong">in</strong> ann:<br>        b = bb_hw(b)<br>        draw_rect(ax, b)<br>        draw_text(ax, b[:2], cats[c], sz=16)<br><br><strong class="markup--strong markup--pre-strong">def</strong> draw_idx(i):<br>    im_a = trn_anno[i]<br>    im = open_image(IMG_PATH/trn_fns[i])<br>    draw_im(im, im_a)</pre><h4 name="153a" id="153a" class="graf graf--h4 graf-after--pre">Multi class&nbsp;[<a href="https://youtu.be/0frKXR-2PBY?t=26m12s" data-href="https://youtu.be/0frKXR-2PBY?t=26m12s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">26:12</a>]</h4><pre name="189d" id="189d" class="graf graf--pre graf-after--h4">MC_CSV = PATH/'tmp/mc.csv'</pre><pre name="9a35" id="9a35" class="graf graf--pre graf-after--pre">trn_anno[12]</pre><pre name="a704" id="a704" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[(array([ 96, 155, 269, 350]), 7)]</em></pre><pre name="2ebe" id="2ebe" class="graf graf--pre graf-after--pre">mc = [set([cats[p[1]] <strong class="markup--strong markup--pre-strong">for</strong> p <strong class="markup--strong markup--pre-strong">in</strong> trn_anno[o]]) <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> trn_ids]<br>mcs = [' '.join(str(p) <strong class="markup--strong markup--pre-strong">for</strong> p <strong class="markup--strong markup--pre-strong">in</strong> o) <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> mc]</pre><pre name="8c39" id="8c39" class="graf graf--pre graf-after--pre">df = pd.DataFrame({'fn': [trn_fns[o] <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> trn_ids], <br>                   'clas': mcs}, columns=['fn','clas'])<br>df.to_csv(MC_CSV, index=<strong class="markup--strong markup--pre-strong">False</strong>)</pre><p name="512b" id="512b" class="graf graf--p graf-after--pre">One of the students pointed out that by using Pandas, we can do things much simpler than using <code class="markup--code markup--p-code">collections.defaultdict</code> and shared <a href="https://gist.github.com/binga/1bc4ebe5e41f670f5954d2ffa9d6c0ed" data-href="https://gist.github.com/binga/1bc4ebe5e41f670f5954d2ffa9d6c0ed" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">this gist</a>. The more you get to know Pandas, the more often you realize it is a good way to solve lots of different problems.</p><p name="2dc8" id="2dc8" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: When you are incrementally building on top of smaller models, do you reuse them as pre-trained weights? or do you toss it away then retrain from scratch [<a href="https://youtu.be/0frKXR-2PBY?t=27m11s" data-href="https://youtu.be/0frKXR-2PBY?t=27m11s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">27:11</a>]? When Jeremy is figuring stuff out as he goes like this, he would generally lean towards tossing away because reusing pre-trained weights introduces unnecessary complexities. However, if he is trying to get to a point where he can train on really big images, he will generally start on much smaller and often re-use these weights.</p><pre name="5b68" id="5b68" class="graf graf--pre graf-after--p">f_model=resnet34<br>sz=224<br>bs=64</pre><pre name="a183" id="a183" class="graf graf--pre graf-after--pre">tfms = tfms_from_model(f_model, sz, crop_type=CropType.NO)<br>md = ImageClassifierData.from_csv(PATH, JPEGS, MC_CSV, tfms=tfms)</pre><pre name="99ad" id="99ad" class="graf graf--pre graf-after--pre">learn = ConvLearner.pretrained(f_model, md)<br>learn.opt_fn = optim.Adam</pre><pre name="cc2f" id="cc2f" class="graf graf--pre graf-after--pre">lr = 2e-2</pre><pre name="17d6" id="17d6" class="graf graf--pre graf-after--pre">learn.fit(lr, 1, cycle_len=3, use_clr=(32,5))</pre><pre name="560a" id="560a" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss   &lt;lambda&gt;                  <br>    0      0.104836   0.085015   0.972356  <br>    1      0.088193   0.079739   0.972461                   <br>    2      0.072346   0.077259   0.974114</em></pre><pre name="60c4" id="60c4" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[0.077258907, 0.9741135761141777]</em></pre><pre name="a327" id="a327" class="graf graf--pre graf-after--pre">lrs = np.array([lr/100, lr/10, lr])</pre><pre name="5faa" id="5faa" class="graf graf--pre graf-after--pre">learn.freeze_to(-2)</pre><pre name="d590" id="d590" class="graf graf--pre graf-after--pre">learn.fit(lrs/10, 1, cycle_len=5, use_clr=(32,5))</pre><pre name="fd47" id="fd47" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss   &lt;lambda&gt;                   <br>    0      0.063236   0.088847   0.970681  <br>    1      0.049675   0.079885   0.973723                   <br>    2      0.03693    0.076906   0.975601                   <br>    3      0.026645   0.075304   0.976187                   <br>    4      0.018805   0.074934   0.975165</em></pre><pre name="5ca7" id="5ca7" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[0.074934497, 0.97516526281833649]</em></pre><pre name="e973" id="e973" class="graf graf--pre graf-after--pre">learn.save('mclas')</pre><pre name="2834" id="2834" class="graf graf--pre graf-after--pre">learn.load('mclas')</pre><pre name="8c3b" id="8c3b" class="graf graf--pre graf-after--pre">y = learn.predict()<br>x,_ = next(iter(md.val_dl))<br>x = to_np(x)</pre><pre name="5b1f" id="5b1f" class="graf graf--pre graf-after--pre">fig, axes = plt.subplots(3, 4, figsize=(12, 8))<br><strong class="markup--strong markup--pre-strong">for</strong> i,ax <strong class="markup--strong markup--pre-strong">in</strong> enumerate(axes.flat):<br>    ima=md.val_ds.denorm(x)[i]<br>    ya = np.nonzero(y[i]&gt;0.4)[0]<br>    b = '<strong class="markup--strong markup--pre-strong">\n</strong>'.join(md.classes[o] <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> ya)<br>    ax = show_img(ima, ax=ax)<br>    draw_text(ax, (0,0), b)<br>plt.tight_layout()</pre><figure name="76a2" id="76a2" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_2m1Qoq3NhsqdYBd4hUTR6A.png"></figure><p name="4bec" id="4bec" class="graf graf--p graf-after--figure">Multi-class classification is pretty straight forward [<a href="https://youtu.be/0frKXR-2PBY?t=28m28s" data-href="https://youtu.be/0frKXR-2PBY?t=28m28s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">28:28</a>]. One minor tweak is the use of <code class="markup--code markup--p-code">set</code> in this line so that each object type appear once.:</p><pre name="de81" id="de81" class="graf graf--pre graf-after--p">mc = [<strong class="markup--strong markup--pre-strong">set</strong>([cats[p[1]] <strong class="markup--strong markup--pre-strong">for</strong> p <strong class="markup--strong markup--pre-strong">in</strong> trn_anno[o]]) <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> trn_ids]</pre><h4 name="f2f7" id="f2f7" class="graf graf--h4 graf-after--pre">SSD and YOLO&nbsp;[<a href="https://youtu.be/0frKXR-2PBY?t=29m10s" data-href="https://youtu.be/0frKXR-2PBY?t=29m10s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">29:10</a>]</h4><p name="4997" id="4997" class="graf graf--p graf-after--h4">We have an input image that goes through a conv net which outputs a vector of size <code class="markup--code markup--p-code">4+c</code> where <code class="markup--code markup--p-code">c=len(cats)</code>&nbsp;. This gives us an object detector for a single largest object. Let’s now create one that finds 16 objects. The obvious way to do this would be to take the last linear layer and rather than having <code class="markup--code markup--p-code">4+c</code> outputs, we could have <code class="markup--code markup--p-code">16x(4+c)</code> outputs. This gives us 16 sets of class probabilities and 16 sets of bounding box coordinates. Then we would just need a loss function that will check whether those 16 sets of bounding boxes correctly represented the up to 16 objects in the image (we will go into the loss function later).</p><figure name="09c8" id="09c8" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_fPHmCosDHcrHmtKvWFK9Mg.png"></figure><p name="8cbc" id="8cbc" class="graf graf--p graf-after--figure">The second way to do this is rather than using <code class="markup--code markup--p-code">nn.linear</code>, what if instead, we took from our ResNet convolutional backbone and added an <code class="markup--code markup--p-code">nn.Conv2d</code> with stride 2 [<a href="https://youtu.be/0frKXR-2PBY?t=31m32s" data-href="https://youtu.be/0frKXR-2PBY?t=31m32s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">31:32</a>]? This will give us a <code class="markup--code markup--p-code">4x4x[# of filters]</code> tensor — here let’s make it <code class="markup--code markup--p-code">4x4x(4+c)</code> so that we get a tensor where the number of elements is exactly equal to the number of elements we wanted. Now if we created a loss function that took a <code class="markup--code markup--p-code">4x4x(4+c)</code> tensor and and mapped it to 16 objects in the image and checked whether each one was correctly represented by these <code class="markup--code markup--p-code">4+c</code> activations, this would work as well. It turns out, both of these approaches are actually used [<a href="https://youtu.be/0frKXR-2PBY?t=33m48s" data-href="https://youtu.be/0frKXR-2PBY?t=33m48s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">33:48</a>]. The approach where the output is one big long vector from a fully connected linear layer is used by a class of models known as <a href="https://arxiv.org/abs/1506.02640" data-href="https://arxiv.org/abs/1506.02640" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">YOLO (You Only Look Once)</a>, where else, the approach of the convolutional activations is used by models which started with something called <a href="https://arxiv.org/abs/1512.02325" data-href="https://arxiv.org/abs/1512.02325" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">SSD (Single Shot Detector)</a>. Since these things came out very similar times in late 2015, things are very much moved towards SSD. So the point where this morning, <a href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" data-href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">YOLO version 3</a> came out and is now doing SSD, so that’s what we are going to do. We will also learn about why this makes more sense as well.</p><h4 name="20e9" id="20e9" class="graf graf--h4 graf-after--p">Anchor boxes&nbsp;[<a href="https://youtu.be/0frKXR-2PBY?t=35m04s" data-href="https://youtu.be/0frKXR-2PBY?t=35m04s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">35:04</a>]</h4><figure name="42e6" id="42e6" class="graf graf--figure graf-after--h4"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_8kpDP3FZFxW99IUQE0C8Xw.png"></figure><p name="4bfa" id="4bfa" class="graf graf--p graf-after--figure">Let’s imagine that we had another <code class="markup--code markup--p-code">Conv2d(stride=2)</code> then we would have <code class="markup--code markup--p-code">2x2x(4+c)</code> tensor. Basically, it is creating a grid that looks something like this:</p><figure name="37fd" id="37fd" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_uA-oJok4-Rng6mnHOOPyNQ.png"></figure><p name="c44d" id="c44d" class="graf graf--p graf-after--figure">This is how the geometry of the activations of the second extra convolutional stride 2 layer are. Remember, stride 2 convolution does the same thing to the geometry of the activations as a stride 1 convolution followed by maxpooling assuming the padding is ok.</p><p name="fd03" id="fd03" class="graf graf--p graf-after--p">Let’s talk about what we might do here [<a href="https://youtu.be/0frKXR-2PBY?t=36m9s" data-href="https://youtu.be/0frKXR-2PBY?t=36m9s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">36:09</a>]. We want each of these grid cell to be responsible for finding the largest object in that part of the image.</p><h4 name="3bae" id="3bae" class="graf graf--h4 graf-after--p">Receptive Field&nbsp;[<a href="https://youtu.be/0frKXR-2PBY?t=37m20s" data-href="https://youtu.be/0frKXR-2PBY?t=37m20s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">37:20</a>]</h4><p name="daf5" id="daf5" class="graf graf--p graf-after--h4">Why do we care about the idea that we would like each convolutional grid cell to be responsible for finding things that are in the corresponding part of the image? The reason is because of something called the receptive field of that convolutional grid cell. The basic idea is that throughout your convolutional layers, every piece of those tensors has a receptive field which means which part of the input image was responsible for calculating that cell. Like all things in life, the easiest way to see this is with Excel [<a href="https://youtu.be/0frKXR-2PBY?t=38m1s" data-href="https://youtu.be/0frKXR-2PBY?t=38m1s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">38:01</a>].</p><figure name="4875" id="4875" class="graf graf--figure graf--layoutOutsetCenter graf-after--p" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_IgL2CMSit3Hh9N2Fq2Zlgg.png"></figure><p name="d0cd" id="d0cd" class="graf graf--p graf-after--figure">Take a single activation (in this case in the maxpool layer)and let’s see where it came from [<a href="https://youtu.be/0frKXR-2PBY?t=38m45s" data-href="https://youtu.be/0frKXR-2PBY?t=38m45s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">38:45</a>]. In excel you can do Formulas → Trace Precedents. Tracing all the way back to the input layer, you can see that it came from this 6 x 6 portion of the image (as well as filters). What is more, the middle portion has lots of weights coming out of where else cells in the outside only have one weight coming out. So we call this 6 x 6 cells the receptive field of the one activation we picked.</p><figure name="719f" id="719f" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_cCBVbJ2WjiPMlqX4nA2bwA.png"><figcaption class="imageCaption">3x3 convolution with opacity 15% — clearly the center of the box has more dependencies</figcaption></figure><p name="10e4" id="10e4" class="graf graf--p graf-after--figure">Note that the receptive field is not just saying it’s this box but also that the center of the box has more dependencies [<a href="https://youtu.be/0frKXR-2PBY?t=40m27s" data-href="https://youtu.be/0frKXR-2PBY?t=40m27s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">40:27</a>] This is a critically important concept when it comes to understanding architectures and understanding why conv nets work the way they do.</p><h4 name="f66c" id="f66c" class="graf graf--h4 graf-after--p">Architecture [<a href="https://youtu.be/0frKXR-2PBY?t=41m18s" data-href="https://youtu.be/0frKXR-2PBY?t=41m18s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">41:18</a>]</h4><p name="19c4" id="19c4" class="graf graf--p graf-after--h4">The architecture is, we will have a ResNet backbone followed by one or more 2D convolutions (one for now) which is going to give us a <code class="markup--code markup--p-code">4x4</code> grid.</p><pre name="ddb9" id="ddb9" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">StdConv</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, nin, nout, stride=2, drop=0.1):<br>        super().__init__()<br>        self.conv = nn.Conv2d(nin, nout, 3, stride=stride, <br>                              padding=1)<br>        self.bn = nn.BatchNorm2d(nout)<br>        self.drop = nn.Dropout(drop)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x): <br>        <strong class="markup--strong markup--pre-strong">return</strong> self.drop(self.bn(F.relu(self.conv(x))))<br>        <br><strong class="markup--strong markup--pre-strong">def</strong> flatten_conv(x,k):<br>    bs,nf,gx,gy = x.size()<br>    x = x.permute(0,2,3,1).contiguous()<br>    <strong class="markup--strong markup--pre-strong">return</strong> x.view(bs,-1,nf//k)</pre><pre name="fa2e" id="fa2e" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">OutConv</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, k, nin, bias):<br>        super().__init__()<br>        self.k = k<br>        self.oconv1 = nn.Conv2d(nin, (len(id2cat)+1)*k, 3, <br>                                padding=1)<br>        self.oconv2 = nn.Conv2d(nin, 4*k, 3, padding=1)<br>        self.oconv1.bias.data.zero_().add_(bias)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x):<br>        <strong class="markup--strong markup--pre-strong">return</strong> [flatten_conv(self.oconv1(x), self.k),<br>                flatten_conv(self.oconv2(x), self.k)]</pre><pre name="d08c" id="d08c" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">SSD_Head</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, k, bias):<br>        super().__init__()<br>        self.drop = nn.Dropout(0.25)<br>        self.sconv0 = StdConv(512,256, stride=1)<br>        self.sconv2 = StdConv(256,256)<br>        self.out = OutConv(k, 256, bias)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x):<br>        x = self.drop(F.relu(x))<br>        x = self.sconv0(x)<br>        x = self.sconv2(x)<br>        <strong class="markup--strong markup--pre-strong">return</strong> self.out(x)<br><br>head_reg4 = SSD_Head(k, -3.)<br>models = ConvnetBuilder(f_model, 0, 0, 0, custom_head=head_reg4)<br>learn = ConvLearner(md, models)<br>learn.opt_fn = optim.Adam</pre><p name="2589" id="2589" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">SSD_Head</strong></p><ol class="postList"><li name="62b3" id="62b3" class="graf graf--li graf-after--p">We start with ReLU and dropout</li><li name="dbc8" id="dbc8" class="graf graf--li graf-after--li">Then stride 1 convolution. The reason we start with a stride 1 convolution is because that does not change the geometry at all — it just lets us add an extra layer of calculation. It lets us create not just a linear layer but now we have a little mini neural network in our custom head. <code class="markup--code markup--li-code">StdConv</code> is defined above — it does convolution, ReLU, BatchNorm, and dropout. Most research code you see won’t define a class like this, instead they write the entire thing again and again. Don’t be like that. Duplicate code leads to errors and poor understanding.</li><li name="74ad" id="74ad" class="graf graf--li graf-after--li">Stride 2 convolution [<a href="https://youtu.be/0frKXR-2PBY?t=44m56s" data-href="https://youtu.be/0frKXR-2PBY?t=44m56s" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">44:56</a>]</li><li name="b4de" id="b4de" class="graf graf--li graf-after--li">At the end, the output of step 3 is <code class="markup--code markup--li-code">4x4</code> which gets passed to <code class="markup--code markup--li-code">OutConv</code>. <code class="markup--code markup--li-code">OutConv</code> has two separate convolutional layers each of which is stride 1 so it is not changing the geometry of the input. One of them is of length of the number of classes (ignore <code class="markup--code markup--li-code">k</code> for now and <code class="markup--code markup--li-code">+1</code> is for “background” — i.e. no object was detected), the other’s length is 4. Rather than having a single conv layer that outputs <code class="markup--code markup--li-code">4+c</code>, let’s have two conv layers and return their outputs in a list. This allows these layers to specialize just a little bit. We talked about this idea that when you have multiple tasks, they can share layers, but they do not have to share all the layers. In this case, our two tasks of creating a classifier and creating and creating bounding box regression share every single layers except the very last one.</li><li name="7bcd" id="7bcd" class="graf graf--li graf-after--li">At the end, we flatten out the convolution because Jeremy wrote the loss function to expect flattened out tensor, but we could totally rewrite it to not do that.</li></ol><h4 name="d6b4" id="d6b4" class="graf graf--h4 graf-after--li"><a href="https://github.com/fastai/fastai/blob/master/docs/style.md" data-href="https://github.com/fastai/fastai/blob/master/docs/style.md" class="markup--anchor markup--h4-anchor" rel="noopener nofollow" target="_blank">Fastai Coding Style</a>&nbsp;[<a href="https://youtu.be/0frKXR-2PBY?t=42m58s" data-href="https://youtu.be/0frKXR-2PBY?t=42m58s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">42:58</a>]</h4><p name="6d61" id="6d61" class="graf graf--p graf-after--h4">The first draft was released this week. It is very heavily orient towards the idea of expository programming which is the idea that programming code should be something that you can use to explain an idea, ideally as readily as mathematical notation, to somebody that understands your coding method. The idea goes back a very long way, but it was best described in the Turing Award lecture of 1979 by probably Jeremy’s greatest computer science hero Ken Iverson. He had been working on it since well before 1964 but 1964 was the first example of this approach of programming he released which is called APL and 25 years later, he won the Turing Award. He then passed on the baton to his son Eric Iverson. Fastai style guide is an attempt at taking some of these ideas.</p><h4 name="2d44" id="2d44" class="graf graf--h4 graf-after--p">Loss Function&nbsp;[<a href="https://youtu.be/0frKXR-2PBY?t=47m44s" data-href="https://youtu.be/0frKXR-2PBY?t=47m44s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">47:44</a>]</h4><p name="aad3" id="aad3" class="graf graf--p graf-after--h4">The loss function needs to look at each of these 16 sets of activations, each of which has four bounding box coordinates and <code class="markup--code markup--p-code">c+1</code> class probabilities and decide if those activations are close or far away from the object which is the closest to this grid cell in the image. If nothing is there, then whether it is predicting background correctly. That turns out to be very hard to do.</p><h4 name="339d" id="339d" class="graf graf--h4 graf-after--p">Matching Problem&nbsp;[<a href="https://youtu.be/0frKXR-2PBY?t=48m43s" data-href="https://youtu.be/0frKXR-2PBY?t=48m43s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">48:43</a>]</h4><figure name="b566" id="b566" class="graf graf--figure graf-after--h4"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_2dqj3hivcOF6ThoL-nhMyA.png"></figure><p name="6b0b" id="6b0b" class="graf graf--p graf-after--figure">The loss function needs to take each of the objects in the image and match them to one of these convolutional grid cells to say “this grid cell is responsible for this particular object” so then it can go ahead and say “okay, how close are the 4 coordinates and how close are the class probabilities.</p><p name="8e9f" id="8e9f" class="graf graf--p graf-after--p">Here is our goal [<a href="https://youtu.be/0frKXR-2PBY?t=49m56s" data-href="https://youtu.be/0frKXR-2PBY?t=49m56s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">49:56</a>]:</p><figure name="f181" id="f181" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_8M9x-WgHNasmuLSJNbKoaQ.png"></figure><p name="3bda" id="3bda" class="graf graf--p graf-after--figure">Our dependent variable looks like the one on the left, and our final convolutional layer is going to be <code class="markup--code markup--p-code">4x4x(c+1)</code> in this case <code class="markup--code markup--p-code">c=20</code>. We then flatten that out into a vector. Our goal is to come up with a function which takes in a dependent variable and also some particular set of activations that ended up coming out of the model and returns a higher number if these activations are not a good reflection of the ground truth bounding boxes; or a lower number if it is a good reflection.</p><h4 name="308c" id="308c" class="graf graf--h4 graf-after--p">Testing [<a href="https://youtu.be/0frKXR-2PBY?t=51m58s" data-href="https://youtu.be/0frKXR-2PBY?t=51m58s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">51:58</a>]</h4><pre name="986e" id="986e" class="graf graf--pre graf-after--h4">x,y = next(iter(md.val_dl))<br>x,y = V(x),V(y)<br>learn.model.eval()<br>batch = learn.model(x)<br>b_clas,b_bb = batch<br>b_clas.size(),b_bb.size()</pre><pre name="209c" id="209c" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(torch.Size([64, 16, 21]), torch.Size([64, 16, 4]))</em></pre><p name="992c" id="992c" class="graf graf--p graf-after--pre">Make sure these shapes make sense. Let’s now look at the ground truth <code class="markup--code markup--p-code">y</code> [<a href="https://youtu.be/0frKXR-2PBY?t=53m24s" data-href="https://youtu.be/0frKXR-2PBY?t=53m24s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">53:24</a>]:</p><pre name="5514" id="5514" class="graf graf--pre graf-after--p">idx=7<br>b_clasi = b_clas[idx]<br>b_bboxi = b_bb[idx]<br>ima=md.val_ds.ds.denorm(to_np(x))[idx]<br>bbox,clas = get_y(y[0][idx], y[1][idx])<br>bbox,clas</pre><pre name="b746" id="b746" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(Variable containing:<br>  0.6786  0.4866  0.9911  0.6250<br>  0.7098  0.0848  0.9911  0.5491<br>  0.5134  0.8304  0.6696  0.9063<br> [torch.cuda.FloatTensor of size 3x4 (GPU 0)], Variable containing:<br>   8<br>  10<br>  17<br> [torch.cuda.LongTensor of size 3 (GPU 0)])</em></pre><p name="a6a5" id="a6a5" class="graf graf--p graf-after--pre">Note that bounding box coordinates have been scaled to between 0 and 1 — basically we are treating the image as being 1x1, so they are relative to the size of the image.</p><p name="b788" id="b788" class="graf graf--p graf-after--p">We already have <code class="markup--code markup--p-code">show_ground_truth</code> function. This <code class="markup--code markup--p-code">torch_gt</code> (gt: ground truth) function simply converts tensors into numpy array.</p><pre name="d582" id="d582" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> torch_gt(ax, ima, bbox, clas, prs=<strong class="markup--strong markup--pre-strong">None</strong>, thresh=0.4):<br>    <strong class="markup--strong markup--pre-strong">return</strong> show_ground_truth(ax, ima, to_np((bbox*224).long()),<br>         to_np(clas), <br>         to_np(prs) <strong class="markup--strong markup--pre-strong">if</strong> prs <strong class="markup--strong markup--pre-strong">is</strong> <strong class="markup--strong markup--pre-strong">not</strong> <strong class="markup--strong markup--pre-strong">None</strong> <strong class="markup--strong markup--pre-strong">else</strong> <strong class="markup--strong markup--pre-strong">None</strong>, thresh)</pre><pre name="4392" id="4392" class="graf graf--pre graf-after--pre">fig, ax = plt.subplots(figsize=(7,7))<br>torch_gt(ax, ima, bbox, clas)</pre><figure name="d862" id="d862" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_Q3ZtSRtk-a2OwKfE1wa5zw.png"></figure><p name="68a9" id="68a9" class="graf graf--p graf-after--figure">The above is a ground truth. Here is our <code class="markup--code markup--p-code">4x4</code> grid cells from our final convolutional layer [<a href="https://youtu.be/0frKXR-2PBY?t=54m44s" data-href="https://youtu.be/0frKXR-2PBY?t=54m44s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">54:44</a>]:</p><pre name="1592" id="1592" class="graf graf--pre graf-after--p">fig, ax = plt.subplots(figsize=(7,7))<br>torch_gt(ax, ima, anchor_cnr, b_clasi.max(1)[1])</pre><figure name="9b0b" id="9b0b" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_xjKmShqdLnD_JX4Aj7U80g.png"></figure><p name="930c" id="930c" class="graf graf--p graf-after--figure">Each of these square boxes, different papers call them different things. The three terms you’ll hear are: anchor boxes, prior boxes, or default boxes. We will stick with the term anchor boxes.</p><p name="e77b" id="e77b" class="graf graf--p graf-after--p">What we are going to do for this loss function is we are going to go through a matching problem where we are going to take every one of these 16 boxes and see which one of these three ground truth objects has the highest amount of overlap with a given square [<a href="https://youtu.be/0frKXR-2PBY?t=55m21s" data-href="https://youtu.be/0frKXR-2PBY?t=55m21s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">55:21</a>]. To do this, we have to have some way of measuring amount of overlap and a standard function for this is called <a href="https://en.wikipedia.org/wiki/Jaccard_index" data-href="https://en.wikipedia.org/wiki/Jaccard_index" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">Jaccard index</a> (IoU).</p><figure name="7378" id="7378" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_10ORjq4HuOc0umcnojiDPA.png"></figure><p name="3347" id="3347" class="graf graf--p graf-after--figure">We are going to go through and find the Jaccard overlap for each one of the three objects versus each of the 16 anchor boxes [<a href="https://youtu.be/0frKXR-2PBY?t=57m11s" data-href="https://youtu.be/0frKXR-2PBY?t=57m11s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">57:11</a>]. That is going to give us a <code class="markup--code markup--p-code">3x16</code> matrix.</p><p name="3634" id="3634" class="graf graf--p graf-after--p">Here are the <em class="markup--em markup--p-em">coordinates</em> of all of our anchor boxes (centers, height, width):</p><pre name="4953" id="4953" class="graf graf--pre graf-after--p">anchors</pre><pre name="2863" id="2863" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">Variable containing:<br> 0.1250  0.1250  0.2500  0.2500<br> 0.1250  0.3750  0.2500  0.2500<br> 0.1250  0.6250  0.2500  0.2500<br> 0.1250  0.8750  0.2500  0.2500<br> 0.3750  0.1250  0.2500  0.2500<br> 0.3750  0.3750  0.2500  0.2500<br> 0.3750  0.6250  0.2500  0.2500<br> 0.3750  0.8750  0.2500  0.2500<br> 0.6250  0.1250  0.2500  0.2500<br> 0.6250  0.3750  0.2500  0.2500<br> 0.6250  0.6250  0.2500  0.2500<br> 0.6250  0.8750  0.2500  0.2500<br> 0.8750  0.1250  0.2500  0.2500<br> 0.8750  0.3750  0.2500  0.2500<br> 0.8750  0.6250  0.2500  0.2500<br> 0.8750  0.8750  0.2500  0.2500<br>[torch.cuda.FloatTensor of size 16x4 (GPU 0)]</em></pre><p name="9f3a" id="9f3a" class="graf graf--p graf-after--pre">Here are the amount of overlap between 3 ground truth objects and 16 anchor boxes:</p><pre name="f523" id="f523" class="graf graf--pre graf-after--p">overlaps = jaccard(bbox.data, anchor_cnr.data)<br>overlaps</pre><pre name="09bc" id="09bc" class="graf graf--pre graf-after--pre">Columns 0 to 7   <br>0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000    0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000    0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000      </pre><pre name="888b" id="888b" class="graf graf--pre graf-after--pre">Columns 8 to 15   <br>0.0000  0.0091 0.0922  0.0000  0.0000  0.0315  0.3985  0.0000  0.0356  0.0549 0.0103  0.0000  0.2598  0.4538  0.0653  0.0000  0.0000  0.0000 0.0000  0.1897  0.0000  0.0000  0.0000  0.0000 [torch.cuda.FloatTensor of size 3x16 (GPU 0)]</pre><p name="8142" id="8142" class="graf graf--p graf-after--pre">What we could do now is we could take the max of dimension 1 (row-wise) which will tell us for each ground truth object, what the maximum amount that overlaps with some grid cell as well as the index:</p><pre name="197d" id="197d" class="graf graf--pre graf-after--p">overlaps.max(1)</pre><pre name="5e49" id="5e49" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(<br>  0.3985<br>  0.4538<br>  0.1897<br> [torch.cuda.FloatTensor of size 3 (GPU 0)], <br>  14<br>  13<br>  11<br> [torch.cuda.LongTensor of size 3 (GPU 0)])</em></pre><p name="fdce" id="fdce" class="graf graf--p graf-after--pre">We will also going to look at max over a dimension 0 (column-wise) which will tell us what is the maximum amount of overlap for each grid cell across all of the ground truth objects [<a href="https://youtu.be/0frKXR-2PBY?t=59m8s" data-href="https://youtu.be/0frKXR-2PBY?t=59m8s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">59:08</a>]:</p><pre name="df2e" id="df2e" class="graf graf--pre graf-after--p">overlaps.max(0)</pre><pre name="757d" id="757d" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(<br>  0.0000<br>  0.0000<br>  0.0000<br>  0.0000<br>  0.0000<br>  0.0000<br>  0.0000<br>  0.0000<br>  0.0356<br>  0.0549<br>  0.0922<br>  0.1897<br>  0.2598<br>  0.4538<br>  0.3985<br>  0.0000<br> [torch.cuda.FloatTensor of size 16 (GPU 0)], <br>  0<br>  0<br>  0<br>  0<br>  0<br>  0<br>  0<br>  0<br>  1<br>  1<br>  0<br>  2<br>  1<br>  1<br>  0<br>  0<br> [torch.cuda.LongTensor of size 16 (GPU 0)])</em></pre><p name="5f19" id="5f19" class="graf graf--p graf-after--pre">What is particularly interesting here is that it tells us for every grid cell what is the index of the ground truth object which overlaps with it the most. Zero is a bit overloaded here — zero could either mean the amount of overlap was zero or its largest overlap is with object index zero. It is going to turn out not to matter but just FYI.</p><p name="d4fd" id="d4fd" class="graf graf--p graf-after--p">There is a function called <code class="markup--code markup--p-code">map_to_ground_truth</code> which we will not worry about for now [<a href="https://youtu.be/0frKXR-2PBY?t=59m57s" data-href="https://youtu.be/0frKXR-2PBY?t=59m57s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">59:57</a>]. It is super simple code but it is slightly awkward to think about. Basically what it does is it combines these two sets of overlaps in a way described in the SSD paper to assign every anchor box to a ground truth object. The way it assign that is each of the three (row-wise max) gets assigned as is. For the rest of the anchor boxes, they get assigned to anything which they have an overlap of at least 0.5 with (column-wise). If neither applies, it is considered to be a cell which contains background.</p><pre name="367d" id="367d" class="graf graf--pre graf-after--p">gt_overlap,gt_idx = map_to_ground_truth(overlaps)<br>gt_overlap,gt_idx</pre><pre name="31ef" id="31ef" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(<br>  0.0000<br>  0.0000<br>  0.0000<br>  0.0000<br>  0.0000<br>  0.0000<br>  0.0000<br>  0.0000<br>  0.0356<br>  0.0549<br>  0.0922<br>  1.9900<br>  0.2598<br>  1.9900<br>  1.9900<br>  0.0000<br> [torch.cuda.FloatTensor of size 16 (GPU 0)], <br>  0<br>  0<br>  0<br>  0<br>  0<br>  0<br>  0<br>  0<br>  1<br>  1<br>  0<br>  2<br>  1<br>  1<br>  0<br>  0<br> [torch.cuda.LongTensor of size 16 (GPU 0)])</em></pre></body></html>