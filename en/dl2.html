
<!-- saved from url=(0063)file:///C:/Users/asus/Desktop/fastai-ml-dl-notes-zh/en/dl2.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></head><body><hr class="section-divider"><h1 name="3fd8" id="3fd8" class="graf graf--h3 graf--leading graf--title">Deep Learning 2: Part 1 Lesson&nbsp;2</h1><p name="43c3" id="43c3" class="graf graf--p graf-after--h3"><em class="markup--em markup--p-em">My personal notes from </em><a href="http://www.fast.ai/" data-href="http://www.fast.ai/" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener" target="_blank"><em class="markup--em markup--p-em">fast.ai course</em></a><em class="markup--em markup--p-em">. These notes will continue to be updated and improved as I continue to review the course to “really” understand it. Much appreciation to </em><a href="https://twitter.com/jeremyphoward" data-href="https://twitter.com/jeremyphoward" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener" target="_blank"><em class="markup--em markup--p-em">Jeremy</em></a><em class="markup--em markup--p-em"> and </em><a href="https://twitter.com/math_rachel" data-href="https://twitter.com/math_rachel" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener" target="_blank"><em class="markup--em markup--p-em">Rachel</em></a><em class="markup--em markup--p-em"> who gave me this opportunity to learn.</em></p><p name="d3c5" id="d3c5" class="graf graf--p graf-after--p graf--trailing">Lessons: <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" class="markup--anchor markup--p-anchor" target="_blank">1</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" class="markup--anchor markup--p-anchor" target="_blank"><strong class="markup--strong markup--p-strong">2</strong></a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" class="markup--anchor markup--p-anchor" target="_blank">3</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" class="markup--anchor markup--p-anchor" target="_blank">4</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" class="markup--anchor markup--p-anchor" target="_blank">5</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" class="markup--anchor markup--p-anchor" target="_blank">6</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" class="markup--anchor markup--p-anchor" target="_blank">7</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" class="markup--anchor markup--p-anchor" target="_blank">8</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" class="markup--anchor markup--p-anchor" target="_blank">9</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" class="markup--anchor markup--p-anchor" target="_blank">10</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" class="markup--anchor markup--p-anchor" target="_blank">11</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" class="markup--anchor markup--p-anchor" target="_blank">12</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" class="markup--anchor markup--p-anchor" target="_blank">13</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" class="markup--anchor markup--p-anchor" target="_blank">14</a></p><hr class="section-divider"><h3 name="5e91" id="5e91" class="graf graf--h3 graf--leading"><a href="http://forums.fast.ai/t/wiki-lesson-2/9399/1" data-href="http://forums.fast.ai/t/wiki-lesson-2/9399/1" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">Lesson 2</a></h3><p name="1e63" id="1e63" class="graf graf--p graf-after--h3"><a href="https://github.com/fastai/fastai/blob/master/courses/dl1/lesson1.ipynb" data-href="https://github.com/fastai/fastai/blob/master/courses/dl1/lesson1.ipynb" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Notebook</a></p><h4 name="5e24" id="5e24" class="graf graf--h4 graf-after--p">Review of last lesson&nbsp;[<a href="https://youtu.be/JNxcznsrRb8?t=1m2s" data-href="https://youtu.be/JNxcznsrRb8?t=1m2s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">01:02</a>]</h4><ul class="postList"><li name="662c" id="662c" class="graf graf--li graf-after--h4">We used 3 lines of code to build an image classifier.</li><li name="a62a" id="a62a" class="graf graf--li graf-after--li">In order to train the model, data needs to be organized in a certain way under <code class="markup--code markup--li-code">PATH</code> (in this case <code class="markup--code markup--li-code">data/dogscats/</code>):</li></ul><figure name="7c4f" id="7c4f" class="graf graf--figure graf-after--li"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_DdsnEeT2DrnAAp_NrHc-jA.png"></figure><ul class="postList"><li name="426e" id="426e" class="graf graf--li graf-after--figure">There should be <code class="markup--code markup--li-code">train</code> folder and <code class="markup--code markup--li-code">valid</code> folder, and under each of these, folders with classification labels (i.e. <code class="markup--code markup--li-code">cats</code> and <code class="markup--code markup--li-code">dogs</code> for this example) with corresponding images in them.</li><li name="f4db" id="f4db" class="graf graf--li graf-after--li">The training output: <em class="markup--em markup--li-em">[</em><code class="markup--code markup--li-code"><em class="markup--em markup--li-em">epoch #</em></code><em class="markup--em markup--li-em">, </em><code class="markup--code markup--li-code"><em class="markup--em markup--li-em">training loss</em></code><em class="markup--em markup--li-em">, </em><code class="markup--code markup--li-code"><em class="markup--em markup--li-em">validation loss</em></code><em class="markup--em markup--li-em">, </em><code class="markup--code markup--li-code"><em class="markup--em markup--li-em">accuracy</em></code><em class="markup--em markup--li-em">]</em></li></ul><pre name="2d14" id="2d14" class="graf graf--pre graf-after--li"><em class="markup--em markup--pre-em">[ 0.       0.04955  0.02605  0.98975]</em></pre><h4 name="f8b7" id="f8b7" class="graf graf--h4 graf-after--pre"><strong class="markup--strong markup--h4-strong">Learning Rate&nbsp;[</strong><a href="https://youtu.be/JNxcznsrRb8?t=4m54s" data-href="https://youtu.be/JNxcznsrRb8?t=4m54s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--h4-strong">4:54</strong></a><strong class="markup--strong markup--h4-strong">]</strong></h4><ul class="postList"><li name="fe22" id="fe22" class="graf graf--li graf-after--h4">The basic idea of learning rate is that it is going to decide how quickly we zoom/hone in on the solution.</li></ul><figure name="5135" id="5135" class="graf graf--figure graf-after--li"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_bl1EuPH_XEGvMcMW6ZloNg.jpeg"></figure><ul class="postList"><li name="30e1" id="30e1" class="graf graf--li graf-after--figure">If the learning rate is too small, it will take very long time to get to the bottom</li><li name="f2f6" id="f2f6" class="graf graf--li graf-after--li">If the learning rate is too big, it could get oscillate away from the bottom.</li><li name="6c2f" id="6c2f" class="graf graf--li graf-after--li">Learning rate finder (<code class="markup--code markup--li-code">learn.lr_find</code>) will increase the learning rate after each mini-batch. Eventually, the learning rate is too high that loss will get worse. We then look at the plot of learning rate against loss, and determine the lowest point and go back by one magnitude and choose that as a learning rate (<code class="markup--code markup--li-code">1e-2</code> in the example below).</li><li name="1dc7" id="1dc7" class="graf graf--li graf-after--li">Mini-batch is a set of few images we look at each time so that we are using the parallel processing power of the GPU effectively (generally 64 or 128 images at a time)</li><li name="31ec" id="31ec" class="graf graf--li graf-after--li">In Python:</li></ul><figure name="f57c" id="f57c" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*3ZW61inLJykJLs0FntGrqA.png" data-width="256" data-height="65" src="../img/1_3ZW61inLJykJLs0FntGrqA.png"></figure><figure name="986f" id="986f" class="graf graf--figure graf--layoutOutsetRow is-partialWidth graf-after--figure" style="width: 49.835%;" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_GgOPv2YCx3QOUpCwolFCyA.png"></figure><figure name="6985" id="6985" class="graf graf--figure graf--layoutOutsetRowContinue is-partialWidth graf-after--figure" style="width: 50.165%;" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_5EdBB9JTXXf-5ccqzDr5Kg.png"></figure><ul class="postList"><li name="bc47" id="bc47" class="graf graf--li graf-after--figure">By adjusting this one number, you should be able to get pretty good results. fast.ai library picks the rest of the hyper parameters for you. But as the course progresses, we will learn that there are some more things we can tweak to get slightly better results. But learning rate is the key number for us to set.</li><li name="c854" id="c854" class="graf graf--li graf-after--li">Learning rate finder sits on top of other optimizers (e.g. momentum, Adam, etc) and help you choose the best learning rate given what other tweaks you are using (such as advanced optimizers but not limited to optimizers).</li><li name="973d" id="973d" class="graf graf--li graf-after--li">Questions: what happens for optimizers that changes learning rate during the epoch? Is this finder choosing an initial learning rate?[<a href="https://youtu.be/JNxcznsrRb8?t=14m5s" data-href="https://youtu.be/JNxcznsrRb8?t=14m5s" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">14:05</a>] We will learn about optimizers in details later, but the basic answer is no. Even Adam has a learning rate which gets divided by the average previous gradient and also the recent sum of squared gradients. Even those so-called “dynamic learning rate” methods have a learning rate.</li><li name="57c2" id="57c2" class="graf graf--li graf-after--li">The most important thing you can do to make the model better is to give it more data. Since these models have millions of parameters, if you train them for a while, they start to do what is called “overfitting”.</li><li name="7f27" id="7f27" class="graf graf--li graf-after--li">Overfitting — the model is starting to see the specific details of the images in the training set rather than learning something general that can be transferred across to the validation set.</li><li name="1ed4" id="1ed4" class="graf graf--li graf-after--li">We can collect more data, but another easy way is data augmentation.</li></ul><h4 name="1581" id="1581" class="graf graf--h4 graf-after--li">Data Augmentation [<a href="https://youtu.be/JNxcznsrRb8?t=15m50s" data-href="https://youtu.be/JNxcznsrRb8?t=15m50s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">15:50</a>]</h4><figure name="9564" id="9564" class="graf graf--figure graf-after--h4"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_7LgHDHSM9jgRLUX6_vYYnA.png"></figure><ul class="postList"><li name="f2f1" id="f2f1" class="graf graf--li graf-after--figure">Every epoch, we will randomly change the image a little bit. In other words, the model is going to see slightly different version of the image each epoch.</li><li name="1383" id="1383" class="graf graf--li graf-after--li">You want to use different types of data augmentation for different types of image (flip horizontally, vertically, zoom in, zoom out, vary contrast and brightness, and many more).</li></ul><h4 name="e776" id="e776" class="graf graf--h4 graf-after--li">Learning Rate Finder Questions [<a href="https://youtu.be/JNxcznsrRb8?t=19m11s" data-href="https://youtu.be/JNxcznsrRb8?t=19m11s" class="markup--anchor markup--h4-anchor" rel="noopener nofollow" target="_blank">19:11</a>]:</h4><ul class="postList"><li name="c26a" id="c26a" class="graf graf--li graf-after--h4">Why not pick the bottom? The point at which the loss was lowest is where the red circle is. But that learning rate was actually too large at that point and will not likely to converge. So the one before that would be a better choice (it is always better to pick a learning rate that is smaller than too big)</li></ul><figure name="c5d0" id="c5d0" class="graf graf--figure graf-after--li"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_vsXrd010HEYLVfoe2F-ZiQ.png"></figure><ul class="postList"><li name="a261" id="a261" class="graf graf--li graf-after--figure">When should we learn <code class="markup--code markup--li-code">lr_find</code>? [<a href="https://youtu.be/JNxcznsrRb8?t=23m2s" data-href="https://youtu.be/JNxcznsrRb8?t=23m2s" class="markup--anchor markup--li-anchor" rel="noopener nofollow" target="_blank">23:02</a>] Run it once at the start, and maybe after unfreezing layers (we will learn it later). Also when I change the thing I am training or change the way I am training it. Never any harm in running it.</li></ul><h4 name="e7d5" id="e7d5" class="graf graf--h4 graf-after--li">Back to Data Augmentation [<a href="https://youtu.be/JNxcznsrRb8?t=24m10s" data-href="https://youtu.be/JNxcznsrRb8?t=24m10s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">24:10</a>]</h4><pre name="ae6d" id="ae6d" class="graf graf--pre graf-after--h4">tfms = tfms_from_model(resnet34, sz, <strong class="markup--strong markup--pre-strong">aug_tfms=transforms_side_on</strong>, max_zoom=1.1)</pre><ul class="postList"><li name="709c" id="709c" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">transform_side_on</code> — a predefined set of transformations for side-on photos (there is also <code class="markup--code markup--li-code">transform_top_down</code>). Later we will learn how to create custom transform lists.</li><li name="b51d" id="b51d" class="graf graf--li graf-after--li">It is not exactly creating new data, but allows the convolutional neural net to learn how to recognize cats or dogs from somewhat different angles.</li></ul><pre name="8305" id="8305" class="graf graf--pre graf-after--li">data = ImageClassifierData.from_paths(PATH, tfms=<strong class="markup--strong markup--pre-strong">tfms</strong>)<br>learn = ConvLearner.pretrained(arch, data, precompute=True)</pre><pre name="e7eb" id="e7eb" class="graf graf--pre graf-after--pre">learn.fit(1e-2, 1)</pre><ul class="postList"><li name="9d7a" id="9d7a" class="graf graf--li graf-after--pre">Now we created a new <code class="markup--code markup--li-code">data</code> object that includes augmentation. Initially, the augmentations actually do nothing because of <code class="markup--code markup--li-code">precompute=True</code>.</li><li name="e2e0" id="e2e0" class="graf graf--li graf-after--li">Convolutional neural network have these things called “activations.” An activation is a number that says “this feature is in this place with this level of confidence (probability)”. We are using a pre-trained network which has already learned to recognize features (i.e. we do not want to change hyper parameters it learned), so what we can do is to pre-compute activations for hidden layers and just train the final linear portion.</li></ul><figure name="f261" id="f261" class="graf graf--figure graf-after--li"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_JxE9HYahNpcbW9mImEJRPA.png"></figure><ul class="postList"><li name="237b" id="237b" class="graf graf--li graf-after--figure">This is why when you train your model for the first time, it takes longer — it is pre-computing these activations.</li><li name="3f70" id="3f70" class="graf graf--li graf-after--li">Even though we are trying to show a different version of the cat each time, we had already pre-computed the activations for a particular version of the cat (i.e. we are not re-calculating the activations with the altered version).</li><li name="5a4c" id="5a4c" class="graf graf--li graf-after--li">To use data augmentation, we have to do <code class="markup--code markup--li-code">learn.precompute=False</code>:</li></ul><pre name="5b79" id="5b79" class="graf graf--pre graf-after--li">learn.precompute=False</pre><pre name="d7e8" id="d7e8" class="graf graf--pre graf-after--pre">learn.fit(1e-2, 3, <strong class="markup--strong markup--pre-strong">cycle_len=1</strong>)</pre><pre name="0d2c" id="0d2c" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[ 0.       0.03597  0.01879  0.99365]                         <br>[ 1.       0.02605  0.01836  0.99365]                         <br>[ 2.       0.02189  0.0196   0.99316]</em></pre><ul class="postList"><li name="5bbc" id="5bbc" class="graf graf--li graf-after--pre">Bad news is that accuracy is not improving. Training loss is decreasing but validation loss is not, but we are not overfitting. Overfitting when the training loss is much lower than the validation loss. In other words, when your model is doing much better job on the training set than it is on the validation set, that means your model is not generalizing.</li><li name="303b" id="303b" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">cycle_len=1</code> [<a href="https://youtu.be/JNxcznsrRb8?t=30m17s" data-href="https://youtu.be/JNxcznsrRb8?t=30m17s" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">30:17</a>]: This enables <strong class="markup--strong markup--li-strong">stochastic gradient descent with restarts (SGDR)</strong>. The basic idea is as you get closer and closer to the spot with the minimal loss, you may want to start decrease the learning rate (taking smaller steps) in order to get to exactly the right spot.</li><li name="4a86" id="4a86" class="graf graf--li graf-after--li">The idea of decreasing the learning rate as you train is called <strong class="markup--strong markup--li-strong">learning rate annealing</strong> which is very common. Most common and “hacky” way to do this is to train a model with a certain learning rate for a while, and when it stops improving, manually drop down the learning rate (stepwise annealing).</li><li name="d9e1" id="d9e1" class="graf graf--li graf-after--li">A better approach is simply to pick some kind of functional form — turns out the really good functional form is one half of the cosign curve which maintains the high learning rate for a while at the beginning, then drop quickly when you get closer.</li></ul><figure name="188d" id="188d" class="graf graf--figure graf--layoutOutsetCenter graf-after--li" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_xmIlOee7PWLc6fa7xdfRkA.png"></figure><ul class="postList"><li name="7bb8" id="7bb8" class="graf graf--li graf-after--figure">However, we may find ourselves in a part of the weight space that isn’t very resilient — that is, small changes to the weights may result in big changes to the loss. We want to encourage our model to find parts of the weight space that are both accurate and stable. Therefore, from time to time we increase the learning rate (this is the ‘restarts’ in ‘SGDR’), which will force the model to jump to a different part of the weight space if the current area is “spiky”. Here’s a picture of how that might look if we reset the learning rates 3 times (in <a href="https://arxiv.org/abs/1704.00109" data-href="https://arxiv.org/abs/1704.00109" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">this paper</a> they call it a “cyclic LR schedule”):</li></ul><figure name="7b24" id="7b24" class="graf graf--figure graf--layoutOutsetCenter graf-after--li" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_TgAz1qaKu_SzuRmsO-6WGQ.png"></figure><ul class="postList"><li name="fd2c" id="fd2c" class="graf graf--li graf-after--figure">The number of epochs between resetting the learning rate is set by <code class="markup--code markup--li-code">cycle_len</code>, and the number of times this happens is referred to as the <em class="markup--em markup--li-em">number of cycles</em>, and is what we're actually passing as the 2nd parameter to <code class="markup--code markup--li-code">fit()</code>. So here's what our actual learning rates looked like:</li></ul><figure name="4269" id="4269" class="graf graf--figure graf-after--li"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_OKmsY6RR0DirLaLU2cIXtQ.png"></figure><ul class="postList"><li name="55e8" id="55e8" class="graf graf--li graf-after--figure">Question: Could we get the same effect by using random starting point? [<a href="https://youtu.be/JNxcznsrRb8?t=35m40s" data-href="https://youtu.be/JNxcznsrRb8?t=35m40s" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">35:40</a>] Before SGDR was created, people used to create “ensembles” where they would relearn a whole new model ten times in the hope that one of them would end up being better. In SGDR, once we get close enough to the optimal and stable area, resetting will not actually “reset” but the weights keeps better. So SGDR will give you better results than just randomly try a few different starting points.</li><li name="7666" id="7666" class="graf graf--li graf-after--li">It is important to pick a learning rate (which is the highest learning rate SGDR uses) that is big enough to allow the reset to jump to a different part of the function. [<a href="https://youtu.be/JNxcznsrRb8?t=37m25s" data-href="https://youtu.be/JNxcznsrRb8?t=37m25s" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">37:25</a>]</li><li name="caa5" id="caa5" class="graf graf--li graf-after--li">SGDR reduces the learning rate every mini-batch, and reset occurs every <code class="markup--code markup--li-code">cycle_len</code> epoch (in this case it is set to 1).</li><li name="69ef" id="69ef" class="graf graf--li graf-after--li">Question: Our main goal is to generalize and not end up in the narrow optima. In this method, are we keeping track of the minima and averaging them and ensembling them? [<a href="https://youtu.be/JNxcznsrRb8?t=39m27s" data-href="https://youtu.be/JNxcznsrRb8?t=39m27s" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">39:27</a>] That is another level of sophistication and you see “Snapshot Ensemble” in the diagram. We are not currently doing that but if you wanted it to generalize even better, you can save the weights right before the resets and take the average. But for now, we are just going to pick the last one.</li><li name="6e56" id="6e56" class="graf graf--li graf-after--li">If you want to skip ahead, there is a parameter called <code class="markup--code markup--li-code">cycle_save_name</code> which you can add as well as <code class="markup--code markup--li-code">cycle_len</code>, which will save a set of weights at the end of every learning rate cycle and then you can ensemble them [<a href="https://youtu.be/JNxcznsrRb8?t=40m14s" data-href="https://youtu.be/JNxcznsrRb8?t=40m14s" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">40:14</a>].</li></ul><h4 name="1848" id="1848" class="graf graf--h4 graf-after--li">Saving model&nbsp;[<a href="https://youtu.be/JNxcznsrRb8?t=40m31s" data-href="https://youtu.be/JNxcznsrRb8?t=40m31s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">40:31</a>]</h4><pre name="18bf" id="18bf" class="graf graf--pre graf-after--h4">learn.save('224_lastlayer')</pre><pre name="603c" id="603c" class="graf graf--pre graf-after--pre">learn.load('224_lastlayer')</pre><ul class="postList"><li name="257c" id="257c" class="graf graf--li graf-after--pre">When you precompute activations or create resized images (we will learn about it soon), various temporary files get created which you see under <code class="markup--code markup--li-code">data/dogcats/tmp</code> folder. If you are getting weird errors, it might be because of precomputed activations that are only half completed or are in some way incompatible with what you are doing. So you can always go ahead and delete this <code class="markup--code markup--li-code">/tmp</code> folder to see if it makes the error go away (fast.ai equivalent of turning it off and then on again).</li><li name="f3e5" id="f3e5" class="graf graf--li graf-after--li">You will also see there is a directory called <code class="markup--code markup--li-code">/models</code> that is where models get saved when you say <code class="markup--code markup--li-code">learn.save</code></li></ul><figure name="4ee4" id="4ee4" class="graf graf--figure graf--layoutOutsetCenter graf-after--li" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_tzmWttjMDhvuAj1xZ7PeOw.png"></figure><h4 name="2134" id="2134" class="graf graf--h4 graf-after--figure">Fine Tuning and Differential Learning Rate&nbsp;[<a href="https://youtu.be/JNxcznsrRb8?t=43m49s" data-href="https://youtu.be/JNxcznsrRb8?t=43m49s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">43:49</a>]</h4><ul class="postList"><li name="d970" id="d970" class="graf graf--li graf-after--h4">So far, we have not retrained any of pre-trained features — specifically, any of those weights in the convolutional kernels. All we have done is we added some new layers on top and learned how to mix and match pre-trained features.</li><li name="881b" id="881b" class="graf graf--li graf-after--li">Images like satellite images, CT scans, etc have totally different kinds of features all together (compare to ImageNet images), so you want to re-train many layers.</li><li name="73b1" id="73b1" class="graf graf--li graf-after--li">For dogs and cats, images are similar to what the model was pre-trained with, but we still may find it is helpful to slightly tune some of the later layers.</li><li name="4f42" id="4f42" class="graf graf--li graf-after--li">Here is how you tell the learner that we want to start actually changing the convolutional filters themselves:</li></ul><pre name="76b2" id="76b2" class="graf graf--pre graf-after--li">learn.unfreeze()</pre><ul class="postList"><li name="3f8f" id="3f8f" class="graf graf--li graf--startsWithDoubleQuote graf-after--pre">“frozen” layer is a layer which is not being trained/updated. <code class="markup--code markup--li-code">unfreeze</code> unfreezes all the layers.</li><li name="206a" id="206a" class="graf graf--li graf-after--li">Earlier layers like the first layer (which detects diagonal edges or gradient) or the second layer (which recognizes corners or curves) probably do not need to change by much, if at all.</li><li name="3575" id="3575" class="graf graf--li graf-after--li">Later layers are much more likely to need more learning. So we create an array of learning rates (differential learning rate):</li></ul><pre name="8800" id="8800" class="graf graf--pre graf-after--li">lr=np.array([1e-4,1e-3,1e-2])</pre><ul class="postList"><li name="0b68" id="0b68" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">1e-4</code>&nbsp;: for the first few layers (basic geometric features)</li><li name="2969" id="2969" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">1e-3</code>&nbsp;: for the middle layers (sophisticated convolutional features)</li><li name="3c4b" id="3c4b" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">1e-2</code>&nbsp;: for layers we added on top</li><li name="956a" id="956a" class="graf graf--li graf-after--li">Why 3? Actually they are 3 ResNet blocks but for now, think of it as a group of layers.</li></ul><p name="0942" id="0942" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">Question</strong>: What if I have a bigger images than the model is trained with? [<a href="https://youtu.be/JNxcznsrRb8?t=50m30s" data-href="https://youtu.be/JNxcznsrRb8?t=50m30s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">50:30</a>] The short answer is, with this library and modern architectures we are using, we can use any size we like.</p><p name="59ca" id="59ca" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: Can we unfreeze just specific layers? [<a href="https://youtu.be/JNxcznsrRb8?t=51m3s" data-href="https://youtu.be/JNxcznsrRb8?t=51m3s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">51:03</a>] We are not doing it yet, but if you wanted, you can do <code class="markup--code markup--p-code">lean.unfreeze_to(n)</code> (which will unfreeze layers from layer <code class="markup--code markup--p-code">n</code> onwards). Jeremy almost never finds it helpful and he thinks it is because we are using differential learning rates, and the optimizer can learn just as much as it needs to. The one place he found it helpful is if he is using a really big memory intensive model and he is running out of GPU, the less layers you unfreeze, the less memory and time it takes.</p><p name="d441" id="d441" class="graf graf--p graf-after--p">Using differential learning rate, we are up to 99.5%! [<a href="https://youtu.be/JNxcznsrRb8?t=52m28s" data-href="https://youtu.be/JNxcznsrRb8?t=52m28s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">52:28</a>]</p><pre name="0a81" id="0a81" class="graf graf--pre graf-after--p">learn.fit(lr, 3, cycle_len=1, <strong class="markup--strong markup--pre-strong">cycle_mult</strong>=2)</pre><pre name="d5c3" id="d5c3" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[ 0.       0.04538  0.01965  0.99268]                          <br>[ 1.       0.03385  0.01807  0.99268]                          <br>[ 2.       0.03194  0.01714  0.99316]                          <br>[ 3.       0.0358   0.0166   0.99463]                          <br>[ 4.       0.02157  0.01504  0.99463]                          <br>[ 5.       0.0196   0.0151   0.99512]                          <br>[ 6.       0.01356  0.01518  0.9956 ]</em></pre><ul class="postList"><li name="e57f" id="e57f" class="graf graf--li graf-after--pre">Earlier we said <code class="markup--code markup--li-code">3</code> is the number of epochs, but it is actually <strong class="markup--strong markup--li-strong">cycles</strong>. So if <code class="markup--code markup--li-code">cycle_len=2</code>&nbsp;, it will do 3 cycles where each cycle is 2 epochs (i.e. 6 epochs). Then why did it 7? It is because of <code class="markup--code markup--li-code">cycle_mult</code></li><li name="8751" id="8751" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">cycle_mult=2</code>&nbsp;: this multiplies the length of the cycle after each cycle (1 epoch + 2 epochs + 4 epochs = 7 epochs).</li></ul><figure name="8b25" id="8b25" class="graf graf--figure graf-after--li"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_SA5MA3z-jOBwvzF2e6-E6Q.png"></figure><p name="e754" id="e754" class="graf graf--p graf-after--figure">Intuitively speaking [<a href="https://youtu.be/JNxcznsrRb8?t=53m57s" data-href="https://youtu.be/JNxcznsrRb8?t=53m57s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">53:57</a>], if the cycle length is too short, it starts going down to find a good spot, then pops out, and goes down trying to find a good spot and pops out, and never actually get to find a good spot. Earlier on, you want it to do that because it is trying to find a spot that is smoother, but later on, you want it to do more exploring. That is why <code class="markup--code markup--p-code">cycle_mult=2</code> seems to be a good approach.</p><p name="e4e2" id="e4e2" class="graf graf--p graf-after--p">We are introducing more and more hyper parameters having told you that there are not many. You can get away with just choosing a good learning rate, but then adding these extra tweaks helps get that extra level-up without any effort. In general, good starting points are:</p><ul class="postList"><li name="ecc1" id="ecc1" class="graf graf--li graf-after--p"><code class="markup--code markup--li-code">n_cycle=3, cycle_len=1, cycle_mult=2</code></li><li name="b8bd" id="b8bd" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">n_cycle=3, cycle_len=2</code> (no <code class="markup--code markup--li-code">cycle_mult</code>)</li></ul><p name="1b45" id="1b45" class="graf graf--p graf-after--li">Question: why do smoother surfaces correlate to more generalized networks? [<a href="https://youtu.be/JNxcznsrRb8?t=55m28s" data-href="https://youtu.be/JNxcznsrRb8?t=55m28s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">55:28</a>]</p><figure name="d934" id="d934" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_fNvevN5qLDf9dgq4632J7A.png"></figure><p name="cea0" id="cea0" class="graf graf--p graf-after--figure">Say you have something spiky (blue line). X-axis is showing how good this is at recognizing dogs vs. cats as you change this particular parameter. Something to be generalizable means that we want it to work when we give it a slightly different dataset. Slightly different dataset may have a slightly different relationship between this parameter and how cat-like vs. dog-like it is. It may, instead look like the red line. In other words, if we end up at the blue pointy part, then it will not going to do a good job on this slightly different dataset. Or else, if we end up on the wider blue part, it will still do a good job on the red dataset.</p><ul class="postList"><li name="098b" id="098b" class="graf graf--li graf-after--p"><a href="http://forums.fast.ai/t/why-do-we-care-about-resilency-of-where-we-are-in-the-weight-space/7323" data-href="http://forums.fast.ai/t/why-do-we-care-about-resilency-of-where-we-are-in-the-weight-space/7323" class="markup--anchor markup--li-anchor" rel="noopener nofollow" target="_blank">Here</a> is some interesting discussion about spiky minima.</li></ul><h4 name="af4c" id="af4c" class="graf graf--h4 graf-after--li">Test Time Augmentation (TTA)&nbsp;[<a href="https://youtu.be/JNxcznsrRb8?t=56m49s" data-href="https://youtu.be/JNxcznsrRb8?t=56m49s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">56:49</a>]</h4><p name="fd99" id="fd99" class="graf graf--p graf-after--h4">Our model has achieved 99.5%. But can we make it better still? Let’s take a look at pictures we predicted incorrectly:</p><figure name="217c" id="217c" class="graf graf--figure graf--layoutOutsetCenter graf-after--p" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_5jSFmwaQRmn4HaMZm1qyZw.png"></figure><p name="ffaf" id="ffaf" class="graf graf--p graf-after--figure">Here, Jeremy printed out the whole of these pictures. When we do the validation set, all of our inputs to our model must be square. The reason is kind of a minor technical detail, but GPU does not go very quickly if you have different dimensions for different images. It needs to be consistent so that every part of the GPU can do the same thing. This may probably be fixable but for now that is the state of the technology we have.</p><p name="7ea1" id="7ea1" class="graf graf--p graf-after--p">To make it square, we just pick out the square in the middle — as you can see below, it is understandable why this picture was classified incorrectly:</p><figure name="f991" id="f991" class="graf graf--figure graf--layoutOutsetCenter graf-after--p" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_u8pjW6L-FhCn0DO-utX7cA.png"></figure><p name="e62b" id="e62b" class="graf graf--p graf-after--figure">We are going to do what is called “<strong class="markup--strong markup--p-strong">Test Time Augmentation</strong>”. What this means is that we are going to take 4 data augmentations at random as well as the un-augmented original (center-cropped). We will then calculate predictions for all these images, take the average, and make that our final prediction. Note that this is only for validation set and/or test set.</p><p name="3d23" id="3d23" class="graf graf--p graf-after--p">To do this, all you have to do is <code class="markup--code markup--p-code">learn.TTA()</code> — which brings up the accuracy to 99.65%!</p><pre name="7368" id="7368" class="graf graf--pre graf-after--p">log_preds,y = <strong class="markup--strong markup--pre-strong">learn.TTA()</strong><br>probs = np.mean(np.exp(log_preds),0)</pre><pre name="7136" id="7136" class="graf graf--pre graf-after--pre">accuracy(probs, y)</pre><pre name="0482" id="0482" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">0.99650000000000005</em></pre><p name="387b" id="387b" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">Questions on augmentation approach[</strong><a href="https://youtu.be/JNxcznsrRb8?t=1h1m36s" data-href="https://youtu.be/JNxcznsrRb8?t=1h1m36s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong">01:01:36</strong></a><strong class="markup--strong markup--p-strong">]:</strong> Why not border or padding to make it square? Typically Jeremy does not do much padding, but instead he does a little bit of <strong class="markup--strong markup--p-strong">zooming</strong>. There is a thing called <strong class="markup--strong markup--p-strong">reflection padding</strong> that works well with satellite imagery. Generally speaking, using TTA plus data augmentation, the best thing to do is try to use as large image as possible. Also, having fixed crop locations plus random contrast, brightness, rotation changes might be better for TTA.</p><p name="41de" id="41de" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question:</strong> Data augmentation for non-image dataset? [<a href="https://youtu.be/JNxcznsrRb8?t=1h3m35s" data-href="https://youtu.be/JNxcznsrRb8?t=1h3m35s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">01:03:35</a>] No one seems to know. It seems like it would be helpful, but there are very few number of examples. In natural language processing, people tried replacing synonyms for instance, but on the whole the area is under researched and under developed.</p><p name="ae09" id="ae09" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: Is fast.ai library open source?[<a href="https://youtu.be/JNxcznsrRb8?t=1h5m34s" data-href="https://youtu.be/JNxcznsrRb8?t=1h5m34s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">01:05:34</a>] Yes. He then covered the reason <a href="http://www.fast.ai/2017/09/08/introducing-pytorch-for-fastai/" data-href="http://www.fast.ai/2017/09/08/introducing-pytorch-for-fastai/" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">why Fast.ai switched from Keras + TensorFlow to PyTorch</a></p><p name="bf94" id="bf94" class="graf graf--p graf-after--p">Random note: PyTorch is much more than just a deep learning library. It actually lets us write arbitrary GPU accelerated algorithms from scratch — Pyro is a great example of what people are now doing with PyTorch outside of deep learning.</p><h4 name="1b93" id="1b93" class="graf graf--h4 graf-after--p">Analyzing results [<a href="https://youtu.be/JNxcznsrRb8?t=1h11m50s" data-href="https://youtu.be/JNxcznsrRb8?t=1h11m50s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">01:11:50</a>]</h4><h4 name="e637" id="e637" class="graf graf--h4 graf-after--h4"><strong class="markup--strong markup--h4-strong">Confusion matrix</strong></h4><p name="c92a" id="c92a" class="graf graf--p graf-after--h4">The simple way to look at the result of a classification is called confusion matrix — which is used not only for deep learning but in any kind of machine learning classifier. It is helpful particularly if there are four or five classes you are trying to predict to see which group you are having the most trouble with.</p><figure name="c420" id="c420" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_IeGRqM88ZW0-7V0Za_FaQw.png"></figure><pre name="6e57" id="6e57" class="graf graf--pre graf-after--figure">preds = np.argmax(probs, axis=1)<br>probs = probs[:,1]</pre><pre name="e078" id="e078" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">sklearn.metrics</strong> <strong class="markup--strong markup--pre-strong">import</strong> confusion_matrix<br>cm = confusion_matrix(y, preds)</pre><pre name="b6da" id="b6da" class="graf graf--pre graf-after--pre">plot_confusion_matrix(cm, data.classes)</pre><h4 name="598b" id="598b" class="graf graf--h4 graf-after--pre">Let’s look at the pictures again [<a href="https://youtu.be/JNxcznsrRb8?t=1h13m" data-href="https://youtu.be/JNxcznsrRb8?t=1h13m" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">01:13:00</a>]</h4><p name="dd38" id="dd38" class="graf graf--p graf-after--h4">Most incorrect cats (only the left two were incorrect — it displays 4 by default):</p><figure name="cc6c" id="cc6c" class="graf graf--figure graf--layoutOutsetCenter graf-after--p" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_IeVm7iR9u3Iy-NPIC73pLQ.png"></figure><p name="5fa9" id="5fa9" class="graf graf--p graf-after--figure">Most incorrect dots:</p><figure name="c03e" id="c03e" class="graf graf--figure graf--layoutOutsetCenter graf-after--p" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_UtNl3fx4vWnEdCL6Zed4Jw.png"></figure><h3 name="ddf7" id="ddf7" class="graf graf--h3 graf-after--figure">Review: easy steps to train a world-class image classifier [<a href="https://youtu.be/JNxcznsrRb8?t=1h14m9s" data-href="https://youtu.be/JNxcznsrRb8?t=1h14m9s" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">01:14:09</a>]</h3><ol class="postList"><li name="6987" id="6987" class="graf graf--li graf-after--h3">Enable data augmentation, and <code class="markup--code markup--li-code">precompute=True</code></li><li name="9179" id="9179" class="graf graf--li graf-after--li">Use <code class="markup--code markup--li-code">lr_find()</code> to find highest learning rate where loss is still clearly improving</li><li name="b8c6" id="b8c6" class="graf graf--li graf-after--li">Train last layer from precomputed activations for 1–2 epochs</li><li name="048d" id="048d" class="graf graf--li graf-after--li">Train last layer with data augmentation (i.e. <code class="markup--code markup--li-code">precompute=False</code>) for 2–3 epochs with <code class="markup--code markup--li-code">cycle_len=1</code></li><li name="40ac" id="40ac" class="graf graf--li graf-after--li">Unfreeze all layers</li><li name="6eb3" id="6eb3" class="graf graf--li graf-after--li">Set earlier layers to 3x-10x lower learning rate than next higher layer. Rule of thumb: 10x for ImageNet like images, 3x for satellite or medical imaging</li><li name="d4d1" id="d4d1" class="graf graf--li graf-after--li">Use <code class="markup--code markup--li-code">lr_find()</code> again (Note: if you call <code class="markup--code markup--li-code">lr_find</code> having set differential learning rates, what it prints out is the learning rate of the last layers.)</li><li name="9a45" id="9a45" class="graf graf--li graf-after--li">Train full network with <code class="markup--code markup--li-code">cycle_mult=2</code> until over-fitting</li></ol><h4 name="600f" id="600f" class="graf graf--h4 graf-after--li">Let’s do it again: <a href="https://www.kaggle.com/c/dog-breed-identification" data-href="https://www.kaggle.com/c/dog-breed-identification" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">Dog Breed Challenge</a> [<a href="https://youtu.be/JNxcznsrRb8?t=1h16m37s" data-href="https://youtu.be/JNxcznsrRb8?t=1h16m37s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">01:16:37</a>]</h4><ul class="postList"><li name="a23f" id="a23f" class="graf graf--li graf-after--h4">You can use <a href="https://github.com/floydwch/kaggle-cli" data-href="https://github.com/floydwch/kaggle-cli" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">Kaggle CLI</a> to download data for Kaggle competitions</li><li name="73cd" id="73cd" class="graf graf--li graf-after--li">Notebook is not made public since it is an active competition</li></ul><pre name="d75d" id="d75d" class="graf graf--pre graf-after--li">%reload_ext autoreload<br>%autoreload 2<br>%matplotlib inline</pre><pre name="8de5" id="8de5" class="graf graf--pre graf-after--pre">from fastai.imports import *<br>from fastai.transforms import *<br>from fastai.conv_learner import *<br>from fastai.model import *<br>from fastai.dataset import *<br>from fastai.sgdr import *<br>from fastai.plots import *</pre><pre name="12bb" id="12bb" class="graf graf--pre graf-after--pre">PATH = 'data/dogbreed/'<br>sz = 224<br>arch = resnext101_64<br>bs=16</pre><pre name="9ff0" id="9ff0" class="graf graf--pre graf-after--pre">label_csv = f'{PATH}labels.csv'<br>n = len(list(open(label_csv)))-1<br>val_idxs = get_cv_idxs(n)</pre><pre name="b1e0" id="b1e0" class="graf graf--pre graf-after--pre">!ls {PATH}</pre><figure name="e487" id="e487" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*RBwxSLtYOv61Ry13ayUv4w.png" data-width="1094" data-height="88" data-action="zoom" data-action-value="1*RBwxSLtYOv61Ry13ayUv4w.png" src="../img/1_RBwxSLtYOv61Ry13ayUv4w.png"></figure><p name="e259" id="e259" class="graf graf--p graf-after--figure">This is a little bit different to our previous dataset. Instead of <code class="markup--code markup--p-code">train</code> folder which has a separate folder for each breed of dog, it has a CSV file with the correct labels. We will read CSV file with Pandas. Pandas is what we use in Python to do structured data analysis like CSV and usually imported as <code class="markup--code markup--p-code">pd</code>:</p><pre name="9713" id="9713" class="graf graf--pre graf-after--p">label_df = pd.read_csv(label_csv)<br>label_df.head()</pre><figure name="0cc3" id="0cc3" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_fTMGpsB_jK7Pp1cjErZ8vw.png"></figure><pre name="bab0" id="bab0" class="graf graf--pre graf-after--figure">label_df.<strong class="markup--strong markup--pre-strong">pivot_table</strong>(index='breed', aggfunc=len).sort_values('id', ascending=False)</pre><figure name="dac4" id="dac4" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_0-PpgltveKXnyjaMHPhCPA.png"><figcaption class="imageCaption">How many dog images per&nbsp;breed</figcaption></figure><pre name="547a" id="547a" class="graf graf--pre graf-after--figure">tfms = tfms_from_model(arch, sz, aug_tfms=transforms_side_on, <br>                       max_zoom=1.1)</pre><pre name="1580" id="1580" class="graf graf--pre graf-after--pre">data = ImageClassifierData.from_csv(PATH, 'train', <br>                 f'{PATH}labels.csv', test_name='test', <br>                 val_idxs=val_idxs, suffix='.jpg', tfms=tfms, bs=bs)</pre><ul class="postList"><li name="fed1" id="fed1" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">max_zoom</code> — we will zoom in up to 1.1 times</li><li name="65dd" id="65dd" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">ImageClassifierData.from_csv</code> — last time, we used <code class="markup--code markup--li-code">from_paths</code> but since the labels are in CSV file, we will call <code class="markup--code markup--li-code">from_csv</code> instead.</li><li name="7a43" id="7a43" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">test_name</code> — we need to specify where the test set is if you want to submit to Kaggle competitions</li><li name="ee6d" id="ee6d" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">val_idx</code> — there is no <code class="markup--code markup--li-code">validation</code> folder but we still want to track how good our performance is locally. So above you will see:</li></ul><p name="3c3d" id="3c3d" class="graf graf--p graf-after--li"><code class="markup--code markup--p-code">n = len(list(open(label_csv)))-1</code>&nbsp;: Open CSV file, create a list of rows, then take the length. <code class="markup--code markup--p-code">-1</code> because the first row is a header. Hence <code class="markup--code markup--p-code">n</code> is the number of images we have.</p><p name="fd66" id="fd66" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code u-paddingRight0 u-marginRight0">val_idxs = <strong class="markup--strong markup--p-strong">get_cv_idxs</strong>(n)</code>&nbsp;: “get cross validation indexes” — this will return, by default, random 20% of the rows (indexes to be precise) to use as a validation set. You can also send <code class="markup--code markup--p-code">val_pct</code> to get different amount.</p><figure name="b6d5" id="b6d5" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_ug-ihQFW21b4P68dJlADpg.png"></figure><ul class="postList"><li name="ac70" id="ac70" class="graf graf--li graf-after--figure"><code class="markup--code markup--li-code">suffix=’.jpg’</code> — File names has&nbsp;<code class="markup--code markup--li-code">.jpg</code> at the end, but CSV file does not. So we will set <code class="markup--code markup--li-code">suffix</code> so it knows the full file names.</li></ul><pre name="9f49" id="9f49" class="graf graf--pre graf-after--li">fn = PATH + data.trn_ds.fnames[0]; fn</pre><pre name="3b7b" id="3b7b" class="graf graf--pre graf-after--pre"><code class="markup--code markup--pre-code"><em class="markup--em markup--pre-em">'data/dogbreed/train/001513dfcb2ffafc82cccf4d8bbaba97.jpg'</em></code></pre><ul class="postList"><li name="24c2" id="24c2" class="graf graf--li graf-after--pre">You can access to training dataset by saying <code class="markup--code markup--li-code">data.trn_ds</code> and <code class="markup--code markup--li-code">trn_ds</code> contains a lot of things including file names (<code class="markup--code markup--li-code">fnames</code>)</li></ul><pre name="52ca" id="52ca" class="graf graf--pre graf-after--li">img = PIL.Image.open(fn); img</pre><figure name="c5e5" id="c5e5" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_1eb6vEpa8SOrxaoNNs7f0g.png"></figure><pre name="daff" id="daff" class="graf graf--pre graf-after--figure">img.size</pre><pre name="8a62" id="8a62" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(500, 375)</em></pre><ul class="postList"><li name="8d02" id="8d02" class="graf graf--li graf-after--pre">Now we check image size. If they are huge, then you have to think really carefully about how to deal with them. If they are tiny, it is also challenging. Most of ImageNet models are trained on either 224 by 224 or 299 by 299 images</li></ul><pre name="ab28" id="ab28" class="graf graf--pre graf-after--li">size_d = {k: PIL.Image.open(PATH+k).size for k in data.trn_ds.fnames}</pre><ul class="postList"><li name="c339" id="c339" class="graf graf--li graf-after--pre">Dictionary comprehension — <code class="markup--code markup--li-code">key: name of the file</code>, <code class="markup--code markup--li-code">value: size of the file</code></li></ul><pre name="d9d6" id="d9d6" class="graf graf--pre graf-after--li">row_sz, col_sz = list(zip(*size_d.values()))</pre><ul class="postList"><li name="ab1c" id="ab1c" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">*size_d.values()</code> will unpack a list. <code class="markup--code markup--li-code">zip</code> will pair up elements of tuples to create a list of tuples.</li></ul><pre name="0550" id="0550" class="graf graf--pre graf-after--li">plt.hist(row_sz);</pre><figure name="046b" id="046b" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_KPYOb0uGgAmaqLr6JWZmSg.png"><figcaption class="imageCaption">Histogram of&nbsp;rows</figcaption></figure><ul class="postList"><li name="63be" id="63be" class="graf graf--li graf-after--figure">Matplotlib is something you want to be very familiar with if you do any kind of data science or machine learning in Python. Matplotlib is always referred to as <code class="markup--code markup--li-code">plt</code>&nbsp;.</li></ul><p name="be91" id="be91" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">Question</strong>: How many images should we use as a validation set? [<a href="https://youtu.be/JNxcznsrRb8?t=1h26m28s" data-href="https://youtu.be/JNxcznsrRb8?t=1h26m28s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">01:26:28</a>] Using 20% is fine unless the dataset is small — then 20% is not enough. If you train the same model multiple times and you are getting very different validation set results, then your validation set is too small. If the validation set is smaller than a thousand, it is hard to interpret how well you are doing. If you care about the third decimal place of accuracy and you only have a thousand things in your validation set, a single image changes the accuracy. If you care about the difference between 0.01 and 0.02, you want that to represent 10 or 20 rows. Normally 20% seems to work fine.</p><pre name="1133" id="1133" class="graf graf--pre graf-after--p">def get_data(sz, bs):<br>    tfms = tfms_from_model(arch, sz, aug_tfms=transforms_side_on,<br>                           max_zoom=1.1)<br>    data = ImageClassifierData.from_csv(PATH, 'train', <br>               f'{PATH}labels.csv', test_name='test', num_workers=4,<br>               val_idxs=val_idxs, suffix='.jpg', tfms=tfms, bs=bs)</pre><pre name="506c" id="506c" class="graf graf--pre graf-after--pre">    return data if sz&gt;300 else data.resize(340, 'tmp')<br><br></pre><ul class="postList"><li name="6bd3" id="6bd3" class="graf graf--li graf-after--pre">Here is the regular two lines of code. When we start working with new dataset, we want everything to go super fast. So we made it possible to specify the size and start with something like 64 which will run fast. Later, we will use bigger images and bigger architectures at which point, you may run out of GPU memory. If you see CUDA out of memory error, the first thing you need to do is to restart kernel (you cannot recover from it), then make the batch size smaller.</li></ul><pre name="4234" id="4234" class="graf graf--pre graf-after--li">data = get_data(224, bs)</pre><pre name="6aba" id="6aba" class="graf graf--pre graf-after--pre">learn = ConvLearner.pretrained(arch, data, precompute=True)</pre><pre name="3588" id="3588" class="graf graf--pre graf-after--pre">learn.fit(1e-2, 5)</pre><pre name="9aeb" id="9aeb" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[0.      1.99245 1.0733  0.76178]                             <br>[1.      1.09107 0.7014  0.8181 ]                             <br>[2.      0.80813 0.60066 0.82148]                             <br>[3.      0.66967 0.55302 0.83125]                             <br>[4.      0.57405 0.52974 0.83564]</em></pre><ul class="postList"><li name="c71c" id="c71c" class="graf graf--li graf-after--pre">83% for 120 classes is pretty good.</li></ul><pre name="6545" id="6545" class="graf graf--pre graf-after--li"><strong class="markup--strong markup--pre-strong">learn.precompute = False</strong></pre><pre name="9252" id="9252" class="graf graf--pre graf-after--pre">learn.fit(1e-2, 5, <strong class="markup--strong markup--pre-strong">cycle_len</strong>=1)</pre><ul class="postList"><li name="0e4a" id="0e4a" class="graf graf--li graf-after--pre">Reminder: a <code class="markup--code markup--li-code">epoch</code> is one pass through the data, a <code class="markup--code markup--li-code">cycle</code> is how many epochs you said is in a cycle</li></ul><pre name="314b" id="314b" class="graf graf--pre graf-after--li">learn.save('224_pre')<br>learn.load('224_pre')</pre><h4 name="2858" id="2858" class="graf graf--h4 graf-after--pre">Increase image size [1:32:55]</h4><pre name="8f00" id="8f00" class="graf graf--pre graf-after--h4">learn.set_data(get_data(299, bs))</pre><ul class="postList"><li name="4814" id="4814" class="graf graf--li graf-after--pre">If you trained a model on smaller size images, you can then call <code class="markup--code markup--li-code">learn.set_data</code> and pass in a larger size dataset. That is going to take your model, however it has been trained so far, and it is going to let you continue to train on larger images.</li></ul><blockquote name="6182" id="6182" class="graf graf--pullquote graf-after--li"><span class="markup--quote markup--pullquote-quote is-other" name="anon_bb85fa28d116" data-creator-ids="anon">Starting training on small images for a few epochs, then switching to bigger images, and continuing training is an amazingly effective way to avoid overfitting.</span></blockquote><pre name="4428" id="4428" class="graf graf--pre graf-after--pullquote">learn.fit(1e-2, 3, cycle_len=1)</pre><pre name="ab98" id="ab98" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[0.      0.35614 0.22239 0.93018]                            <br>[1.      0.28341 0.2274  0.92627]<br>[2.      </em><strong class="markup--strong markup--pre-strong"><em class="markup--em markup--pre-em">0.28341</em></strong><em class="markup--em markup--pre-em"> </em><strong class="markup--strong markup--pre-strong"><em class="markup--em markup--pre-em">0.2274</em></strong><em class="markup--em markup--pre-em">  0.92627]</em></pre><ul class="postList"><li name="623d" id="623d" class="graf graf--li graf-after--pre">As you see, validation set loss (0.2274) is much lower than training set loss (0.28341) — which means it is <strong class="markup--strong markup--li-strong">under fitting</strong>. When you are under fitting, it means <code class="markup--code markup--li-code">cycle_len=1</code> is too short (learning rate is getting reset before it had the chance to zoom in properly). So we will add <code class="markup--code markup--li-code">cycle_mult=2</code> (i.e. 1st cycle is 1 epoch, 2nd cycle is 2 epochs, and 3rd cycle is 4 epochs)</li></ul><pre name="d699" id="d699" class="graf graf--pre graf-after--li">learn.fit(1e-2, 3, cycle_len=1, <strong class="markup--strong markup--pre-strong">cycle_mult=2</strong>)</pre><pre name="5305" id="5305" class="graf graf--pre graf-after--pre">[0.      0.27171 0.2118  0.93192]                            <br>[1.      0.28743 0.21008 0.9324 ]<br>[2.      0.25328 0.20953 0.93288]                            <br>[3.      0.23716 0.20868 0.93001]<br>[4.      0.23306 0.20557 0.93384]                            <br>[5.      0.22175 0.205   0.9324 ]<br>[6.      0.2067  0.20275 0.9348 ]</pre><ul class="postList"><li name="15cc" id="15cc" class="graf graf--li graf-after--pre">Now the validation loss and training loss are about the same — this is about the right track. Then we try <code class="markup--code markup--li-code">TTA</code>&nbsp;:</li></ul><pre name="8110" id="8110" class="graf graf--pre graf-after--li">log_preds, y = learn.TTA()<br>probs = np.exp(log_preds)<br>accuracy(log_preds,y), metrics.log_loss(y, probs)</pre><pre name="7147" id="7147" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(0.9393346379647749, 0.20101565705592733)</em></pre><p name="2976" id="2976" class="graf graf--p graf-after--pre">Other things to try:</p><ul class="postList"><li name="03ef" id="03ef" class="graf graf--li graf-after--p">Try running one more cycle of 2 epochs</li><li name="90b9" id="90b9" class="graf graf--li graf-after--li">Unfreezing (in this case, training convolutional layers did not help in the slightest since the images actually came from ImageNet)</li><li name="ee86" id="ee86" class="graf graf--li graf-after--li">Remove validation set and just re-run the same steps, and submit that — which lets us use 100% of the data.</li></ul><p name="e162" id="e162" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">Question</strong>: How do we deal with unbalanced dataset? [<a href="https://youtu.be/JNxcznsrRb8?t=1h38m46s" data-href="https://youtu.be/JNxcznsrRb8?t=1h38m46s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">01:38:46</a>] This dataset is not totally balanced (between 60 and 100) but it is not unbalanced enough that Jeremy would give it a second thought. A recent paper says the best way to deal with very unbalanced dataset is to make copies of the rare cases.</p><p name="bdd0" id="bdd0" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: Difference between <code class="markup--code markup--p-code">precompute=True</code> and <code class="markup--code markup--p-code">unfreeze</code>?</p><ul class="postList"><li name="1ab9" id="1ab9" class="graf graf--li graf-after--p">We started with a pre-trained network</li><li name="89e1" id="89e1" class="graf graf--li graf-after--li">We added a couple of layers on the end of it which start out random. With everything frozen and <code class="markup--code markup--li-code">precompute=True</code>, all we are learning is the layers we have added.</li><li name="f896" id="f896" class="graf graf--li graf-after--li">With <code class="markup--code markup--li-code">precompute=True</code>, data augmentation does not do anything because we are showing exactly the same activations each time.</li><li name="4069" id="4069" class="graf graf--li graf-after--li">We then set <code class="markup--code markup--li-code">precompute=False</code> which means we are still only training the layers we added because it is frozen but data augmentation is now working because it is actually going through and recalculating all of the activations from scratch.</li><li name="a9a8" id="a9a8" class="graf graf--li graf-after--li">Then finally, we unfreeze which is saying “okay, now you can go ahead and change all of these earlier convolutional filters”.</li></ul><p name="49ab" id="49ab" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">Question</strong>: Why not just set <code class="markup--code markup--p-code">precompute=False</code> from the beginning? The only reason to have <code class="markup--code markup--p-code">precompute=True</code> is it is much faster (10 or more times). If you are working with quite a large dataset, it can save quite a bit of time. There is no accuracy reason ever to use <code class="markup--code markup--p-code">precompute=True</code>&nbsp;.</p><p name="d391" id="d391" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Minimal steps to get good results:</strong></p><ol class="postList"><li name="e5ea" id="e5ea" class="graf graf--li graf-after--p">Use <code class="markup--code markup--li-code">lr_find()</code> to find highest learning rate where loss is still clearly improving</li><li name="283f" id="283f" class="graf graf--li graf-after--li">Train last layer with data augmentation (i.e.<code class="markup--code markup--li-code"> precompute=False</code>) for 2–3 epochs with <code class="markup--code markup--li-code">cycle_len=1</code></li><li name="e9cb" id="e9cb" class="graf graf--li graf-after--li">Unfreeze all layers</li><li name="f608" id="f608" class="graf graf--li graf-after--li">Set earlier layers to 3x-10x lower learning rate than next higher layer</li><li name="322c" id="322c" class="graf graf--li graf-after--li">Train full network with <code class="markup--code markup--li-code">cycle_mult=2</code> until over-fitting</li></ol><p name="0492" id="0492" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">Question</strong>: Does reducing the batch size only affect the speed of training? [<a href="https://youtu.be/JNxcznsrRb8?t=1h43m34s" data-href="https://youtu.be/JNxcznsrRb8?t=1h43m34s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:43:34</a>] Yes, pretty much. If you are showing it less images each time, then it is calculating the gradient with less images — hence less accurate. In other words, knowing which direction to go and how far to go in that direction is less accurate. So as you make the batch size smaller, you are making it more volatile. It impacts the optimal learning rate that you would need to use, but in practice, dividing the batch size by 2 vs. 4 does not seem to change things very much. If you change the batch size by much, you can re-run learning rate finder to check.</p><p name="e33f" id="e33f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question:</strong> What are the grey images vs. the ones on the right?</p><figure name="c939" id="c939" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_IHxFF49erSrWw02s8H6BiQ.png"><figcaption class="imageCaption"><a href="https://arxiv.org/abs/1311.2901" data-href="https://arxiv.org/abs/1311.2901" class="markup--anchor markup--figure-anchor" rel="noopener nofollow" target="_blank">Visualizing and Understanding Convolutional Networks</a></figcaption></figure><p name="e94e" id="e94e" class="graf graf--p graf-after--figure">Layer 1, they are exactly what the filters look like. It is easy to visualize because input to it are pixels. Later on, it gets harder because inputs are themselves activations which is a combination of activations. Zeiler and Fergus came up with a clever technique to show what the filters tend to look like on average — called <strong class="markup--strong markup--p-strong">deconvolution</strong> (we will learn in Part 2).<strong class="markup--strong markup--p-strong"> </strong>Ones on the right are the examples of patches of image which activated that filter highly.</p><p name="bdb3" id="bdb3" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: What would you have done if the dog was off to the corner or tiny (re: dog breed identification)? [<a href="https://youtu.be/JNxcznsrRb8?t=1h47m16s" data-href="https://youtu.be/JNxcznsrRb8?t=1h47m16s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">01:47:16</a>] We will learn about it in Part 2, but there is a technique that allows you to figure out roughly which parts of an image most likely have the interesting things in them. Then you can crop out that area.</p><h4 name="9c7b" id="9c7b" class="graf graf--h4 graf-after--p">Further improvement [<a href="https://youtu.be/JNxcznsrRb8?t=1h48m16s" data-href="https://youtu.be/JNxcznsrRb8?t=1h48m16s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">01:48:16</a>]</h4><p name="808b" id="808b" class="graf graf--p graf-after--h4">Two things we can do immediately to make it better:</p><ol class="postList"><li name="0660" id="0660" class="graf graf--li graf-after--p">Assuming the size of images you were using is smaller than the average size of images you have been given, you can increase the size. As we have seen before, you can increase it during training.</li><li name="637d" id="637d" class="graf graf--li graf-after--li">Use better architecture. There are different ways of putting together what size convolutional filters and how they are connected to each other, and different architectures have different number of layers, size of kernels, filters, etc.</li></ol><p name="00b1" id="00b1" class="graf graf--p graf-after--li">We have been using ResNet34 — a great starting point and often a good finishing point because it does not have too many parameters and works well with small dataset. There is another architecture called ResNext which was the second-place winner in last year’s ImageNet competition.ResNext50 takes twice as long and 2–4 times more memory than ResNet34.</p><p name="0221" id="0221" class="graf graf--p graf-after--p"><a href="https://github.com/fastai/fastai/blob/master/courses/dl1/lesson1-rxt50.ipynb" data-href="https://github.com/fastai/fastai/blob/master/courses/dl1/lesson1-rxt50.ipynb" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Here</a> is the notebook which is almost identical to the original dogs. vs. cats. which uses ResNext50 which achieved 99.75% accuracy.</p><h4 name="2d66" id="2d66" class="graf graf--h4 graf-after--p">Satellite Imagery [01:53:01]</h4><p name="219d" id="219d" class="graf graf--p graf-after--h4"><a href="https://github.com/fastai/fastai/blob/master/courses/dl1/lesson2-image_models.ipynb" data-href="https://github.com/fastai/fastai/blob/master/courses/dl1/lesson2-image_models.ipynb" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Notebook</a></p><figure name="755f" id="755f" class="graf graf--figure graf--layoutOutsetCenter graf-after--p" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_oZ3NUOr9KNljuB4jkwWguQ.png"></figure><p name="76be" id="76be" class="graf graf--p graf-after--figure">Code is pretty much the same as what we had seen before. Here are some differences:</p><ul class="postList"><li name="fa94" id="fa94" class="graf graf--li graf-after--p"><code class="markup--code markup--li-code">transforms_top_down</code> — Since they satellite imagery, they still make sense when they were flipped vertically.</li><li name="2064" id="2064" class="graf graf--li graf-after--li">Much higher learning rate — something to do with this particular dataset</li><li name="654c" id="654c" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">lrs = np.array([lr/9,lr/3,lr])</code> — differential learning rate now change by 3x because images are quite different from ImageNet images</li><li name="7ba6" id="7ba6" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">sz=64</code> — this helped to avoid over fitting for satellite images but he would not do that for dogs. vs. cats or dog breed (similar images to ImageNet) as 64 by 64 is quite tiny and might destroy pre-trained weights.</li></ul><h4 name="a1ae" id="a1ae" class="graf graf--h4 graf-after--li">How to get your AWS setup [<a href="https://youtu.be/JNxcznsrRb8?t=1h58m54s" data-href="https://youtu.be/JNxcznsrRb8?t=1h58m54s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">01:58:54</a>]</h4><p name="2d85" id="2d85" class="graf graf--p graf-after--h4 graf--trailing">You can follow along the video or <a href="https://github.com/reshamas/fastai_deeplearn_part1/blob/master/tools/aws_ami_gpu_setup.md" data-href="https://github.com/reshamas/fastai_deeplearn_part1/blob/master/tools/aws_ami_gpu_setup.md" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">here</a> is a great write up by one of the students.</p><hr class="section-divider"><p name="6ef3" id="6ef3" class="graf graf--p graf--leading graf--trailing">Lessons: <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" class="markup--anchor markup--p-anchor" target="_blank">1</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" class="markup--anchor markup--p-anchor" target="_blank"><strong class="markup--strong markup--p-strong">2</strong></a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" class="markup--anchor markup--p-anchor" target="_blank">3</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" class="markup--anchor markup--p-anchor" target="_blank">4</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" class="markup--anchor markup--p-anchor" target="_blank">5</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" class="markup--anchor markup--p-anchor" target="_blank">6</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" class="markup--anchor markup--p-anchor" target="_blank">7</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" class="markup--anchor markup--p-anchor" target="_blank">8</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" class="markup--anchor markup--p-anchor" target="_blank">9</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" class="markup--anchor markup--p-anchor" target="_blank">10</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" class="markup--anchor markup--p-anchor" target="_blank">11</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" class="markup--anchor markup--p-anchor" target="_blank">12</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" class="markup--anchor markup--p-anchor" target="_blank">13</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" class="markup--anchor markup--p-anchor" target="_blank">14</a></p></body></html>