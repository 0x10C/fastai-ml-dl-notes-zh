
<!-- saved from url=(0063)file:///C:/Users/asus/Desktop/fastai-ml-dl-notes-zh/en/dl7.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></head><body><hr class="section-divider"><h1 name="2f7f" id="2f7f" class="graf graf--h3 graf--leading graf--title">Deep Learning 2: Part 1 Lesson&nbsp;7</h1><p name="724a" id="724a" class="graf graf--p graf-after--h3"><em class="markup--em markup--p-em">My personal notes from </em><a href="http://www.fast.ai/" data-href="http://www.fast.ai/" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank"><em class="markup--em markup--p-em">fast.ai course</em></a><em class="markup--em markup--p-em">. These notes will continue to be updated and improved as I continue to review the course to “really” understand it. Much appreciation to </em><a href="https://twitter.com/jeremyphoward" data-href="https://twitter.com/jeremyphoward" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank"><em class="markup--em markup--p-em">Jeremy</em></a><em class="markup--em markup--p-em"> and </em><a href="https://twitter.com/math_rachel" data-href="https://twitter.com/math_rachel" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank"><em class="markup--em markup--p-em">Rachel</em></a><em class="markup--em markup--p-em"> who gave me this opportunity to learn.</em></p><p name="8a99" id="8a99" class="graf graf--p graf-after--p graf--trailing">Lessons: <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" class="markup--anchor markup--p-anchor" target="_blank">1</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" class="markup--anchor markup--p-anchor" target="_blank">2</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" class="markup--anchor markup--p-anchor" target="_blank">3</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" class="markup--anchor markup--p-anchor" target="_blank">4</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" class="markup--anchor markup--p-anchor" target="_blank">5</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" class="markup--anchor markup--p-anchor" target="_blank">6</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" class="markup--anchor markup--p-anchor" target="_blank"><strong class="markup--strong markup--p-strong">7</strong></a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" class="markup--anchor markup--p-anchor" target="_blank">8</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" class="markup--anchor markup--p-anchor" target="_blank">9</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" class="markup--anchor markup--p-anchor" target="_blank">10</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" class="markup--anchor markup--p-anchor" target="_blank">11</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" class="markup--anchor markup--p-anchor" target="_blank">12</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" class="markup--anchor markup--p-anchor" target="_blank">13</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" class="markup--anchor markup--p-anchor" target="_blank">14</a></p><hr class="section-divider"><h3 name="46b3" id="46b3" class="graf graf--h3 graf--leading"><a href="http://forums.fast.ai/t/wiki-lesson-7/9405" data-href="http://forums.fast.ai/t/wiki-lesson-7/9405" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">Lesson 7</a></h3><p name="dc17" id="dc17" class="graf graf--p graf-after--h3">The theme of Part 1 is:</p><ul class="postList"><li name="a260" id="a260" class="graf graf--li graf-after--p">classification and regression with deep learning</li><li name="239c" id="239c" class="graf graf--li graf-after--li">identifying and learning best and established practices</li><li name="1df8" id="1df8" class="graf graf--li graf-after--li">focus is on classification and regression which is predicting “a thing” (e.g. a number, a small number of labels)</li></ul><p name="ce50" id="ce50" class="graf graf--p graf-after--li">Part 2 of the course:</p><ul class="postList"><li name="bdb8" id="bdb8" class="graf graf--li graf-after--p">focus is on generative modeling which means predicting “lots of things” — for example, creating a sentence as in neural translation, image captioning, or question answering while creating an image such as in style transfer, super-resolution, segmentation and so forth.</li><li name="7f27" id="7f27" class="graf graf--li graf-after--li">not as much best practices but a little more speculative from recent papers that may not be fully tested.</li></ul><h4 name="687e" id="687e" class="graf graf--h4 graf-after--li">Review of Char3Model [<a href="https://youtu.be/H3g26EVADgY?t=2m49s" data-href="https://youtu.be/H3g26EVADgY?t=2m49s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">02:49</a>]</h4><p name="0fd0" id="0fd0" class="graf graf--p graf-after--h4">Reminder: RNN is not in any way different or unusual or magical — just a standard fully connected network.</p><figure name="9769" id="9769" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_9XXQ3J7G3rD92tFkusi4bA.png"><figcaption class="imageCaption">Standard fully connected network</figcaption></figure><ul class="postList"><li name="4a92" id="4a92" class="graf graf--li graf-after--figure">Arrows represent one or more layer operations — generally speaking a linear followed by a non-linear function, in this case matrix multiplications followed by <code class="markup--code markup--li-code">relu</code> or <code class="markup--code markup--li-code">tanh</code></li><li name="9313" id="9313" class="graf graf--li graf-after--li">Arrows of the same color represent exactly the same weight matrix being used.</li><li name="5393" id="5393" class="graf graf--li graf-after--li">One slight difference from previous is that there are inputs coming in at the second and third layers. We tried two approaches — concatenating and adding these inputs to the current activations.</li></ul><pre name="b1a6" id="b1a6" class="graf graf--pre graf-after--li"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">Char3Model</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, vocab_size, n_fac):<br>        super().__init__()<br>        self.e = nn.Embedding(vocab_size, n_fac)<br><br>        <em class="markup--em markup--pre-em"># The 'green arrow' from our diagram</em><br>        self.l_in = nn.Linear(n_fac, n_hidden)<br><br>        <em class="markup--em markup--pre-em"># The 'orange arrow' from our diagram</em><br>        self.l_hidden = nn.Linear(n_hidden, n_hidden)<br>        <br>        <em class="markup--em markup--pre-em"># The 'blue arrow' from our diagram</em><br>        self.l_out = nn.Linear(n_hidden, vocab_size)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, c1, c2, c3):<br>        in1 = F.relu(self.l_in(self.e(c1)))<br>        in2 = F.relu(self.l_in(self.e(c2)))<br>        in3 = F.relu(self.l_in(self.e(c3)))<br>        <br>        h = V(torch.zeros(in1.size()).cuda())<br>        h = F.tanh(self.l_hidden(h+in1))<br>        h = F.tanh(self.l_hidden(h+in2))<br>        h = F.tanh(self.l_hidden(h+in3))<br>        <br>        <strong class="markup--strong markup--pre-strong">return</strong> F.log_softmax(self.l_out(h))</pre><ul class="postList"><li name="844e" id="844e" class="graf graf--li graf-after--pre">By using <code class="markup--code markup--li-code">nn.Linear</code> we get both the weight matrix and the bias vector wrapped up for free for us.</li><li name="7795" id="7795" class="graf graf--li graf-after--li">To deal with the fact that there is no orange arrow coming in for the first ellipse&nbsp;, we invented an empty matrix</li></ul><pre name="ff33" id="ff33" class="graf graf--pre graf-after--li"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">CharLoopModel</strong>(nn.Module):<br>    <em class="markup--em markup--pre-em"># This is an RNN!</em><br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, vocab_size, n_fac):<br>        super().__init__()<br>        self.e = nn.Embedding(vocab_size, n_fac)<br>        self.l_in = nn.Linear(n_fac, n_hidden)<br>        self.l_hidden = nn.Linear(n_hidden, n_hidden)<br>        self.l_out = nn.Linear(n_hidden, vocab_size)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, *cs):<br>        bs = cs[0].size(0)<br>        h = V(torch.zeros(bs, n_hidden).cuda())<br>        <strong class="markup--strong markup--pre-strong">for</strong> c <strong class="markup--strong markup--pre-strong">in</strong> cs:<br>            inp = F.relu(self.l_in(self.e(c)))<br>            h = F.tanh(self.l_hidden(h+inp))<br>        <br>        <strong class="markup--strong markup--pre-strong">return</strong> F.log_softmax(self.l_out(h), dim=-1)</pre><ul class="postList"><li name="d5f9" id="d5f9" class="graf graf--li graf-after--pre">Almost identical except for the <code class="markup--code markup--li-code">for</code> loop</li></ul><pre name="399b" id="399b" class="graf graf--pre graf-after--li"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">CharRnn</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, vocab_size, n_fac):<br>        super().__init__()<br>        self.e = nn.Embedding(vocab_size, n_fac)<br>        self.rnn = nn.RNN(n_fac, n_hidden)<br>        self.l_out = nn.Linear(n_hidden, vocab_size)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, *cs):<br>        bs = cs[0].size(0)<br>        h = V(torch.zeros(1, bs, n_hidden))<br>        inp = self.e(torch.stack(cs))<br>        outp,h = self.rnn(inp, h)<br>        <br>        <strong class="markup--strong markup--pre-strong">return</strong> F.log_softmax(self.l_out(outp[-1]), dim=-1)</pre><ul class="postList"><li name="b74e" id="b74e" class="graf graf--li graf-after--pre">PyTorch version — <code class="markup--code markup--li-code">nn.RNN</code> will create the loop and keep track of <code class="markup--code markup--li-code">h</code> as it goes along.</li><li name="8ce5" id="8ce5" class="graf graf--li graf-after--li">We are using white section to predict the green character — which seems wasteful as the next section mostly overlaps with the current section.</li></ul><figure name="cc6c" id="cc6c" class="graf graf--figure graf-after--li"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_4v68iwTS32RHplB8c-egmg.png"></figure><ul class="postList"><li name="17ad" id="17ad" class="graf graf--li graf-after--figure">We then tried splitting it into non-overlapping pieces in multi-output model:</li></ul><figure name="0b91" id="0b91" class="graf graf--figure graf-after--li"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_5LY1Sdql1_VLHDfdd2e8lw.png"></figure><ul class="postList"><li name="83c2" id="83c2" class="graf graf--li graf-after--figure">In this approach, we are throwing away our <code class="markup--code markup--li-code">h</code> activation after processing each section and started a new one. In order to predict the second character using the first one in the next section, it has nothing to go on but a default activation. Let’s not throw away <code class="markup--code markup--li-code">h</code>&nbsp;.</li></ul><h4 name="3ed0" id="3ed0" class="graf graf--h4 graf-after--li">Stateful RNN&nbsp;[<a href="https://youtu.be/H3g26EVADgY?t=8m52s" data-href="https://youtu.be/H3g26EVADgY?t=8m52s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">08:52</a>]</h4><pre name="2fc5" id="2fc5" class="graf graf--pre graf-after--h4"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">CharSeqStatefulRnn</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, vocab_size, n_fac, bs):<br>        self.vocab_size = vocab_size<br>        super().__init__()<br>        self.e = nn.Embedding(vocab_size, n_fac)<br>        self.rnn = nn.RNN(n_fac, n_hidden)<br>        self.l_out = nn.Linear(n_hidden, vocab_size)<br>        <strong class="markup--strong markup--pre-strong">self.init_hidden(bs)</strong><br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, cs):<br>        bs = cs[0].size(0)<br>        <strong class="markup--strong markup--pre-strong">if</strong> self.h.size(1) != bs: self.init_hidden(bs)<br>        outp,h = self.rnn(self.e(cs), self.h)<br>        <strong class="markup--strong markup--pre-strong">self.h = repackage_var(h)</strong><br>        <strong class="markup--strong markup--pre-strong">return</strong> F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)<br>    <br>    <strong class="markup--strong markup--pre-strong">def</strong> init_hidden(self, bs): self.h = V(torch.zeros(1, bs, n_hidden))</pre><ul class="postList"><li name="1f50" id="1f50" class="graf graf--li graf-after--pre">One additional line in constructor. <code class="markup--code markup--li-code">self.init_hidden(bs)</code> sets <code class="markup--code markup--li-code">self.h</code> to bunch of zeros.</li><li name="f027" id="f027" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Wrinkle #1</strong> [<a href="https://youtu.be/H3g26EVADgY?t=10m51s" data-href="https://youtu.be/H3g26EVADgY?t=10m51s" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">10:51</a>] — if we were to simply do <code class="markup--code markup--li-code">self.h = h</code>&nbsp;, and we trained on a document that is a million characters long, then the size of unrolled version of the RNN has a million layers (ellipses). One million layer fully connected network is going to be very memory intensive because in order to do a chain rule, we have to multiply one million layers while remembering all one million gradients every batch.</li><li name="7a3c" id="7a3c" class="graf graf--li graf-after--li">To avoid this, we tell it to forget its history from time to time. We can still remember the state (the values in our hidden matrix) without remembering everything about how we got there.</li></ul><pre name="1c88" id="1c88" class="graf graf--pre graf-after--li">def repackage_var(h):<em class="markup--em markup--pre-em"><br>    </em>return Variable(h.data) if type(h) == Variable else tuple(repackage_var(v) for v in h)</pre><ul class="postList"><li name="297b" id="297b" class="graf graf--li graf-after--pre">Grab the tensor out of <code class="markup--code markup--li-code">Variable</code> <code class="markup--code markup--li-code">h</code> (remember, a tensor itself does not have any concept of history), and create a new <code class="markup--code markup--li-code">Variable</code> out of that. The new variable has the same value but no history of operations, therefore when it tries to back-propagate, it will stop there.</li><li name="abcd" id="abcd" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">forward</code> will process 8 characters, it then back propagate through eight layers, keep track of the values in out hidden state, but it will throw away its history of operations. This is called <strong class="markup--strong markup--li-strong">back-prop through time (bptt)</strong>.</li><li name="2296" id="2296" class="graf graf--li graf-after--li">In other words, after the <code class="markup--code markup--li-code">for</code> loop, just throw away the history of operations and start afresh. So we are keeping our hidden state but we are not keeping our hidden state history.</li><li name="bfe4" id="bfe4" class="graf graf--li graf-after--li">Another good reason not to back-propagate through too many layers is that if you have any kind of gradient instability (e.g. gradient explosion or gradient banishing), the more layers you have, the harder the network gets to train (slower and less resilient).</li><li name="bb28" id="bb28" class="graf graf--li graf-after--li">On the other hand, the longer <code class="markup--code markup--li-code">bptt</code> means that you are able to explicitly capture a longer memory and more state.</li><li name="8b4d" id="8b4d" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Wrinkle #2</strong> [<a href="https://youtu.be/H3g26EVADgY?t=16m" data-href="https://youtu.be/H3g26EVADgY?t=16m" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">16:00</a>] — how to create mini-batches. We do not want to process one section at a time, but a bunch in parallel at a time.</li><li name="1165" id="1165" class="graf graf--li graf-after--li">When we started looking at TorchText for the first time, we talked about how it creates these mini-batches.</li><li name="2ca6" id="2ca6" class="graf graf--li graf-after--li">Jeremy said we take a whole long document consisting of the entire works of Nietzsche or all of the IMDB reviews concatenated together, we split this into 64 equal sized chunks (NOT chunks of size 64).</li></ul><figure name="01a4" id="01a4" class="graf graf--figure graf-after--li"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_YOUoCz-p7semcNuDFZqp_w.png"></figure><ul class="postList"><li name="1066" id="1066" class="graf graf--li graf-after--figure">For a document that is 64 million characters long, each “chunk” will be 1 million characters. We stack them together and now split them by <code class="markup--code markup--li-code">bptt</code> — 1 mini-bach consists of 64 by <code class="markup--code markup--li-code">bptt</code> matrix.</li><li name="8a37" id="8a37" class="graf graf--li graf-after--li">The first character of the second chunk(1,000,001th character) is likely be in the middle of a sentence. But it is okay since it only happens once every million characters.</li></ul><h4 name="92e7" id="92e7" class="graf graf--h4 graf-after--li">Question: Data augmentation for this kind of dataset?&nbsp;[<a href="https://youtu.be/H3g26EVADgY?t=20m34s" data-href="https://youtu.be/H3g26EVADgY?t=20m34s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">20:34</a>]</h4><p name="f30b" id="f30b" class="graf graf--p graf-after--h4">There is no known good way. Somebody recently won a Kaggle competition by doing data augmentation which randomly inserted parts of different rows — something like that may be useful here. But there has not been any recent state-of-the-art NLP papers that are doing this kind of data augmentation.</p><h4 name="175d" id="175d" class="graf graf--h4 graf-after--p">Question: How do we choose the size of bptt?&nbsp;[<a href="https://youtu.be/H3g26EVADgY?t=21m36s" data-href="https://youtu.be/H3g26EVADgY?t=21m36s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">21:36</a>]</h4><p name="0182" id="0182" class="graf graf--p graf-after--h4">There are a couple things to think about:</p><ul class="postList"><li name="69fb" id="69fb" class="graf graf--li graf-after--p">the first is that mini-batch matrix has a size of <code class="markup--code markup--li-code">bs</code> (# of chunks) by <code class="markup--code markup--li-code">bptt</code> so your GPU RAM must be able to fit that by your embedding matrix. So if you get CUDA out of memory error, you need reduce one of these.</li><li name="3504" id="3504" class="graf graf--li graf-after--li">If your training is unstable (e.g. your loss is shooting off to NaN suddenly), then you could try decreasing your <code class="markup--code markup--li-code">bptt</code> because you have less layers to gradient explode through.</li><li name="ed23" id="ed23" class="graf graf--li graf-after--li">If it is too slow [<a href="https://youtu.be/H3g26EVADgY?t=22m44s" data-href="https://youtu.be/H3g26EVADgY?t=22m44s" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">22:44</a>], try decreasing your <code class="markup--code markup--li-code">bptt</code> because it will do one of those steps at a time. <code class="markup--code markup--li-code">for</code> loop cannot be parallelized (for the current version). There is a recent thing called QRNN (Quasi-Recurrent Neural Network) which does parallelize it and we hope to cover in part 2.</li><li name="e917" id="e917" class="graf graf--li graf-after--li">So pick the highest number that satisfies all these.</li></ul><h4 name="a9ad" id="a9ad" class="graf graf--h4 graf-after--li">Stateful RNN &amp; TorchText [<a href="https://youtu.be/H3g26EVADgY?t=23m23s" data-href="https://youtu.be/H3g26EVADgY?t=23m23s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">23:23</a>]</h4><p name="89bd" id="89bd" class="graf graf--p graf-after--h4">When using an existing API which expects data to be certain format, you can either change your data to fit that format or you can write your own dataset sub-class to handle the format that your data is already in. Either is fine, but in this case, we will put our data in the format TorchText already support. Fast.ai wrapper around TorchText already has something where you can have a training path and validation path, and one or more text files in each path containing bunch of text that are concatenated together for your language model.</p><pre name="0c59" id="0c59" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">torchtext</strong> <strong class="markup--strong markup--pre-strong">import</strong> vocab, data  </pre><pre name="1025" id="1025" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">fastai.nlp</strong> <strong class="markup--strong markup--pre-strong">import</strong> * <br><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">fastai.lm_rnn</strong> <strong class="markup--strong markup--pre-strong">import</strong> *  </pre><pre name="8a75" id="8a75" class="graf graf--pre graf-after--pre">PATH='data/nietzsche/'  </pre><pre name="ffd3" id="ffd3" class="graf graf--pre graf-after--pre">TRN_PATH = 'trn/' <br>VAL_PATH = 'val/' <br>TRN = f'<strong class="markup--strong markup--pre-strong">{PATH}{TRN_PATH}</strong>' <br>VAL = f'<strong class="markup--strong markup--pre-strong">{PATH}{VAL_PATH}</strong>'</pre><pre name="a49b" id="a49b" class="graf graf--pre graf-after--pre">%ls {PATH}<br><em class="markup--em markup--pre-em">models/  nietzsche.txt  trn/  val/</em></pre><pre name="f7d1" id="f7d1" class="graf graf--pre graf-after--pre">%ls {PATH}trn<br><em class="markup--em markup--pre-em">trn.txt</em></pre><ul class="postList"><li name="d4bc" id="d4bc" class="graf graf--li graf-after--pre">Made a copy of Nietzsche file, pasted into training and validation directory. Then deleted the last 20% of the rows from training set, and deleted everything but the last 20% from the validation set [<a href="https://youtu.be/H3g26EVADgY?t=25m15s" data-href="https://youtu.be/H3g26EVADgY?t=25m15s" class="markup--anchor markup--li-anchor" rel="noopener nofollow" target="_blank">25:15</a>].</li><li name="7a31" id="7a31" class="graf graf--li graf-after--li">The other benefit of doing it this way is that it seems like it is more realistic to have a validation set that was not a random shuffled set of rows of text, but it was totally separate part of the corpus.</li><li name="f4a0" id="f4a0" class="graf graf--li graf-after--li">When you are doing a language model, you do not really need separate files. You can have multiple files but they just get concatenated together anyway.</li></ul><pre name="678c" id="678c" class="graf graf--pre graf-after--li">TEXT = data.Field(lower=<strong class="markup--strong markup--pre-strong">True</strong>, tokenize=list)<br>bs=64; bptt=8; n_fac=42; n_hidden=256<br><br>FILES = dict(train=TRN_PATH, validation=VAL_PATH, test=VAL_PATH)<br>md = LanguageModelData.from_text_files(PATH, TEXT, **FILES, bs=bs, bptt=bptt, min_freq=3)<br><br>len(md.trn_dl), md.nt, len(md.trn_ds), len(md.trn_ds[0].text)<br><em class="markup--em markup--pre-em">(963, 56, 1, 493747)</em></pre><ul class="postList"><li name="83dc" id="83dc" class="graf graf--li graf-after--pre">In TorchText, we make this thing called <code class="markup--code markup--li-code">Field</code> and initially <code class="markup--code markup--li-code">Field</code> is just a description of how to go about pre-processing the text.</li><li name="2db6" id="2db6" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">lower</code> — we told it to lowercase the text</li><li name="609b" id="609b" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">tokenize</code> — Last time, we used a function that splits on whitespace that gave us a word model. This time, we want a character model, so use <code class="markup--code markup--li-code">list</code> function to tokenize strings. Remember, in Python, <code class="markup--code markup--li-code">list('abc')</code> will return <code class="markup--code markup--li-code">['a', 'b', 'c']</code>&nbsp;.</li><li name="17bc" id="17bc" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">bs</code>&nbsp;: batch size, <code class="markup--code markup--li-code">bptt</code>&nbsp;: we renamed <code class="markup--code markup--li-code">cs</code>&nbsp;, <code class="markup--code markup--li-code">n_fac</code>&nbsp;: size of embedding, <code class="markup--code markup--li-code">n_hidden</code>&nbsp;: size of our hidden state</li><li name="8935" id="8935" class="graf graf--li graf-after--li">We do not have a separate test set, so we’ll just use validation set for testing</li><li name="8014" id="8014" class="graf graf--li graf-after--li">TorchText randomize the length of <code class="markup--code markup--li-code">bptt</code> a little bit each time. It does not always give us exactly 8 characters; 5% of the time, it will cut it in half and add on a small standard deviation to make it slightly bigger or smaller than 8. We cannot shuffle the data since it needs to be contiguous, so this is a way to introduce some randomness.</li><li name="7ce8" id="7ce8" class="graf graf--li graf-after--li">Question [<a href="https://youtu.be/H3g26EVADgY?t=31m46s" data-href="https://youtu.be/H3g26EVADgY?t=31m46s" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">31:46</a>]: Does the size remain constant per mini-batch? Yes, we need to do matrix multiplication with <code class="markup--code markup--li-code">h</code> weight matrix, so mini-batch size must remain constant. But sequence length can change no problem.</li><li name="79b4" id="79b4" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">len(md.trn_dl)</code>&nbsp;: length of data loader (i.e. how many mini-batches), <code class="markup--code markup--li-code">md.nt</code>&nbsp;: number of tokens (i.e. how many unique things are in the vocabulary)</li><li name="c9d4" id="c9d4" class="graf graf--li graf-after--li">Once you run <code class="markup--code markup--li-code">LanguageModelData.from_text_files</code>&nbsp;, <code class="markup--code markup--li-code">TEXT</code> will contain an extra attribute called <code class="markup--code markup--li-code">vocab</code>. <code class="markup--code markup--li-code">TEXT.vocab.itos</code> list of unique items in the vocabulary, and <code class="markup--code markup--li-code">TEXT.vocab.stoi</code> is a reverse mapping from each item to number.</li></ul><pre name="e596" id="e596" class="graf graf--pre graf-after--li"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">CharSeqStatefulRnn</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, vocab_size, n_fac, bs):<br>        self.vocab_size = vocab_size<br>        super().__init__()<br>        self.e = nn.Embedding(vocab_size, n_fac)<br>        self.rnn = nn.RNN(n_fac, n_hidden)<br>        self.l_out = nn.Linear(n_hidden, vocab_size)<br>        self.init_hidden(bs)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, cs):<br>        bs = cs[0].size(0)<br>        <strong class="markup--strong markup--pre-strong">if self.h.size(1) != bs: self.init_hidden(bs)</strong><br>        outp,h = self.rnn(self.e(cs), self.h)<br>        self.h = repackage_var(h)<br>        <strong class="markup--strong markup--pre-strong">return</strong> <strong class="markup--strong markup--pre-strong">F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)</strong><br>    <br>    <strong class="markup--strong markup--pre-strong">def</strong> init_hidden(self, bs): self.h = V(torch.zeros(1, bs, n_hidden))</pre><ul class="postList"><li name="f663" id="f663" class="graf graf--li graf-after--pre"><strong class="markup--strong markup--li-strong">Wrinkle #3</strong> [<a href="https://youtu.be/H3g26EVADgY?t=33m51s" data-href="https://youtu.be/H3g26EVADgY?t=33m51s" class="markup--anchor markup--li-anchor" rel="noopener nofollow" target="_blank">33:51</a>]: Jeremy lied to us when he said that mini-batch size remains constant. It is very likely that the last mini-batch is shorter than the rest unless the dataset is exactly divisible by <code class="markup--code markup--li-code">bptt</code> times <code class="markup--code markup--li-code">bs</code>&nbsp;. That is why we check whether <code class="markup--code markup--li-code">self.h</code> ‘s second dimension is the same as <code class="markup--code markup--li-code">bs</code> of the input. If it is not the same, set it back to zero with the input’s <code class="markup--code markup--li-code">bs</code>&nbsp;. This happens at the end of the epoch and the beginning of the epoch (setting back to the full batch size).</li><li name="b2fc" id="b2fc" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Wrinkle #4 </strong>[<a href="https://youtu.be/H3g26EVADgY?t=35m44s" data-href="https://youtu.be/H3g26EVADgY?t=35m44s" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">35:44</a>]: The last wrinkle is something that slightly sucks about PyTorch and maybe somebody can be nice enough to try and fix it with a PR. Loss functions are not happy receiving a rank 3 tensor (i.e. three dimensional array). There is no particular reason they ought to not be happy receiving a rank 3 tensor (sequence length by batch size by results — so you can just calculate loss for each of the two initial axis). Works for rank 2 or 4, but not 3.</li><li name="235d" id="235d" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">.view</code> will reshape rank 3 tensor into rank 2 of <code class="markup--code markup--li-code">-1</code> (however big as necessary) by <code class="markup--code markup--li-code">vocab_size</code>. TorchText automatically changes the <strong class="markup--strong markup--li-strong">target</strong> to be flattened out, so we do not need to do that for actual values (when we looked at a mini-batch in lesson 4, we noticed that it was flattened. Jeremy said we will learn about why later, so later is now.)</li><li name="c881" id="c881" class="graf graf--li graf-after--li">PyTorch (as of 0.3), <code class="markup--code markup--li-code">log_softmax</code> requires us to specify which axis we want to do the softmax over (i.e. which axis we want to sum to one). In this case we want to do it over the last axis <code class="markup--code markup--li-code">dim = -1</code>.</li></ul><pre name="0378" id="0378" class="graf graf--pre graf-after--li">m = CharSeqStatefulRnn(md.nt, n_fac, 512).cuda() <br>opt = optim.Adam(m.parameters(), 1e-3)</pre><pre name="1896" id="1896" class="graf graf--pre graf-after--pre">fit(m, md, 4, opt, F.nll_loss)</pre><h4 name="0554" id="0554" class="graf graf--h4 graf-after--pre">Let’s gain more insight by unpacking RNN&nbsp;[<a href="https://youtu.be/H3g26EVADgY?t=42m48s" data-href="https://youtu.be/H3g26EVADgY?t=42m48s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">42:48</a>]</h4><p name="2c84" id="2c84" class="graf graf--p graf-after--h4">We remove the use of <code class="markup--code markup--p-code">nn.RNN</code> and replace it with <code class="markup--code markup--p-code">nn.RNNCell</code>&nbsp;. PyTorch source code looks like the following. You should be able to read and understand (Note: they do not concatenate the input and the hidden state, but they sum them together — which was our first approach):</p><pre name="7d7c" id="7d7c" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> RNNCell(input, hidden, w_ih, w_hh, b_ih, b_hh):<br>    <strong class="markup--strong markup--pre-strong">return</strong> F.tanh(F.linear(input, w_ih, b_ih) + F.linear(hidden, w_hh, b_hh))</pre><p name="38a2" id="38a2" class="graf graf--p graf-after--pre">Question about <code class="markup--code markup--p-code">tanh</code> [<a href="https://youtu.be/H3g26EVADgY?t=44m6s" data-href="https://youtu.be/H3g26EVADgY?t=44m6s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">44:06</a>]: As we have seen last week, <code class="markup--code markup--p-code">tanh</code> is forcing the value to be between -1 and 1. Since we are multiplying by this weight matrix again and again, we would worry that <code class="markup--code markup--p-code">relu</code> (since it is unbounded) might have more gradient explosion problem. Having said that, you can specify <code class="markup--code markup--p-code">RNNCell</code> to use different <code class="markup--code markup--p-code">nonlineality</code> whose default is <code class="markup--code markup--p-code">tanh</code> and ask it to use <code class="markup--code markup--p-code">relu</code> if you wanted to.</p><pre name="87cc" id="87cc" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">CharSeqStatefulRnn2</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, vocab_size, n_fac, bs):<br>        super().__init__()<br>        self.vocab_size = vocab_size<br>        self.e = nn.Embedding(vocab_size, n_fac)<br>        self.rnn = <strong class="markup--strong markup--pre-strong">nn.RNNCell</strong>(n_fac, n_hidden)<br>        self.l_out = nn.Linear(n_hidden, vocab_size)<br>        self.init_hidden(bs)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, cs):<br>        bs = cs[0].size(0)<br>        <strong class="markup--strong markup--pre-strong">if</strong> self.h.size(1) != bs: self.init_hidden(bs)<br>        outp = []<br>        o = self.h<br>        <strong class="markup--strong markup--pre-strong">for</strong> c <strong class="markup--strong markup--pre-strong">in</strong> cs: <br>            o = self.rnn(self.e(c), o)<br>            outp.append(o)<br>        outp = self.l_out(torch.stack(outp))<br>        self.h = repackage_var(o)<br>        <strong class="markup--strong markup--pre-strong">return</strong> F.log_softmax(outp, dim=-1).view(-1, self.vocab_size)<br>    <br>    <strong class="markup--strong markup--pre-strong">def</strong> init_hidden(self, bs): self.h = V(torch.zeros(1, bs, n_hidden))</pre><ul class="postList"><li name="1bfc" id="1bfc" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">for</code> loop is back and append the result of linear function to a list — which in end gets stacked up together.</li><li name="9cc0" id="9cc0" class="graf graf--li graf-after--li">fast.ai library actually does exactly this in order to use regularization approaches that are not supported by PyTorch.</li></ul><h4 name="8f68" id="8f68" class="graf graf--h4 graf-after--li">Gated Recurrent Unit (GRU)&nbsp;[<a href="https://youtu.be/H3g26EVADgY?t=46m44s" data-href="https://youtu.be/H3g26EVADgY?t=46m44s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">46:44</a>]</h4><p name="7cd3" id="7cd3" class="graf graf--p graf-after--h4">In practice, nobody really uses <code class="markup--code markup--p-code">RNNCell</code> since even with <code class="markup--code markup--p-code">tanh</code>&nbsp;, gradient explosions are still a problem and we need use low learning rate and small <code class="markup--code markup--p-code">bptt</code> to get them to train. So what we do is to replace <code class="markup--code markup--p-code">RNNCell</code> with something like <code class="markup--code markup--p-code">GRUCell</code>&nbsp;.</p><figure name="57e4" id="57e4" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1__29x3zNI1C0vM3fxiIpiVA.png"><figcaption class="imageCaption"><a href="http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/" data-href="http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/</a></figcaption></figure><ul class="postList"><li name="0be5" id="0be5" class="graf graf--li graf-after--figure">Normally, the input gets multiplied by a weight matrix to create new activations <code class="markup--code markup--li-code">h</code> and get added to the existing activations straight away. That is not wha happens here.</li><li name="78e7" id="78e7" class="graf graf--li graf-after--li">Input goes into <code class="markup--code markup--li-code">h˜</code> and it doesn’t just get added to the previous activations, but the previous activation gets multiplied by <code class="markup--code markup--li-code">r</code> (reset gate) which has a value of 0 or 1.</li><li name="97d7" id="97d7" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">r</code> is calculated as below — matrix multiplication of some weight matrix and the concatenation of our previous hidden state and new input. In other words, this is a little one hidden layer neural net. It gets put through the sigmoid function as well. This mini neural net learns to determine how much of the hidden states to remember (maybe forget it all when it sees a full-stop character — beginning of a new sentence).</li><li name="8d12" id="8d12" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">z</code> gate (update gate) determines what degree to use <code class="markup--code markup--li-code">h˜</code> (the new input version of hidden states) and what degree to leave the hidden state the same as before.</li></ul><figure name="3ba3" id="3ba3" class="graf graf--figure graf-after--li"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_qzfburCutJ3p-FYu1T6Q3Q.png"><figcaption class="imageCaption"><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" data-href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></figcaption></figure><figure name="08f3" id="08f3" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*M7ujxxzjQfL5e33BjJQViw.png" data-width="420" data-height="77" src="../img/1_M7ujxxzjQfL5e33BjJQViw.png"></figure><ul class="postList"><li name="f8f5" id="f8f5" class="graf graf--li graf-after--figure">Linear interpolation</li></ul><pre name="5f0e" id="5f0e" class="graf graf--pre graf-after--li"><strong class="markup--strong markup--pre-strong">def</strong> GRUCell(input, hidden, w_ih, w_hh, b_ih, b_hh):<br>    gi = F.linear(input, w_ih, b_ih)<br>    gh = F.linear(hidden, w_hh, b_hh)<br>    i_r, i_i, i_n = gi.chunk(3, 1)<br>    h_r, h_i, h_n = gh.chunk(3, 1)<br><br>    resetgate = F.sigmoid(i_r + h_r)<br>    inputgate = F.sigmoid(i_i + h_i)<br>    newgate = F.tanh(i_n + resetgate * h_n)<br>    <strong class="markup--strong markup--pre-strong">return</strong> newgate + inputgate * (hidden - newgate)</pre><p name="2734" id="2734" class="graf graf--p graf-after--pre">Above is what <code class="markup--code markup--p-code">GRUCell</code> code looks like, and our new model that utilize this is below:</p><pre name="5830" id="5830" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">CharSeqStatefulGRU</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, vocab_size, n_fac, bs):<br>        super().__init__()<br>        self.vocab_size = vocab_size<br>        self.e = nn.Embedding(vocab_size, n_fac)<br>        self.rnn = nn.GRU(n_fac, n_hidden)<br>        self.l_out = nn.Linear(n_hidden, vocab_size)<br>        self.init_hidden(bs)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, cs):<br>        bs = cs[0].size(0)<br>        <strong class="markup--strong markup--pre-strong">if</strong> self.h.size(1) != bs: self.init_hidden(bs)<br>        outp,h = self.rnn(self.e(cs), self.h)<br>        self.h = repackage_var(h)<br>        <strong class="markup--strong markup--pre-strong">return</strong> F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)<br>    <br>    <strong class="markup--strong markup--pre-strong">def</strong> init_hidden(self, bs): self.h = V(torch.zeros(1, bs, n_hidden))</pre><p name="10d3" id="10d3" class="graf graf--p graf-after--pre">As a result, we can lower the loss down to 1.36 (<code class="markup--code markup--p-code">RNNCell</code> one was 1.54). In practice, GRU and LSTM are what people uses.</p><h4 name="2c05" id="2c05" class="graf graf--h4 graf-after--p">Putting it all together: Long Short-Term Memory&nbsp;[<a href="https://youtu.be/H3g26EVADgY?t=54m9s" data-href="https://youtu.be/H3g26EVADgY?t=54m9s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">54:09</a>]</h4><p name="be04" id="be04" class="graf graf--p graf-after--h4">LSTM has one more piece of state in it called “cell state” (not just hidden state), so if you do use a LSTM, you have to return a tuple of matrices in <code class="markup--code markup--p-code">init_hidden</code> (exactly the same size as hidden state):</p><pre name="ba6b" id="ba6b" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">fastai</strong> <strong class="markup--strong markup--pre-strong">import</strong> sgdr<br><br>n_hidden=512</pre><pre name="d419" id="d419" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">CharSeqStatefulLSTM</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, vocab_size, n_fac, bs, nl):<br>        super().__init__()<br>        self.vocab_size,self.nl = vocab_size,nl<br>        self.e = nn.Embedding(vocab_size, n_fac)<br>        self.rnn = nn.LSTM(n_fac, n_hidden, nl, <strong class="markup--strong markup--pre-strong">dropout</strong>=0.5)<br>        self.l_out = nn.Linear(n_hidden, vocab_size)<br>        self.init_hidden(bs)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, cs):<br>        bs = cs[0].size(0)<br>        <strong class="markup--strong markup--pre-strong">if</strong> self.h[0].size(1) != bs: self.init_hidden(bs)<br>        outp,h = self.rnn(self.e(cs), self.h)<br>        self.h = repackage_var(h)<br>        <strong class="markup--strong markup--pre-strong">return</strong> F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)<br>    <br>    <strong class="markup--strong markup--pre-strong">def</strong> init_hidden(self, bs):<br><strong class="markup--strong markup--pre-strong">        self.h = (V(torch.zeros(self.nl, bs, n_hidden)),<br>                  V(torch.zeros(self.nl, bs, n_hidden)))</strong></pre><p name="663f" id="663f" class="graf graf--p graf-after--pre">The code is identical to GRU one. The one thing that was added was <code class="markup--code markup--p-code">dropout</code> which does dropout after each time step and doubled the hidden layer — in a hope that it will be able to learn more and be resilient as it does so.</p><h4 name="c653" id="c653" class="graf graf--h4 graf-after--p">Callbacks (specifically SGDR) without Learner class&nbsp;[<a href="https://youtu.be/H3g26EVADgY?t=55m23s" data-href="https://youtu.be/H3g26EVADgY?t=55m23s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">55:23</a>]</h4><pre name="40cf" id="40cf" class="graf graf--pre graf-after--h4">m = CharSeqStatefulLSTM(md.nt, n_fac, 512, 2).cuda()<br>lo = LayerOptimizer(optim.Adam, m, 1e-2, 1e-5)</pre><ul class="postList"><li name="198a" id="198a" class="graf graf--li graf-after--pre">After creating a standard PyTorch model, we usually do something like <code class="markup--code markup--li-code">opt = optim.Adam(m.parameters(), 1e-3)</code>. Instead, we will use fast.ai <code class="markup--code markup--li-code">LayerOptimizer</code> which takes an optimizer <code class="markup--code markup--li-code">optim.Adam</code>&nbsp;, our model <code class="markup--code markup--li-code">m</code>&nbsp;, learning rate <code class="markup--code markup--li-code">1e-2</code>&nbsp;, and optionally weight decay <code class="markup--code markup--li-code">1e-5</code>&nbsp;.</li><li name="0ddb" id="0ddb" class="graf graf--li graf-after--li">A key reason <code class="markup--code markup--li-code">LayerOptimizer</code> exists is to do differential learning rates and differential weight decay. The reason we need to use it is that all of the mechanics inside fast.ai assumes that you have one of these. If you want to use callbacks or SGDR in code you are not using the Learner class, you need to use this.</li><li name="07f2" id="07f2" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">lo.opt</code> returns the optimizer.</li></ul><pre name="4b38" id="4b38" class="graf graf--pre graf-after--li">on_end = <strong class="markup--strong markup--pre-strong">lambda</strong> sched, cycle: save_model(m, f'<strong class="markup--strong markup--pre-strong">{PATH}</strong>models/cyc_<strong class="markup--strong markup--pre-strong">{cycle}</strong>')</pre><pre name="8627" id="8627" class="graf graf--pre graf-after--pre">cb = [CosAnneal(lo, len(md.trn_dl), cycle_mult=2, on_cycle_end=on_end)]</pre><pre name="632c" id="632c" class="graf graf--pre graf-after--pre">fit(m, md, 2**4-1, lo.opt, F.nll_loss, callbacks=cb)</pre><ul class="postList"><li name="b163" id="b163" class="graf graf--li graf-after--pre">When we call <code class="markup--code markup--li-code">fit</code>, we can now pass the <code class="markup--code markup--li-code">LayerOptimizer</code> and also <code class="markup--code markup--li-code">callbacks</code>.</li><li name="890b" id="890b" class="graf graf--li graf-after--li">Here, we use cosine annealing callback — which requires a <code class="markup--code markup--li-code">LayerOptimizer</code> object. It does cosine annealing by changing learning rate in side the <code class="markup--code markup--li-code">lo</code> object.</li><li name="871a" id="871a" class="graf graf--li graf-after--li">Concept: Create a cosine annealing callback which is going to update the learning rates in the layer optimizer <code class="markup--code markup--li-code">lo</code>&nbsp;. The length of an epoch is equal to <code class="markup--code markup--li-code">len(md.trn_dl)</code> — how many mini-batches are there in an epoch is the length of the data loader. Since it is doing cosine annealing, it needs to know how often to reset. You can pass in <code class="markup--code markup--li-code">cycle_mult</code> in usual way. We can even save our model automatically just like we did with <code class="markup--code markup--li-code">cycle_save_name</code> in <code class="markup--code markup--li-code">Learner.fit</code>.</li><li name="75f8" id="75f8" class="graf graf--li graf-after--li">We can do callback at a start of a training, epoch or a batch, or at the end of a training, an epoch, or a batch.</li><li name="c3af" id="c3af" class="graf graf--li graf-after--li">It has been used for <code class="markup--code markup--li-code">CosAnneal</code> (SGDR), and decoupled weight decay (AdamW), loss-over-time graph, etc.</li></ul><h4 name="6061" id="6061" class="graf graf--h4 graf-after--li">Testing [<a href="https://youtu.be/H3g26EVADgY?t=59m55s" data-href="https://youtu.be/H3g26EVADgY?t=59m55s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">59:55</a>]</h4><pre name="1657" id="1657" class="graf graf--pre graf-after--h4"><strong class="markup--strong markup--pre-strong">def</strong> get_next(inp):<br>    idxs = TEXT.numericalize(inp)<br>    p = m(VV(idxs.transpose(0,1)))<br>    r = <strong class="markup--strong markup--pre-strong">torch.multinomial(p[-1].exp(), 1)</strong><br>    <strong class="markup--strong markup--pre-strong">return</strong> TEXT.vocab.itos[to_np(r)[0]]</pre><pre name="6c76" id="6c76" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> get_next_n(inp, n):<br>    res = inp<br>    <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(n):<br>        c = get_next(inp)<br>        res += c<br>        inp = inp[1:]+c<br>    <strong class="markup--strong markup--pre-strong">return</strong> res</pre><pre name="6fad" id="6fad" class="graf graf--pre graf-after--pre">print(get_next_n('for thos', 400))</pre><pre name="b8d1" id="b8d1" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">for those the skemps), or imaginates, though they deceives. it should so each ourselvess and new present, step absolutely for the science." the contradity and measuring,  the whole!  </em></pre><pre name="1600" id="1600" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">293. perhaps, that every life a values of blood of intercourse when it senses there is unscrupulus, his very rights, and still impulse, love? just after that thereby how made with the way anything, and set for harmless philos</em></pre><ul class="postList"><li name="3ade" id="3ade" class="graf graf--li graf-after--pre">In lesson 6, when we were testing <code class="markup--code markup--li-code">CharRnn</code> model, we noticed that it repeated itself over and over. <code class="markup--code markup--li-code">torch.multinomial</code> used in this new version deals with this problem. <code class="markup--code markup--li-code">p[-1]</code> to get the final output (the triangle), <code class="markup--code markup--li-code">exp</code> to convert log probability to probability. We then use <code class="markup--code markup--li-code">torch.multinomial</code> function which will give us a sample using the given probabilities. If probability is [0, 1, 0, 0] and ask it to give us a sample, it will always return the second item. If it was [0.5, 0, 0.5], it will give the first item 50% of the time, and second item&nbsp;. 50% of the time (<a href="http://onlinestatbook.com/2/probability/multinomial.html" data-href="http://onlinestatbook.com/2/probability/multinomial.html" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">review of multinomial distribution</a>)</li><li name="fbfc" id="fbfc" class="graf graf--li graf-after--li">To play around with training character based language models like this, try running <code class="markup--code markup--li-code">get_next_n</code> at different levels of loss to get a sense of what it looks like. The example above is at 1.25, but at 1.3, it looks like a total junk.</li><li name="bfb9" id="bfb9" class="graf graf--li graf-after--li">When you are playing around with NLP, particularly generative model like this, and the results are kind of okay but not great, do not be disheartened because that means you are actually very VERY nearly there!</li></ul><h3 name="48dd" id="48dd" class="graf graf--h3 graf-after--li"><a href="https://github.com/fastai/fastai/blob/master/courses/dl1/lesson7-cifar10.ipynb" data-href="https://github.com/fastai/fastai/blob/master/courses/dl1/lesson7-cifar10.ipynb" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">Back to computer vision: CIFAR 10</a> [<a href="https://youtu.be/H3g26EVADgY?t=1h1m58s" data-href="https://youtu.be/H3g26EVADgY?t=1h1m58s" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">1:01:58</a>]</h3><p name="da72" id="da72" class="graf graf--p graf-after--h3">CIFAR 10 is an old and well known dataset in academia — well before ImageNet, there was CIFAR 10. It is small both in terms of number of images and size of images which makes it interesting and challenging. You will likely be working with thousands of images rather than one and a half million images. Also a lot of the things we are looking at like in medical imaging, we are looking at a specific area where there is a lung nodule, you are probably looking at 32 by 32 pixels at most.</p><p name="04f9" id="04f9" class="graf graf--p graf-after--p">It also runs quickly, so it is much better to test our your algorithms. As Ali Rahini mentioned in NIPS 2017, Jeremy has the concern that many people are not doing carefully tuned and throught-about experiments in deep learning, but instead, they throw lots of GPUs and TPUs or lots of data and consider that a day. It is important to test many versions of your algorithm on dataset like CIFAR 10 rather than ImageNet that takes weeks. MNIST is also good for studies and experiments even though people tend to complain about it.</p><p name="d725" id="d725" class="graf graf--p graf-after--p">CIFAR 10 data in image format is available <a href="http://pjreddie.com/media/files/cifar.tgz" data-href="http://pjreddie.com/media/files/cifar.tgz" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">here</a></p><pre name="75a0" id="75a0" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">fastai.conv_learner</strong> <strong class="markup--strong markup--pre-strong">import</strong> *<br>PATH = "data/cifar10/"<br>os.makedirs(PATH,exist_ok=<strong class="markup--strong markup--pre-strong">True</strong>)</pre><pre name="9eb0" id="9eb0" class="graf graf--pre graf-after--pre">classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')<br>stats = (np.array([ 0.4914 ,  0.48216,  0.44653]), np.array([ 0.24703,  0.24349,  0.26159]))</pre><pre name="fb08" id="fb08" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> get_data(sz,bs):<br>     tfms = <strong class="markup--strong markup--pre-strong">tfms_from_stats</strong>(stats, sz, aug_tfms=[RandomFlipXY()], pad=sz//8)<br>     <strong class="markup--strong markup--pre-strong">return</strong> ImageClassifierData.from_paths(PATH, val_name='test', tfms=tfms, bs=bs)</pre><pre name="c93a" id="c93a" class="graf graf--pre graf-after--pre">bs=256</pre><ul class="postList"><li name="a707" id="a707" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">classes</code> — image labels</li><li name="6af1" id="6af1" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">stats</code> —When we use pre-trained models, you can call <code class="markup--code markup--li-code">tfms_from_model</code> which creates the necessary transforms to convert our data set into a normalized dataset based on the means and standard deviations of each channel in the original model that was trained in. Since we are training a model from scratch, we ned to tell it the mean and standard deviation of our data to normalize it. Make sure you can calculate the mean and the standard deviation for each channel.</li><li name="a90b" id="a90b" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">tfms</code> — For CIFAR 10 data augmentation, people typically do horizontal flip and black padding around the edge and randomly select 32 by 32 area within the padded image.</li></ul><pre name="a4dc" id="a4dc" class="graf graf--pre graf-after--li">data = get_data(32,bs)<br><br>lr=1e-2</pre><p name="8e48" id="8e48" class="graf graf--p graf-after--pre">From <a href="https://github.com/KeremTurgutlu/deeplearning/blob/master/Exploring%20Optimizers.ipynb" data-href="https://github.com/KeremTurgutlu/deeplearning/blob/master/Exploring%20Optimizers.ipynb" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">this notebook</a> by our student Kerem Turgutlu:</p><pre name="9fca" id="9fca" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">SimpleNet</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, layers):<br>        super().__init__()<br>        self.layers = <strong class="markup--strong markup--pre-strong">nn.ModuleList</strong>([<br>            nn.Linear(layers[i], layers[i + 1]) <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(len(layers) - 1)])<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x):<br>        x = x.view(x.size(0), -1)<br>        <strong class="markup--strong markup--pre-strong">for</strong> l <strong class="markup--strong markup--pre-strong">in</strong> self.layers:<br>            l_x = l(x)<br>            x = F.relu(l_x)<br>        <strong class="markup--strong markup--pre-strong">return</strong> F.log_softmax(l_x, dim=-1)</pre><ul class="postList"><li name="42f7" id="42f7" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">nn.ModuleList</code> — whenever you create a list of layers in PyTorch, you have to wrap it in <code class="markup--code markup--li-code">ModuleList</code> to register these as attributes.</li></ul><pre name="afaa" id="afaa" class="graf graf--pre graf-after--li">learn = ConvLearner.from_model_data(SimpleNet([32*32*3, 40,10]), data)</pre><ul class="postList"><li name="013f" id="013f" class="graf graf--li graf-after--pre">Now we step up one level of API higher — rather than calling <code class="markup--code markup--li-code">fit</code> function, we create a <code class="markup--code markup--li-code">learn</code> object <em class="markup--em markup--li-em">from a custom model</em>. <code class="markup--code markup--li-code">ConfLearner.from_model_data</code> takes standard PyTorch model and model data object.</li></ul><pre name="0fa9" id="0fa9" class="graf graf--pre graf-after--li">learn, [o.numel() <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> learn.model.parameters()]</pre><pre name="2012" id="2012" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(SimpleNet(<br>   (layers): ModuleList(<br>     (0): Linear(in_features=3072, out_features=40)<br>     (1): Linear(in_features=40, out_features=10)<br>   )<br> ), [122880, 40, 400, 10])</em></pre><pre name="85fd" id="85fd" class="graf graf--pre graf-after--pre">learn.summary()</pre><pre name="88f0" id="88f0" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">OrderedDict([('Linear-1',<br>              OrderedDict([('input_shape', [-1, 3072]),<br>                           ('output_shape', [-1, 40]),<br>                           ('trainable', True),<br>                           ('nb_params', 122920)])),<br>             ('Linear-2',<br>              OrderedDict([('input_shape', [-1, 40]),<br>                           ('output_shape', [-1, 10]),<br>                           ('trainable', True),<br>                           ('nb_params', 410)]))])</em></pre><pre name="3f7e" id="3f7e" class="graf graf--pre graf-after--pre">learn.lr_find()</pre><pre name="b7af" id="b7af" class="graf graf--pre graf-after--pre">learn.sched.plot()</pre><figure name="f8c3" id="f8c3" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1__5sTAdoWHTBQUzbaVrc4HA.png"></figure><pre name="15f5" id="15f5" class="graf graf--pre graf-after--figure">%time learn.fit(lr, 2)</pre><pre name="e001" id="e001" class="graf graf--pre graf-after--pre">A Jupyter Widget</pre><pre name="81cd" id="81cd" class="graf graf--pre graf-after--pre">[ 0.       1.7658   1.64148  0.42129]                       <br>[ 1.       1.68074  1.57897  0.44131]                       <br><br>CPU times: user 1min 11s, sys: 32.3 s, total: 1min 44s<br>Wall time: 55.1 s</pre><pre name="8ec0" id="8ec0" class="graf graf--pre graf-after--pre">%time learn.fit(lr, 2, cycle_len=1)</pre><pre name="a5d9" id="a5d9" class="graf graf--pre graf-after--pre">A Jupyter Widget</pre><pre name="4db5" id="4db5" class="graf graf--pre graf-after--pre">[ 0.       1.60857  1.51711  0.46631]                       <br>[ 1.       1.59361  1.50341  0.46924]                       <br><br>CPU times: user 1min 12s, sys: 31.8 s, total: 1min 44s<br>Wall time: 55.3 s</pre><p name="d12e" id="d12e" class="graf graf--p graf-after--pre">With a simple one hidden layer model with 122,880 parameters, we achieved 46.9% accuracy. Let’s improve this and gradually build up to a basic ResNet architecture.</p><h4 name="e35d" id="e35d" class="graf graf--h4 graf-after--p">CNN [<a href="https://youtu.be/H3g26EVADgY?t=1h12m30s" data-href="https://youtu.be/H3g26EVADgY?t=1h12m30s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">01:12:30</a>]</h4><ul class="postList"><li name="f56f" id="f56f" class="graf graf--li graf-after--h4">Let’s replace a fully connected model with a convolutional model. Fully connected layer is simply doing a dot product. That is why the weight matrix is big (3072 input * 40 = 122880). We are not using the parameters very efficiently because every single pixel in the input has a different weight. What we want to do is a group of 3 by 3 pixels that have particular patterns to them (i.e. convolution).</li><li name="c4c9" id="c4c9" class="graf graf--li graf-after--li">We will use a filter with three by three kernel. When there are multiple filters, the output will have additional dimension.</li></ul><pre name="956d" id="956d" class="graf graf--pre graf-after--li"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">ConvNet</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, layers, c):<br>        super().__init__()<br>        self.layers = nn.ModuleList([<br>            <strong class="markup--strong markup--pre-strong">nn.Conv2d(layers[i], layers[i + 1], kernel_size=3, stride=2)</strong><br>            <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(len(layers) - 1)])<br>        self.pool = nn.AdaptiveMaxPool2d(1)<br>        self.out = nn.Linear(layers[-1], c)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x):<br>        <strong class="markup--strong markup--pre-strong">for</strong> l <strong class="markup--strong markup--pre-strong">in</strong> self.layers: x = F.relu(l(x))<br>        x = self.pool(x)<br>        x = x.view(x.size(0), -1)<br>        <strong class="markup--strong markup--pre-strong">return</strong> F.log_softmax(self.out(x), dim=-1)</pre><ul class="postList"><li name="ae05" id="ae05" class="graf graf--li graf-after--pre">Replace <code class="markup--code markup--li-code">nn.Linear</code> with <code class="markup--code markup--li-code">nn.Conv2d</code></li><li name="b44e" id="b44e" class="graf graf--li graf-after--li">First two parameters are exactly the same as <code class="markup--code markup--li-code">nn.Linear</code> — the number of features coming in, and the number of features coming out</li><li name="7e41" id="7e41" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">kernel_size=3</code>&nbsp;, the size of the filter</li><li name="fd62" id="fd62" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">stride=2</code> will use every other 3 by 3 area which will halve the output resolution in each dimension (i.e. it has the same effect as 2 by 2 max-pooling)</li></ul><pre name="cf96" id="cf96" class="graf graf--pre graf-after--li">learn = ConvLearner.from_model_data(ConvNet([3, 20, 40, 80], 10), data)</pre><pre name="efa6" id="efa6" class="graf graf--pre graf-after--pre">learn.summary()</pre><pre name="6614" id="6614" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">OrderedDict([('Conv2d-1',<br>              OrderedDict([('input_shape', [-1, 3, 32, 32]),<br>                           ('output_shape', [-1, 20, 15, 15]),<br>                           ('trainable', True),<br>                           ('nb_params', 560)])),<br>             ('Conv2d-2',<br>              OrderedDict([('input_shape', [-1, 20, 15, 15]),<br>                           ('output_shape', [-1, 40, 7, 7]),<br>                           ('trainable', True),<br>                           ('nb_params', 7240)])),<br>             ('Conv2d-3',<br>              OrderedDict([('input_shape', [-1, 40, 7, 7]),<br>                           ('output_shape', [-1, 80, 3, 3]),<br>                           ('trainable', True),<br>                           ('nb_params', 28880)])),<br>             ('AdaptiveMaxPool2d-4',<br>              OrderedDict([('input_shape', [-1, 80, 3, 3]),<br>                           ('output_shape', [-1, 80, 1, 1]),<br>                           ('nb_params', 0)])),<br>             ('Linear-5',<br>              OrderedDict([('input_shape', [-1, 80]),<br>                           ('output_shape', [-1, 10]),<br>                           ('trainable', True),<br>                           ('nb_params', 810)]))])</em></pre><ul class="postList"><li name="0106" id="0106" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">ConvNet([3, 20, 40, 80], 10)</code> — It start with 3 RGB channels, 20, 40, 80 features, then 10 classes to predict.</li><li name="7f89" id="7f89" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">AdaptiveMaxPool2d</code> — This followed by a linear layer is how you get from 3 by 3 down to a prediction of one of 10 classes and is now a standard for state-of-the-art algorithms. The very last layer, we do a special kind of max-pooling for which you specify the output activation resolution rather than how big of an area to poll. In other words, here we do 3 by 3 max-pool which is equivalent of 1 by 1 <em class="markup--em markup--li-em">adaptive</em> max-pool.</li><li name="dcf2" id="dcf2" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">x = x.view(x.size(0), -1)</code> — <code class="markup--code markup--li-code">x</code> has a shape of # of the features by 1 by 1, so it will remove the last two layers.</li><li name="00b5" id="00b5" class="graf graf--li graf-after--li">This model is called “fully convolutional network” — where every layer is convolutional except for the very last.</li></ul><pre name="5c4c" id="5c4c" class="graf graf--pre graf-after--li">learn.lr_find(<strong class="markup--strong markup--pre-strong">end_lr=100</strong>)<br>learn.sched.plot()</pre><figure name="1e20" id="1e20" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_YuNvyUac9HvAv0XZn08-3g.png"></figure><ul class="postList"><li name="eb07" id="eb07" class="graf graf--li graf-after--figure">The default final learning rate <code class="markup--code markup--li-code">lr_find</code> tries is 10. If the loss is still getting better at that point, you can overwrite by specifying <code class="markup--code markup--li-code">end_lr</code>&nbsp;.</li></ul><pre name="1c6f" id="1c6f" class="graf graf--pre graf-after--li">%time learn.fit(1e-1, 2)</pre><pre name="5546" id="5546" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">A Jupyter Widget</em></pre><pre name="f992" id="f992" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[ 0.       1.72594  1.63399  0.41338]                       <br>[ 1.       1.51599  1.49687  0.45723]                       <br><br>CPU times: user 1min 14s, sys: 32.3 s, total: 1min 46s<br>Wall time: 56.5 s</em></pre><pre name="3b9b" id="3b9b" class="graf graf--pre graf-after--pre">%time learn.fit(1e-1, 4, cycle_len=1)</pre><pre name="d9f2" id="d9f2" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">A Jupyter Widget</em></pre><pre name="af51" id="af51" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[ 0.       1.36734  1.28901  0.53418]                       <br>[ 1.       1.28854  1.21991  0.56143]                       <br>[ 2.       1.22854  1.15514  0.58398]                       <br>[ 3.       1.17904  1.12523  0.59922]                       <br><br>CPU times: user 2min 21s, sys: 1min 3s, total: 3min 24s<br>Wall time: 1min 46s</em></pre><ul class="postList"><li name="bf9c" id="bf9c" class="graf graf--li graf-after--pre">It flattened out around 60% accuracy. Considering it uses about 30,000 parameters (compared to 47% with 122k parameters)</li><li name="49ed" id="49ed" class="graf graf--li graf-after--li">Time per epoch is about the same since their architectures are both simple and most of time is spent doing memory transfer.</li></ul><h4 name="cc39" id="cc39" class="graf graf--h4 graf-after--li">Refactored [<a href="https://youtu.be/H3g26EVADgY?t=1h21m57s" data-href="https://youtu.be/H3g26EVADgY?t=1h21m57s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">01:21:57</a>]</h4><p name="340e" id="340e" class="graf graf--p graf-after--h4">Simplify <code class="markup--code markup--p-code">forward</code> function by creating <code class="markup--code markup--p-code">ConvLayer</code> (our first custom layer!). In PyTorch, layer definition and neural network definitions are identical. Anytime you have a layer, you can use it as a neural net, when you have a neural net, you can use it as a layer.</p><pre name="41d6" id="41d6" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">ConvLayer</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, ni, nf):<br>        super().__init__()<br>        self.conv = nn.Conv2d(ni, nf, kernel_size=3, stride=2, padding=1)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x): <strong class="markup--strong markup--pre-strong">return</strong> F.relu(self.conv(x))</pre><ul class="postList"><li name="393b" id="393b" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">padding=1</code> — When you do convolution the image shrink by 1 pixel on each side. So it does not go from 32 by 32 to 16 by 16 but actually 15 by 15. <code class="markup--code markup--li-code">padding</code> will add a border so we can keep the edge pixel information. It is not as big of a deal for a big image, but when it’s down to 4 by 4, you really don’t want to throw away a whole piece.</li></ul><pre name="d8b6" id="d8b6" class="graf graf--pre graf-after--li"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">ConvNet2</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, layers, c):<br>        super().__init__()<br>        self.layers = nn.ModuleList([ConvLayer(layers[i], layers[i + 1])<br>            <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(len(layers) - 1)])<br>        self.out = nn.Linear(layers[-1], c)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x):<br>        <strong class="markup--strong markup--pre-strong">for</strong> l <strong class="markup--strong markup--pre-strong">in</strong> self.layers: x = l(x)<br>        x = <strong class="markup--strong markup--pre-strong">F.adaptive_max_pool2d(x, 1)</strong><br>        x = x.view(x.size(0), -1)<br>        <strong class="markup--strong markup--pre-strong">return</strong> F.log_softmax(self.out(x), dim=-1)</pre><ul class="postList"><li name="89d9" id="89d9" class="graf graf--li graf-after--pre">Another difference from the last model is that <code class="markup--code markup--li-code">nn.AdaptiveMaxPool2d</code> does not have any state (i.e. no weights). So we can just call it as a function <code class="markup--code markup--li-code">F.adaptive_max_pool2d</code>&nbsp;.</li></ul><h4 name="acdf" id="acdf" class="graf graf--h4 graf-after--li">BatchNorm [<a href="https://youtu.be/H3g26EVADgY?t=1h25m10s" data-href="https://youtu.be/H3g26EVADgY?t=1h25m10s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:25:10</a>]</h4><ul class="postList"><li name="6334" id="6334" class="graf graf--li graf-after--h4">The last model, when we tried to add more layers, we had trouble training. The reason we had trouble training was that if we used larger learning rates, it would go off to NaN and if we used smaller learning rate, it would take forever and doesn’t have a chance to explore properly — so it was not resilient.</li><li name="82ea" id="82ea" class="graf graf--li graf-after--li">To make it resilient, we will use something called batch normalization. BatchNorm came out about two years ago and it has been quite transformative since it suddenly makes it really easy to train deeper networks.</li><li name="e576" id="e576" class="graf graf--li graf-after--li">We can simply use <code class="markup--code markup--li-code">nn.BatchNorm</code> but to learn about it, we will write it from scratch.</li><li name="786a" id="786a" class="graf graf--li graf-after--li">It is unlikely that the weight matrices on average are not going to cause your activations to keep getting smaller and smaller or keep getting bigger and bigger. It is important to keep them at reasonable scale. So we start things off with zero-mean standard deviation one by normalizing the input. What we really want to do is to do this for all layers, not just the inputs.</li></ul><pre name="e8b6" id="e8b6" class="graf graf--pre graf-after--li"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">BnLayer</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, ni, nf, stride=2, kernel_size=3):<br>        super().__init__()<br>        self.conv = nn.Conv2d(ni, nf, kernel_size=kernel_size, <br>                              stride=stride, bias=<strong class="markup--strong markup--pre-strong">False</strong>, padding=1)<br>        self.a = nn.Parameter(torch.zeros(nf,1,1))<br>        self.m = nn.Parameter(torch.ones(nf,1,1))<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x):<br>        x = F.relu(self.conv(x))<br>        x_chan = x.transpose(0,1).contiguous().view(x.size(1), -1)<br>        <strong class="markup--strong markup--pre-strong">if</strong> self.training:<br>            <strong class="markup--strong markup--pre-strong">self.means = x_chan.mean(1)[:,None,None]</strong><br>           <strong class="markup--strong markup--pre-strong"> self.stds  = x_chan.std (1)[:,None,None]</strong><br>        <strong class="markup--strong markup--pre-strong">return</strong> <strong class="markup--strong markup--pre-strong">(x-self.means) / self.stds *self.m + self.a</strong></pre></body></html>