<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></head><body><p name="1811" id="1811" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: Can GANs be used for data augmentation [<a href="https://youtu.be/ondivPiwQho?t=1h45m33s" data-href="https://youtu.be/ondivPiwQho?t=1h45m33s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:45:33</a>]? Yeah, absolutely you can use GAN for data augmentation. Should you? I don’t know. There are some papers that try to do semi-supervised learning with GANs. I haven’t found any that are particularly compelling showing state-of-the-art results on really interesting datasets that have been widely studied. I’m a little skeptical and the reason I’m a little skeptical is because in my experience, if you train a model with synthetic data, the neural net will become fantastically good at recognizing the specific problems of your synthetic data and that’ll end up what it’s learning from. There are lots of other ways of doing semi-supervised models which do work well. There are some places that can work. For example, you might remember Otavio Good created that fantastic visualization in part 1 of the zooming conv net where it showed letter going through MNIST, he, at least at that time, was the number one in autonomous remote control car competitions, and he trained his model using synthetically augmented data where he basically took real videos of a car driving around the circuit and added fake people and fake other cars. I think that worked well because A. he is kind of a genius and B. because I think he had a well defined little subset that he had to work in. But in general, it’s really really hard to use synthetic data. I’ve tried using synthetic data and models for decades now (obviously not GANs because they’re pretty new) but in general it’s very hard to do. Very interesting research question.</p><h3 name="520c" id="520c" class="graf graf--h3 graf-after--p">Cycle GAN [<a href="https://youtu.be/ondivPiwQho?t=1h41m8s" data-href="https://youtu.be/ondivPiwQho?t=1h41m8s" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">1:41:08</a>]</h3><p name="f1dd" id="f1dd" class="graf graf--p graf-after--h3"><a href="https://arxiv.org/abs/1703.10593" data-href="https://arxiv.org/abs/1703.10593" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Paper</a> / <a href="https://github.com/fastai/fastai/blob/master/courses/dl2/cyclegan.ipynb" data-href="https://github.com/fastai/fastai/blob/master/courses/dl2/cyclegan.ipynb" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Notebook</a></p><p name="a6c8" id="a6c8" class="graf graf--p graf-after--p">We are going to use cycle GAN to turn horses into zebras. You can also use it to turn Monet prints into photos or to turn photos of Yosemite in summer into winter.</p><figure name="239a" id="239a" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_dWd0lVTbnu80UZM641gCbw.gif"></figure><p name="6582" id="6582" class="graf graf--p graf-after--figure">This is going to be really straight forward because it’s just a neural net [<a href="https://youtu.be/ondivPiwQho?t=1h44m46s" data-href="https://youtu.be/ondivPiwQho?t=1h44m46s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:44:46</a>]. All we are going to do is we are going to create an input containing lots of zebra photos and with each one we’ll pair it with an equivalent horse photo and we’ll just train a neural net that goes from one to the other. Or you could do the same thing for every Monet painting — create a dataset containing the photo of the place&nbsp;…oh wait, that’s not possible because the places that Monet painted aren’t there anymore and there aren’t exact zebra versions of horses&nbsp;…how the heck is this going to work? This seems to break everything we know about what neural nets can do and how they do them.</p><p name="e271" id="e271" class="graf graf--p graf-after--p">So somehow these folks at Berkeley cerated a model that can turn a horse into a zebra despite not having any photos. Unless they went out there and painted horses and took before-and-after shots but I believe they didn’t [<a href="https://youtu.be/ondivPiwQho?t=1h47m51s" data-href="https://youtu.be/ondivPiwQho?t=1h47m51s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:47:51</a>]. So how the heck did they do this? It’s kind of genius.</p><p name="e0f6" id="e0f6" class="graf graf--p graf-after--p">The person I know who is doing the most interesting practice of cycle GAN right now is one of our students Helena Sarin <a href="https://twitter.com/glagolista" data-href="https://twitter.com/glagolista" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong">@</strong>glagolista</a>. She is the only artist I know of who is a cycle GAN artist.</p><figure name="c22d" id="c22d" class="graf graf--figure graf--layoutOutsetRow is-partialWidth graf-after--p" style="width: 49.105%;"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_y0xHbQJvxcwUsx7EEK4nHQ.jpeg"></figure><figure name="a429" id="a429" class="graf graf--figure graf--layoutOutsetRowContinue is-partialWidth graf-after--figure" style="width: 50.895%;"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_QZWqdoLXR1TjgeWDivTlnA.jpeg"></figure><figure name="8521" id="8521" class="graf graf--figure graf--layoutOutsetRow is-partialWidth graf-after--figure" style="width: 41.493%;"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_JIF1OaO04wxkWIP_7b14uA.jpeg"></figure><figure name="be01" id="be01" class="graf graf--figure graf--layoutOutsetRowContinue is-partialWidth graf-after--figure" style="width: 58.507%;"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_xn7L_rsu2J6Py2Mjq_q1LA.jpeg"></figure><p name="4467" id="4467" class="graf graf--p graf-after--figure">Here are some more of her amazing works and I think it’s really interesting. I mentioned at the start of this class that GANs are in the category of stuff that is not there yet, but it’s nearly there. And in this case, there is at least one person in the world who is creating beautiful and extraordinary artworks using GANs (specifically cycle GANs). At least a dozen people I know of who are just doing interesting creative work with neural nets more generally. And the field of creative AI is going to expand dramatically.</p><figure name="9c03" id="9c03" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_oqSRuiHT8Z9pWl0Zq9_Sjw.png"></figure><p name="349f" id="349f" class="graf graf--p graf-after--figure">Here is the basic trick [<a href="https://youtu.be/ondivPiwQho?t=1h50m11s" data-href="https://youtu.be/ondivPiwQho?t=1h50m11s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:50:11</a>]. This is from the cycle GAN paper. We are going to have two images (assuming we are doing this with images). The key thing is they are not paired images, so we don’t have a dataset of horses and the equivalent zebras. We have bunch of horses, and bunch of zebras. Grab one horse <em class="markup--em markup--p-em">X</em>, grab one zebra <em class="markup--em markup--p-em">Y</em>. We are going to train a generator (what they call here a “mapping function”) that turns horse into zebra. We’ll call that mapping function <em class="markup--em markup--p-em">G</em> and we’ll create one mapping function (a.k.a. generator) that turns a zebra into a horse and we will call that <em class="markup--em markup--p-em">F. </em>We will create a discriminator just like we did before which is going to get as good as possible at recognizing real from fake horses so that will be <em class="markup--em markup--p-em">Dx. </em>Another discriminator which is going to be as good as possible at recognizing real from fake zebras, we will call that <em class="markup--em markup--p-em">Dy</em>. That is our starting point.</p><p name="146d" id="146d" class="graf graf--p graf-after--p">The key thing to making this work [<a href="https://youtu.be/ondivPiwQho?t=1h51m27s" data-href="https://youtu.be/ondivPiwQho?t=1h51m27s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:51:27</a>]— so we are generating a loss function here (<em class="markup--em markup--p-em">Dx</em> and <em class="markup--em markup--p-em">Dy</em>). We are going to create something called <strong class="markup--strong markup--p-strong">cycle-consistency loss</strong> which says after you turn your horse into a zebra with your generator, and check whether or not I can recognize that it’s a real. We turn our horse into a zebra and then going to try and turn that zebra back into the same horse that we started with. Then we are going to have another function that is going to check whether this horse which are generated knowing nothing about <em class="markup--em markup--p-em">x</em> — generated entirely from this zebra <em class="markup--em markup--p-em">Y </em>is similar to the original horse or not. So the idea would be if your generated zebra doesn’t look anything like your original horse, you’ve got no chance of turning it back into the original horse. So a loss which compares <em class="markup--em markup--p-em">x-hat</em> to <em class="markup--em markup--p-em">x</em> is going to be really bad unless you can go into <em class="markup--em markup--p-em">Y</em> and back out again and you’re probably going to be able to do that if you’re able to create a zebra that looks like the original horse so that you know what the original horse looked like. And vice versa — take your zebra, turn it into a fake horse, and check that you can recognize that and then try and turn it back into the original zebra and check that it looks like the original.</p><p name="ed14" id="ed14" class="graf graf--p graf-after--p">So notice <em class="markup--em markup--p-em">F</em> (zebra to horse) and <em class="markup--em markup--p-em">G</em> (horse to zebra) are doing two things [<a href="https://youtu.be/ondivPiwQho?t=1h53m9s" data-href="https://youtu.be/ondivPiwQho?t=1h53m9s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:53:09</a>]. They are both turning the original horse into the zebra, and then turning the zebra back into the original horse. So there are only two generators. There isn’t a separate generator for the reverse mapping. You have to use the same generator that was used for the original mapping. So this is the cycle-consistency loss. I think this is genius. The idea that this is a thing that could even be possible. Honestly when this came out, it just never occurred to me as a thing that I could even try and solve. It seems so obviously impossible and then the idea that you can solve it like this — I just think it’s so darn smart.</p><p name="2d94" id="2d94" class="graf graf--p graf-after--p">It’s good to look at the equations in this paper because they are good examples — they are written pretty simply and it’s not like some of the Wasserstein GAN paper which is lots of theoretical proofs and whatever else [<a href="https://youtu.be/ondivPiwQho?t=1h54m5s" data-href="https://youtu.be/ondivPiwQho?t=1h54m5s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:54:05</a>]. In this case, they are just equations that lay out what’s going on. You really want to get to a point where you can read them and understand them.</p><figure name="96ba" id="96ba" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_Mygxs_TWrjycbanbH5aUeQ.png"></figure><p name="e152" id="e152" class="graf graf--p graf-after--figure">So we’ve got a horse <em class="markup--em markup--p-em">X</em> and a zebra <em class="markup--em markup--p-em">Y</em>[<a href="https://youtu.be/ondivPiwQho?t=1h54m34s" data-href="https://youtu.be/ondivPiwQho?t=1h54m34s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:54:34</a>]. For some mapping function <em class="markup--em markup--p-em">G</em> which is our horse to zebra mapping function then there is a GAN loss which is a bit we are already familiar with it says we have a horse, a zebra, a fake zebra recognizer, and a horse-zebra generator. The loss is what we saw before — it’s our ability to draw one zebra out of our zebras and recognize whether it is real or fake. Then take a horse and turn it into a zebra and recognize whether that’s real or fake. You then do one minus the other (in this case, they have a log in there but the log is not terribly important). So this is the thing we just saw. That is why we did Wasserstein GAN first. This is just a standard GAN loss in math form.</p><p name="8979" id="8979" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: All of this sounds awfully like translating in one language to another then back to the original. Have GANs or any equivalent been tried in translation [<a href="https://youtu.be/ondivPiwQho?t=1h55m54s" data-href="https://youtu.be/ondivPiwQho?t=1h55m54s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:55:54</a>]? <a href="https://arxiv.org/abs/1711.00043" data-href="https://arxiv.org/abs/1711.00043" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">Paper from the forum</a>. Back up to what I do know — normally with translation you require this kind of paired input (i.e. parallel text — “this is the French translation of this English sentence”). There has been a couple of recent papers that show the ability to create good quality translation models without paired data. I haven’t implemented them and I don’t understand anything I haven’t implemented, but they may well be doing the same basic idea. We’ll look at it during the week and get back to you.</p><p name="3009" id="3009" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Cycle-consistency loss</strong> [<a href="https://youtu.be/ondivPiwQho?t=1h57m14s" data-href="https://youtu.be/ondivPiwQho?t=1h57m14s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:57:14</a>]: So we’ve got a GAN loss and the next piece is the cycle-consistency loss. So the basic idea here is that we start with our horse, use our zebra generator on that to create a zebra, use our horse generator on that to create a horse and compare that to the original horse. This double lines with the 1 is the L1 loss — sum of the absolute value of differences [<a href="https://youtu.be/ondivPiwQho?t=1h57m35s" data-href="https://youtu.be/ondivPiwQho?t=1h57m35s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:57:35</a>]. Where else if this was 2, it would be the L2 loss so the 2-norm which would be the sum of the squared differences.</p><figure name="c900" id="c900" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_0wq511kW9eRhBMWS94G0Bw.png"></figure><p name="193d" id="193d" class="graf graf--p graf-after--figure">We now know this squiggle idea which is from our horses grab a horse. This is what we mean by sample from a distribution. There’s all kinds of distributions but most commonly in these papers we’re using an empirical distribution, in other words we’ve got some rows of data, grab a row. So here, it is saying grab something from the data and we are going to call that thing <em class="markup--em markup--p-em">x</em>. To recapture:</p><ol class="postList"><li name="df84" id="df84" class="graf graf--li graf-after--p">From our horse pictures, grab a horse</li><li name="c679" id="c679" class="graf graf--li graf-after--li">Turn it into a zebra</li><li name="8318" id="8318" class="graf graf--li graf-after--li">Turn it back into a horse</li><li name="8543" id="8543" class="graf graf--li graf-after--li">Compare it to the original and sum of the absolute values</li><li name="c6fa" id="c6fa" class="graf graf--li graf-after--li">Do it for zebra to horse as well</li><li name="44c8" id="44c8" class="graf graf--li graf-after--li">And add the two together</li></ol><p name="6f8b" id="6f8b" class="graf graf--p graf-after--li">That is our cycle-consistency loss.</p><p name="0993" id="0993" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Full objective</strong> [<a href="https://youtu.be/ondivPiwQho?t=1h58m54s" data-href="https://youtu.be/ondivPiwQho?t=1h58m54s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:58:54</a>]</p><figure name="3bfd" id="3bfd" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_84eYJ5eck_7r3zVJzrzGzA.png"></figure><p name="9647" id="9647" class="graf graf--p graf-after--figure">Now we get our loss function and the whole loss function depends on:</p><ul class="postList"><li name="836d" id="836d" class="graf graf--li graf-after--p">our horse generator</li><li name="5bd6" id="5bd6" class="graf graf--li graf-after--li">a zebra generator</li><li name="eb3b" id="eb3b" class="graf graf--li graf-after--li">our horse recognizer</li><li name="31fd" id="31fd" class="graf graf--li graf-after--li">our zebra recognizer (a.k.a. discriminator)</li></ul><p name="0b41" id="0b41" class="graf graf--p graf-after--li">We are going to add up&nbsp;:</p><ul class="postList"><li name="c459" id="c459" class="graf graf--li graf-after--p">the GAN loss for recognizing horses</li><li name="b5ea" id="b5ea" class="graf graf--li graf-after--li">GAN loss for recognizing zebras</li><li name="d133" id="d133" class="graf graf--li graf-after--li">the cycle-consistency loss for our two generators</li></ul><p name="2933" id="2933" class="graf graf--p graf-after--li">We have a lambda here which hopefully we are kind of used to this idea now that is when you have two different kinds of loss, you chuck in a parameter there you can multiply them by so they are about the same scale [<a href="https://youtu.be/ondivPiwQho?t=1h59m23s" data-href="https://youtu.be/ondivPiwQho?t=1h59m23s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:59:23</a>]. We did a similar thing with our bounding box loss compared to our classifier loss when we did the localization.</p><p name="fa0c" id="fa0c" class="graf graf--p graf-after--p">Then for this loss function, we are going to try to maximize the capability of the discriminators to discriminate, whilst minimizing that for the generators. So the generators and the discriminators are going to be facing off against each other. When you see this <em class="markup--em markup--p-em">min max </em>thing in papers, it basically means this idea that in your training loop, one thing is trying to make something better, the other is trying to make something worse, and there’re lots of ways to do it but most commonly, you’ll alternate between the two. You will often see this just referred to in math papers as min-max. So when you see min-max, you should immediately think <strong class="markup--strong markup--p-strong">adversarial training</strong>.</p><h4 name="3df1" id="3df1" class="graf graf--h4 graf-after--p">Implementing cycle GAN [<a href="https://youtu.be/ondivPiwQho?t=2h41s" data-href="https://youtu.be/ondivPiwQho?t=2h41s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">2:00:41</a>]</h4><p name="a096" id="a096" class="graf graf--p graf-after--h4">Let’s look at the code. We are going to do something almost unheard of which is I started looking at somebody else’s code and I was not so disgusted that I threw the whole thing away and did it myself. I actually said I quite like this, I like it enough I’m going to show it to my students. <a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix" data-href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">This</a> is where the code came from, and this is one of the people that created the original code for cycle GANs and they created a PyTorch version. I had to clean it up a little bit but it’s actually pretty darn good. The cool thing about this is that you are now going to get to see almost all the bits of fast.ai or all the relevant bits of fast.ai written in a different way by somebody else. So you’re going to get to see how they do datasets, data loaders, models, training loops, and so forth.</p><p name="56cc" id="56cc" class="graf graf--p graf-after--p">You’ll find there is a <code class="markup--code markup--p-code">cgan</code> directory [<a href="https://youtu.be/ondivPiwQho?t=2h2m12s" data-href="https://youtu.be/ondivPiwQho?t=2h2m12s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:02:12</a>] which is basically nearly the original with some cleanups which I hope to submit as a PR sometime&nbsp;. It was written in a way that unfortunately made it a bit over connected to how they were using it as a script, so I cleaned it up a little bit so I could use it as a module. But other than that, it’s pretty similar.</p><pre name="e22b" id="e22b" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">fastai.conv_learner</strong> <strong class="markup--strong markup--pre-strong">import</strong> *<br><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">fastai.dataset</strong> <strong class="markup--strong markup--pre-strong">import</strong> *<br><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">cgan.options.train_options</strong> <strong class="markup--strong markup--pre-strong">import</strong> *</pre><p name="778a" id="778a" class="graf graf--p graf-after--pre">So <code class="markup--code markup--p-code">cgan</code> is their code copied from their github repo with some minor changes. The way <code class="markup--code markup--p-code">cgan</code> mini library has been set up is that the configuration options, they are assuming, are being passed into like a script. So they have <code class="markup--code markup--p-code">TrainOptions().parse</code> method and I’m basically passing in an array of script options (where’s my data, how many threads, do I want to dropout, how many iterations, what am I going to call this model, which GPU do I want run it on). That gives us an <code class="markup--code markup--p-code">opt</code> object which you can see what it contains. You’ll see that it contains some things we didn’t mention that is because it has defaults for everything else that we didn’t mention.</p><pre name="41b7" id="41b7" class="graf graf--pre graf-after--p">opt = TrainOptions().parse(['--dataroot',    <br>   '/data0/datasets/cyclegan/horse2zebra', '--nThreads', '8', <br>   '--no_dropout', '--niter', '100', '--niter_decay', '100', <br>   '--name', 'nodrop', '--gpu_ids', '2'])</pre><p name="60aa" id="60aa" class="graf graf--p graf-after--pre">So rather than using fast.ai stuff, we are going to largely use cgan stuff.</p><pre name="98b8" id="98b8" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">cgan.data.data_loader</strong> <strong class="markup--strong markup--pre-strong">import</strong> CreateDataLoader<br><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">cgan.models.models</strong> <strong class="markup--strong markup--pre-strong">import</strong> create_model</pre><p name="dc65" id="dc65" class="graf graf--p graf-after--pre">The first thing we are going to need is a data loader. So this is also a great opportunity for you again to practice your ability to navigate through code with your editor or IDE of choice. We are going to start with <code class="markup--code markup--p-code">CreateDataLoader</code>. You should be able to go find symbol or in vim tag to jump straight to <code class="markup--code markup--p-code">CreateDataLoader</code> and we can see that’s creating a <code class="markup--code markup--p-code">CustomDatasetDataLoader</code>. Then we can see <code class="markup--code markup--p-code">CustomDatasetDataLoader</code> is a <code class="markup--code markup--p-code">BaseDataLoader</code>. We can see that it’s going to use a standard PyTorch DataLoader, so that’s good. We know if you are going to use a standard PyTorch DataLoader, you have pass it a dataset, and we know that a dataset is something that contains a length and an indexer so presumably when we look at <code class="markup--code markup--p-code">CreateDataset</code> it’s going to do that.</p><p name="bc6d" id="bc6d" class="graf graf--p graf-after--p">Here is <code class="markup--code markup--p-code">CreateDataset</code> and this library does more than just cycle GAN — it handles both aligned and unaligned image pairs [<a href="https://youtu.be/ondivPiwQho?t=2h4m46s" data-href="https://youtu.be/ondivPiwQho?t=2h4m46s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:04:46</a>]. We know that our image pairs are unaligned so we are going to <code class="markup--code markup--p-code">UnalignedDataset</code>.</p><figure name="e786" id="e786" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_wDbxkFlSWbEnC9QDtymlZA.png"></figure><p name="fb25" id="fb25" class="graf graf--p graf-after--figure">As expected, it has <code class="markup--code markup--p-code">__getitem__</code> and <code class="markup--code markup--p-code">__len__</code>. For length, A and B are our horses and zebras, we got two sets, so whichever one is longer is the length of the <code class="markup--code markup--p-code">DataLoader</code>. <code class="markup--code markup--p-code">__getitem__</code> is going to:</p><ul class="postList"><li name="42ce" id="42ce" class="graf graf--li graf-after--p">Randomly grab something from each of our two horses and zebras</li><li name="053d" id="053d" class="graf graf--li graf-after--li">Open them up with pillow (PIL)</li><li name="c9ec" id="c9ec" class="graf graf--li graf-after--li">Run them through some transformations</li><li name="d1f0" id="d1f0" class="graf graf--li graf-after--li">Then we could either be turning horses into zebras or zebras into horses, so there’s some direction</li><li name="8fab" id="8fab" class="graf graf--li graf-after--li">Return our horse, zebra, a path to the horse, and a path of zebra</li></ul><p name="702d" id="702d" class="graf graf--p graf-after--li">Hopefully you can kind of see that this is looking pretty similar to the kind of things fast.ai does. Fast.ai obviously does quite a lot more when it comes to transforms and performance, but remember, this is research code for this one thing and it’s pretty cool that they did all this work.</p><figure name="c412" id="c412" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_zWN8sgzWry6qu7R9FS0Ydw.png"></figure><pre name="35db" id="35db" class="graf graf--pre graf-after--figure">data_loader = CreateDataLoader(opt)<br>dataset = data_loader.load_data()<br>dataset_size = len(data_loader)<br>dataset_size</pre><pre name="caf1" id="caf1" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">1334</em></pre><p name="4826" id="4826" class="graf graf--p graf-after--pre">We’ve got a data loader so we can go and load our data into it [<a href="https://youtu.be/ondivPiwQho?t=2h6m17s" data-href="https://youtu.be/ondivPiwQho?t=2h6m17s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:06:17</a>]. That will tell us how many mini-batches are in it (that’s the length of the data loader in PyTorch).</p><p name="c169" id="c169" class="graf graf--p graf-after--p">Next step is to create a model. Same idea, we’ve got different kind of models and we’re going to be doing a cycle GAN.</p><figure name="a2bb" id="a2bb" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_TmC6TtfaP2xRyS9KK1ryjA.png"></figure><p name="15e0" id="15e0" class="graf graf--p graf-after--figure">Here is our <code class="markup--code markup--p-code">CycleGANModel</code>. There is quite a lot of stuff in <code class="markup--code markup--p-code">CycleGANModel</code>, so let’s go through and find out what’s going to be used. At this stage, we’ve just called initializer so when we initialize it, it’s going to go through and define two generators which is not surprising a generator for our horses and a generator for zebras. There is some way for it to generate a pool of fake data and then we’re going to grab our GAN loss, and as we talked about our cycle-consistency loss is an L1 loss. They are going to use Adam, so obviously for cycle GANS they found Adam works pretty well. Then we are going to have an optimizer for our horse discriminator, an optimizer for our zebra discriminator, and an optimizer for our generator. The optimizer for the generator is going to contain the parameters both for the horse generator and the zebra generator all in one place.</p><figure name="1f75" id="1f75" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_eDn2CkHKsIDaAz1M5WnWBg.png"></figure><p name="7e31" id="7e31" class="graf graf--p graf-after--figure">So the initializer is going to set up all of the different networks and loss functions we need and they are going to be stored inside this <code class="markup--code markup--p-code">model</code> [<a href="https://youtu.be/ondivPiwQho?t=2h8m14s" data-href="https://youtu.be/ondivPiwQho?t=2h8m14s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:08:14</a>].</p><pre name="b9f2" id="b9f2" class="graf graf--pre graf-after--p">model = create_model(opt)</pre><p name="80f0" id="80f0" class="graf graf--p graf-after--pre">It then prints out and shows us exactly the PyTorch model we have. It’s interesting to see that they are using ResNets and so you can see the ResNets look pretty familiar, so we have conv, batch norm, Relu. <code class="markup--code markup--p-code">InstanceNorm</code> is just the same as batch norm basically but it applies to one image at a time and the difference isn’t particularly important. And you can see they are doing reflection padding just like we are. You can kind of see when you try to build everything from scratch like this, it is a lot of work and you can forget the nice little things that fast.ai does automatically for you. You have to do all of them by hand and only you end up with a subset of them. So over time, hopefully soon, we’ll get all of this GAN stuff into fast.ai and it’ll be nice and easy.</p><figure name="c2b9" id="c2b9" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_YTCDe7-xeLelfeQNiKiq4A.png"></figure><p name="01f6" id="01f6" class="graf graf--p graf-after--figure">We’ve got our model and remember the model contains the loss functions, generators, discriminators, all in one convenient place [<a href="https://youtu.be/ondivPiwQho?t=2h9m32s" data-href="https://youtu.be/ondivPiwQho?t=2h9m32s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:09:32</a>]. I’ve gone ahead and copied and pasted and slightly refactored the training loop from their code so that we can run it inside the notebook. So this one should look a lot familiar. A loop to go through each epoch and a loop to go through the data. Before we did this, we set up <code class="markup--code markup--p-code">dataset</code>. This is actually not a PyTorch dataset, I think this is what they used slightly confusingly to talk about their combined what we would call a model data object — all the data that they need. Loop through that with <code class="markup--code markup--p-code">tqdm</code> to get a progress bar, and so now we can go through and see what happens in the model.</p><pre name="d096" id="d096" class="graf graf--pre graf-after--p">total_steps = 0<br><br><strong class="markup--strong markup--pre-strong">for</strong> epoch <strong class="markup--strong markup--pre-strong">in</strong> range(opt.epoch_count, opt.niter + opt.niter_decay+1):<br>    epoch_start_time = time.time()<br>    iter_data_time = time.time()<br>    epoch_iter = 0<br><br>    <strong class="markup--strong markup--pre-strong">for</strong> i, data <strong class="markup--strong markup--pre-strong">in</strong> tqdm(enumerate(dataset)):<br>        iter_start_time = time.time()<br>        <strong class="markup--strong markup--pre-strong">if</strong> total_steps % opt.print_freq == 0: <br>             t_data = iter_start_time - iter_data_time<br>        total_steps += opt.batchSize<br>        epoch_iter += opt.batchSize<br>        model.set_input(data)<br>        model.optimize_parameters()<br><br>        <strong class="markup--strong markup--pre-strong">if</strong> total_steps % opt.display_freq == 0:<br>            save_result = total_steps % opt.update_html_freq == 0<br><br>        <strong class="markup--strong markup--pre-strong">if</strong> total_steps % opt.print_freq == 0:<br>            errors = model.get_current_errors()<br>            t = (time.time() - iter_start_time) / opt.batchSize<br><br>        <strong class="markup--strong markup--pre-strong">if</strong> total_steps % opt.save_latest_freq == 0:<br>            print('saving the latest model(epoch <strong class="markup--strong markup--pre-strong">%d</strong>,total_steps <strong class="markup--strong markup--pre-strong">%d</strong>)'<br>                    % (epoch, total_steps))<br>            model.save('latest')<br><br>        iter_data_time = time.time()<br>    <strong class="markup--strong markup--pre-strong">if</strong> epoch % opt.save_epoch_freq == 0:<br>        print('saving the model at the end of epoch <strong class="markup--strong markup--pre-strong">%d</strong>, iters <strong class="markup--strong markup--pre-strong">%d</strong>' <br>               % (epoch, total_steps))<br>        model.save('latest')<br>        model.save(epoch)<br><br>    print('End of epoch <strong class="markup--strong markup--pre-strong">%d</strong> / <strong class="markup--strong markup--pre-strong">%d</strong> <strong class="markup--strong markup--pre-strong">\t</strong> Time Taken: <strong class="markup--strong markup--pre-strong">%d</strong> sec' %<br>          (epoch, opt.niter + opt.niter_decay, time.time() <br>          - epoch_start_time))<br>    model.update_learning_rate()</pre><p name="ad20" id="ad20" class="graf graf--p graf-after--pre"><code class="markup--code markup--p-code">set_input</code> [<a href="https://youtu.be/ondivPiwQho?t=2h10m32s" data-href="https://youtu.be/ondivPiwQho?t=2h10m32s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:10:32</a>]: It’s a different approach to what we do in fast.ai. This is kind of neat, it’s quite specific to cycle GANs but basically internally inside this model is this idea that we are going to go into our data and grab the appropriate one. We are either going horse to zebra or zebra to horse, depending on which way we go, <code class="markup--code markup--p-code">A</code> is either horse or zebra, and vice versa. If necessary put it on the appropriate GPU, then grab the appropriate paths. So the model now has a mini-batch of horses and a mini-batch of zebras.</p><figure name="2853" id="2853" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1__s9OBHq4z1OBiR9SJORySw.png"></figure><p name="7fb5" id="7fb5" class="graf graf--p graf-after--figure">Now we optimize the parameters [<a href="https://youtu.be/ondivPiwQho?t=2h11m19s" data-href="https://youtu.be/ondivPiwQho?t=2h11m19s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:11:19</a>]. It’s kind of nice to see it like this. You can see each step. First of all, try to optimize the generators, then try to optimize the horse discriminators, then try to optimize the zebra discriminator. <code class="markup--code markup--p-code">zero_grad()</code> is a part of PyTorch, as well as <code class="markup--code markup--p-code">step()</code>. So the interesting bit is the actual thing that does the back propagation on the generator.</p><figure name="bdce" id="bdce" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_CXawhHC0Mc9pgBFBIWg22Q.png"></figure><p name="5e55" id="5e55" class="graf graf--p graf-after--figure">Here it is [<a href="https://youtu.be/ondivPiwQho?t=2h12m4s" data-href="https://youtu.be/ondivPiwQho?t=2h12m4s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:12:04</a>]. Let’s jump to the key pieces. There’s all the formula that we just saw in the paper. Let’s take a horse and generate a zebra. Let’s now use the discriminator to see if we can tell whether it’s fake or not (<code class="markup--code markup--p-code">pred_fake</code>). Then let’s pop that into our loss function which we set up earlier to get a GAN loss based on that prediction. Let’s do the same thing going the opposite direction using the opposite discriminator then put that through the loss function again. Then let’s do the cycle consistency loss. Again, we take our fake which we created and try and turn it back again into the original. Let’s use the cycle consistency loss function we created earlier to compare it to the real original. And here is that lambda — so there’s some weight that we used and that would set up actually we just use the default that they suggested in their options. Then do the same for the opposite direction and then add them all together. We then do the backward step. That’s it.</p><figure name="73c7" id="73c7" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_q-ir1SHyywXmO5EkTDVq1w.png"></figure><p name="cb04" id="cb04" class="graf graf--p graf-after--figure">So we can do the same thing for the first discriminator [<a href="https://youtu.be/ondivPiwQho?t=2h13m50s" data-href="https://youtu.be/ondivPiwQho?t=2h13m50s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:13:50</a>]. Since basically all the work has been done now, there’s much less to do here. There that is. We won’t step all through it but it’s basically the same basic stuff that we’ve already seen.</p><figure name="e538" id="e538" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_PPZdNJDrTHrrQLVRjzucgg.png"></figure><p name="a2fb" id="a2fb" class="graf graf--p graf-after--figure">So <code class="markup--code markup--p-code">optimize_parameters()</code> is calculating the losses and doing the optimizer step. From time to time, save and print out some results. Then from time to time, update the learning rate so they’ve got some learning rate annealing built in here as well. Kind of like fast.ai, they’ve got this idea of schedulers which you can then use to update your learning rates.</p><figure name="b657" id="b657" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Xrc3Dxs8hKV7pWHQBfZSsQ.png" data-width="439" data-height="96" src="../img/1_Xrc3Dxs8hKV7pWHQBfZSsQ.png"></figure><p name="059e" id="059e" class="graf graf--p graf-after--figure">For those of you are interested in better understanding deep learning APIs, contributing more to fast.ai, or creating your own version of some of this stuff in some different back-end, it’s cool to look at a second API that covers some subset of some of the similar things to get a sense for how they are solving some of these problems and what the similarities/differences are.</p><pre name="b222" id="b222" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> show_img(im, ax=<strong class="markup--strong markup--pre-strong">None</strong>, figsize=<strong class="markup--strong markup--pre-strong">None</strong>):<br>    <strong class="markup--strong markup--pre-strong">if</strong> <strong class="markup--strong markup--pre-strong">not</strong> ax: fig,ax = plt.subplots(figsize=figsize)<br>    ax.imshow(im)<br>    ax.get_xaxis().set_visible(<strong class="markup--strong markup--pre-strong">False</strong>)<br>    ax.get_yaxis().set_visible(<strong class="markup--strong markup--pre-strong">False</strong>)<br>    <strong class="markup--strong markup--pre-strong">return</strong> ax</pre><pre name="d679" id="d679" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> get_one(data):<br>    model.set_input(data)<br>    model.test()<br>    <strong class="markup--strong markup--pre-strong">return</strong> list(model.get_current_visuals().values())</pre><pre name="9ec8" id="9ec8" class="graf graf--pre graf-after--pre">model.save(201)</pre><pre name="e8e0" id="e8e0" class="graf graf--pre graf-after--pre">test_ims = []<br><strong class="markup--strong markup--pre-strong">for</strong> i,o <strong class="markup--strong markup--pre-strong">in</strong> enumerate(dataset):<br>    <strong class="markup--strong markup--pre-strong">if</strong> i&gt;10: <strong class="markup--strong markup--pre-strong">break</strong><br>    test_ims.append(get_one(o))</pre><pre name="fa28" id="fa28" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> show_grid(ims):<br>    fig,axes = plt.subplots(2,3,figsize=(9,6))<br>    <strong class="markup--strong markup--pre-strong">for</strong> i,ax <strong class="markup--strong markup--pre-strong">in</strong> enumerate(axes.flat): show_img(ims[i], ax);<br>    fig.tight_layout()</pre><pre name="8260" id="8260" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(8): show_grid(test_ims[i])</pre><p name="7294" id="7294" class="graf graf--p graf-after--pre">We train that for a little while and then we can just grab a few examples and here we have them [<a href="https://youtu.be/ondivPiwQho?t=2h15m29s" data-href="https://youtu.be/ondivPiwQho?t=2h15m29s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:15:29</a>]. Here are horses, zebras, and back again as horses.</p><figure name="e03c" id="e03c" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_CcsmcW4TlZvn7eywxQPHUQ.png"></figure><figure name="7327" id="7327" class="graf graf--figure graf-after--figure"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_uMqqilXzEmTMXf8x5ry0CQ.png"></figure><figure name="a1e0" id="a1e0" class="graf graf--figure graf-after--figure"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1__9FdL_2vB30MCQ1V8fU-qw.png"></figure><figure name="ead1" id="ead1" class="graf graf--figure graf-after--figure"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_SanvgXWJHOoucKANA6A36A.png"></figure><figure name="fea0" id="fea0" class="graf graf--figure graf-after--figure"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_TcjrwAtTdYLkV1x5kCqdxA.png"></figure><figure name="06e9" id="06e9" class="graf graf--figure graf-after--figure"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_Rlpp3gVTYSaKknsAq4qOig.png"></figure><figure name="506e" id="506e" class="graf graf--figure graf-after--figure"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_xxPYAgd8hRxgvGv2mBQ2Vg.png"></figure><figure name="0884" id="0884" class="graf graf--figure graf-after--figure"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_MxQzw0SwBT_iYfbyd4BD_w.png"></figure><p name="c730" id="c730" class="graf graf--p graf-after--figure">It took me like 24 hours to train it even that far so it’s kind of slow [<a href="https://youtu.be/ondivPiwQho?t=2h16m39s" data-href="https://youtu.be/ondivPiwQho?t=2h16m39s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:16:39</a>]. I know Helena is constantly complaining on Twitter about how long these things take. I don’t know how she’s so productive with them.</p><pre name="b869" id="b869" class="graf graf--pre graf-after--p">#! wget https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip</pre><p name="cae9" id="cae9" class="graf graf--p graf-after--pre">I will mention one more thing that just came out yesterday [<a href="https://youtu.be/ondivPiwQho?t=2h16m54s" data-href="https://youtu.be/ondivPiwQho?t=2h16m54s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:16:54</a>]:</p><p name="44c4" id="44c4" class="graf graf--p graf-after--p"><a href="https://arxiv.org/abs/1804.04732" data-href="https://arxiv.org/abs/1804.04732" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">Multimodal Unsupervised Image-to-Image Translation</a></p><p name="a7be" id="a7be" class="graf graf--p graf-after--p">There is now a multi-modal image to image translation of unpaired. So you can basically now create different cats for instance from this dog.</p><figure name="5b2b" id="5b2b" class="graf graf--figure graf--iframe graf-after--p"><iframe data-width="854" data-height="480" width="700" height="393" data-src="/media/6af412979eb92c5c8e7ee3439d5c31b3?postId=215dfbf04a94" data-media-id="6af412979eb92c5c8e7ee3439d5c31b3" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Fi.ytimg.com%2Fvi%2Fab64TWzWn40%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="../img/saved_resource.html"></iframe><iframe data-width="854" data-height="480" width="700" height="393" src="../img/6af412979eb92c5c8e7ee3439d5c31b3.html" data-media-id="6af412979eb92c5c8e7ee3439d5c31b3" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Fi.ytimg.com%2Fvi%2Fab64TWzWn40%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen="" frameborder="0"></iframe></figure><p name="8f99" id="8f99" class="graf graf--p graf-after--figure graf--trailing">This is basically not just creating one example of the output that you want, but creating multiple ones. This came out yesterday or the day before. I think it’s pretty amazing. So you can kind of see how this technology is developing and I think there’s so many opportunities to maybe do this with music, speech, writing, or to create kind of tools for artists.</p><hr class="section-divider"><p name="18d1" id="18d1" class="graf graf--p graf--leading graf--trailing">Lessons: <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" class="markup--anchor markup--p-anchor" target="_blank">1</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" class="markup--anchor markup--p-anchor" target="_blank">2</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" class="markup--anchor markup--p-anchor" target="_blank">3</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" class="markup--anchor markup--p-anchor" target="_blank">4</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" class="markup--anchor markup--p-anchor" target="_blank">5</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" class="markup--anchor markup--p-anchor" target="_blank">6</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" class="markup--anchor markup--p-anchor" target="_blank">7</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" class="markup--anchor markup--p-anchor" target="_blank">8</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" class="markup--anchor markup--p-anchor" target="_blank">9</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" class="markup--anchor markup--p-anchor" target="_blank">10</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" class="markup--anchor markup--p-anchor" target="_blank">11</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" class="markup--anchor markup--p-anchor" target="_blank"><strong class="markup--strong markup--p-strong">12</strong></a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" class="markup--anchor markup--p-anchor" target="_blank">13</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" class="markup--anchor markup--p-anchor" target="_blank">14</a></p></body></html>