
<!-- saved from url=(0064)file:///C:/Users/asus/Desktop/fastai-ml-dl-notes-zh/en/dl13.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></head><body><hr class="section-divider"><h1 name="f41f" id="f41f" class="graf graf--h3 graf--leading graf--title">Deep Learning 2: Part 2 Lesson&nbsp;13</h1><p name="5787" id="5787" class="graf graf--p graf-after--h3"><em class="markup--em markup--p-em">My personal notes from </em><a href="http://www.fast.ai/" data-href="http://www.fast.ai/" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener noopener noopener nofollow noopener nofollow noopener noopener nofollow noopener noopener nofollow noopener" target="_blank"><em class="markup--em markup--p-em">fast.ai course</em></a><em class="markup--em markup--p-em">. These notes will continue to be updated and improved as I continue to review the course to “really” understand it. Much appreciation to </em><a href="https://twitter.com/jeremyphoward" data-href="https://twitter.com/jeremyphoward" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener noopener noopener nofollow noopener nofollow noopener noopener nofollow noopener noopener nofollow noopener" target="_blank"><em class="markup--em markup--p-em">Jeremy</em></a><em class="markup--em markup--p-em"> and </em><a href="https://twitter.com/math_rachel" data-href="https://twitter.com/math_rachel" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener noopener noopener nofollow noopener nofollow noopener noopener nofollow noopener noopener nofollow noopener" target="_blank"><em class="markup--em markup--p-em">Rachel</em></a><em class="markup--em markup--p-em"> who gave me this opportunity to learn.</em></p><p name="0c92" id="0c92" class="graf graf--p graf-after--p graf--trailing">Lessons: <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" class="markup--anchor markup--p-anchor" target="_blank">1</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" class="markup--anchor markup--p-anchor" target="_blank">2</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" class="markup--anchor markup--p-anchor" target="_blank">3</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" class="markup--anchor markup--p-anchor" target="_blank">4</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" class="markup--anchor markup--p-anchor" target="_blank">5</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" class="markup--anchor markup--p-anchor" target="_blank">6</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" class="markup--anchor markup--p-anchor" target="_blank">7</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" class="markup--anchor markup--p-anchor" target="_blank">8</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" class="markup--anchor markup--p-anchor" target="_blank">9</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" class="markup--anchor markup--p-anchor" target="_blank">10</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" class="markup--anchor markup--p-anchor" target="_blank">11</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" class="markup--anchor markup--p-anchor" target="_blank">12</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" class="markup--anchor markup--p-anchor" target="_blank"><strong class="markup--strong markup--p-strong">13</strong></a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" class="markup--anchor markup--p-anchor" target="_blank">14</a></p><hr class="section-divider"><p name="8d15" id="8d15" class="graf graf--p graf--leading"><a href="http://forums.fast.ai/t/lesson-13-discussion-and-wiki/15297/1" data-href="http://forums.fast.ai/t/lesson-13-discussion-and-wiki/15297/1" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Forum</a> / <a href="https://youtu.be/xXXiC4YRGrQ" data-href="https://youtu.be/xXXiC4YRGrQ" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Video</a></p><figure name="2b7d" id="2b7d" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_hnLrcMUPpHa6RyMy1DqTMA.png"></figure><figure name="2860" id="2860" class="graf graf--figure graf--layoutOutsetCenter graf-after--figure" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_M8g-O4rEHrAO62K9J_AG6w.png"></figure><p name="370a" id="370a" class="graf graf--p graf-after--figure">Image enhancement — we’ll cover things like this painting that you might be familiar with. However, you might not have noticed before that this painting of an eagle in it. The reason you may not have noticed that before is this painting didn’t used to have an eagle in it. By the same token, the painting on the first slide did not used to have Captain America’s shield on it either.</p><figure name="9cde" id="9cde" class="graf graf--figure graf--layoutOutsetCenter graf-after--p" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_7jEj71y4syKb2ylnSGrUAg.png"></figure><p name="aaca" id="aaca" class="graf graf--p graf-after--figure">This is a cool new paper that just came out a couple of days ago called <a href="https://arxiv.org/abs/1804.03189" data-href="https://arxiv.org/abs/1804.03189" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Deep Painterly Harmonization</a> and it uses almost exactly the technique we are going to learn in this lesson with some minor tweaks. But you can see the basic idea is to take one picture pasted on top of another picture, and then use some kind of approach to combine the two. The approach is called a “style transfer”.</p><p name="3c6c" id="3c6c" class="graf graf--p graf-after--p">Before we talk about that, I wanted to mention this really cool contribution by William Horton who added this stochastic weight averaging technique to the fastai library that is now all merged and ready to go. He’s written a whole post about that which I strongly recommend you check out not just because stochastic weight averaging lets you get higher performance from your existing neural network with basically no extra work (it’s as simple as adding two parameters to your fit function: <code class="markup--code markup--p-code">use_swa</code>, <code class="markup--code markup--p-code">swa_start</code>) but also he’s described his process of building this and how he tested it and how he contributed to the library. So I think it’s interesting if you are interested in doing something like this. I think William had not built this kind of library before so he describes how he did it.</p><figure name="ce07" id="ce07" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_hJeKM7VaXaDyvVTTXWlFqg.png"><figcaption class="imageCaption"><a href="https://medium.com/@hortonhearsafoo/adding-a-cutting-edge-deep-learning-training-technique-to-the-fast-ai-library-2cd1dba90a49" data-href="https://medium.com/@hortonhearsafoo/adding-a-cutting-edge-deep-learning-training-technique-to-the-fast-ai-library-2cd1dba90a49" class="markup--anchor markup--figure-anchor" rel="nofollow" target="_blank">https://medium.com/@hortonhearsafoo/adding-a-cutting-edge-deep-learning-training-technique-to-the-fast-ai-library-2cd1dba90a49</a></figcaption></figure><h4 name="639a" id="639a" class="graf graf--h4 graf-after--figure">TrainPhase [<a href="https://youtu.be/xXXiC4YRGrQ?t=2m1s" data-href="https://youtu.be/xXXiC4YRGrQ?t=2m1s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">2:01</a>]</h4><p name="d6b6" id="d6b6" class="graf graf--p graf-after--h4"><a href="https://github.com/fastai/fastai/blob/master/courses/dl2/training_phase.ipynb" data-href="https://github.com/fastai/fastai/blob/master/courses/dl2/training_phase.ipynb" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Notebook</a></p><p name="11f8" id="11f8" class="graf graf--p graf-after--p">Another vert cool contribution to the fastai library is a new train phase API. And I’m going to do something I’ve never done before which is I’m going to present somebody else’s notebook. The reason I haven’t done it before is because I haven’t liked any notebooks enough to think they are worth presenting it, but Sylvain has done a fantastic job here of not just creating this new API but also creating a beautiful notebook describing what it is and how it works and so forth. The background here is as you guys know we’ve been trying to train networks faster, partly as part of this Dawn bench competition and also for a reason that you’ll learn about next week. I mentioned on the forum last week it would be really handy for our experiments if we had an easier way to try out different learning rate schedules etc, and I laid out an API that I had in mind as it’d be really cool if somebody could write this because I am going to bed now and I kind of need it by tomorrow. And Sylvain replied on the forum well that sounds like a good challenge and by 24 hours later, it was done and it’s been super cool. I want to take you through it because it’s going to allow you to research things that nobody has tried before.</p><p name="4935" id="4935" class="graf graf--p graf-after--p">It’s called the TrainPhase API [<a href="https://youtu.be/xXXiC4YRGrQ?t=3m32s" data-href="https://youtu.be/xXXiC4YRGrQ?t=3m32s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">3:32</a>] and the easiest way to show it is to show an example of what it does. Here is an iteration against learning rate chart as you are familiar with seeing. This is one where we train for a while at the learning rate of 0.01 and then we train for a while at the learning rate of 0.001. I actually wanted to create something very much like that learning rate chart because most people that trained ImageNet use this stepwise approach and it’s actually not something that’s built into fastai because it’s not generally something we recommend. But in order to replicate existing papers, I wanted to do it the same way. So rather than writing a number of fit, fit, fit calls with different learning rates, it would be nice to be able to say train for n epochs at this learning rate and then m epochs at that learning rate.</p><figure name="7377" id="7377" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_aWc3pHrwlAI7kfIPDANQpw.png"></figure><p name="f44d" id="f44d" class="graf graf--p graf-after--figure">So here is how you do that:</p><pre name="6899" id="6899" class="graf graf--pre graf-after--p">phases = [TrainingPhase(epochs=1, opt_fn=optim.SGD, lr = 1e-2),   <br>          TrainingPhase(epochs=2, opt_fn=optim.SGD, lr = 1e-3)]</pre><p name="a384" id="a384" class="graf graf--p graf-after--pre">A phase is a period of training with particular optimizer parameters and <code class="markup--code markup--p-code">phases</code> consist of a number of training phase objects. A training phase object says how many epochs to train for, what optimization function to use, and what learning rate amongst other things that we will see. Here, you’ll see the two training phases that you just saw on that graph. So now, rather than calling <code class="markup--code markup--p-code">learn.fit</code>, you say:</p><pre name="f856" id="f856" class="graf graf--pre graf-after--p">learn.fit_opt_sched(phases)</pre><p name="56fc" id="56fc" class="graf graf--p graf-after--pre">In other words, <code class="markup--code markup--p-code">learn.fit</code> with an optimizer scheduler with these phases. From there, most of the things you pass in can just get sent across to the fit function as per usual, so most of the usual parameter will work fine. Generally speaking, we can just use these training phases and you will see it fits in a usual way. Then when you say <code class="markup--code markup--p-code">plot_lr</code> you will see the graphs above. Not only does it plot the learning rate, it also plots momentum, and for each phase, it tells you what optimizer it used. You can turn off the printing of the optimizers (<code class="markup--code markup--p-code">show_text=False</code>), you can turn off the printing of momentums (<code class="markup--code markup--p-code">show_moms=False</code>), and you can do other little things like a training phase could have a <code class="markup--code markup--p-code">lr_decay</code> parameter [<a href="https://youtu.be/xXXiC4YRGrQ?t=5m47s" data-href="https://youtu.be/xXXiC4YRGrQ?t=5m47s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">5:47</a> ]:</p><pre name="3dcd" id="3dcd" class="graf graf--pre graf-after--p">phases = [TrainingPhase(epochs=1, opt_fn=optim.SGD, lr = 1e-2), <br>          TrainingPhase(epochs=1, opt_fn=optim.SGD, <br>                       lr = (1e-2,1e-3), lr_decay=DecayType.LINEAR),<br>          TrainingPhase(epochs=1, opt_fn=optim.SGD, lr = 1e-3)]</pre><p name="0ce8" id="0ce8" class="graf graf--p graf-after--pre">So here is a fixed learning rate, then a linear decay learning rate, and then a fixed learning rate which gives up this picture:</p><pre name="0ece" id="0ece" class="graf graf--pre graf-after--p">lr_i = start_lr + (end_lr - start_lr) * i/n</pre><figure name="fc22" id="fc22" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_duLsu6JhsWXtVxSTJ3LvTw.png"></figure><p name="f5a4" id="f5a4" class="graf graf--p graf-after--figure">This might be quite a good way to train because we know at high learning rates, you get to explore better, and at low learning rates, you get to fine-tune better. And it’s probably better to gradually slide between the two. So this actually isn’t a bad approach, I suspect.</p><p name="f18b" id="f18b" class="graf graf--p graf-after--p">You can use other decay types such as cosine [<a href="https://youtu.be/xXXiC4YRGrQ?t=6m25s" data-href="https://youtu.be/xXXiC4YRGrQ?t=6m25s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">6:25</a>]:</p><pre name="a8e8" id="a8e8" class="graf graf--pre graf-after--p">phases = [TrainingPhase(epochs=1, opt_fn=optim.SGD, lr = 1e-2),    <br>          TrainingPhase(epochs=1, opt_fn=optim.SGD, lr =(1e-2,1e-3),<br>                  lr_decay=DecayType.COSINE),           <br>          TrainingPhase(epochs=1, opt_fn=optim.SGD, lr = 1e-3)]</pre><p name="21fb" id="21fb" class="graf graf--p graf-after--pre">This probably makes even more sense as a genuinely potentially useful learning rate annealing shape.</p><pre name="982b" id="982b" class="graf graf--pre graf-after--p">lr_i = end_lr + (start_lr - end_lr)/2 * (1 + np.cos(i * np.pi)/n)</pre><figure name="70f0" id="70f0" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_ABxYYgRpKEiadfQj7tSWHg.png"></figure><p name="458c" id="458c" class="graf graf--p graf-after--figure">Exponential which is super popular approach:</p><pre name="1cb9" id="1cb9" class="graf graf--pre graf-after--p">lr_i = start_lr * (end_lr/start_lr)**(i/n)</pre><figure name="f619" id="f619" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_cDjz1ZSPmDfHWQGDt4nFUQ.png"></figure><p name="f0b4" id="f0b4" class="graf graf--p graf-after--figure">Polynomial which isn’t terribly popular but actually in the literature works better than just about anything else, but seems to have been largely ignored. So polynomial is good to be aware of. And what Sylvain has done is he’s given us the formula for each of these curves. So with a polynomial, you get to pick what polynomial to use. I believe p of 0.9 is the one I’ve seen really good results for — FYI.</p><pre name="053e" id="053e" class="graf graf--pre graf-after--p">lr_i = end_lr + (start_lr - end_lr) * (1 - i/n) ** p</pre><figure name="5f52" id="5f52" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_Ku8WXqHiEI4_Q_XtfrL4Qg.png"></figure><p name="4070" id="4070" class="graf graf--p graf-after--figure">If you don’t give a tuple of learning rates when there is an LR decay, then it will decay all the way down to zero [<a href="https://youtu.be/xXXiC4YRGrQ?t=7m26s" data-href="https://youtu.be/xXXiC4YRGrQ?t=7m26s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">7:26</a>]. And as you can see, you can happily start the next cycle at a different point.</p><pre name="140a" id="140a" class="graf graf--pre graf-after--p">phases = [TrainingPhase(epochs=1, opt_fn=optim.SGD, lr = 1e-2), <br>          TrainingPhase(epochs=1, opt_fn=optim.SGD, lr = 1e-2, <br>                        lr_decay=DecayType.COSINE),<br>          TrainingPhase(epochs=1, opt_fn=optim.SGD, lr = 1e-3)]</pre><figure name="a98a" id="a98a" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1__2LEpZ_eiPa_Oh6uaUz3Ig.png"></figure><h4 name="0432" id="0432" class="graf graf--h4 graf-after--figure">SGDR [<a href="https://youtu.be/xXXiC4YRGrQ?t=7m43s" data-href="https://youtu.be/xXXiC4YRGrQ?t=7m43s" class="markup--anchor markup--h4-anchor" rel="noopener nofollow" target="_blank">7:43</a>]</h4><p name="cf7b" id="cf7b" class="graf graf--p graf-after--h4">So the cool thing is, now we can replicate all of our existing schedules using nothing but these training phases&nbsp;. So here is a function called <code class="markup--code markup--p-code">phases_sgdr</code> which does SGDR using the new training phase API.</p><pre name="75ab" id="75ab" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> phases_sgdr(lr, opt_fn, num_cycle,cycle_len,cycle_mult):<br>    phases = [TrainingPhase(epochs = cycle_len/ 20, opt_fn=opt_fn, <br>                       lr=lr/100),<br>              TrainingPhase(epochs = cycle_len * 19/20, <br>                   opt_fn=opt_fn, lr=lr, lr_decay=DecayType.COSINE)]<br>    <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(1,num_cycle):<br>        phases.append(TrainingPhase(epochs=cycle_len*<br>                      (cycle_mult**i), opt_fn=opt_fn, lr=lr, <br>                      lr_decay=DecayType.COSINE))<br>    <strong class="markup--strong markup--pre-strong">return</strong> phases</pre><p name="8f05" id="8f05" class="graf graf--p graf-after--pre">So you can see, if he runs this schedule, here is what it looks like:</p><figure name="31be" id="31be" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_4ZZswpXpsQ5ZAwLpnMlUiw.png"></figure><p name="6e95" id="6e95" class="graf graf--p graf-after--figure">He’s even done the little trick I have where you’re training at really low learning rate just for a little bit and then pop up and do a few cycles, and the cycles are increasing in length [<a href="https://youtu.be/xXXiC4YRGrQ?t=8m5s" data-href="https://youtu.be/xXXiC4YRGrQ?t=8m5s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">8:05</a>]. And that’s all done in a single function.</p><h4 name="5e00" id="5e00" class="graf graf--h4 graf-after--p">1cycle [<a href="https://youtu.be/xXXiC4YRGrQ?t=8m20s" data-href="https://youtu.be/xXXiC4YRGrQ?t=8m20s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">8:20</a>]</h4><p name="a7e8" id="a7e8" class="graf graf--p graf-after--h4">The new 1cycle we can now implement with, again, a single little function.</p><pre name="7bcc" id="7bcc" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> phases_1cycle(cycle_len,lr,div,pct,max_mom,min_mom):<br>    tri_cyc = (1-pct/100) * cycle_len<br>    <strong class="markup--strong markup--pre-strong">return</strong> [TrainingPhase(epochs=tri_cyc/2, opt_fn=optim.SGD, <br>                          lr=(lr/div,lr), lr_decay=DecayType.LINEAR,<br>                          momentum=(max_mom,min_mom),  <br>                          momentum_decay=DecayType.LINEAR),<br>           TrainingPhase(epochs=tri_cyc/2, opt_fn=optim.SGD, <br>                         lr=(lr,lr/div), lr_decay=DecayType.LINEAR, <br>                          momentum=(min_mom,max_mom), <br>                          momentum_decay=DecayType.LINEAR),<br>           TrainingPhase(epochs=cycle_len-tri_cyc, opt_fn=optim.SGD,<br>                         lr=(lr/div,lr/(100*div)), <br>                         lr_decay=DecayType.LINEAR, <br>                         momentum=max_mom)]</pre><p name="2508" id="2508" class="graf graf--p graf-after--pre">So if we fit with that, we get this triangle followed by a little flatter bit and the momentum is a cool thing — the momentum has a momentum decay. And in the third TrainingPhase, we have a fixed momentum. So it’s doing the momentum and the learning rate at the same time.</p><figure name="f767" id="f767" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_Wws6eOCVXppEMOtORM1dSQ.png"></figure><h4 name="4643" id="4643" class="graf graf--h4 graf-after--figure">Discriminative learning rates + 1cycle&nbsp;[<a href="https://youtu.be/xXXiC4YRGrQ?t=8m53s" data-href="https://youtu.be/xXXiC4YRGrQ?t=8m53s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">8:53</a>]</h4><p name="c12f" id="c12f" class="graf graf--p graf-after--h4">So something that I haven’t tried yet, but I think would be really interesting is to use the combination of discriminative learning rates and 1cycle. No one has tried yet. So that would be really interesting. The only paper I’ve come across which has discriminative learning rate uses something called LARS. It was used to train ImageNet with very very large batch sizes by looking at the ratio between the gradient and the mean at each layer and using that to change the learning rate of each layer automatically. They found that they could use much larger batch sizes. That’s the only other place I’ve seen this kind of approach used, but there’s lots of interesting things you could try with combining discriminative learning rates and different interesting schedules.</p><h4 name="cf75" id="cf75" class="graf graf--h4 graf-after--p">Your own LR finder&nbsp;[<a href="https://youtu.be/xXXiC4YRGrQ?t=10m6s" data-href="https://youtu.be/xXXiC4YRGrQ?t=10m6s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">10:06</a>]</h4><p name="5fca" id="5fca" class="graf graf--p graf-after--h4">You can now write your own LR finger of different types, specifically because there is now this <code class="markup--code markup--p-code">stop_div</code> parameter which basically means that it’ll use whatever schedule you asked for but when the loss gets too bad, it’ll stop training.</p><p name="1ea6" id="1ea6" class="graf graf--p graf-after--p">One useful thing that’s been added is the <code class="markup--code markup--p-code">linear</code> parameter to the <code class="markup--code markup--p-code">plot</code> function. If you use linear schedule rather than an exponential schedule in your learning rate finder which is a good idea if you fine-tuned into roughly the right area, then you can use linear to find exactly the right area. Then you probably want to plot it with a linear scale. So that’s why you can also pass linear to plot now as well.</p><p name="c248" id="c248" class="graf graf--p graf-after--p">You can change the optimizer each phase [<a href="https://youtu.be/xXXiC4YRGrQ?t=11m6s" data-href="https://youtu.be/xXXiC4YRGrQ?t=11m6s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">11:06</a>]. That’s more important than you might imagine because actually the current state-of-the-art for training on really large batch sizes really quickly for ImageNet actually starts with RMSProp for the first bit, then they switch to SGD for the second bit. So that could be something interesting to experiment more with because at least one paper has now shown that that can work well. Again, it’s something that isn’t well appreciated as yet.</p><h4 name="902d" id="902d" class="graf graf--h4 graf-after--p">Changing data&nbsp;[<a href="https://youtu.be/xXXiC4YRGrQ?t=11m49s" data-href="https://youtu.be/xXXiC4YRGrQ?t=11m49s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">11:49</a>]</h4><p name="1b44" id="1b44" class="graf graf--p graf-after--h4">Then the bit I find most interesting is you can change your data. Why would we want to change our data? Because you remember from lesson 1 and 2, you could use small images at the start and bigger images later. The theory is that you could use that to train the first bit more quickly with smaller images, and remember if you halve the height and halve the width, you’ve got the quarter of the activations every layer, so it can be a lot faster. It might even generalize better. So you can now create a couple of different sizes, for example, he’s got 28 and 32 sized images. This is CIFAR10 so there’s only so much you can do. Then if you pass in an array of data in this <code class="markup--code markup--p-code">data_list</code> parameter when you call <code class="markup--code markup--p-code">fit_opt_sched</code>, it’ll use different dataset for each phase.</p><pre name="64e7" id="64e7" class="graf graf--pre graf-after--p">data1 = get_data(28,batch_size)<br>data2 = get_data(32,batch_size)</pre><pre name="7c03" id="7c03" class="graf graf--pre graf-after--pre">learn = ConvLearner.from_model_data(ShallowConvNet(), data1)</pre><pre name="5976" id="5976" class="graf graf--pre graf-after--pre">phases = [TrainingPhase(epochs=1, opt_fn=optim.Adam, lr=1e-2, <br>                        lr_decay=DecayType.COSINE),<br>          TrainingPhase(epochs=2, opt_fn=optim.Adam, lr=1e-2, <br>                        lr_decay=DecayType.COSINE)]</pre><pre name="81f7" id="81f7" class="graf graf--pre graf-after--pre">learn.fit_opt_sched(phases, data_list=[data1,data2])</pre><p name="b9bd" id="b9bd" class="graf graf--p graf-after--pre">That’s really cool because we can use that now like we could use that in our DAWN bench entries and see what happens when we actually increase the size with very little code. So what happens when we do that [<a href="https://youtu.be/xXXiC4YRGrQ?t=13m2s" data-href="https://youtu.be/xXXiC4YRGrQ?t=13m2s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">13:02</a>]? The answer is here in DAWN bench training on ImageNet:</p><figure name="c2bd" id="c2bd" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_5CGZBcVMwjuz7N6J9906xQ.png"></figure><p name="ab67" id="ab67" class="graf graf--p graf-after--figure">You can see here that Google has won this with half an hour on a cluster of TPUs. The best non-cluster of TPU result is fast.ai + students under 3 hours beating out Intel on 128 computers, where else, we ran on a single computer. We also beat Google running on a TPU so using this approach, we’ve shown:</p><ul class="postList"><li name="5b73" id="5b73" class="graf graf--li graf-after--p">the fastest GPU result</li><li name="27c7" id="27c7" class="graf graf--li graf-after--li">the fastest single machine result</li><li name="51e1" id="51e1" class="graf graf--li graf-after--li">the fastest publicly available infrastructure result</li></ul><p name="b7ec" id="b7ec" class="graf graf--p graf-after--li">These TPU pods, you can’t use unless you’re Google. Also the cost is tiny ($72.54), this Intel one costs them $1,200 worth of compute — they haven’t even written it here, but that’s what you get if you use 128 computers in parallel each one with 36 cores, each one with 140G compare to our single AWS instance. So this is kind of a breakthrough in what we can do. The idea that we can train ImageNet on a single publicly available machine and this is $72, by the way, it was actually $25 because we used a spot instance. One of our students Andrew Shaw built this whole system to allow us to throw a whole bunch of spot instance experiments up and run them simultaneously and pretty much automatically, but DAWN bench doesn’t quote the actual number we used. So it’s actually $25, not $72. So this <code class="markup--code markup--p-code">data_list</code> idea is super important and helpful.</p><h4 name="5745" id="5745" class="graf graf--h4 graf-after--p">CIFAR10 result&nbsp;[<a href="https://youtu.be/xXXiC4YRGrQ?t=15m15s" data-href="https://youtu.be/xXXiC4YRGrQ?t=15m15s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">15:15</a>]</h4><p name="d2e0" id="d2e0" class="graf graf--p graf-after--h4">Our CIFAR10 results are also now up there officially and you might remember the previous best was a bit over an hour. The trick here was using 1cycle, so all of this stuff that’s in Sylvain’s training phase API is really all the stuff that we used to get these top results. And another fast.ai student who goes by the name bkj has taken that and done his own version, he took a Resnet18 and added the concat pooling that you might remember that we learnt about on top, and used Leslie Smith’s 1cycle and so he’s got on the leaderboard. So all the top 3 are fast.ai students which wonderful.</p><figure name="227f" id="227f" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_9UOqTbSEEsBMtEp5qKdUQg.png"></figure><h4 name="2625" id="2625" class="graf graf--h4 graf-after--figure">CIFAR10 cost result&nbsp;[<a href="https://youtu.be/xXXiC4YRGrQ?t=16m5s" data-href="https://youtu.be/xXXiC4YRGrQ?t=16m5s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">16:05</a>]</h4><p name="4f7c" id="4f7c" class="graf graf--p graf-after--h4">Same for cost — the top 3 and you can see, Paperspace. Brett ran this on Paperspace and got the cheapest result just ahead of bkj.</p><figure name="89ad" id="89ad" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_c2xPu_54XdllFPo8mrQmaA.png"></figure><p name="8b53" id="8b53" class="graf graf--p graf-after--figure">So I think you can see [<a href="https://youtu.be/xXXiC4YRGrQ?t=16m25s" data-href="https://youtu.be/xXXiC4YRGrQ?t=16m25s" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">16:25</a>], a lot of the interesting opportunities at the moment for the training stuff more quickly and cheaply are all about learning rate annealing, size annealing, and training with different parameters at different times, and I still think everybody is scratching the surface. I think we can go a lot faster and a lot cheaper. That’s really helpful for people in resource constrained environment which is basically everybody except Google, maybe Facebook.</p><p name="9664" id="9664" class="graf graf--p graf-after--p">Architectures are interesting as well though [<a href="https://youtu.be/xXXiC4YRGrQ?t=17m" data-href="https://youtu.be/xXXiC4YRGrQ?t=17m" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">17:00</a>], and one of the things we looked at last week was creating a simpler version of darknet architecture. But there’s a piece of architecture we haven’t talk about which is necessary to understand the Inception network. The Inception network is actually pretty interesting because they use some tricks to make things more efficient. We are not currently using these tricks and I feel that maybe we should try it. The most interesting and most successful Inception network is their Inception-ResNet-v2 network and most of the blocks in that looks something like this:</p><figure name="e88a" id="e88a" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_HUqLof5PFa8QA0imCbzjgw.png"></figure><p name="289e" id="289e" class="graf graf--p graf-after--figure">It looks a lot like a standard ResNet block in that there’s an identity connection, and there’s a conv path, and we add them up together [<a href="https://youtu.be/xXXiC4YRGrQ?t=17m47s" data-href="https://youtu.be/xXXiC4YRGrQ?t=17m47s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">17:47</a>]. But it’s not quite that. The first is the middle conv path is a 1x1 conv, and it’s worth thinking about what a 1x1 conv actually is.</p><h4 name="6678" id="6678" class="graf graf--h4 graf-after--p">1x1 convolution [<a href="https://youtu.be/xXXiC4YRGrQ?t=18m23s" data-href="https://youtu.be/xXXiC4YRGrQ?t=18m23s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">18:23</a>]</h4><p name="a55c" id="a55c" class="graf graf--p graf-after--h4">1x1 conv is simply saying for each grid cell in your input, you’ve got basically a vector. 1 by 1 by number of filters tensor is basically a vector. For each grid cell in your input, you’re just doing a dot product with that tensor. Then of course, it’s going to be one of those vectors for each of the 192 activations we are creating. So basically do 192 dot products with grid cell (1, 1) and then 192 with grid cell (1, 2) or (1, 3) and so forth. So you will end up with something which has the same grid size as the input and 192 channels in the output. So that’s a really good way to either reduce the dimensionality or increase the dimensionality of an input without changing the grid size. That’s normally what we use 1x1 convs for. Here, we have a 1x1 conv and another 1x1 conv, and then they add it together. Then there is a third path and this third path is not added. It is not explicitly mentioned but this third path is concatenated. There is a form of ResNet which is basically identical to ResNet but we don’t do plus, we do concat. That’s called a DenseNet. It’s just a ResNet where we do concat instead of plus. That’s an interesting approach because then the kind of the identity path is literally being copied. So you get that flow all the way through and so as we’ll see next week, that tends to be good for segmentation and stuff like that whe re you really want to keep the original pixels, the first layer of pixels, and the second layer of pixels untouched.</p><p name="efb2" id="efb2" class="graf graf--p graf-after--p">Concatenating rather than adding branches is a very useful thing to do and we are concatenating the middle branch and the right right branch [<a href="https://youtu.be/xXXiC4YRGrQ?t=20m22s" data-href="https://youtu.be/xXXiC4YRGrQ?t=20m22s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">20:22</a>]. The right most branch is doing something interesting which is it’s doing, first of all, the 1x1 conv, and then a 1x7, and then 7x1. What’s going on there? So, what’s going on there is basically what we really want to do is do 7x7 conv. The reason we want to do 7x7 conv is that if you have multiple paths (each of which has different kernel sizes), then it’s able to look at different amounts of the image. The original Inception network had 1x1, 3x3, 5x5, 7x7 getting concatenated together or something like that. So if we can have a 7x7 filter, then we get to look at a lot of the image at once and create a really rich representation. So the stem of the Inception network that is the first few layers of the Inception network actually also used this kind fo 7x7 conv because you start out with this 224 by 224 by 3, and you want to turn it into something that’s 112 by 112 by 64. By using a 7x7 conv, you can get a lot of information in each one of those outputs to get those 64 filters. But the problem is that 7x7 conv is a lot of work. You’ve got 49 kernel values to multiply by 49 inputs for every input pixel across every channel. So the compute is crazy. You can kind of get away with it (maybe) for the very first layer, and in fact, the very first conv of ResNet is a 7x7 conv.</p><p name="9436" id="9436" class="graf graf--p graf-after--p">But not so for Inception [<a href="https://youtu.be/xXXiC4YRGrQ?t=22m30s" data-href="https://youtu.be/xXXiC4YRGrQ?t=22m30s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">22:30</a>]. They don’t do a 7x7 conv, instead, they do a 1x7 followed by 7x1. So to explain, the basic idea of the Inception networks or all the different versions of it that you have a number of separate paths which have different convolution widths. In this case, conceptually the idea is the middle path is 1x1 convolution width, and the right path is going to be a 7 convolution width, so they are looking at different amount of data and then we combine them together. But we don’t want to have a 7x7 conv through out the network because it’s just too computationally expensive.</p><p name="d071" id="d071" class="graf graf--p graf-after--p">But if you think about it [<a href="https://youtu.be/xXXiC4YRGrQ?t=23m18s" data-href="https://youtu.be/xXXiC4YRGrQ?t=23m18s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">23:18</a>], if we’ve got some input coming in and we have some big filter that we want and it’s too big to deal with. What could we do? Let’s do 5x5. What we can do is to create two filters — one which is 1x5, one which is 5x1. We take our activations of the previous layer, and we put it through the 1x5. We take the activations out of that, and put it through the 5x1, and something comes out the other end. Now what comes out the other end? Rather than thinking of it as, first of all, we take the activations, then we put it through the 1x5 then we put it through the 5x1, what if instead we think of these two operations together and say what is a 5x1 dot product and a 1x5 dot product do together? Effectively, you could take a 1x5 and 5x1 and the outer product of that is going to give you a 5x5. Now you can’t create any possible 5x5 matrix by taking that product, but there’s a lot of 5x5 matrices that you can create. So the basic idea here is when you think about the order of operations (if you are interested in more of the theory here, you should check out Rachel’s numerical linear algebra course which is basically a whole course about this). But conceptually, the idea is that very often the computation you want to do is actually more simple than an entire 5x5 convolution. Very often, the term we use in linear algebra is that there’s some lower rank approximation. In other words, that the 1x5 and the 5x1 combined together — that 5x5 matrix is nearly as good as the 5x5 matrix you ideally would have computed if you were able to. So this is very often the case in practice — just because the nature of the real world is that the real world tends to have more structure than randomness.</p><p name="e761" id="e761" class="graf graf--p graf-after--p">The cool thing is [26:16], if we replace our 7x7 conv with a 1x7 and 7x1, for each cell, it has 14 by input channel by output channel dot products to do, whereas 7x7 one has 49 to do. So it’s going to be a lot faster and we have to hope that it’s going to be nearly as good. It’s certainly capturing as much width of information by definition.</p><figure name="4dee" id="4dee" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_V1k6QCPEuoJpD2GmRrCfAQ.png"></figure><p name="ff4f" id="ff4f" class="graf graf--p graf-after--figure">If you are interested in learning more about this, specifically in a deep learning area, you can google for <strong class="markup--strong markup--p-strong">Factored Convolutions</strong>. The idea was come up with 3 or 4 years ago now. It’s probably been around for longer, but that was when I first saw it. It turned out to work really well and the Inception network uses it quite widely. They actually use it in their stem. We’ve talked before about how we tend to add-on — we tend to say this is main backbone when we have ResNet34, for example. This is main backbone which is all of the convolutions, and then we can add on to it a custom head that tends to be a max pooling or a fully connected layer. It’s better to talk about the backbone is containing two pieces: one is the stem and the other is the main backbone. The reason is that the thing that’s coming in has only 3 channels, so we want some sequence of operations which is going to expand that out into something richer — generally something like 64 channels.</p><figure name="8574" id="8574" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_5JWfOol_FWA5gjThyktI7g.png"></figure><p name="b014" id="b014" class="graf graf--p graf-after--figure">In ResNet, the stem is super simple. It’s a 7x7 stride 2 conv followed by a stride 2 max pool (I think that’s it if memory serves correctly). Inception have a much more complex stem with multiple paths getting combined and concatenated including factored conv (1x7 and 7x1). I’m interested in what would happen if you stacked a standard ResNet on top of an Inception stem, for instance. I think that would be a really interesting thing to try because an Inception stem is quite a carefully engineered thing, and this thing of how you take 3 channel input and turn it into something richer seems really important. And all of that work seems to have gotten thrown away for ResNet. We like ResNet, it works really well. But what if we put a dense net backbone on top of an Inception stem? Or what if we replaced the 7x7 conv with a 1x7 and 7x1 factored conv in standard ResNet? There are lots of things we could try and I think it would be really interesting. So there’s some more thoughts about potential research directions.</p><p name="c9a7" id="c9a7" class="graf graf--p graf-after--p">So that was kind of my little bunch of random stuff section [<a href="https://youtu.be/xXXiC4YRGrQ?t=29m51s" data-href="https://youtu.be/xXXiC4YRGrQ?t=29m51s" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">29:51</a>]. Moving a little bit closer to the actual main topic of this which is image enhancement. I’m going to talk about a new paper briefly because it really connects what I just discussed with what we are going to discuss next. It’s a paper on progressive GANs which came from Nvidia: <a href="http://research.nvidia.com/publication/2017-10_Progressive-Growing-of" data-href="http://research.nvidia.com/publication/2017-10_Progressive-Growing-of" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">Progressive Growing of GANs for Improved Quality, Stability, and Variation</a>. Progressive GANs takes this idea of gradually increasing the image size. It’s the only other direction I am aware of that people have actually gradually increase the image size. It surprises me because this paper is actually very popular, well known, and well liked and yet, people haven’t taken the basic idea of gradually increasing the image size and use it anywhere else which shows you the general level of creativity you can expect to find in the deep learning research community, perhaps.</p><figure name="4568" id="4568" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_QZSuQJD2MhWOyQ5bxTOSEg.png"></figure><p name="f12d" id="f12d" class="graf graf--p graf-after--figure">They really go back and they start with 4x4 GAN [<a href="https://youtu.be/xXXiC4YRGrQ?t=31m47s" data-href="https://youtu.be/xXXiC4YRGrQ?t=31m47s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">31:47</a>]. Literally, they are trying to replicate 4x4 pixel, and then 8x8 (the upper left ones above). This is the CelebA dataset so we are trying to recreate pictures of celebrities. Then they go 16x16, 32, 64, 128, then 256. One of the really nifty things they do is that as they increase the size, they also add more layers to the network. Which kind of makes sense because if you are doing more of a ResNet-y type thing, then you are spitting out something which hopefully makes sense at each grid cell size, so you should be able to layer stuff on top. They do another nifty thing where they add a skip connection when they do that, and they gradually change the linear interpolation parameter that moves it more and more away from the old 4x4 network and towards the new 8x8 network. Then once this totally moved it across, they throw away that extra connection. The details don’t matter too much but it uses the basic ideas we’ve talked about, gradually increasing the image size and skip connections. It’s a great paper to study because it is one of these rare things where good engineers actually built something that just works in a really sensible way. Now it’s not surprising this actually comes from Nvidia themselves. Nvidia don’t do a lot of papers and it’s interesting that when they do, they build something that is so throughly practical and sensible. So I think it’s a great paper to study if you want to put together lots of the different things we’ve learned and there aren’t many re-implementation of this so it’s an interesting thing to project, and maybe you could build on and find something else.</p><p name="2e32" id="2e32" class="graf graf--p graf-after--p">Here is what happens next [<a href="https://youtu.be/xXXiC4YRGrQ?t=33m45s" data-href="https://youtu.be/xXXiC4YRGrQ?t=33m45s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">33:45</a>]. We eventually go up to 1024x1024, and you’ll see that the images are not only getting higher resolution but they are getting better. So I am going to see if you can guess which one of the following is fake:</p><figure name="01ec" id="01ec" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_xKpY8dRD8lSeL6Twd2lcgw.png"></figure><p name="8368" id="8368" class="graf graf--p graf-after--figure">They are all fake. That’s the next stage. You go up up up up and them BOOM. So GANs and stuff are getting crazy and some of you may have seen this during the week [<a href="https://youtu.be/xXXiC4YRGrQ?t=34m16s" data-href="https://youtu.be/xXXiC4YRGrQ?t=34m16s" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">34:16</a>]. This video just came out and it’s a speech by Barack Obama and let’s check it out:</p><figure name="d94c" id="d94c" class="graf graf--figure graf--iframe graf-after--p"><iframe data-width="854" data-height="480" width="700" height="393" data-src="/media/d1eb9049368b3a8bf7a4dd9b5a92a8c2?postId=43454b21a5d0" data-media-id="d1eb9049368b3a8bf7a4dd9b5a92a8c2" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Fi.ytimg.com%2Fvi%2FcQ54GDm1eL0%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="../img/saved_resource.html"></iframe><iframe data-width="854" data-height="480" width="700" height="393" src="../img/d1eb9049368b3a8bf7a4dd9b5a92a8c2.html" data-media-id="d1eb9049368b3a8bf7a4dd9b5a92a8c2" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Fi.ytimg.com%2Fvi%2FcQ54GDm1eL0%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen="" frameborder="0"></iframe></figure><p name="7d39" id="7d39" class="graf graf--p graf-after--figure">As you can see, they’ve used this kind of technology to literally move Obama’s face in the way that Jordan Peele’s face was moving. You basically have all the techniques you need now to do that. Is that a good idea?</p><h3 name="8878" id="8878" class="graf graf--h3 graf-after--p">Ethics in AI&nbsp;[<a href="https://youtu.be/xXXiC4YRGrQ?t=35m31s" data-href="https://youtu.be/xXXiC4YRGrQ?t=35m31s" class="markup--anchor markup--h3-anchor" rel="noopener nofollow" target="_blank">35:31</a>]</h3><p name="ce1d" id="ce1d" class="graf graf--p graf-after--h3">This is the bit where we talk about what’s most important which is now that we can do all this stuff, what should we be doing and how do we think about that? The TL;DR version is I actually don’t know. Recently a lot of you saw the founders of the spaCy prodigy folks down at the Explosion AI did a talk, Matthew and Ines, and I went to dinner with them afterwards, and we basically spent the entire evening talking, debating, arguing about what does it mean the companies like ours are building tools that are democratizing access to tools that can be used in harmful ways. They are incredibly thoughtful people and we, I wouldn’t say we didn’t agree, we just couldn’t come to a conclusion ourselves. So I’m just going to lay out some of the questions and point to some of the research, and when I say research, most of the actual literature review and putting this together was done by Rachel, so thanks Rachel.</p><p name="f433" id="f433" class="graf graf--p graf-after--p">Let me start by saying the models we build are often pretty crappy in ways which are not immediately apparent [<a href="https://youtu.be/xXXiC4YRGrQ?t=36m52s" data-href="https://youtu.be/xXXiC4YRGrQ?t=36m52s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">36:52</a>]. You won’t know how crappy they are unless the people that are building them with you are a range of people and the people that are using them with you are a range of people. For example, a couple of wonderful researchers, <a href="https://twitter.com/timnitGebru" data-href="https://twitter.com/timnitGebru" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Timnit Gebru</a> is at Microsoft and <a href="https://twitter.com/jovialjoy" data-href="https://twitter.com/jovialjoy" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Joy Buolamwini</a> just finished PhD from MIT, they did this really interesting research where they looked at some off-the-shelf face recognizers, one from FACE++ which is a huge Chinese company, IBM’s, and Microsoft’s, and they looked for a range of different face types.</p><figure name="8b71" id="8b71" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_nELJUHaM-pD_MHwLz4ThEg.png"></figure><p name="c13e" id="c13e" class="graf graf--p graf-after--figure">Generally speaking, Microsoft one in particular was incredibly accurate unless the face type happened to be dark-skinned when suddenly it went 25 times worse. IBM got it wrong nearly half the time. For a big company like this to release a product that, for large percentage of the world, doesn’t work is more than a technical failure. It’s a really deep failure of understanding what kind of team needs to be used to create such a technology and to test such a technology or even an understanding of who your customers are. Some of your customers have dark skin. “I was also going to add that the classifiers all did worse on women than on men” (Rachel). Shocking. It’s funny that Rachel tweeted about something like this the other day, and some guy said “What’s this all about? What are you saying? Don’t you know people made cars for a long time — are you saying you need women to make cars too?” And Rachel pointed out — well actually yes. For most of the history of car safety, women in cars have been far more at risk of death than men in cars because the men created male looking, feeling, sized crash test dummies, so car safety was literally not tested on women size bodies. Crappy product management with a total failure of diversity and understanding is not new to our field.</p><p name="61e6" id="61e6" class="graf graf--p graf--startsWithDoubleQuote graf-after--p">“I was just going to say that was comparing impacts of similar strength for men and women ” (Rachel). I don’t know why whenever you say something like this on Twitter, Rachel has to say this because anytime you say something like this on Twitter, there’s about 10 people who’ll say “oh, you have to compare all these other things” as if we didn’t know that.</p><figure name="73a1" id="73a1" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_mHJOIpE3W1Q4Z4NAl_JWlw.png"></figure><p name="2a1b" id="2a1b" class="graf graf--p graf-after--figure">Other things our very best most famous systems do like Microsoft’s face recognizer or Google’s language translator, you turn “She is a doctor. He is a nurse.” into Turkish and quite correctly — both pronouns become O because there is no gendered pronouns in Turkish. Go the other direction, what does it get turned into? “He is a doctor. She is a nurse.” So we’ve got these kind of biases built into tools that we are all using every day. And again, people say “oh, it’s just showing us what’s in the world” and okay, there’s lots of problems with that basic assertion, but as you know, machine learning algorithms love to generalize.</p><figure name="fb59" id="fb59" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_kAtnaDbTmcC_uVWSzxOGEg.png"></figure><p name="933f" id="933f" class="graf graf--p graf-after--figure">So because they love to generalize, this is one fo the cool things about you guys knowing the technical details now, because they love to generalize when you see something like 60% of people cooking are women in the pictures they used to build this model and then you run the model on a separate set of pictures, then 84% of the people they choose as cooking are women rather than the correct 67%. Which is a really understandable thing for an algorithm to do as it took a biased input and created a more biased output because for this particular loss function, that’s where it ended up. This is a really common kind of model amplification.</p></body></html>