<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></head><body><p name="183e" id="183e" class="graf graf--p graf-after--pre">Here is the list of what each word is for wikitext103 model, and we can do the same <code class="markup--code markup--p-code">defaultdict</code> trick to map it in reverse. We’ll use -1 to mean that it is not in the wikitext dictionary.</p><pre name="64fc" id="64fc" class="graf graf--pre graf-after--p">itos2 = pickle.load((PRE_PATH<strong class="markup--strong markup--pre-strong">/</strong>'itos_wt103.pkl').open('rb'))</pre><pre name="172c" id="172c" class="graf graf--pre graf-after--pre">stoi2 = collections.defaultdict(<strong class="markup--strong markup--pre-strong">lambda</strong>:<strong class="markup--strong markup--pre-strong">-</strong>1, {v:k <strong class="markup--strong markup--pre-strong">for</strong> k,v <br>                                              <strong class="markup--strong markup--pre-strong">in</strong> enumerate(itos2)})</pre><p name="719f" id="719f" class="graf graf--p graf-after--pre">So now we can just say our new set of weights is just a whole bunch of zeros with vocab size by embedding size (i.e. we are going to create an embedding matrix) [<a href="https://youtu.be/h5Tz7gZT9Fo?t=47m57s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=47m57s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">47:57</a>]. We then go through every one of the words in our IMDb vocabulary. We are going to look it up in <code class="markup--code markup--p-code">stoi2</code> (string-to-integer for the wikitext103 vocabulary) and see if it’s a word there. If that is a word there, then we won’t get the <code class="markup--code markup--p-code">-1</code>. So <code class="markup--code markup--p-code">r</code> will be greater than or equal to zero, so in that case, we will just set that row of the embedding matrix to the weight which was stored inside the named element <code class="markup--code markup--p-code">‘0.encoder.weight’</code>. You can look at this dictionary <code class="markup--code markup--p-code">wgts</code> and it’s pretty obvious what each name corresponds to. It looks very similar to the names that you gave it when you set up your module, so here are the encoder weights.</p><p name="fda9" id="fda9" class="graf graf--p graf-after--p">If we don’t find it [<a href="https://youtu.be/h5Tz7gZT9Fo?t=49m2s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=49m2s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">49:02</a>], we will use the row mean — in other words, here is the average embedding weight across all of the wikitext103. So we will end up with an embedding matrix for every word that’s in both our vocabulary for IMDb and the wikitext103 vocab, we will use the wikitext103 embedding matrix weights; for anything else, we will just use whatever was the average weight from the wikitext103 embedding matrix.</p><pre name="cbbc" id="cbbc" class="graf graf--pre graf-after--p">new_w = np.zeros((vs, em_sz), dtype=np.float32)<br><strong class="markup--strong markup--pre-strong">for</strong> i,w <strong class="markup--strong markup--pre-strong">in</strong> enumerate(itos):<br>    r = stoi2[w]<br>    new_w[i] = enc_wgts[r] <strong class="markup--strong markup--pre-strong">if</strong> r<strong class="markup--strong markup--pre-strong">&gt;</strong>=0 <strong class="markup--strong markup--pre-strong">else</strong> row_m</pre><p name="40e2" id="40e2" class="graf graf--p graf-after--pre">We will then replace the encoder weights with <code class="markup--code markup--p-code">new_w</code> turn into a tensor [<a href="https://youtu.be/h5Tz7gZT9Fo?t=49m35s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=49m35s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">49:35</a>]. We haven’t talked much about weight tying, but basically the decoder (the thing that turns the final prediction back into a word) uses exactly the same weights, so we pop it there as well. Then there is a bit of weird thing with how we do embedding dropout that ends up with a whole separate copy of them for a reason that doesn’t matter much. So we popped the weights back where they need to go. So this is now a set of torch state which we can load in.</p><pre name="3dd9" id="3dd9" class="graf graf--pre graf-after--p">wgts['0.encoder.weight'] = T(new_w)<br>wgts['0.encoder_with_dropout.embed.weight'] = T(np.copy(new_w))<br>wgts['1.decoder.weight'] = T(np.copy(new_w))</pre><h4 name="0b0b" id="0b0b" class="graf graf--h4 graf-after--pre">Language model&nbsp;[<a href="https://youtu.be/h5Tz7gZT9Fo?t=50m18s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=50m18s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">50:18</a>]</h4><p name="5e97" id="5e97" class="graf graf--p graf-after--h4">Let’s create our language model. Basic approach we are going to use is we are going to concatenate all of the documents together into a single list of tokens of length 24,998,320. That is going to be what we pass in as a training set. So for the language model:</p><ul class="postList"><li name="a474" id="a474" class="graf graf--li graf-after--p">We take all our documents and just concatenate them back to back.</li><li name="5859" id="5859" class="graf graf--li graf-after--li">We are going to be continuously trying to predict what’s the next word after these words.</li><li name="7752" id="7752" class="graf graf--li graf-after--li">We will set up a whole bunch of dropouts.</li><li name="8f84" id="8f84" class="graf graf--li graf-after--li">Once we have a model data object, we can grab the model from it, so that’s going to give us a learner.</li><li name="db38" id="db38" class="graf graf--li graf-after--li">Then as per usual, we can call <code class="markup--code markup--li-code">learner.fit</code>. We do a single epoch on the last layer just to get that okay. The way it’s set up is the last layer is the embedding words because that’s obviously the thing that’s going to be the most wrong because a lot of those embedding weights didn’t even exist in the vocab. So we will train a single epoch of just the embedding weights.</li><li name="51c2" id="51c2" class="graf graf--li graf-after--li">Then we’ll start doing a few epochs of the full model. How is it looking? In lesson 4, we had the loss of 4.23 after 14 epochs. In this case, we have 4.12 loss after 1 epoch. So by pre-training on wikitext103, we have a better loss after 1 epoch than the best loss we got for the language model otherwise.</li></ul><p name="3bb4" id="3bb4" class="graf graf--p graf-after--li graf--trailing"><strong class="markup--strong markup--p-strong">Question</strong>: What is the wikitext103 model? Is it a AWD LSTM again [<a href="https://youtu.be/h5Tz7gZT9Fo?t=52m41s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=52m41s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">52:41</a>]? Yes, we are about to dig into that. The way I trained it was literally the same lines of code that you see above, but without pre-training it on wikitext103.</p><hr class="section-divider"><h4 name="c4bf" id="c4bf" class="graf graf--h4 graf--leading">A quick discussion about fastai doc project&nbsp;[<a href="https://youtu.be/h5Tz7gZT9Fo?t=53m7s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=53m7s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">53:07</a>]</h4><p name="7bad" id="7bad" class="graf graf--p graf-after--h4">The goal of fastai doc project is to create documentation that makes readers say “wow, that’s the most fantastic documentation I’ve ever read” and we have some specific ideas about how to do that. It’s the same kind of idea of top-down, thoughtful, take full advantage of the medium approach, interactive experimental code first that we are all familiar with. If you are interested in getting involved, you can see the basic approach in <a href="https://github.com/fastai/fastai/tree/master/docs" data-href="https://github.com/fastai/fastai/tree/master/docs" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">the docs directory</a>. In there, there is, amongst other things, <a href="https://raw.githubusercontent.com/fastai/fastai/master/docs/transforms-tmpl.adoc" data-href="https://raw.githubusercontent.com/fastai/fastai/master/docs/transforms-tmpl.adoc" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">transforms-tmpl.adoc</a>. <code class="markup--code markup--p-code">adoc</code> is <a href="http://asciidoc.org/" data-href="http://asciidoc.org/" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">AsciiDoc</a>. AsciiDoc is like markdown but it’s like what markdown needs to be to create actual books. A lot of actual books are written in AsciiDoc and it’s as easy to use as markdown but there’s way more cool stuff you can do with it. <a href="https://raw.githubusercontent.com/fastai/fastai/master/docs/transforms.adoc" data-href="https://raw.githubusercontent.com/fastai/fastai/master/docs/transforms.adoc" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Here</a> is more standard asciiDoc example. You can do things like inserting a table of contents (<code class="markup--code markup--p-code">:toc:</code>).&nbsp;<code class="markup--code markup--p-code">::</code> means put a definition list here. <code class="markup--code markup--p-code">+</code> means this is a continuation of the previous list item. So there are many super handy features and it is like turbo-charged markdown. So this asciidoc creates this HTML and no custom CSS or anything added:</p><figure name="19e0" id="19e0" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_9UfkC1UD_8TZP0PpTbJhdg.png"></figure><p name="c69b" id="c69b" class="graf graf--p graf-after--figure">We literally started this project 4 hours ago. So you have a table of contents with hyper links to specific sections. We have cross reference we can click on to jump straight to the cross reference. Each method comes along with its details and so on. To make things even easier, they’ve created a special template for argument, cross reference, method, etc. The idea is, it will almost be like a book. There will be tables, pictures, video segments, and hyperlink throughout.</p><p name="13c4" id="13c4" class="graf graf--p graf-after--p graf--trailing">You might be wondering what about docstrings. But actually, if you look at the Python standard library and look at the docstring for <code class="markup--code markup--p-code">re.compile()</code>, for example, it’s a single line. Nearly every docstring in Python is a single line. And Python then does exactly this — they have a website containing the documentation that says “this is what regular expressions are, and this is what you need to know about them, and if you want do them fast, you need to compile, and here is some information about compile” etc. These information is not in the docstring and that’s how we are going to do as well — our docstring will be one line unless you need like two sometimes. Everybody is welcome to help contribute to the documentation.</p><hr class="section-divider"><p name="f4f0" id="f4f0" class="graf graf--p graf--leading"><strong class="markup--strong markup--p-strong">Question</strong>: Hoes this compare to word2vec [<a href="https://youtu.be/h5Tz7gZT9Fo?t=58m31s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=58m31s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">58:31</a>]? This is actually a great thing for you to spend time thinking about during the week. I’ll give you the summary now but it’s a very important conceptual difference. The main conceptual difference is “what is word2vec?” Word2vec is a single embedding matrix — each word has a vector and that’s it. In other words, it’s a single layer from a pre-trained model — specifically that layer is the input layer. Also specifically that pre-trained model is a linear model that is pre-trained on something called a co-occurrence matrix. So we have no particular reason to believe that this model has learned anything much about English language or that it has any particular capabilities because it’s just a single linear layer and that’s it. What’s this wikitext103 model? It’s a language model and it has a 400 dimensional embedding matrix, 3 hidden layers with 1,150 activations per layer, and regularization and all that stuff tied input output matrices — it’s basically a state-of-the-art AWD LSTM. What’s the difference between a single layer of a single linear model vs. a three layer recurrent neural network? Everything! They are very different levels of capabilities. So you will see when you try using a pre-trained language model vs. word2vec layer, you’ll get very different results for the vast majority of tasks.</p><p name="a03a" id="a03a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: What if the numpy array does not fit in memory? Is it possible to write a PyTorch data loader directly from a large CSV file [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h32s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h32s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:00:32</a>]? It almost certainly won’t come up, so I’m not going to spend time on it. These things are tiny — they are just integers. Think about how many integers you would need to run out of memories? That’s not gonna happen. They don’t have to fit in GPU memory, just in your memory. I’ve actually done another Wikipedia model which I called giga wiki which was on all of Wikipedia and even that easily fits in memory. The reason I’m not using it is because it turned out not to really help very much vs. wikitext103. I’ve built a bigger model than anybody else I’ve found in the academic literature and it fits in memory on a single machine.</p><p name="5a1d" id="5a1d" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: What is the idea behind averaging the weights of embeddings [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h1m24s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h1m24s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:01:24</a>]? They have to be set to something. These are words that weren’t there, so the other option is we could leave them as zero. But that seems like a very extreme thing to do. Zero is a very extreme number. Why would it be zero? We could set it equal to some random numbers, but if so, what would be the mean and standard deviation of those random numbers? Should they be uniform? If we just average the rest of the embeddings, then we have something that’s reasonably scaled. Just to clarify, this is how we are initializing words that didn’t appear in the training corpus.</p><h4 name="5f8b" id="5f8b" class="graf graf--h4 graf-after--p">Back to Language Model [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h2m20s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h2m20s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:02:20</a>]</h4><p name="19c7" id="19c7" class="graf graf--p graf-after--h4">This is a ton of stuff we’ve seen before, but it’s changed a little bit. It’s actually a lot easier than it was in part 1, but I want to go a little bit deeper into the language model loader.</p><pre name="869e" id="869e" class="graf graf--pre graf-after--p">wd=1e-7<br>bptt=70<br>bs=52<br>opt_fn = partial(optim.Adam, betas=(0.8, 0.99))</pre><pre name="f38a" id="f38a" class="graf graf--pre graf-after--pre">t = len(np.concatenate(trn_lm))<br>t, t//64</pre><pre name="9669" id="9669" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(24998320, 390598)</em></pre><p name="3d67" id="3d67" class="graf graf--p graf-after--pre">This is the <code class="markup--code markup--p-code">LanguageModelLoader</code> and I really hope that by now, you’ve learned in your editor or IDE how to jump to symbols [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h2m37s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h2m37s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:02:37</a>]. I don’t want it to be a burden for you to find out what the source code of <code class="markup--code markup--p-code">LanguageModelLoader</code> is. If your editor doesn’t make it easy, don’t use that editor anymore. There’s lots of good free editors that make this easy.</p><p name="e810" id="e810" class="graf graf--p graf-after--p">So this is the source code for LanguageModelLoader, and it’s interesting to notice that it’s not doing anything particularly tricky. It’s not deriving from anything at all. What makes something that’s capable of being a data loader is that it’s something you can iterate over.</p><figure name="b3cc" id="b3cc" class="graf graf--figure graf--layoutOutsetCenter graf-after--p" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_ttM96lLbHQn06byFwmHj0g.png"></figure><p name="9816" id="9816" class="graf graf--p graf-after--figure">Here is the <code class="markup--code markup--p-code">fit</code> function inside fastai.model [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h3m41s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h3m41s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:03:41</a>]. This is where everything ends up eventually which goes through each epoch, creates an iterator from the data loader, and then just does a for loop through it. So anything you can do a for loop through can be a data loader. Specifically it needs to return tuples of independent and dependent variables for mini-batches.</p><figure name="23de" id="23de" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_560U29nWI0xNGLsHgnWFNQ.png"></figure><p name="500c" id="500c" class="graf graf--p graf-after--figure">So anything with a <code class="markup--code markup--p-code">__iter__</code> method is something that can act as an iterator [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h4m9s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h4m9s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:04:09</a>]. <code class="markup--code markup--p-code">yield</code> is a neat little Python keywords you probably should learn about if you don’t already know it. But it basically spits out a thing and waits for you to ask for another thing — normally in a for loop or something. In this case, we start by initializing the language model passing it in the numbers <code class="markup--code markup--p-code">nums</code> this is the numericalized long list of all of our documents concatenated together. The first thing we do is to “batchfy” it. This is the thing which quite a few of you got confused about last time. If our batch size is 64 and we have 25 million numbers in our list. We are not creating items of length 64 — we are creating 64 items in total. So each of them is of size <code class="markup--code markup--p-code">t</code> divided by 64 which is 390k. So that’s what we do here:</p><p name="b3ca" id="b3ca" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">data = data.view(self.bs, -1).t().contiguous()</code></p><p name="d852" id="d852" class="graf graf--p graf-after--p">We reshape it so that this axis is of length 64 and <code class="markup--code markup--p-code">-1</code> is everything else (390k blob), and we transpose it. So that means that we now have 64 columns, 390k rows. Then what we do each time we do an iterate is we grab one batch of some sequence length, which is approximately equal to <code class="markup--code markup--p-code">bptt</code> (back prop through time) which we set to 70. We just grab that many rows. So from <code class="markup--code markup--p-code">i</code> to <code class="markup--code markup--p-code">i+70</code> rows, we try to predict that plus one. Remember, we are trying to predict one past where we are up to.</p><p name="8e87" id="8e87" class="graf graf--p graf-after--p">So we have 64 columns and each of those is 1/64th of our 25 million tokens, and hundreds of thousands long, and we just grab 70 at a time [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h6m29s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h6m29s" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">1:06:29</a>]. So each of those columns, each time we grab it, it’s going to kind of hook up to the previous column. That’s why we get this consistency. This language model is stateful which is really important.</p><p name="aff7" id="aff7" class="graf graf--p graf-after--p">Pretty much all of the cool stuff in the language model is stolen from Stephen Merity’s AWD-LSTM [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h6m59s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h6m59s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:06:59</a>] including this little trick here:</p><figure name="6739" id="6739" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Mv0c41-UvTGlNKHMuPlsHw.png" data-width="476" data-height="48" src="../img/1_Mv0c41-UvTGlNKHMuPlsHw.png"></figure><p name="4cba" id="4cba" class="graf graf--p graf-after--figure">If we always grab 70 at a time and then we go back and do a new epoch, we’re going to grab exactly the same batches every time — there is no randomness. Normally, we shuffle our data every time we do an epoch or every time we grab some data we grab it at random. You can’t do that with a language model because this set has to join up to the previous set because it’s trying to learn the sentence. If you suddenly jump somewhere else, that doesn’t make any sense as a sentence. So Stephen’s idea is to say “okay, since we can’t shuffle the order, let’s instead randomly change the sequence length”. Basically, 95% of the time, we will use <code class="markup--code markup--p-code">bptt</code> (i.e. 70) but 5% of the time, we’ll use half that. Then he says “you know what, I’m not even going to make that the sequence length, I’m going to create a normally distributed random number with that average and a standard deviation of 5, and I’ll make that the sequence length.” So the sequence length is seventy-ish and that means every time we go through, we are getting slightly different batches. So we’ve got that little bit of extra randomness. Jeremy asked Stephen Merity where he came up with this idea, did he think of it? and he said “I think I thought of it, but it seemed so obvious that I bet I didn’t think of it” — which is true of every time Jeremy comes up with an idea in deep learning. It always seems so obvious that you just assume somebody else has thought of it. But Jeremy thinks Stephen thought of it.</p><p name="cb8e" id="cb8e" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">LanguageModelLoader</code> is a nice thing to look at if you are trying to do something a bit unusual with a data loader [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h8m55s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h8m55s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:08:55</a>]. It’s a simple role model you can use as to creating a data loader from scratch — something that spits out batches of data.</p><p name="e73e" id="e73e" class="graf graf--p graf-after--p">Our language model loader took in all of the documents concatenated together along with batch size and bptt [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h9m14s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h9m14s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:09:14</a>].</p><pre name="f6cc" id="f6cc" class="graf graf--pre graf-after--p">trn_dl = LanguageModelLoader(np.concatenate(trn_lm), bs, bptt)<br>val_dl = LanguageModelLoader(np.concatenate(val_lm), bs, bptt)<br>md = LanguageModelData(PATH, 1, vs, trn_dl, val_dl, bs=bs, <br>                       bptt=bptt)</pre><p name="9fe9" id="9fe9" class="graf graf--p graf-after--pre">Now generally speaking, we want to create a learner and the way we normally do that is by getting a model data object and calling some kind of method which have various names but often we call that method <code class="markup--code markup--p-code">get_model</code>. The idea is that the model data object has enough information to know what kind of model to give you. So we have to create that model data object which means we need LanguageModelData class which is very easy to do [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h9m51s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h9m51s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:09:51</a>].</p><p name="0965" id="0965" class="graf graf--p graf-after--p">Here are all of the pieces. We are going to create a custom learner, a custom model data class, and a custom model class. So a model data class, again this one doesn’t inherit from anything so you really see there’s almost nothing to do. You need to tell it most importantly what’s your training set (give it a data loader), what’s the validation set (give it a data loader), and optionally, give it a test set (data loader), plus anything else that needs to know. It might need to know the bptt, it needs to know number of tokens(i.e. the vocab size), and it needs to know what is the padding index. And so that it can save temporary files and models, model datas as always need to know the path. So we just grab all that stuff and we dump it. That’s it. That’s the entire initializer. There is no logic there at all.</p><figure name="d1c5" id="d1c5" class="graf graf--figure graf--layoutOutsetCenter graf-after--p" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_GPeBIZ7A9P8gdCulCCrREw.png"></figure><p name="7ffb" id="7ffb" class="graf graf--p graf-after--figure">Then all of the work happens inside <code class="markup--code markup--p-code">get_model</code>[<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h10m55s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h10m55s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:10:55</a>]. get_model calls something we will look at later, which just grabs a normal PyTorch nn.Module architecture, and chucks it on GPU. Note: with PyTorch, we would say&nbsp;<code class="markup--code markup--p-code">.cuda()</code>, with fastai it’s better to say <code class="markup--code markup--p-code">to_gpu()</code>, the reason is that if you don’t have GPU, it will leave it on the CPU. It also provides a global variable you can set to choose whether it goes on the GPU or not, so it’s a better approach. We wrapped the model in a <code class="markup--code markup--p-code">LanguageModel</code> and the <code class="markup--code markup--p-code">LanguageModel</code> is a subclass of <code class="markup--code markup--p-code">BasicModel</code> which almost does nothing except it defines layer groups. Remember when we do discriminative learning rates where different layers have different learning rates or we freeze different amounts, we don’t provide a different learning rate for every layer because there can be a thousand layers. We provide a different learning rate for every layer group. So when you create a custom model, you just have to override this one thing which returns a list of all of your layer groups. In this case, the last layer group contains the last part of the model and one bit of dropout. The rest of it (<code class="markup--code markup--p-code">*</code> here means pull this apart) so this is going to be one layer per RNN layer. So that’s all that is.</p><p name="f8e3" id="f8e3" class="graf graf--p graf-after--p">Then finally turn that into a learner [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h12m41s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h12m41s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:12:41</a>]. So a learner, you just pass in the model and it turns it into a learner. In this case, we have overridden learner and the only thing we’ve done is to say I want the default loss function to be cross entropy. This entire set of custom model, custom model data, custom learner all fits on a single screen. They always basically look like this.</p><p name="3c71" id="3c71" class="graf graf--p graf-after--p">The interesting part of this code base is <code class="markup--code markup--p-code">get_language_model</code> [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h13m18s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h13m18s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:13:18</a>]. Because that gives us our AWD LSTM. It actually contains the big idea. The big, incredibly simple idea that everybody else here thinks it’s really obvious that everybody in the NLP community Jeremy spoke to thought was insane. That is, every model can be thought of as a backbone plus a head, and if you pre-train the backbone and stick on a random head, you can do fine-tuning and that’s a good idea.</p><figure name="52e0" id="52e0" class="graf graf--figure graf--layoutOutsetCenter graf-after--p" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_QoAsI-zGJ3XKMBDY-3o1Rg.png"></figure><p name="2276" id="2276" class="graf graf--p graf-after--figure">These two bits of code, literally right next to each other, this is all there is inside <code class="markup--code markup--p-code">fastai.lm_rnn</code>.</p><p name="c171" id="c171" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">get_language_model</code>: Creates an RNN encoder and then creates a sequential model that sticks on top of that — a linear decoder.</p><p name="3f23" id="3f23" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">get_rnn_classifier</code>: Creates an RNN encoder, then a sequential model that sticks on top of that — a pooling linear classifier.</p><p name="0de8" id="0de8" class="graf graf--p graf-after--p">We’ll see what these differences are in a moment, but you get the basic idea. They are doing pretty much the same thing. They’ve got this head and they are sticking on a simple linear layer on top.</p><p name="e2b0" id="e2b0" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: There was a question earlier about whether that any of this translates to other languages [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h14m52s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h14m52s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:14:52</a>]. Yes, this whole thing works in any languages. Would you have to retrain your language model on a corpus from that language? Absolutely! So the wikitext103 pre-trained language model knows English. You could use it maybe as a pre-trained start for like French or German model, start by retraining the embedding layer from scratch might be helpful. Chinese, maybe not so much. But given that a language model can be trained from any unlabeled documents at all, you’ll never have to do that. Because almost every language in the world has plenty of documents — you can grab newspapers, web pages, parliamentary records, etc. As long as you have a few thousand documents showing somewhat normal usage of that language, you can create a language model. One of our students tried this approach for Thai and he said the first model he built easily beat the previous state-of-the-art Thai classifier. For those of you that are international fellow, this is an easy way for you to whip out a paper in which you either create the first ever classifier in your language or beat everybody else’s classifier in your language. Then you can tell them that you’ve been a student of deep learning for six months and piss off all the academics in your country.</p><p name="34a9" id="34a9" class="graf graf--p graf-after--p">Here is our RNN encoder [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h16m49s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h16m49s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:16:49</a>]. It is a standard nn.Module. It looks like there is more going on in it than there actually is, but really all there is is we create an embedding layer, create an LSTM for each layer that’s been asked for, that’s it. Everything else in it is dropout. Basically all of the interesting stuff (just about) in the AWS LSTM paper is all of the places you can put dropout. Then the forward is basically the same thing. Call the embedding layer, add some dropout, go through each layer, call that RNN layer, append it to our list of outputs, add dropout, that’s about it. So it’s pretty straight forward.</p><figure name="41b8" id="41b8" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_HrRraVW1kuyghw-PIhV89g.png"></figure><p name="09ca" id="09ca" class="graf graf--p graf-after--figure">The paper you want to be reading is the AWD LSTM paper which is <a href="https://arxiv.org/abs/1708.02182" data-href="https://arxiv.org/abs/1708.02182" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">Regularizing and Optimizing LSTM Language Models</a>. It’s well written, pretty accessible, and entirely implemented inside fastai as well — so you can see all of the code for that paper. A lot of the code actually is shamelessly plagiarized with Stephen’s permission from his excellent GitHub repo <a href="https://github.com/Smerity/awd-lstm-lm" data-href="https://github.com/Smerity/awd-lstm-lm" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">AWD LSTM</a>.</p><p name="b0ac" id="b0ac" class="graf graf--p graf-after--p">The paper refers to other papers. For things like why is it that the encoder weight and the decoder weight are the same. It’s because there is this thing called “tie weights.” Inside <code class="markup--code markup--p-code">get_language_model</code>, there is a thing called <code class="markup--code markup--p-code">tie_weights</code> which defaults to true. If it’s true, then we literally use the same weight matrix for the encoder and the decoder. They are pointing at the same block of memory. Why is that? What’s the result of it? That’s one of the citations in Stephen’s paper which is also a well written paper you can look up and learn about weight tying.</p><figure name="f68b" id="f68b" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_b0FeRkWrz1MxE96PMak8xw.png"></figure><p name="aceb" id="aceb" class="graf graf--p graf-after--figure">We have basically a standard RNN [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h19m52s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h19m52s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:19:52</a>]. The only reason where it’s not standard is it has lots more types of dropout in it. In a sequential model on top of the RNN, we stick a linear decoder which is literally half the screen of code. It has a single linear layer, we initialize the weights to some range, we add some dropout, and that’s it. So it’s a linear layer with dropout.</p><figure name="26f4" id="26f4" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_8qFWffVOekS8lvZYmZIUdA.png"></figure><p name="2eff" id="2eff" class="graf graf--p graf-after--figure">So the language model is:</p><ul class="postList"><li name="c7d8" id="c7d8" class="graf graf--li graf-after--p">RNN → A linear layer with dropout</li></ul><h4 name="2201" id="2201" class="graf graf--h4 graf-after--li">Choosing dropout [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h20m36s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h20m36s" class="markup--anchor markup--h4-anchor" rel="noopener nofollow" target="_blank">1:20:36</a>]</h4><p name="3d4a" id="3d4a" class="graf graf--p graf-after--h4">What dropout you choose matters a lot&nbsp;.Through a lot of experimentation, Jeremy found a bunch of dropouts that tend to work pretty well for language models. But if you have less data for your language model, you’ll need more dropout. If you have more data, you can benefit from less dropout. You don’t want to regularize more than you have to. Rather than having to tune every one of these five things, Jeremy’s claim is they are already pretty good ratios to each other, so just tune this number (<code class="markup--code markup--p-code">0.7</code> below), we just multiply it all by something. If you are overfitting, then you’ll need to increase the number, if you are underfitting, you’ll need to decrease this. Because other than that, these ratio seem pretty good.</p><pre name="b878" id="b878" class="graf graf--pre graf-after--p">drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*<strong class="markup--strong markup--pre-strong">0.7</strong></pre><pre name="f140" id="f140" class="graf graf--pre graf-after--pre">learner= md.get_model(opt_fn, em_sz, nh, nl, <br>    dropouti=drops[0], dropout=drops[1], wdrop=drops[2],<br>    dropoute=drops[3], dropouth=drops[4])</pre><pre name="2af8" id="2af8" class="graf graf--pre graf-after--pre">learner.metrics = [accuracy]<br>learner.freeze_to(-1)</pre><h4 name="9764" id="9764" class="graf graf--h4 graf-after--pre">Measuring accuracy [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h21m45s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h21m45s" class="markup--anchor markup--h4-anchor" rel="noopener nofollow" target="_blank">1:21:45</a>]</h4><p name="4625" id="4625" class="graf graf--p graf-after--h4">One important idea which may seem minor but again it’s incredibly controversial is that we should measure accuracy when we look at a language model&nbsp;. Normally for language models, we look at a loss value which is just cross entropy loss but specifically we nearly always take e to the power of that which the NLP community calls “perplexity”. So perplexity is just <code class="markup--code markup--p-code">e^(cross entropy)</code>. There is a lot of problems with comparing things based on cross entropy loss. Not sure if there’s time to go into it in detail now, but the basic problem is that it is like that thing we learned about focal loss. Cross entropy loss — if you are right, it wants you to be really confident that you are right. So it really penalizes a model that doesn’t say “I’m so sure this is wrong” and it’s wrong. Whereas accuracy doesn’t care at all about how confident you are — it cares about whether you are right. This is much more often the thing which you care about in real life. The accuracy is how often do we guess the next word correctly and it’s a much more stable number to keep track of. So that’s a simple little thing that Jeremy does.</p><pre name="a00b" id="a00b" class="graf graf--pre graf-after--p">learner.model.load_state_dict(wgts)</pre><pre name="41d0" id="41d0" class="graf graf--pre graf-after--pre">lr=1e-3<br>lrs = lr</pre><pre name="60c5" id="60c5" class="graf graf--pre graf-after--pre">learner.fit(lrs/2, 1, wds=wd, use_clr=(32,2), cycle_len=1)</pre><pre name="65c1" id="65c1" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss   accuracy                     <br>    0      4.398856   4.175343   0.28551</em></pre><pre name="5aa5" id="5aa5" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[4.175343, 0.2855095456305303]</em></pre><pre name="694c" id="694c" class="graf graf--pre graf-after--pre">learner.save('lm_last_ft')</pre><pre name="82ad" id="82ad" class="graf graf--pre graf-after--pre">learner.load('lm_last_ft')</pre><pre name="c5f8" id="c5f8" class="graf graf--pre graf-after--pre">learner.unfreeze()</pre><pre name="ab35" id="ab35" class="graf graf--pre graf-after--pre">learner.lr_find(start_lr=lrs/10, end_lr=lrs*10, linear=<strong class="markup--strong markup--pre-strong">True</strong>)</pre><pre name="5757" id="5757" class="graf graf--pre graf-after--pre">learner.sched.plot()</pre><p name="f2da" id="f2da" class="graf graf--p graf-after--pre">We train for a while and we get down to a 3.9 cross entropy loss which is equivalent of ~49.40 perplexity (<code class="markup--code markup--p-code">e^3.9</code>) [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h23m14s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h23m14s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:23:14</a>]. To give you a sense of what’s happening with language models, if you look at academic papers from about 18 months ago, you’ll see them talking about state-of-the-art perplexity of over a hundred. The rate at which our ability to understand language and measuring language model accuracy or perplexity is not a terrible proxy for understanding language. If I can guess what you are going to say next, I need to understand language well and the kind of things you might talk about pretty well. The perplexity number has just come down so much that it’s been amazing, and it will come down a lot more. NLP in the last 12–18 months, it really feels like 2011–2012 computer vision. We are starting to understand transfer learning and fine-tuning, and basic models are getting so much better. Everything you thought about what NLP can and can’t do is rapidly going out of date. There’s still lots of things NLP is not good at to be clear. Just like in 2012, there were lots of stuff computer vision wasn’t good at. But it’s changing incredibly rapidly and now is a very very good time to be getting very good at NLP or starting startups base on NLP because there is a whole bunch of stuff which computers would absolutely terrible at two years ago and now not quite good as people and then next year, they’ll be much better than people.</p><pre name="700a" id="700a" class="graf graf--pre graf-after--p">learner.fit(lrs, 1, wds=wd, use_clr=(20,10), cycle_len=15)</pre><pre name="849a" id="849a" class="graf graf--pre graf-after--pre">epoch      trn_loss   val_loss   accuracy                     <br>    0      4.332359   4.120674   0.289563  <br>    1      4.247177   4.067932   0.294281                     <br>    2      4.175848   4.027153   0.298062                     <br>    3      4.140306   4.001291   0.300798                     <br>    4      4.112395   3.98392    0.302663                     <br>    5      4.078948   3.971053   0.304059                     <br>    6      4.06956    3.958152   0.305356                     <br>    7      4.025542   3.951509   0.306309                     <br>    8      4.019778   3.94065    0.30756                      <br>    9      4.027846   3.931385   0.308232                     <br>    10     3.98106    3.928427   0.309011                     <br>    11     3.97106    3.920667   0.30989                      <br>    12     3.941096   3.917029   0.310515                     <br>    13     3.924818   3.91302    0.311015                     <br>    14     3.923296   3.908476   0.311586</pre><pre name="7027" id="7027" class="graf graf--pre graf-after--pre">[3.9084756, 0.3115861900150776]</pre><p name="2c4f" id="2c4f" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">Question</strong>: What is your ratio of paper reading vs. coding in a week [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h25m24s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h25m24s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:25:24</a>]? Gosh, what do you think, Rachel? You see me. I mean, it’s more coding, right? “It’s a lot more coding. I feel like it also really varies from week to week” (Rachel). With that bounding box stuff, there were all these papers and no map through them, so I didn’t even know which one to read first and then I’d read the citations and didn’t understand any of them. So there was a few weeks of just kind of reading papers before I even know what to start coding. That’s unusual though. Anytime I start reading a paper, I’m always convinced that I’m not smart enough to understand it, always, regardless of the paper. And somehow eventually I do. But I try to spend as much time as I can coding.</p><p name="4b3e" id="4b3e" class="graf graf--p graf-after--p">Nearly always after I’ve read a paper [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h26m34s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h26m34s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:26:34</a>], even after I’ve read the bit that says this is the problem I’m trying to solve, I’ll stop there and try to implement something that I think might solve that problem. And then I’ll go back and read the paper, and I read little bits about these are how I solve these problem bits, and I’ll be like “oh that’s a good idea” and then I’ll try to implement those. That’s why for example, I didn’t actually implement SSD. My custom head is not the same as their head. It’s because I kind of read the gist of it and then I tried to create something as best as I could, then go back to the papers and try to see why. So by the time I got to the focal loss paper, Rachel will tell you, I was driving myself crazy with how come I can’t find small objects? How come it’s always predicting background? I read the focal loss paper and I was like “that’s why!!” It’s so much better when you deeply understand the problem they are trying to solve. I do find the vast majority of the time, by the time I read that bit of the paper which is solving a problem, I’m then like “yeah, but these three ideas I came up with, they didn’t try.” Then you suddenly realize that you’ve got new ideas. Or else, if you just implement the paper mindlessly, you tend not to have these insights about better ways to do it&nbsp;.</p><p name="6bd2" id="6bd2" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: Is your dropout rate the same through the training or do you adjust it and weights accordingly [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h26m27s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h26m27s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:26:27</a>]? Varying dropout is really interesting and there are some recent papers that suggest gradually changing dropout [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h28m9s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h28m9s" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">1:28:09</a>]. It was either good idea to gradually make it smaller or gradually make it bigger, I’m not sure which. Maybe one of us can try and find it during the week. I haven’t seen it widely used. I tried it a little bit with the most recent paper I wrote and I had some good results. I think I was gradually make it smaller, but I can’t remember.</p><p name="34e1" id="34e1" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: Am I correct in thinking that this language model is build on word embeddings? Would it be valuable to try this with phrase or sentence embeddings? I ask this because I saw from Google the other day, universal sentence encoder [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h28m45s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h28m45s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:28:45</a>]. This is much better than that. This is not just an embedding of a sentence, this is an entire model. An embedding by definition is like a fixed thing. A sentence or a phase embedding is always a model that creates that. We’ve got a model that’s trying to understand language. It’s not just as phrase or as sentence — it’s a document in the end, and it’s not just an embedding that we are training through the whole thing. This has been a huge problem with NLP for years now is this attachment they have to embeddings. Even the paper that the community has been most excited about recently from <a href="http://allenai.org/" data-href="http://allenai.org/" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">AI2</a> (Allen Institute for Artificial Intelligence) called ELMo — they found much better results across lots of models, but again it was an embedding. They took a fixed model and created a fixed set of numbers which they then fed into a model. But in computer vision, we’ve known for years that that approach of having fixed set of features, they’re called hyper columns in computer vision, people stopped using them like 3 or 4 years ago because fine-tuning the entire model works much better. For those of you that have spent quite a lot of time with NLP and not much time with computer vision, you’re going to have to start re-learning. All that stuff you have been told about this idea that there are these things called embeddings and that you learn them ahead of time and then you apply these fixed things whether it be word level or phrase level or whatever level — don’t do that. You want to actually create a pre-trained model and fine-tune it end-to-end, then you’ll see some specific results.</p><p name="b97f" id="b97f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: For using accuracy instead of perplexity as a metric for the model, could we work that into the loss function rather than just use it as a metric [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h31m21s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h31m21s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:31:21</a>]? No, you never want to do that whether it be computer vision or NLP or whatever. It’s too bumpy. So cross entropy is fine as a loss function. And I’m not saying instead of, I use it in addition to. I think it’s good to look at the accuracy and to look at the cross entropy. But for your loss function, you need something nice and smoothy. Accuracy doesn’t work very well.</p><pre name="8db6" id="8db6" class="graf graf--pre graf-after--p">learner.save('lm1')<br>learner.save_encoder('lm1_enc')</pre><h4 name="0ad3" id="0ad3" class="graf graf--h4 graf-after--pre"><code class="markup--code markup--h4-code">save_encoder</code> [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h31m55s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h31m55s" class="markup--anchor markup--h4-anchor" rel="noopener nofollow" target="_blank">1:31:55</a>]</h4><p name="de3b" id="de3b" class="graf graf--p graf-after--h4">You’ll see there are two different versions of <code class="markup--code markup--p-code">save</code>. <code class="markup--code markup--p-code">save</code> saves the whole model as per usual. <code class="markup--code markup--p-code">save_encoder</code> just saves that bit:</p><figure name="66f5" id="66f5" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_H8mfqVgmT04qnT1ludJRFQ.png"></figure><p name="2569" id="2569" class="graf graf--p graf-after--figure">In other words, in the sequential model, it saves just <code class="markup--code markup--p-code">rnn_enc</code> and not <code class="markup--code markup--p-code">LinearDecoder(n_tok, emb_sz, dropout, tie_encoder=enc)</code> (which is the bit that actually makes it into a language model). We don’t care about that bit in the classifier, we just care about <code class="markup--code markup--p-code">rnn_end</code>. That’s why we save two different models here.</p><pre name="d43a" id="d43a" class="graf graf--pre graf-after--p">learner.sched.plot_loss()</pre><figure name="9e5a" id="9e5a" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_NI2INKONs4lYhviEqp3zFQ.png"></figure><h4 name="daab" id="daab" class="graf graf--h4 graf-after--figure">Classifier tokens [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h32m31s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h32m31s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:32:31</a>]</h4><p name="a303" id="a303" class="graf graf--p graf-after--h4">Let’s now create the classifier. We will go through this pretty quickly because it’s the same. But when you go back during the week and look at the code, convince yourself it’s the same.</p><pre name="d86d" id="d86d" class="graf graf--pre graf-after--p">df_trn = pd.read_csv(CLAS_PATH/'train.csv', header=<strong class="markup--strong markup--pre-strong">None</strong>, <br>                     chunksize=chunksize)<br>df_val = pd.read_csv(CLAS_PATH/'test.csv', header=<strong class="markup--strong markup--pre-strong">None</strong>, <br>                     chunksize=chunksize)</pre><pre name="4c43" id="4c43" class="graf graf--pre graf-after--pre">tok_trn, trn_labels = get_all(df_trn, 1)<br>tok_val, val_labels = get_all(df_val, 1)</pre><pre name="d47b" id="d47b" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">0<br>1<br>0<br>1</em></pre><pre name="af54" id="af54" class="graf graf--pre graf-after--pre">(CLAS_PATH/'tmp').mkdir(exist_ok=<strong class="markup--strong markup--pre-strong">True</strong>)</pre><pre name="48d8" id="48d8" class="graf graf--pre graf-after--pre">np.save(CLAS_PATH/'tmp'/'tok_trn.npy', tok_trn)<br>np.save(CLAS_PATH/'tmp'/'tok_val.npy', tok_val)</pre><pre name="40a1" id="40a1" class="graf graf--pre graf-after--pre">np.save(CLAS_PATH/'tmp'/'trn_labels.npy', trn_labels)<br>np.save(CLAS_PATH/'tmp'/'val_labels.npy', val_labels)</pre><pre name="6ab6" id="6ab6" class="graf graf--pre graf-after--pre">tok_trn = np.load(CLAS_PATH/'tmp'/'tok_trn.npy')<br>tok_val = np.load(CLAS_PATH/'tmp'/'tok_val.npy')</pre><p name="8557" id="8557" class="graf graf--p graf-after--pre">We don’t create a new <code class="markup--code markup--p-code">itos</code> vocabulary, we obviously want to use the same vocabulary we had in the language model because we are about to reload the same encoder [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h32m48s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h32m48s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:32:48</a>].</p><pre name="64b2" id="64b2" class="graf graf--pre graf-after--p">itos = pickle.load((LM_PATH/'tmp'/'itos.pkl').open('rb'))<br>stoi = collections.defaultdict(<strong class="markup--strong markup--pre-strong">lambda</strong>:0, {v:k <strong class="markup--strong markup--pre-strong">for</strong> k,v <strong class="markup--strong markup--pre-strong">in</strong> <br>                                          enumerate(itos)})<br>len(itos)</pre><pre name="0e12" id="0e12" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">60002</em></pre><pre name="26ca" id="26ca" class="graf graf--pre graf-after--pre">trn_clas = np.array([[stoi[o] <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> p] <strong class="markup--strong markup--pre-strong">for</strong> p <strong class="markup--strong markup--pre-strong">in</strong> tok_trn])<br>val_clas = np.array([[stoi[o] <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> p] <strong class="markup--strong markup--pre-strong">for</strong> p <strong class="markup--strong markup--pre-strong">in</strong> tok_val])</pre><pre name="221e" id="221e" class="graf graf--pre graf-after--pre">np.save(CLAS_PATH/'tmp'/'trn_ids.npy', trn_clas)<br>np.save(CLAS_PATH/'tmp'/'val_ids.npy', val_clas)</pre><h4 name="bbd3" id="bbd3" class="graf graf--h4 graf-after--pre">Classifier</h4><pre name="e422" id="e422" class="graf graf--pre graf-after--h4">trn_clas = np.load(CLAS_PATH/'tmp'/'trn_ids.npy')<br>val_clas = np.load(CLAS_PATH/'tmp'/'val_ids.npy')</pre><pre name="4e4e" id="4e4e" class="graf graf--pre graf-after--pre">trn_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'trn_labels.npy'))<br>val_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'val_labels.npy'))</pre><p name="83cb" id="83cb" class="graf graf--p graf-after--pre">The construction of the model hyper parameters are the same [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h33m16s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h33m16s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:33:16</a>]. We can change the dropout. Pick a batch size that is as big as you can that doesn’t run out of memory.</p><pre name="75ea" id="75ea" class="graf graf--pre graf-after--p">bptt,em_sz,nh,nl = 70,400,1150,3<br>vs = len(itos)<br>opt_fn = partial(optim.Adam, betas=(0.8, 0.99))<br>bs = 48</pre><pre name="4ac1" id="4ac1" class="graf graf--pre graf-after--pre">min_lbl = trn_labels.min()<br>trn_labels -= min_lbl<br>val_labels -= min_lbl<br>c=int(trn_labels.max())+1</pre><h4 name="c689" id="c689" class="graf graf--h4 graf-after--pre">TextDataset [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h33m37s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h33m37s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:33:37</a>]</h4><p name="1d9f" id="1d9f" class="graf graf--p graf-after--h4">This bit is interesting. There’s fun stuff going on here.</p><pre name="f3a7" id="f3a7" class="graf graf--pre graf-after--p">trn_ds = TextDataset(trn_clas, trn_labels)<br>val_ds = TextDataset(val_clas, val_labels)</pre><p name="bcf5" id="bcf5" class="graf graf--p graf-after--pre">The basic idea here is that for the classifier, we do really want to look at one document. Is this document positive or negative? So we do want to shuffle the documents. But those documents have different lengths and so if we stick them all into one batch (this is a handy thing that fastai does for you) — you can stick things of different lengths into a batch and it will automatically pat them, so you don’t have to worry about that. But if they are wildly different lengths, then you’re going to be wasting a lot of computation times. If there is one thing that’s 2,000 words long and everything else is 50 words long, that means you end up with 2000 wide tensor. That’s pretty annoying. So James Bradbury who is one of Stephen Merity’s colleagues and the guy who came up with torchtext came up with a neat idea which was “let’s sort the dataset by length-ish”. So kind of make it so the first things in the list are, on the whole, shorter than the things at the end, but a little bit random as well.</p><p name="dad1" id="dad1" class="graf graf--p graf-after--p">Here is how Jeremy implemented that [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h35m10s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h35m10s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:35:10</a>]. The first thing we need is a Dataset. So we have a Dataset passing in the documents and their labels. Here is <code class="markup--code markup--p-code">TextDataSet</code> which inherits from <code class="markup--code markup--p-code">Dataset</code> and <code class="markup--code markup--p-code">Dataset</code> from PyTorch is also shown below:</p><figure name="6707" id="6707" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_5X1u6uQ6ywmiDVOa8qzbgg.png"></figure><p name="fd1b" id="fd1b" class="graf graf--p graf-after--figure">Actually <code class="markup--code markup--p-code">Dataset</code> doesn’t do anything at all [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h35m34s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h35m34s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:35:34</a>]. It says you need <code class="markup--code markup--p-code">__getitem__</code> if you don’t have one, you’re going to get an error. Same is true for <code class="markup--code markup--p-code">__len__</code>. So this is an abstract class. To <code class="markup--code markup--p-code">TextDataset</code>, we are going to pass in our <code class="markup--code markup--p-code">x</code> and <code class="markup--code markup--p-code">y</code>, and <code class="markup--code markup--p-code">__getitem__</code> will grab <code class="markup--code markup--p-code">x</code> and <code class="markup--code markup--p-code">y</code>, and return them — it couldn’t be much simpler. Optionally, 1. they could reverse it, 2. stick an end of stream at the end, 3. stick start of stream at the beginning. But we are not doing any of those things, so literally all we are doing is putting <code class="markup--code markup--p-code">x</code> and <code class="markup--code markup--p-code">y</code> and <code class="markup--code markup--p-code">__getitem__</code> returns them as a tuple. The length is however long the <code class="markup--code markup--p-code">x</code> is. That’s all <code class="markup--code markup--p-code">Dataset</code> is — something with a length that you can index.</p><h4 name="8a5a" id="8a5a" class="graf graf--h4 graf-after--p">Turning it to a DataLoader [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h36m27s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h36m27s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:36:27</a>]</h4><pre name="f03a" id="f03a" class="graf graf--pre graf-after--h4">trn_samp = SortishSampler(trn_clas, key=<strong class="markup--strong markup--pre-strong">lambda</strong> x: len(trn_clas[x]), <br>                          bs=bs//2)<br>val_samp = SortSampler(val_clas, key=<strong class="markup--strong markup--pre-strong">lambda</strong> x: len(val_clas[x]))</pre><pre name="34c1" id="34c1" class="graf graf--pre graf-after--pre">trn_dl = DataLoader(trn_ds, bs//2, transpose=<strong class="markup--strong markup--pre-strong">True</strong>, num_workers=1,<br>                    pad_idx=1, sampler=trn_samp)<br>val_dl = DataLoader(val_ds, bs, transpose=<strong class="markup--strong markup--pre-strong">True</strong>, num_workers=1, <br>                    pad_idx=1, sampler=val_samp)<br>md = ModelData(PATH, trn_dl, val_dl)</pre><p name="0bc9" id="0bc9" class="graf graf--p graf-after--pre">To turn it into a DataLoader, you simply pass the Dataset to the DataLoader constructor, and it’s now going to give you a batch of that at a time. Normally you can say shuffle equals true or shuffle equals false, it’ll decide whether to randomize it for you. In this case though, we are actually going to pass in a sampler parameter and sampler is a class we are going to define that tells the data loader how to shuffle.</p><ul class="postList"><li name="6ca8" id="6ca8" class="graf graf--li graf-after--p">For validation set, we are going to define something that actually just sorts. It just deterministically sorts it so that all the shortest documents will be at the start, all the longest documents will be at the end, and that’s going to minimize the amount of padding.</li><li name="7bb6" id="7bb6" class="graf graf--li graf-after--li">For training sampler, we are going to create this thing called sort-ish sampler which also sorts (ish!)</li></ul><figure name="efce" id="efce" class="graf graf--figure graf--layoutOutsetCenter graf-after--li" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_Z_0F0rRH8odcUq8n7bRDVg.png"></figure><p name="6e2b" id="6e2b" class="graf graf--p graf-after--figure">What’s great about PyTorch is that they came up with this idea for an API for their data loader where we can hook in new classes to make it behave in different ways [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h37m27s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h37m27s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:37:27</a>]. SortSampler is something which has a length which is the length of the data source and has an iterator which is simply an iterator which goes through the data source sorted by length (which is passed in as <code class="markup--code markup--p-code">key</code>). For the SortishSampler, it basically does the same thing with a little bit of randomness. It’s just another of those beautiful design things in PyTorch that Jeremy discovered. He could take James Bradbury’s ideas which he had written a whole new set of classes around, and he could just use inbuilt hooks inside PyTorch. You will notice data loader is not actually PyTorch’s data loader — it’s actually fastai’s data loader. But it’s basically almost entirely plagiarized from PyTorch but customized in some ways to make it faster mainly using multi-threading instead of multi-processing.</p><p name="174d" id="174d" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: Does the pre-trained LSTM depth and <code class="markup--code markup--p-code">bptt</code> need to match with the new one we are training [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h39m" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h39m" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:39:00</a>]? No, the <code class="markup--code markup--p-code">bptt</code> doesn’t need to match at all. That’s just like how many things we look at at a time. It has nothing to do with the architecture.</p><p name="7f93" id="7f93" class="graf graf--p graf-after--p">So now we can call that function we just saw before <code class="markup--code markup--p-code">get_rnn_classifier</code> [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h39m16s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h39m16s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:39:16</a>]. It’s going to create exactly the same encoder more or less, and we are going to pass in the same architectural details as before. But this time, with the head we add on, you have a few more things you can do. One is you can add more than one hidden layer. In <code class="markup--code markup--p-code">layers=[em_sz*3, 50, c]</code>:</p><ul class="postList"><li name="dd32" id="dd32" class="graf graf--li graf-after--p"><code class="markup--code markup--li-code">em_sz * 3</code>: this is what the input to my head (i.e. classifier section) is going to be.</li><li name="8d9c" id="8d9c" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">50</code>: this is the output of the first layer</li><li name="2a02" id="2a02" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">c</code>: this is the output of the second layer</li></ul><p name="8a42" id="8a42" class="graf graf--p graf-after--li">And you can add as many as you like. So you can basically create a little multi-layer neural net classifier at the end. Similarly, for <code class="markup--code markup--p-code">drops=[dps[4], 0.1]</code>, these are the dropouts to go after each of these layers.</p><pre name="a36b" id="a36b" class="graf graf--pre graf-after--p"><em class="markup--em markup--pre-em"> # part 1</em><br>dps = np.array([0.4, 0.5, 0.05, 0.3, 0.1])</pre><pre name="a511" id="a511" class="graf graf--pre graf-after--pre">dps = np.array([0.4,0.5,0.05,0.3,0.4])*0.5</pre><pre name="10ee" id="10ee" class="graf graf--pre graf-after--pre">m = get_rnn_classifer(bptt, 20*70, c, vs, emb_sz=em_sz, n_hid=nh, <br>                      n_layers=nl, pad_token=1,<br>                      layers=[em_sz*3, 50, c], drops=[dps[4], 0.1],<br>                      dropouti=dps[0], wdrop=dps[1],        <br>                      dropoute=dps[2], dropouth=dps[3])</pre><pre name="189d" id="189d" class="graf graf--pre graf-after--pre">opt_fn = partial(optim.Adam, betas=(0.7, 0.99))</pre><p name="dad9" id="dad9" class="graf graf--p graf-after--pre">We are going to use RNN_Learner just like before.</p><pre name="4683" id="4683" class="graf graf--pre graf-after--p">learn = RNN_Learner(md, TextModel(to_gpu(m)), opt_fn=opt_fn)<br>learn.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)<br>learn.clip=25.<br>learn.metrics = [accuracy]</pre><p name="2c19" id="2c19" class="graf graf--p graf-after--pre">We are going to use discriminative learning rates for different layers [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h40m20s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h40m20s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:40:20</a>].</p><pre name="68d6" id="68d6" class="graf graf--pre graf-after--p">lr=3e-3<br>lrm = 2.6<br>lrs = np.array([lr/(lrm**4), lr/(lrm**3), lr/(lrm**2), lr/lrm, lr])</pre><pre name="8626" id="8626" class="graf graf--pre graf-after--pre">lrs=np.array([1e-4,1e-4,1e-4,1e-3,1e-2])</pre><p name="0a57" id="0a57" class="graf graf--p graf-after--pre">You can try using weight decay or not. Jeremy has been fiddling around a bit with that to see what happens.</p><pre name="b02e" id="b02e" class="graf graf--pre graf-after--p">wd = 1e-7<br>wd = 0<br>learn.load_encoder('lm2_enc')</pre><p name="03e7" id="03e7" class="graf graf--p graf-after--pre">We start out just training the last layer and we get 92.9% accuracy:</p><pre name="e20a" id="e20a" class="graf graf--pre graf-after--p">learn.freeze_to(-1)</pre><pre name="edf5" id="edf5" class="graf graf--pre graf-after--pre">learn.lr_find(lrs/1000)<br>learn.sched.plot()</pre><pre name="9107" id="9107" class="graf graf--pre graf-after--pre">learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))</pre><pre name="3962" id="3962" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss   accuracy                      <br>    0      0.365457   0.185553   0.928719</em></pre><pre name="6209" id="6209" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[0.18555279, 0.9287188090884525]</em></pre><pre name="9c99" id="9c99" class="graf graf--pre graf-after--pre">learn.save('clas_0')<br>learn.load('clas_0')</pre><p name="bba1" id="bba1" class="graf graf--p graf-after--pre">Then we unfreeze one more layer, get 93.3% accuracy:</p><pre name="10d9" id="10d9" class="graf graf--pre graf-after--p">learn.freeze_to(-2)</pre><pre name="2e70" id="2e70" class="graf graf--pre graf-after--pre">learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))</pre><pre name="f93f" id="f93f" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss   accuracy                      <br>    0      0.340473   0.17319    0.933125</em></pre><pre name="9338" id="9338" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[0.17319041, 0.9331253991245995]</em></pre><pre name="2ab8" id="2ab8" class="graf graf--pre graf-after--pre">learn.save('clas_1')<br>learn.load('clas_1')</pre><pre name="6017" id="6017" class="graf graf--pre graf-after--pre">learn.unfreeze()</pre><pre name="d275" id="d275" class="graf graf--pre graf-after--pre">learn.fit(lrs, 1, wds=wd, cycle_len=14, use_clr=(32,10))</pre><pre name="80e4" id="80e4" class="graf graf--pre graf-after--pre">epoch      trn_loss   val_loss   accuracy                      <br>    0      0.337347   0.186812   0.930782  <br>    1      0.284065   0.318038   0.932062                      <br>    2      0.246721   0.156018   0.941747                      <br>    3      0.252745   0.157223   0.944106                      <br>    4      0.24023    0.159444   0.945393                      <br>    5      0.210046   0.202856   0.942858                      <br>    6      0.212139   0.149009   0.943746                      <br>    7      0.21163    0.186739   0.946553                      <br>    8      0.186233   0.1508     0.945218                      <br>    9      0.176225   0.150472   0.947985                      <br>    10     0.198024   0.146215   0.948345                      <br>    11     0.20324    0.189206   0.948145                      <br>    12     0.165159   0.151402   0.947745                      <br>    13     0.165997   0.146615   0.947905</pre><pre name="e90e" id="e90e" class="graf graf--pre graf-after--pre">[0.14661488, 0.9479046703071374]</pre><pre name="bd91" id="bd91" class="graf graf--pre graf-after--pre">learn.sched.plot_loss()</pre><pre name="c033" id="c033" class="graf graf--pre graf-after--pre">learn.save('clas_2')</pre><p name="af6f" id="af6f" class="graf graf--p graf-after--pre">Then we fine-tune the whole thing [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h40m47s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h40m47s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:40:47</a>]. This was the main attempt before our paper came along at using a pre-trained model:</p><p name="2042" id="2042" class="graf graf--p graf-after--p"><a href="https://arxiv.org/abs/1708.00107" data-href="https://arxiv.org/abs/1708.00107" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Learned in Translation: Contextualized Word Vectors</a></p><p name="1d42" id="1d42" class="graf graf--p graf-after--p">What they did is they used a pre-trained translation model but they didn’t fine tune the whole thing. They just took the activations of the translation model and when they tried IMDb, they got 91.8% — which we beat easily after only fine-tuning one layer. They weren’t state-of-the-art, the state-of-the-art is 94.1% which we beat after fine-tuning the whole thing for 3 epochs and by the end, we are at 94.8% which is obviously a huge difference because in terms of error rate, that’s gone done from 5.9%. A simple little trick is go back to the start of this notebook and reverse the order of all of the documents, and then re-run the whole thing. When you get to the bit that says <code class="markup--code markup--p-code">fwd_wt_103</code>, replace <code class="markup--code markup--p-code">fwd</code> for forward with <code class="markup--code markup--p-code">bwd</code> for backward. That’s a backward English language model that learns to read English backward. So if you redo this whole thing, put all the documents in reverse, and change this to backward, you now have a second classifier which classifies things by positive or negative sentiment based on the reverse document. If you then take the two predictions and take the average of them, you basically have a bi-directional model (which you trained each bit separately)and that gets you to 95.4% accuracy. So we basically lowered it from 5.9% to 4.6%. So this kind of 20% change in the state-of-the-art is almost unheard of. It doesn’t happen very often. So you can see this idea of using transfer learning, it’s ridiculously powerful that every new field thinks their new field is too special and you can’t do it. So it’s a big opportunity for all of us.</p><h4 name="71a0" id="71a0" class="graf graf--h4 graf-after--p">Universal Language Model Fine-tuning for Text Classification [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h44m2s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h44m2s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:44:02</a>]</h4><figure name="688d" id="688d" class="graf graf--figure graf-after--h4"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_XzWZUyxcsTu-ehYucd_vFQ.png"></figure><p name="7751" id="7751" class="graf graf--p graf-after--figure">So we turned this into a paper, and when I say we, I did it with this guy Sebastian Ruder. Now you might remember his name because in lesson 5, I told you that I actually had shared lesson 4 with Sebastian because I think he is an awesome researcher who I thought might like it. I didn’t know him personally at all. Much to my surprise, he actually watched the video. He watched the whole video and said:</p><p name="7caa" id="7caa" class="graf graf--p graf-after--p">Sebastian: “That’s actually quite fantastic! We should turn this into a paper.”</p><p name="ac56" id="ac56" class="graf graf--p graf-after--p">Jeremy: “I don’t write papers. I don’t care about papers and am not interested in papers — that sounds really boring”</p><p name="95e4" id="95e4" class="graf graf--p graf-after--p">Sebastian: “Okay, how about I write the paper for you.”</p><p name="fbf3" id="fbf3" class="graf graf--p graf-after--p">Jeremy: “You can’t really write a paper about this yet because you’d have to do like studies to compare it to other things (they are called ablation studies) to see which bit actually works. There’s no rigor here, I just put in everything that came in my head and chucked it all together and it happened to work”</p><p name="2db0" id="2db0" class="graf graf--p graf-after--p">Sebastian: “Okay, what if I write all the paper and do all your ablation studies, then can we write the paper?”</p><p name="f6a3" id="f6a3" class="graf graf--p graf-after--p">Jeremy: “Well, it’s like a whole library that I haven’t documented and I’m not going to yet and you don’t know how it all works”</p><p name="d987" id="d987" class="graf graf--p graf-after--p">Sebastian: “Okay, if I wrote the paper, and do the ablation studies, and figure out from scratch how the code works without bothering you, then can we write the paper?”</p><p name="a55f" id="a55f" class="graf graf--p graf-after--p">Jeremy: “Um… yeah, if you did all those things, then we can write the paper. Okay!”</p><p name="cc4f" id="cc4f" class="graf graf--p graf-after--p">Then two days later, he comes back and says “okay, I’ve done a draft of the paper.” So, I share this story to say, if you are some student in Ireland and you want to do good work, don’t let anybody stop you. I did not encourage him to say the least. But in the end, he said “I want to do this work, I think it’s going to be good, and I’ll figure it out” and he wrote a fantastic paper. He did the ablation study and he figured out how fastai works, and now we are planning to write another paper together. You’ve got to be a bit careful because sometimes I get messages from random people saying like “I’ve got lots of good ideas, can we have coffee?” — “I don’t want… I can have coffee in my office anytime, thank you”. But it’s very different to say “hey, I took your ideas and I wrote a paper, and I did a bunch of experiments, and I figured out how your code works, and I added documentation to it — should we submit this to a conference?” You see what I mean? There is nothing to stop you doing amazing work and if you do amazing work that helps somebody else, in this case, I’m happy that we have a paper. I don’t particularly care about papers but I think it’s cool that these ideas now have this rigorous study.</p><h4 name="b47f" id="b47f" class="graf graf--h4 graf-after--p">Let me show you what he did [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h47m19s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h47m19s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:47:19</a>]</h4><p name="47af" id="47af" class="graf graf--p graf-after--h4">He took all my code, so I’d already done all the fastai.text and as you have seen, it lets us work with large corpuses. Sebastian is fantastically well-read and he said “here’s a paper that Yann LeCun and some guys just came out with where they tried lots of classification datasets so I’m going to try running your code on all these datasets.” So these are the datasets:</p><figure name="4fe9" id="4fe9" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_NFanphEYzNa9uMV4iSY2bw.png"></figure><p name="b6a8" id="b6a8" class="graf graf--p graf-after--figure">Some of them had many many hundreds of thousands of documents and they were far bigger than I had tried — but I thought it should work.</p><p name="d97a" id="d97a" class="graf graf--p graf-after--p">And he had a few good ideas as we went along and so you should totally make sure you read the paper. He said “well, this thing that you called in the lessons differential learning rates, differential kind of means something else. Maybe we should rename it” so we renamed it. It’s now called discriminative learning rate. So this idea that we had from part one where we use different learning rates for different layers, after doing some literature research, it does seem like that hasn’t been done before so it’s now officially a thing — discriminative learning rates. This is something we learnt in lesson 1 but it now has an equation with Greek and everything [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h48m41s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h48m41s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:48:41</a>]:</p><figure name="ac0d" id="ac0d" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_KeaQyBreXN5QHfKCG-dJ0Q.png"></figure><p name="8140" id="8140" class="graf graf--p graf-after--figure">When you see an equation with Greek and everything, that doesn’t necessarily mean it’s more complex than anything we did in lesson 1 because this one isn’t.</p><p name="81a0" id="81a0" class="graf graf--p graf-after--p">Again, that idea of like unfreezing a layer at a time, also seems to never been done before so it’s now a thing and it’s got the very clever name “gradual unfreezing” [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h48m57s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h48m57s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:48:57</a>].</p><figure name="6d34" id="6d34" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_W3JSe1RPeRaYhMrr-RZoWw.png"></figure><h4 name="53d4" id="53d4" class="graf graf--h4 graf-after--figure">Slanted triangular learning rate [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h49m10s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h49m10s" class="markup--anchor markup--h4-anchor" rel="noopener nofollow" target="_blank">1:49:10</a>]</h4><p name="11e3" id="11e3" class="graf graf--p graf-after--h4">So then, as promised, we will look at slanted triangular learning rates&nbsp;. This actually was not my idea. Leslie Smith, one of my favorite researchers who you all now know about, emailed me a while ago and said “I’m so over cyclical learning rates. I don’t do that anymore. I now do a slightly different version where I have one cycle which goes up quickly at the start, and then slowly down afterwards. I often find it works better.” I’ve tried going back over all of my old datasets and it works better for all of them — every one I tried. So this is what the learning rate look like. You can use it in fastai just by adding <code class="markup--code markup--p-code">use_clr=</code> to your <code class="markup--code markup--p-code">fit</code>. The first number is the ratio between the highest learning rate and the lowest learning rate so the initial learning rate is 1/32 of the peak. The second number is the ratio between the first peak and the last peak. The basic idea is if you are doing a cycle length 10, that you want the first epoch to be the upward bit and the other 9 epochs to be the downward bit, then you would use 10. I find that works pretty well and that was also Leslie’s suggestion is make about 1/10 of it the upward bit and 9/10 the downward bit. Since he told me about it, maybe two days ago, he wrote this amazing paper: <a href="https://arxiv.org/abs/1803.09820" data-href="https://arxiv.org/abs/1803.09820" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">A DISCIPLINED APPROACH TO NEURAL NETWORK HYPER-PARAMETERS</a>. In which, he describes something very slightly different to this again, but the same basic idea. This is a must read paper. It’s got all the kinds of ideas that fastai talks about a lot in great depth and nobody else is talking about this. It’s kind of a slog, unfortunately Leslie had to go away on a trop before he really had time to edit it properly, so it’s a little bit slow reading, but don’t let that stop you. It’s amazing.</p><figure name="19d2" id="19d2" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_ydr4ZUCrsDg71s_C73ggTg.png"></figure><p name="4d28" id="4d28" class="graf graf--p graf-after--figure">The equation on the right is from my paper with Sebastian. Sebastian asked “Jeremy, can you send me the math equation behind that code you wrote?” and I said “no, I just wrote the code. I could not turn it into math” so he figured out the math for it.</p><h4 name="f151" id="f151" class="graf graf--h4 graf-after--p">Concat pooling [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h51m36s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h51m36s" class="markup--anchor markup--h4-anchor" rel="noopener nofollow" target="_blank">1:51:36</a>]</h4><p name="070c" id="070c" class="graf graf--p graf-after--h4">So you might have noticed, the first layer of our classifier was equal to embedding size*3&nbsp;. Why times 3? Times 3 because, and again, this seems to be something which people haven’t done before, so a new idea “concat pooling”. It is that we take the average pooling over the sequence of the activations, the max pooling of the sequence over the activations, and the final set of activations, and just concatenate them all together. This is something which we talked about in part 1 but doesn’t seem to be in the literature before so it’s now called “concat pooling” and it’s now got an equation and everything but this is the entirety of the implementation. So you can go through this paper and see how the fastai code implements each piece.</p><figure name="ae42" id="ae42" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_ilEQlVMIdx3m2WAKzOCjfQ.png"></figure><h4 name="c4b4" id="c4b4" class="graf graf--h4 graf-after--figure">BPT3C [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h52m46s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h52m46s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:52:46</a>]</h4><p name="3fc2" id="3fc2" class="graf graf--p graf-after--h4">One of the kind of interesting pieces is the difference between <code class="markup--code markup--p-code">RNN_Encoder</code> which you’ve already seen and MultiBatchRNN encoder. So what’s the difference there? The key difference is that the normal RNN encoder for the language model, we could just do <code class="markup--code markup--p-code">bptt</code> chunk at a time. But for the classifier, we need to do the whole document. We need to do the whole movie review before we decide if it’s positive or negative. And the whole movie review can easily be 2,000 words long and we can’t fit 2.000 words worth of gradients in my GPU memory for every single one of my weights. So what do we do? So the idea was very simple which is I go through my whole sequence length one batch of <code class="markup--code markup--p-code">bptt</code> at a time. And I call <code class="markup--code markup--p-code">super().forward</code> (in other words, the <code class="markup--code markup--p-code">RNN_Encoder</code>) to grab its outputs, and then I’ve got this maximum sequence length parameter where it says “okay, as long as you are doing no more than that sequence length, then start appending it to my list of outputs.” So in other words, the thing that it sends back to this pooling is only as many activations as we’ve asked it to keep. That way, you can figure out what <code class="markup--code markup--p-code">max_seq</code> can your particular GPU handle. So it’s still using the whole document, but let’s say <code class="markup--code markup--p-code">max_seq</code> is 1,000 words and your longest document length is 2, 000 words. It’s still going through RNN creating states for those first thousand words, but it’s not actually going to store the activations for the backprop of the first thousand. It’s only going to keep the last thousand. So that means that it can’t back-propagate the loss back to any state that was created in the first thousand words — basically that’s now gone. So it’s a really simple piece of code and honestly when I wrote it I didn’t spend much time thinking about it, it seems so obviously the only way this could possibly work. But again, it seems to be a new thing, so we now have backprop through time for text classification. You can see there’s lots of little pieces in this paper.</p><figure name="5a1f" id="5a1f" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_N-GZd5Z6Z3HjbEJnTID43g.png"></figure><h4 name="7110" id="7110" class="graf graf--h4 graf-after--figure">Results [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h55m56s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h55m56s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:55:56</a>]</h4><p name="b323" id="b323" class="graf graf--p graf-after--h4">What was the result? On every single dataset we tried, we got better result than any previous academic paper for text classification. All different types. Honestly, IMDb was the only one I spent any time trying to optimize the model, so most of them, we just did it whatever came out first. So if we actually spent time with it, I think this would be a lot better. The things that these are comparing to, most of them are different on each table because they are customized algorithms on the whole. So this is saying one simple fine-tuning algorithm can beat these really customized algorithms.</p><figure name="227e" id="227e" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_D9ntGwft-g9FgWsuNonGJQ.png"></figure><h4 name="91f9" id="91f9" class="graf graf--h4 graf-after--figure">Ablation studies [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h56m56s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h56m56s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:56:56</a>]</h4><p name="5173" id="5173" class="graf graf--p graf-after--h4">Here is the ablation studies Sebastian did. I was really keen that if you are going to publish a paper, we had to say why it works. So Sebastian went through and tried removing all of those different contributions I mentioned. So what is we don’t use gradual freezing? What if we don’t use discriminative learning rates? What if instead of discrimination rates, we use cosign annealing? What if we don’t do any pre-training with Wikipedia? What if we don’t do any fine tuning? And the really interesting one to me was, what’s the validation error rate on IMDb if we only used a hundred training examples (vs. 200, vs. 500, etc). And you can see, very interestingly, the full version of this approach is nearly as accurate on just a hundred training examples — it’s still very accurate vs. full 20,000 training examples. Where as if you are training from scratch on 100, it’s almost random. It’s what I expected. I’ve said to Sebastian I really think that this is most beneficial when you don’t have much data. This is where fastai is most interested in contributing — small data regimes, small compute regimes, and so forth. So he did these studies to check.</p><figure name="1725" id="1725" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_JsahawCY9ja-kZHTd90lFQ.png"></figure><h3 name="12b5" id="12b5" class="graf graf--h3 graf-after--figure">Tricks to run ablation studies [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h58m32s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h58m32s" class="markup--anchor markup--h3-anchor" rel="noopener nofollow" target="_blank">1:58:32</a>]</h3><h4 name="0528" id="0528" class="graf graf--h4 graf-after--h3">Trick #1:&nbsp;VNC</h4><p name="d186" id="d186" class="graf graf--p graf-after--h4">The first trick is something which I know you’re all going to find really handy. I know you’ve all been annoyed when you are running something in a Jupyter notebook, and you lose your internet connection for long enough that it decides you’ve gone away, and then your session disappears, and you have to start it again from scratch. So what do you do? There is a very simple cool thing called VNC where you can install on your AWS instance or PaperSpace, or whatever:</p><ul class="postList"><li name="ae5a" id="ae5a" class="graf graf--li graf-after--p">X Windows (<code class="markup--code markup--li-code">xorg</code>)</li><li name="628f" id="628f" class="graf graf--li graf-after--li">Lightweight window manager (<code class="markup--code markup--li-code">lxde-core</code>)</li><li name="6f05" id="6f05" class="graf graf--li graf-after--li">VNC server (<code class="markup--code markup--li-code">tightvncserver</code>)</li><li name="53ea" id="53ea" class="graf graf--li graf-after--li">Firefox (<code class="markup--code markup--li-code">firefox</code>)</li><li name="ce58" id="ce58" class="graf graf--li graf-after--li">Terminal (<code class="markup--code markup--li-code">lxterminal</code>)</li><li name="ba38" id="ba38" class="graf graf--li graf-after--li">Some fonts (<code class="markup--code markup--li-code">xfonts-100dpi</code>)</li></ul><p name="27f2" id="27f2" class="graf graf--p graf-after--li">Chuck the lines at the end of your&nbsp;<code class="markup--code markup--p-code">./vnc/xstartup</code> configuration file, and then run this command (<code class="markup--code markup--p-code">tightvncserver&nbsp;:13 -geometry 1200x900</code>):</p><figure name="3bda" id="3bda" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_A6iP79W389q7anG5nyASyg.png"></figure><p name="d8ac" id="d8ac" class="graf graf--p graf-after--figure">It’s now running a server where you can then run the TightVNC Viewer or any VNC viewer on your computer and you point it at your server. But specifically, what you do is you use SSH port forwarding to forward&nbsp;:5913 to localhost:5913:</p><figure name="c7ed" id="c7ed" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*fPDXeYX8HkT_JTuUEIHgSQ.png" data-width="1039" data-height="35" data-action="zoom" data-action-value="1*fPDXeYX8HkT_JTuUEIHgSQ.png" src="../img/1_fPDXeYX8HkT_JTuUEIHgSQ.png"></figure><p name="356e" id="356e" class="graf graf--p graf-after--figure">Then you connect to port 5013 on localhost. It will send it off to port 5913 on your server which is the VNC port (because you said&nbsp;<code class="markup--code markup--p-code">:13</code>) and it will display an X Windows desktop. Then you can click on the Linux start like button and click on Firefox and you now have Firefox. You see here in Firefox, it says localhost because this Firefox is running on my AWS server. So you now run Firefox, you start your thing running, and then you close your VNC viewer remembering that Firefox is displaying on this virtual VNC display, not in a real display, so then later on that day, you log back into VNC viewer and it pops up again. So it’s like a persistent desktop, and it’s shockingly fast. It works really well. There’s lots of different VNC servers and clients, but this one works fine for me.</p><h4 name="e4fa" id="e4fa" class="graf graf--h4 graf-after--p">Trick #2: Google Fire [<a href="https://youtu.be/h5Tz7gZT9Fo?t=2h1m27s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=2h1m27s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">2:01:27</a>]</h4><figure name="5682" id="5682" class="graf graf--figure graf-after--h4"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_03yxHYXeuHZUZbYaqKRs5g.png"></figure><p name="9e32" id="9e32" class="graf graf--p graf-after--figure">Trick #2 is to create Python scripts, and this is what we ended up doing. So I ended up creating a little Python script for Sebastian to kind of say this is the basic steps you need to do, and now you need to create different versions for everything else. And I suggested to him that he tried using this thing called Google Fire. What Google Fire does is, you create a function with tons of parameters, so these are all the things that Sebastian wanted to try doing — different dropout amounts, different learning rates, do I use pre-training or not, do I use CLR or not, do I use discriminative learning rate or not, etc. So you create a function, and then you add something saying:</p><pre name="a1ec" id="a1ec" class="graf graf--pre graf-after--p">if __name__ == '__main__': fire.Fire(train_clas)</pre><p name="93fb" id="93fb" class="graf graf--p graf-after--pre">You do nothing else at all — you don’t have to add any metadata, any docstrings, anything at all, and you then call that script and automatically you now have a command line interface. That’s a super fantastic easy way to run lots of different variations in a terminal. This ends up being easier if you want to do lots of variations than using a notebook because you can just have a bash script that tries all of them and spits them all out.</p><h4 name="aae4" id="aae4" class="graf graf--h4 graf-after--p">Trick #3: IMDb scripts [<a href="https://youtu.be/h5Tz7gZT9Fo?t=2h2m47s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=2h2m47s" class="markup--anchor markup--h4-anchor" rel="noopener nofollow" target="_blank">2:02:47</a>]</h4><p name="5c60" id="5c60" class="graf graf--p graf-after--h4">You’ll find inside the <code class="markup--code markup--p-code">courses/dl2</code>, there’s now something called <code class="markup--code markup--p-code">imdb_scripts</code>, and I put all the scripts Sebastian and I used. Because we needed to tokenize and numericalize every dataset, then train a language model and a classifier for every dataset. And we had to do all of those things in a variety of different ways to compare them, so we had scripts for all those things. You can check out and see all of the scripts that we used.</p><figure name="aae0" id="aae0" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*4wNUZhHpjSgRLj6s6ECddQ.png" data-width="1052" data-height="80" data-action="zoom" data-action-value="1*4wNUZhHpjSgRLj6s6ECddQ.png" src="../img/1_4wNUZhHpjSgRLj6s6ECddQ.png"></figure><figure name="b2e4" id="b2e4" class="graf graf--figure graf--layoutOutsetCenter graf-after--figure" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_SkRiJH47FdHtubjyUeYdLA.png"></figure><h4 name="f084" id="f084" class="graf graf--h4 graf-after--figure">Trick #4: pip install -e [<a href="https://youtu.be/h5Tz7gZT9Fo?t=2h3m32s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=2h3m32s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">2:03:32</a>]</h4><p name="7bc0" id="7bc0" class="graf graf--p graf-after--h4">When you are doing a lot of scripts, you got different code all over the place. Eventually it might get frustrating that you don’t want to symlink your fastai library again and again. But you probably don’t want to pip install it because that version tends to be a little bit old as we move so fast that you want to use the current version in Git. If you say <code class="markup--code markup--p-code">pip install -e&nbsp;.</code> from fastai repo base, it does something quite neat which is basically creates a symlink to the fastai library (i.e. your locally cloned Git repo) inside site-packages directory. Your site-packages directory is your main Python library. So if you do this, you can then access fastai from anywhere but every time you do <code class="markup--code markup--p-code">git pull</code>, you’ve got the most recent version. One downside of this is that it installs any updated versions of packages from pip which can confuse Conda a little bit, so another alternative here is just do symlink the fastai library to your site packages library. That works just as well. You can use fastai from anywhere and it’s quite handy when you want to run scripts that use fastai from different directories on your system.</p><figure name="03b0" id="03b0" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_tg8X-gjGJ6rFAg-aiPIgpQ.png"></figure><h4 name="9510" id="9510" class="graf graf--h4 graf-after--figure">Trick #5: SentencePiece [<a href="https://youtu.be/h5Tz7gZT9Fo?t=2h5m6s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=2h5m6s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">2:05:06</a>]</h4><p name="35cf" id="35cf" class="graf graf--p graf-after--h4">This is something you can try if you like. You don’t have to tokenize. Instead of tokenizing words, you can tokenize what are called sub-word units.For example, “unsupervised” could be tokenized as “un” and “supervised”. “Tokenizer” can be tokenized as [“token”, “izer”]. Then you could do the same thing. The language model that works on sub-word units, a classifier that works on sub-word units, etc. How well does that work? I started playing with it and with not too much playing, I was getting classification results that were nearly as good as using word level tokenization — not quite as good, but nearly as good. I suspect with more careful thinking and playing around, maybe I could have gotten as good or better. But even if I couldn’t, if you create a sub-word-unit wikitext model, then IMDb language model, and then classifier forwards and backwards and then ensemble it with the forwards and backwards word level ones, you should be able to beat us. So here is an approach you may be able to beat our state-of-the-art result.</p><figure name="26e6" id="26e6" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_Ihivmbwld8tPdMracJ-FuQ.png"></figure><p name="b800" id="b800" class="graf graf--p graf-after--figure">Sebastian told me this particular project — Google has a project called sentence peace which actually uses a neural net to figure out the optimal splitting up of words and so you end up with vocabulary of sub-word units. In my playing around, I found that create vocabulary of about 30,000 sub-word units seems to be about optimal. If you are interested, there is something you can try. It is a bit of a pain to install — it’s C++, doesn’t have create error message, but it will work. There is a Python library for it. If anybody tries this, I’m happy to help them get it working. There’s been little, if any, experiments with ensembling sub-word and word level classification, and I do think it should be the best approach.</p><p name="3931" id="3931" class="graf graf--p graf-after--p graf--trailing">Have a great week!</p><hr class="section-divider"><p name="bbc0" id="bbc0" class="graf graf--p graf--leading graf--trailing">Lessons: <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" class="markup--anchor markup--p-anchor" target="_blank">1</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" class="markup--anchor markup--p-anchor" target="_blank">2</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" class="markup--anchor markup--p-anchor" target="_blank">3</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" class="markup--anchor markup--p-anchor" target="_blank">4</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" class="markup--anchor markup--p-anchor" target="_blank">5</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" class="markup--anchor markup--p-anchor" target="_blank">6</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" class="markup--anchor markup--p-anchor" target="_blank">7</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" class="markup--anchor markup--p-anchor" target="_blank">8</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" class="markup--anchor markup--p-anchor" target="_blank">9</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" class="markup--anchor markup--p-anchor" target="_blank"><strong class="markup--strong markup--p-strong">10</strong></a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" class="markup--anchor markup--p-anchor" target="_blank">11</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" class="markup--anchor markup--p-anchor" target="_blank">12</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" class="markup--anchor markup--p-anchor" target="_blank">13</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" class="markup--anchor markup--p-anchor" target="_blank">14</a></p></body></html>