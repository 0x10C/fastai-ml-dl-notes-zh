<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></head><body><p name="183e" id="183e" class="graf graf--p graf-after--pre">Here is the list of what each word is for wikitext103 model, and we can do the same <code class="markup--code markup--p-code">defaultdict</code> trick to map it in reverse. We’ll use -1 to mean that it is not in the wikitext dictionary.</p><pre name="64fc" id="64fc" class="graf graf--pre graf-after--p">itos2 = pickle.load((PRE_PATH<strong class="markup--strong markup--pre-strong">/</strong>'itos_wt103.pkl').open('rb'))</pre><pre name="172c" id="172c" class="graf graf--pre graf-after--pre">stoi2 = collections.defaultdict(<strong class="markup--strong markup--pre-strong">lambda</strong>:<strong class="markup--strong markup--pre-strong">-</strong>1, {v:k <strong class="markup--strong markup--pre-strong">for</strong> k,v <br>                                              <strong class="markup--strong markup--pre-strong">in</strong> enumerate(itos2)})</pre><p name="719f" id="719f" class="graf graf--p graf-after--pre">So now we can just say our new set of weights is just a whole bunch of zeros with vocab size by embedding size (i.e. we are going to create an embedding matrix) [<a href="https://youtu.be/h5Tz7gZT9Fo?t=47m57s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=47m57s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">47:57</a>]. We then go through every one of the words in our IMDb vocabulary. We are going to look it up in <code class="markup--code markup--p-code">stoi2</code> (string-to-integer for the wikitext103 vocabulary) and see if it’s a word there. If that is a word there, then we won’t get the <code class="markup--code markup--p-code">-1</code>. So <code class="markup--code markup--p-code">r</code> will be greater than or equal to zero, so in that case, we will just set that row of the embedding matrix to the weight which was stored inside the named element <code class="markup--code markup--p-code">‘0.encoder.weight’</code>. You can look at this dictionary <code class="markup--code markup--p-code">wgts</code> and it’s pretty obvious what each name corresponds to. It looks very similar to the names that you gave it when you set up your module, so here are the encoder weights.</p><p name="fda9" id="fda9" class="graf graf--p graf-after--p">If we don’t find it [<a href="https://youtu.be/h5Tz7gZT9Fo?t=49m2s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=49m2s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">49:02</a>], we will use the row mean — in other words, here is the average embedding weight across all of the wikitext103. So we will end up with an embedding matrix for every word that’s in both our vocabulary for IMDb and the wikitext103 vocab, we will use the wikitext103 embedding matrix weights; for anything else, we will just use whatever was the average weight from the wikitext103 embedding matrix.</p><pre name="cbbc" id="cbbc" class="graf graf--pre graf-after--p">new_w = np.zeros((vs, em_sz), dtype=np.float32)<br><strong class="markup--strong markup--pre-strong">for</strong> i,w <strong class="markup--strong markup--pre-strong">in</strong> enumerate(itos):<br>    r = stoi2[w]<br>    new_w[i] = enc_wgts[r] <strong class="markup--strong markup--pre-strong">if</strong> r<strong class="markup--strong markup--pre-strong">&gt;</strong>=0 <strong class="markup--strong markup--pre-strong">else</strong> row_m</pre><p name="40e2" id="40e2" class="graf graf--p graf-after--pre">We will then replace the encoder weights with <code class="markup--code markup--p-code">new_w</code> turn into a tensor [<a href="https://youtu.be/h5Tz7gZT9Fo?t=49m35s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=49m35s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">49:35</a>]. We haven’t talked much about weight tying, but basically the decoder (the thing that turns the final prediction back into a word) uses exactly the same weights, so we pop it there as well. Then there is a bit of weird thing with how we do embedding dropout that ends up with a whole separate copy of them for a reason that doesn’t matter much. So we popped the weights back where they need to go. So this is now a set of torch state which we can load in.</p><pre name="3dd9" id="3dd9" class="graf graf--pre graf-after--p">wgts['0.encoder.weight'] = T(new_w)<br>wgts['0.encoder_with_dropout.embed.weight'] = T(np.copy(new_w))<br>wgts['1.decoder.weight'] = T(np.copy(new_w))</pre><h4 name="0b0b" id="0b0b" class="graf graf--h4 graf-after--pre">Language model&nbsp;[<a href="https://youtu.be/h5Tz7gZT9Fo?t=50m18s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=50m18s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">50:18</a>]</h4><p name="5e97" id="5e97" class="graf graf--p graf-after--h4">Let’s create our language model. Basic approach we are going to use is we are going to concatenate all of the documents together into a single list of tokens of length 24,998,320. That is going to be what we pass in as a training set. So for the language model:</p><ul class="postList"><li name="a474" id="a474" class="graf graf--li graf-after--p">We take all our documents and just concatenate them back to back.</li><li name="5859" id="5859" class="graf graf--li graf-after--li">We are going to be continuously trying to predict what’s the next word after these words.</li><li name="7752" id="7752" class="graf graf--li graf-after--li">We will set up a whole bunch of dropouts.</li><li name="8f84" id="8f84" class="graf graf--li graf-after--li">Once we have a model data object, we can grab the model from it, so that’s going to give us a learner.</li><li name="db38" id="db38" class="graf graf--li graf-after--li">Then as per usual, we can call <code class="markup--code markup--li-code">learner.fit</code>. We do a single epoch on the last layer just to get that okay. The way it’s set up is the last layer is the embedding words because that’s obviously the thing that’s going to be the most wrong because a lot of those embedding weights didn’t even exist in the vocab. So we will train a single epoch of just the embedding weights.</li><li name="51c2" id="51c2" class="graf graf--li graf-after--li">Then we’ll start doing a few epochs of the full model. How is it looking? In lesson 4, we had the loss of 4.23 after 14 epochs. In this case, we have 4.12 loss after 1 epoch. So by pre-training on wikitext103, we have a better loss after 1 epoch than the best loss we got for the language model otherwise.</li></ul><p name="3bb4" id="3bb4" class="graf graf--p graf-after--li graf--trailing"><strong class="markup--strong markup--p-strong">Question</strong>: What is the wikitext103 model? Is it a AWD LSTM again [<a href="https://youtu.be/h5Tz7gZT9Fo?t=52m41s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=52m41s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">52:41</a>]? Yes, we are about to dig into that. The way I trained it was literally the same lines of code that you see above, but without pre-training it on wikitext103.</p><hr class="section-divider"><h4 name="c4bf" id="c4bf" class="graf graf--h4 graf--leading">A quick discussion about fastai doc project&nbsp;[<a href="https://youtu.be/h5Tz7gZT9Fo?t=53m7s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=53m7s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">53:07</a>]</h4><p name="7bad" id="7bad" class="graf graf--p graf-after--h4">The goal of fastai doc project is to create documentation that makes readers say “wow, that’s the most fantastic documentation I’ve ever read” and we have some specific ideas about how to do that. It’s the same kind of idea of top-down, thoughtful, take full advantage of the medium approach, interactive experimental code first that we are all familiar with. If you are interested in getting involved, you can see the basic approach in <a href="https://github.com/fastai/fastai/tree/master/docs" data-href="https://github.com/fastai/fastai/tree/master/docs" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">the docs directory</a>. In there, there is, amongst other things, <a href="https://raw.githubusercontent.com/fastai/fastai/master/docs/transforms-tmpl.adoc" data-href="https://raw.githubusercontent.com/fastai/fastai/master/docs/transforms-tmpl.adoc" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">transforms-tmpl.adoc</a>. <code class="markup--code markup--p-code">adoc</code> is <a href="http://asciidoc.org/" data-href="http://asciidoc.org/" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">AsciiDoc</a>. AsciiDoc is like markdown but it’s like what markdown needs to be to create actual books. A lot of actual books are written in AsciiDoc and it’s as easy to use as markdown but there’s way more cool stuff you can do with it. <a href="https://raw.githubusercontent.com/fastai/fastai/master/docs/transforms.adoc" data-href="https://raw.githubusercontent.com/fastai/fastai/master/docs/transforms.adoc" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Here</a> is more standard asciiDoc example. You can do things like inserting a table of contents (<code class="markup--code markup--p-code">:toc:</code>).&nbsp;<code class="markup--code markup--p-code">::</code> means put a definition list here. <code class="markup--code markup--p-code">+</code> means this is a continuation of the previous list item. So there are many super handy features and it is like turbo-charged markdown. So this asciidoc creates this HTML and no custom CSS or anything added:</p><figure name="19e0" id="19e0" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_9UfkC1UD_8TZP0PpTbJhdg.png"></figure><p name="c69b" id="c69b" class="graf graf--p graf-after--figure">We literally started this project 4 hours ago. So you have a table of contents with hyper links to specific sections. We have cross reference we can click on to jump straight to the cross reference. Each method comes along with its details and so on. To make things even easier, they’ve created a special template for argument, cross reference, method, etc. The idea is, it will almost be like a book. There will be tables, pictures, video segments, and hyperlink throughout.</p><p name="13c4" id="13c4" class="graf graf--p graf-after--p graf--trailing">You might be wondering what about docstrings. But actually, if you look at the Python standard library and look at the docstring for <code class="markup--code markup--p-code">re.compile()</code>, for example, it’s a single line. Nearly every docstring in Python is a single line. And Python then does exactly this — they have a website containing the documentation that says “this is what regular expressions are, and this is what you need to know about them, and if you want do them fast, you need to compile, and here is some information about compile” etc. These information is not in the docstring and that’s how we are going to do as well — our docstring will be one line unless you need like two sometimes. Everybody is welcome to help contribute to the documentation.</p><hr class="section-divider"><p name="f4f0" id="f4f0" class="graf graf--p graf--leading"><strong class="markup--strong markup--p-strong">Question</strong>: Hoes this compare to word2vec [<a href="https://youtu.be/h5Tz7gZT9Fo?t=58m31s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=58m31s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">58:31</a>]? This is actually a great thing for you to spend time thinking about during the week. I’ll give you the summary now but it’s a very important conceptual difference. The main conceptual difference is “what is word2vec?” Word2vec is a single embedding matrix — each word has a vector and that’s it. In other words, it’s a single layer from a pre-trained model — specifically that layer is the input layer. Also specifically that pre-trained model is a linear model that is pre-trained on something called a co-occurrence matrix. So we have no particular reason to believe that this model has learned anything much about English language or that it has any particular capabilities because it’s just a single linear layer and that’s it. What’s this wikitext103 model? It’s a language model and it has a 400 dimensional embedding matrix, 3 hidden layers with 1,150 activations per layer, and regularization and all that stuff tied input output matrices — it’s basically a state-of-the-art AWD LSTM. What’s the difference between a single layer of a single linear model vs. a three layer recurrent neural network? Everything! They are very different levels of capabilities. So you will see when you try using a pre-trained language model vs. word2vec layer, you’ll get very different results for the vast majority of tasks.</p><p name="a03a" id="a03a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: What if the numpy array does not fit in memory? Is it possible to write a PyTorch data loader directly from a large CSV file [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h32s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h32s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:00:32</a>]? It almost certainly won’t come up, so I’m not going to spend time on it. These things are tiny — they are just integers. Think about how many integers you would need to run out of memories? That’s not gonna happen. They don’t have to fit in GPU memory, just in your memory. I’ve actually done another Wikipedia model which I called giga wiki which was on all of Wikipedia and even that easily fits in memory. The reason I’m not using it is because it turned out not to really help very much vs. wikitext103. I’ve built a bigger model than anybody else I’ve found in the academic literature and it fits in memory on a single machine.</p><p name="5a1d" id="5a1d" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: What is the idea behind averaging the weights of embeddings [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h1m24s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h1m24s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:01:24</a>]? They have to be set to something. These are words that weren’t there, so the other option is we could leave them as zero. But that seems like a very extreme thing to do. Zero is a very extreme number. Why would it be zero? We could set it equal to some random numbers, but if so, what would be the mean and standard deviation of those random numbers? Should they be uniform? If we just average the rest of the embeddings, then we have something that’s reasonably scaled. Just to clarify, this is how we are initializing words that didn’t appear in the training corpus.</p><h4 name="5f8b" id="5f8b" class="graf graf--h4 graf-after--p">Back to Language Model [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h2m20s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h2m20s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:02:20</a>]</h4><p name="19c7" id="19c7" class="graf graf--p graf-after--h4">This is a ton of stuff we’ve seen before, but it’s changed a little bit. It’s actually a lot easier than it was in part 1, but I want to go a little bit deeper into the language model loader.</p><pre name="869e" id="869e" class="graf graf--pre graf-after--p">wd=1e-7<br>bptt=70<br>bs=52<br>opt_fn = partial(optim.Adam, betas=(0.8, 0.99))</pre><pre name="f38a" id="f38a" class="graf graf--pre graf-after--pre">t = len(np.concatenate(trn_lm))<br>t, t//64</pre><pre name="9669" id="9669" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(24998320, 390598)</em></pre><p name="3d67" id="3d67" class="graf graf--p graf-after--pre">This is the <code class="markup--code markup--p-code">LanguageModelLoader</code> and I really hope that by now, you’ve learned in your editor or IDE how to jump to symbols [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h2m37s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h2m37s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:02:37</a>]. I don’t want it to be a burden for you to find out what the source code of <code class="markup--code markup--p-code">LanguageModelLoader</code> is. If your editor doesn’t make it easy, don’t use that editor anymore. There’s lots of good free editors that make this easy.</p><p name="e810" id="e810" class="graf graf--p graf-after--p">So this is the source code for LanguageModelLoader, and it’s interesting to notice that it’s not doing anything particularly tricky. It’s not deriving from anything at all. What makes something that’s capable of being a data loader is that it’s something you can iterate over.</p><figure name="b3cc" id="b3cc" class="graf graf--figure graf--layoutOutsetCenter graf-after--p" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_ttM96lLbHQn06byFwmHj0g.png"></figure><p name="9816" id="9816" class="graf graf--p graf-after--figure">Here is the <code class="markup--code markup--p-code">fit</code> function inside fastai.model [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h3m41s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h3m41s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:03:41</a>]. This is where everything ends up eventually which goes through each epoch, creates an iterator from the data loader, and then just does a for loop through it. So anything you can do a for loop through can be a data loader. Specifically it needs to return tuples of independent and dependent variables for mini-batches.</p><figure name="23de" id="23de" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_560U29nWI0xNGLsHgnWFNQ.png"></figure><p name="500c" id="500c" class="graf graf--p graf-after--figure">So anything with a <code class="markup--code markup--p-code">__iter__</code> method is something that can act as an iterator [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h4m9s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h4m9s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:04:09</a>]. <code class="markup--code markup--p-code">yield</code> is a neat little Python keywords you probably should learn about if you don’t already know it. But it basically spits out a thing and waits for you to ask for another thing — normally in a for loop or something. In this case, we start by initializing the language model passing it in the numbers <code class="markup--code markup--p-code">nums</code> this is the numericalized long list of all of our documents concatenated together. The first thing we do is to “batchfy” it. This is the thing which quite a few of you got confused about last time. If our batch size is 64 and we have 25 million numbers in our list. We are not creating items of length 64 — we are creating 64 items in total. So each of them is of size <code class="markup--code markup--p-code">t</code> divided by 64 which is 390k. So that’s what we do here:</p><p name="b3ca" id="b3ca" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">data = data.view(self.bs, -1).t().contiguous()</code></p><p name="d852" id="d852" class="graf graf--p graf-after--p">We reshape it so that this axis is of length 64 and <code class="markup--code markup--p-code">-1</code> is everything else (390k blob), and we transpose it. So that means that we now have 64 columns, 390k rows. Then what we do each time we do an iterate is we grab one batch of some sequence length, which is approximately equal to <code class="markup--code markup--p-code">bptt</code> (back prop through time) which we set to 70. We just grab that many rows. So from <code class="markup--code markup--p-code">i</code> to <code class="markup--code markup--p-code">i+70</code> rows, we try to predict that plus one. Remember, we are trying to predict one past where we are up to.</p><p name="8e87" id="8e87" class="graf graf--p graf-after--p">So we have 64 columns and each of those is 1/64th of our 25 million tokens, and hundreds of thousands long, and we just grab 70 at a time [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h6m29s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h6m29s" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">1:06:29</a>]. So each of those columns, each time we grab it, it’s going to kind of hook up to the previous column. That’s why we get this consistency. This language model is stateful which is really important.</p><p name="aff7" id="aff7" class="graf graf--p graf-after--p">Pretty much all of the cool stuff in the language model is stolen from Stephen Merity’s AWD-LSTM [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h6m59s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h6m59s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:06:59</a>] including this little trick here:</p><figure name="6739" id="6739" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*Mv0c41-UvTGlNKHMuPlsHw.png" data-width="476" data-height="48" src="../img/1_Mv0c41-UvTGlNKHMuPlsHw.png"></figure><p name="4cba" id="4cba" class="graf graf--p graf-after--figure">If we always grab 70 at a time and then we go back and do a new epoch, we’re going to grab exactly the same batches every time — there is no randomness. Normally, we shuffle our data every time we do an epoch or every time we grab some data we grab it at random. You can’t do that with a language model because this set has to join up to the previous set because it’s trying to learn the sentence. If you suddenly jump somewhere else, that doesn’t make any sense as a sentence. So Stephen’s idea is to say “okay, since we can’t shuffle the order, let’s instead randomly change the sequence length”. Basically, 95% of the time, we will use <code class="markup--code markup--p-code">bptt</code> (i.e. 70) but 5% of the time, we’ll use half that. Then he says “you know what, I’m not even going to make that the sequence length, I’m going to create a normally distributed random number with that average and a standard deviation of 5, and I’ll make that the sequence length.” So the sequence length is seventy-ish and that means every time we go through, we are getting slightly different batches. So we’ve got that little bit of extra randomness. Jeremy asked Stephen Merity where he came up with this idea, did he think of it? and he said “I think I thought of it, but it seemed so obvious that I bet I didn’t think of it” — which is true of every time Jeremy comes up with an idea in deep learning. It always seems so obvious that you just assume somebody else has thought of it. But Jeremy thinks Stephen thought of it.</p><p name="cb8e" id="cb8e" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">LanguageModelLoader</code> is a nice thing to look at if you are trying to do something a bit unusual with a data loader [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h8m55s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h8m55s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:08:55</a>]. It’s a simple role model you can use as to creating a data loader from scratch — something that spits out batches of data.</p><p name="e73e" id="e73e" class="graf graf--p graf-after--p">Our language model loader took in all of the documents concatenated together along with batch size and bptt [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h9m14s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h9m14s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:09:14</a>].</p><pre name="f6cc" id="f6cc" class="graf graf--pre graf-after--p">trn_dl = LanguageModelLoader(np.concatenate(trn_lm), bs, bptt)<br>val_dl = LanguageModelLoader(np.concatenate(val_lm), bs, bptt)<br>md = LanguageModelData(PATH, 1, vs, trn_dl, val_dl, bs=bs, <br>                       bptt=bptt)</pre><p name="9fe9" id="9fe9" class="graf graf--p graf-after--pre">Now generally speaking, we want to create a learner and the way we normally do that is by getting a model data object and calling some kind of method which have various names but often we call that method <code class="markup--code markup--p-code">get_model</code>. The idea is that the model data object has enough information to know what kind of model to give you. So we have to create that model data object which means we need LanguageModelData class which is very easy to do [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h9m51s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h9m51s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:09:51</a>].</p><p name="0965" id="0965" class="graf graf--p graf-after--p">Here are all of the pieces. We are going to create a custom learner, a custom model data class, and a custom model class. So a model data class, again this one doesn’t inherit from anything so you really see there’s almost nothing to do. You need to tell it most importantly what’s your training set (give it a data loader), what’s the validation set (give it a data loader), and optionally, give it a test set (data loader), plus anything else that needs to know. It might need to know the bptt, it needs to know number of tokens(i.e. the vocab size), and it needs to know what is the padding index. And so that it can save temporary files and models, model datas as always need to know the path. So we just grab all that stuff and we dump it. That’s it. That’s the entire initializer. There is no logic there at all.</p><figure name="d1c5" id="d1c5" class="graf graf--figure graf--layoutOutsetCenter graf-after--p" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_GPeBIZ7A9P8gdCulCCrREw.png"></figure><p name="7ffb" id="7ffb" class="graf graf--p graf-after--figure">Then all of the work happens inside <code class="markup--code markup--p-code">get_model</code>[<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h10m55s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h10m55s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:10:55</a>]. get_model calls something we will look at later, which just grabs a normal PyTorch nn.Module architecture, and chucks it on GPU. Note: with PyTorch, we would say&nbsp;<code class="markup--code markup--p-code">.cuda()</code>, with fastai it’s better to say <code class="markup--code markup--p-code">to_gpu()</code>, the reason is that if you don’t have GPU, it will leave it on the CPU. It also provides a global variable you can set to choose whether it goes on the GPU or not, so it’s a better approach. We wrapped the model in a <code class="markup--code markup--p-code">LanguageModel</code> and the <code class="markup--code markup--p-code">LanguageModel</code> is a subclass of <code class="markup--code markup--p-code">BasicModel</code> which almost does nothing except it defines layer groups. Remember when we do discriminative learning rates where different layers have different learning rates or we freeze different amounts, we don’t provide a different learning rate for every layer because there can be a thousand layers. We provide a different learning rate for every layer group. So when you create a custom model, you just have to override this one thing which returns a list of all of your layer groups. In this case, the last layer group contains the last part of the model and one bit of dropout. The rest of it (<code class="markup--code markup--p-code">*</code> here means pull this apart) so this is going to be one layer per RNN layer. So that’s all that is.</p><p name="f8e3" id="f8e3" class="graf graf--p graf-after--p">Then finally turn that into a learner [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h12m41s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h12m41s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:12:41</a>]. So a learner, you just pass in the model and it turns it into a learner. In this case, we have overridden learner and the only thing we’ve done is to say I want the default loss function to be cross entropy. This entire set of custom model, custom model data, custom learner all fits on a single screen. They always basically look like this.</p><p name="3c71" id="3c71" class="graf graf--p graf-after--p">The interesting part of this code base is <code class="markup--code markup--p-code">get_language_model</code> [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h13m18s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h13m18s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:13:18</a>]. Because that gives us our AWD LSTM. It actually contains the big idea. The big, incredibly simple idea that everybody else here thinks it’s really obvious that everybody in the NLP community Jeremy spoke to thought was insane. That is, every model can be thought of as a backbone plus a head, and if you pre-train the backbone and stick on a random head, you can do fine-tuning and that’s a good idea.</p><figure name="52e0" id="52e0" class="graf graf--figure graf--layoutOutsetCenter graf-after--p" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_QoAsI-zGJ3XKMBDY-3o1Rg.png"></figure><p name="2276" id="2276" class="graf graf--p graf-after--figure">These two bits of code, literally right next to each other, this is all there is inside <code class="markup--code markup--p-code">fastai.lm_rnn</code>.</p><p name="c171" id="c171" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">get_language_model</code>: Creates an RNN encoder and then creates a sequential model that sticks on top of that — a linear decoder.</p><p name="3f23" id="3f23" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">get_rnn_classifier</code>: Creates an RNN encoder, then a sequential model that sticks on top of that — a pooling linear classifier.</p><p name="0de8" id="0de8" class="graf graf--p graf-after--p">We’ll see what these differences are in a moment, but you get the basic idea. They are doing pretty much the same thing. They’ve got this head and they are sticking on a simple linear layer on top.</p><p name="e2b0" id="e2b0" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: There was a question earlier about whether that any of this translates to other languages [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h14m52s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h14m52s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:14:52</a>]. Yes, this whole thing works in any languages. Would you have to retrain your language model on a corpus from that language? Absolutely! So the wikitext103 pre-trained language model knows English. You could use it maybe as a pre-trained start for like French or German model, start by retraining the embedding layer from scratch might be helpful. Chinese, maybe not so much. But given that a language model can be trained from any unlabeled documents at all, you’ll never have to do that. Because almost every language in the world has plenty of documents — you can grab newspapers, web pages, parliamentary records, etc. As long as you have a few thousand documents showing somewhat normal usage of that language, you can create a language model. One of our students tried this approach for Thai and he said the first model he built easily beat the previous state-of-the-art Thai classifier. For those of you that are international fellow, this is an easy way for you to whip out a paper in which you either create the first ever classifier in your language or beat everybody else’s classifier in your language. Then you can tell them that you’ve been a student of deep learning for six months and piss off all the academics in your country.</p><p name="34a9" id="34a9" class="graf graf--p graf-after--p">Here is our RNN encoder [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h16m49s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h16m49s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:16:49</a>]. It is a standard nn.Module. It looks like there is more going on in it than there actually is, but really all there is is we create an embedding layer, create an LSTM for each layer that’s been asked for, that’s it. Everything else in it is dropout. Basically all of the interesting stuff (just about) in the AWS LSTM paper is all of the places you can put dropout. Then the forward is basically the same thing. Call the embedding layer, add some dropout, go through each layer, call that RNN layer, append it to our list of outputs, add dropout, that’s about it. So it’s pretty straight forward.</p><figure name="41b8" id="41b8" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_HrRraVW1kuyghw-PIhV89g.png"></figure><p name="09ca" id="09ca" class="graf graf--p graf-after--figure">The paper you want to be reading is the AWD LSTM paper which is <a href="https://arxiv.org/abs/1708.02182" data-href="https://arxiv.org/abs/1708.02182" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">Regularizing and Optimizing LSTM Language Models</a>. It’s well written, pretty accessible, and entirely implemented inside fastai as well — so you can see all of the code for that paper. A lot of the code actually is shamelessly plagiarized with Stephen’s permission from his excellent GitHub repo <a href="https://github.com/Smerity/awd-lstm-lm" data-href="https://github.com/Smerity/awd-lstm-lm" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">AWD LSTM</a>.</p><p name="b0ac" id="b0ac" class="graf graf--p graf-after--p">The paper refers to other papers. For things like why is it that the encoder weight and the decoder weight are the same. It’s because there is this thing called “tie weights.” Inside <code class="markup--code markup--p-code">get_language_model</code>, there is a thing called <code class="markup--code markup--p-code">tie_weights</code> which defaults to true. If it’s true, then we literally use the same weight matrix for the encoder and the decoder. They are pointing at the same block of memory. Why is that? What’s the result of it? That’s one of the citations in Stephen’s paper which is also a well written paper you can look up and learn about weight tying.</p><figure name="f68b" id="f68b" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_b0FeRkWrz1MxE96PMak8xw.png"></figure><p name="aceb" id="aceb" class="graf graf--p graf-after--figure">We have basically a standard RNN [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h19m52s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h19m52s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:19:52</a>]. The only reason where it’s not standard is it has lots more types of dropout in it. In a sequential model on top of the RNN, we stick a linear decoder which is literally half the screen of code. It has a single linear layer, we initialize the weights to some range, we add some dropout, and that’s it. So it’s a linear layer with dropout.</p><figure name="26f4" id="26f4" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_8qFWffVOekS8lvZYmZIUdA.png"></figure><p name="2eff" id="2eff" class="graf graf--p graf-after--figure">So the language model is:</p><ul class="postList"><li name="c7d8" id="c7d8" class="graf graf--li graf-after--p">RNN → A linear layer with dropout</li></ul><h4 name="2201" id="2201" class="graf graf--h4 graf-after--li">Choosing dropout [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h20m36s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h20m36s" class="markup--anchor markup--h4-anchor" rel="noopener nofollow" target="_blank">1:20:36</a>]</h4><p name="3d4a" id="3d4a" class="graf graf--p graf-after--h4">What dropout you choose matters a lot&nbsp;.Through a lot of experimentation, Jeremy found a bunch of dropouts that tend to work pretty well for language models. But if you have less data for your language model, you’ll need more dropout. If you have more data, you can benefit from less dropout. You don’t want to regularize more than you have to. Rather than having to tune every one of these five things, Jeremy’s claim is they are already pretty good ratios to each other, so just tune this number (<code class="markup--code markup--p-code">0.7</code> below), we just multiply it all by something. If you are overfitting, then you’ll need to increase the number, if you are underfitting, you’ll need to decrease this. Because other than that, these ratio seem pretty good.</p><pre name="b878" id="b878" class="graf graf--pre graf-after--p">drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*<strong class="markup--strong markup--pre-strong">0.7</strong></pre><pre name="f140" id="f140" class="graf graf--pre graf-after--pre">learner= md.get_model(opt_fn, em_sz, nh, nl, <br>    dropouti=drops[0], dropout=drops[1], wdrop=drops[2],<br>    dropoute=drops[3], dropouth=drops[4])</pre><pre name="2af8" id="2af8" class="graf graf--pre graf-after--pre">learner.metrics = [accuracy]<br>learner.freeze_to(-1)</pre><h4 name="9764" id="9764" class="graf graf--h4 graf-after--pre">Measuring accuracy [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h21m45s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h21m45s" class="markup--anchor markup--h4-anchor" rel="noopener nofollow" target="_blank">1:21:45</a>]</h4><p name="4625" id="4625" class="graf graf--p graf-after--h4">One important idea which may seem minor but again it’s incredibly controversial is that we should measure accuracy when we look at a language model&nbsp;. Normally for language models, we look at a loss value which is just cross entropy loss but specifically we nearly always take e to the power of that which the NLP community calls “perplexity”. So perplexity is just <code class="markup--code markup--p-code">e^(cross entropy)</code>. There is a lot of problems with comparing things based on cross entropy loss. Not sure if there’s time to go into it in detail now, but the basic problem is that it is like that thing we learned about focal loss. Cross entropy loss — if you are right, it wants you to be really confident that you are right. So it really penalizes a model that doesn’t say “I’m so sure this is wrong” and it’s wrong. Whereas accuracy doesn’t care at all about how confident you are — it cares about whether you are right. This is much more often the thing which you care about in real life. The accuracy is how often do we guess the next word correctly and it’s a much more stable number to keep track of. So that’s a simple little thing that Jeremy does.</p><pre name="a00b" id="a00b" class="graf graf--pre graf-after--p">learner.model.load_state_dict(wgts)</pre><pre name="41d0" id="41d0" class="graf graf--pre graf-after--pre">lr=1e-3<br>lrs = lr</pre><pre name="60c5" id="60c5" class="graf graf--pre graf-after--pre">learner.fit(lrs/2, 1, wds=wd, use_clr=(32,2), cycle_len=1)</pre><pre name="65c1" id="65c1" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss   accuracy                     <br>    0      4.398856   4.175343   0.28551</em></pre><pre name="5aa5" id="5aa5" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[4.175343, 0.2855095456305303]</em></pre><pre name="694c" id="694c" class="graf graf--pre graf-after--pre">learner.save('lm_last_ft')</pre><pre name="82ad" id="82ad" class="graf graf--pre graf-after--pre">learner.load('lm_last_ft')</pre><pre name="c5f8" id="c5f8" class="graf graf--pre graf-after--pre">learner.unfreeze()</pre><pre name="ab35" id="ab35" class="graf graf--pre graf-after--pre">learner.lr_find(start_lr=lrs/10, end_lr=lrs*10, linear=<strong class="markup--strong markup--pre-strong">True</strong>)</pre><pre name="5757" id="5757" class="graf graf--pre graf-after--pre">learner.sched.plot()</pre><p name="f2da" id="f2da" class="graf graf--p graf-after--pre">We train for a while and we get down to a 3.9 cross entropy loss which is equivalent of ~49.40 perplexity (<code class="markup--code markup--p-code">e^3.9</code>) [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h23m14s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h23m14s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:23:14</a>]. To give you a sense of what’s happening with language models, if you look at academic papers from about 18 months ago, you’ll see them talking about state-of-the-art perplexity of over a hundred. The rate at which our ability to understand language and measuring language model accuracy or perplexity is not a terrible proxy for understanding language. If I can guess what you are going to say next, I need to understand language well and the kind of things you might talk about pretty well. The perplexity number has just come down so much that it’s been amazing, and it will come down a lot more. NLP in the last 12–18 months, it really feels like 2011–2012 computer vision. We are starting to understand transfer learning and fine-tuning, and basic models are getting so much better. Everything you thought about what NLP can and can’t do is rapidly going out of date. There’s still lots of things NLP is not good at to be clear. Just like in 2012, there were lots of stuff computer vision wasn’t good at. But it’s changing incredibly rapidly and now is a very very good time to be getting very good at NLP or starting startups base on NLP because there is a whole bunch of stuff which computers would absolutely terrible at two years ago and now not quite good as people and then next year, they’ll be much better than people.</p><pre name="700a" id="700a" class="graf graf--pre graf-after--p">learner.fit(lrs, 1, wds=wd, use_clr=(20,10), cycle_len=15)</pre><pre name="849a" id="849a" class="graf graf--pre graf-after--pre">epoch      trn_loss   val_loss   accuracy                     <br>    0      4.332359   4.120674   0.289563  <br>    1      4.247177   4.067932   0.294281                     <br>    2      4.175848   4.027153   0.298062                     <br>    3      4.140306   4.001291   0.300798                     <br>    4      4.112395   3.98392    0.302663                     <br>    5      4.078948   3.971053   0.304059                     <br>    6      4.06956    3.958152   0.305356                     <br>    7      4.025542   3.951509   0.306309                     <br>    8      4.019778   3.94065    0.30756                      <br>    9      4.027846   3.931385   0.308232                     <br>    10     3.98106    3.928427   0.309011                     <br>    11     3.97106    3.920667   0.30989                      <br>    12     3.941096   3.917029   0.310515                     <br>    13     3.924818   3.91302    0.311015                     <br>    14     3.923296   3.908476   0.311586</pre><pre name="7027" id="7027" class="graf graf--pre graf-after--pre">[3.9084756, 0.3115861900150776]</pre><p name="2c4f" id="2c4f" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">Question</strong>: What is your ratio of paper reading vs. coding in a week [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h25m24s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h25m24s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:25:24</a>]? Gosh, what do you think, Rachel? You see me. I mean, it’s more coding, right? “It’s a lot more coding. I feel like it also really varies from week to week” (Rachel). With that bounding box stuff, there were all these papers and no map through them, so I didn’t even know which one to read first and then I’d read the citations and didn’t understand any of them. So there was a few weeks of just kind of reading papers before I even know what to start coding. That’s unusual though. Anytime I start reading a paper, I’m always convinced that I’m not smart enough to understand it, always, regardless of the paper. And somehow eventually I do. But I try to spend as much time as I can coding.</p><p name="4b3e" id="4b3e" class="graf graf--p graf-after--p">Nearly always after I’ve read a paper [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h26m34s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h26m34s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:26:34</a>], even after I’ve read the bit that says this is the problem I’m trying to solve, I’ll stop there and try to implement something that I think might solve that problem. And then I’ll go back and read the paper, and I read little bits about these are how I solve these problem bits, and I’ll be like “oh that’s a good idea” and then I’ll try to implement those. That’s why for example, I didn’t actually implement SSD. My custom head is not the same as their head. It’s because I kind of read the gist of it and then I tried to create something as best as I could, then go back to the papers and try to see why. So by the time I got to the focal loss paper, Rachel will tell you, I was driving myself crazy with how come I can’t find small objects? How come it’s always predicting background? I read the focal loss paper and I was like “that’s why!!” It’s so much better when you deeply understand the problem they are trying to solve. I do find the vast majority of the time, by the time I read that bit of the paper which is solving a problem, I’m then like “yeah, but these three ideas I came up with, they didn’t try.” Then you suddenly realize that you’ve got new ideas. Or else, if you just implement the paper mindlessly, you tend not to have these insights about better ways to do it&nbsp;.</p><p name="6bd2" id="6bd2" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: Is your dropout rate the same through the training or do you adjust it and weights accordingly [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h26m27s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h26m27s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:26:27</a>]? Varying dropout is really interesting and there are some recent papers that suggest gradually changing dropout [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h28m9s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h28m9s" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">1:28:09</a>]. It was either good idea to gradually make it smaller or gradually make it bigger, I’m not sure which. Maybe one of us can try and find it during the week. I haven’t seen it widely used. I tried it a little bit with the most recent paper I wrote and I had some good results. I think I was gradually make it smaller, but I can’t remember.</p><p name="34e1" id="34e1" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: Am I correct in thinking that this language model is build on word embeddings? Would it be valuable to try this with phrase or sentence embeddings? I ask this because I saw from Google the other day, universal sentence encoder [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h28m45s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h28m45s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:28:45</a>]. This is much better than that. This is not just an embedding of a sentence, this is an entire model. An embedding by definition is like a fixed thing. A sentence or a phase embedding is always a model that creates that. We’ve got a model that’s trying to understand language. It’s not just as phrase or as sentence — it’s a document in the end, and it’s not just an embedding that we are training through the whole thing. This has been a huge problem with NLP for years now is this attachment they have to embeddings. Even the paper that the community has been most excited about recently from <a href="http://allenai.org/" data-href="http://allenai.org/" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">AI2</a> (Allen Institute for Artificial Intelligence) called ELMo — they found much better results across lots of models, but again it was an embedding. They took a fixed model and created a fixed set of numbers which they then fed into a model. But in computer vision, we’ve known for years that that approach of having fixed set of features, they’re called hyper columns in computer vision, people stopped using them like 3 or 4 years ago because fine-tuning the entire model works much better. For those of you that have spent quite a lot of time with NLP and not much time with computer vision, you’re going to have to start re-learning. All that stuff you have been told about this idea that there are these things called embeddings and that you learn them ahead of time and then you apply these fixed things whether it be word level or phrase level or whatever level — don’t do that. You want to actually create a pre-trained model and fine-tune it end-to-end, then you’ll see some specific results.</p><p name="b97f" id="b97f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: For using accuracy instead of perplexity as a metric for the model, could we work that into the loss function rather than just use it as a metric [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h31m21s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h31m21s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:31:21</a>]? No, you never want to do that whether it be computer vision or NLP or whatever. It’s too bumpy. So cross entropy is fine as a loss function. And I’m not saying instead of, I use it in addition to. I think it’s good to look at the accuracy and to look at the cross entropy. But for your loss function, you need something nice and smoothy. Accuracy doesn’t work very well.</p><pre name="8db6" id="8db6" class="graf graf--pre graf-after--p">learner.save('lm1')<br>learner.save_encoder('lm1_enc')</pre><h4 name="0ad3" id="0ad3" class="graf graf--h4 graf-after--pre"><code class="markup--code markup--h4-code">save_encoder</code> [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h31m55s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h31m55s" class="markup--anchor markup--h4-anchor" rel="noopener nofollow" target="_blank">1:31:55</a>]</h4><p name="de3b" id="de3b" class="graf graf--p graf-after--h4">You’ll see there are two different versions of <code class="markup--code markup--p-code">save</code>. <code class="markup--code markup--p-code">save</code> saves the whole model as per usual. <code class="markup--code markup--p-code">save_encoder</code> just saves that bit:</p><figure name="66f5" id="66f5" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_H8mfqVgmT04qnT1ludJRFQ.png"></figure><p name="2569" id="2569" class="graf graf--p graf-after--figure">In other words, in the sequential model, it saves just <code class="markup--code markup--p-code">rnn_enc</code> and not <code class="markup--code markup--p-code">LinearDecoder(n_tok, emb_sz, dropout, tie_encoder=enc)</code> (which is the bit that actually makes it into a language model). We don’t care about that bit in the classifier, we just care about <code class="markup--code markup--p-code">rnn_end</code>. That’s why we save two different models here.</p><pre name="d43a" id="d43a" class="graf graf--pre graf-after--p">learner.sched.plot_loss()</pre><figure name="9e5a" id="9e5a" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_NI2INKONs4lYhviEqp3zFQ.png"></figure><h4 name="daab" id="daab" class="graf graf--h4 graf-after--figure">Classifier tokens [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h32m31s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h32m31s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:32:31</a>]</h4><p name="a303" id="a303" class="graf graf--p graf-after--h4">Let’s now create the classifier. We will go through this pretty quickly because it’s the same. But when you go back during the week and look at the code, convince yourself it’s the same.</p><pre name="d86d" id="d86d" class="graf graf--pre graf-after--p">df_trn = pd.read_csv(CLAS_PATH/'train.csv', header=<strong class="markup--strong markup--pre-strong">None</strong>, <br>                     chunksize=chunksize)<br>df_val = pd.read_csv(CLAS_PATH/'test.csv', header=<strong class="markup--strong markup--pre-strong">None</strong>, <br>                     chunksize=chunksize)</pre><pre name="4c43" id="4c43" class="graf graf--pre graf-after--pre">tok_trn, trn_labels = get_all(df_trn, 1)<br>tok_val, val_labels = get_all(df_val, 1)</pre><pre name="d47b" id="d47b" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">0<br>1<br>0<br>1</em></pre><pre name="af54" id="af54" class="graf graf--pre graf-after--pre">(CLAS_PATH/'tmp').mkdir(exist_ok=<strong class="markup--strong markup--pre-strong">True</strong>)</pre><pre name="48d8" id="48d8" class="graf graf--pre graf-after--pre">np.save(CLAS_PATH/'tmp'/'tok_trn.npy', tok_trn)<br>np.save(CLAS_PATH/'tmp'/'tok_val.npy', tok_val)</pre><pre name="40a1" id="40a1" class="graf graf--pre graf-after--pre">np.save(CLAS_PATH/'tmp'/'trn_labels.npy', trn_labels)<br>np.save(CLAS_PATH/'tmp'/'val_labels.npy', val_labels)</pre><pre name="6ab6" id="6ab6" class="graf graf--pre graf-after--pre">tok_trn = np.load(CLAS_PATH/'tmp'/'tok_trn.npy')<br>tok_val = np.load(CLAS_PATH/'tmp'/'tok_val.npy')</pre><p name="8557" id="8557" class="graf graf--p graf-after--pre">We don’t create a new <code class="markup--code markup--p-code">itos</code> vocabulary, we obviously want to use the same vocabulary we had in the language model because we are about to reload the same encoder [<a href="https://youtu.be/h5Tz7gZT9Fo?t=1h32m48s" data-href="https://youtu.be/h5Tz7gZT9Fo?t=1h32m48s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:32:48</a>].</p></body></html>