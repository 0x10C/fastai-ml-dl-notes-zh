
<!-- saved from url=(0063)file:///C:/Users/asus/Desktop/fastai-ml-dl-notes-zh/en/dl5.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></head><body><hr class="section-divider"><h1 name="7073" id="7073" class="graf graf--h3 graf--leading graf--title">Deep Learning 2: Part 1 Lesson&nbsp;5</h1><p name="885c" id="885c" class="graf graf--p graf-after--h3"><em class="markup--em markup--p-em">My personal notes from </em><a href="http://www.fast.ai/" data-href="http://www.fast.ai/" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank"><em class="markup--em markup--p-em">fast.ai course</em></a><em class="markup--em markup--p-em">. These notes will continue to be updated and improved as I continue to review the course to “really” understand it. Much appreciation to </em><a href="https://twitter.com/jeremyphoward" data-href="https://twitter.com/jeremyphoward" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank"><em class="markup--em markup--p-em">Jeremy</em></a><em class="markup--em markup--p-em"> and </em><a href="https://twitter.com/math_rachel" data-href="https://twitter.com/math_rachel" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank"><em class="markup--em markup--p-em">Rachel</em></a><em class="markup--em markup--p-em"> who gave me this opportunity to learn.</em></p><p name="8979" id="8979" class="graf graf--p graf-after--p graf--trailing">Lessons: <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" class="markup--anchor markup--p-anchor" target="_blank">1</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" class="markup--anchor markup--p-anchor" target="_blank">2</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" class="markup--anchor markup--p-anchor" target="_blank">3</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" class="markup--anchor markup--p-anchor" target="_blank">4</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" class="markup--anchor markup--p-anchor" target="_blank"><strong class="markup--strong markup--p-strong">5</strong></a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" class="markup--anchor markup--p-anchor" target="_blank">6</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" class="markup--anchor markup--p-anchor" target="_blank">7</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" class="markup--anchor markup--p-anchor" target="_blank">8</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" class="markup--anchor markup--p-anchor" target="_blank">9</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" class="markup--anchor markup--p-anchor" target="_blank">10</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" class="markup--anchor markup--p-anchor" target="_blank">11</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" class="markup--anchor markup--p-anchor" target="_blank">12</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" class="markup--anchor markup--p-anchor" target="_blank">13</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" class="markup--anchor markup--p-anchor" target="_blank">14</a></p><hr class="section-divider"><h3 name="3e49" id="3e49" class="graf graf--h3 graf--leading"><a href="http://forums.fast.ai/t/wiki-lesson-5/9403" data-href="http://forums.fast.ai/t/wiki-lesson-5/9403" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">Lesson 5</a></h3><h3 name="6320" id="6320" class="graf graf--h3 graf-after--h3">I. Introduction</h3><p name="e7b7" id="e7b7" class="graf graf--p graf-after--h3">There is not enough publications on structured deep learning, but it is definitely happening in industries:</p><a href="https://towardsdatascience.com/structured-deep-learning-b8ca4138b848" data-href="https://towardsdatascience.com/structured-deep-learning-b8ca4138b848" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://towardsdatascience.com/structured-deep-learning-b8ca4138b848" rel="nofollow"><strong class="markup--strong markup--mixtapeEmbed-strong">Structured Deep Learning</strong><br><em class="markup--em markup--mixtapeEmbed-em">by Kerem Turgutlu</em>towardsdatascience.com</a><a href="https://towardsdatascience.com/structured-deep-learning-b8ca4138b848" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="a36062ade2923e4c6e23fbd086b1b88c" data-thumbnail-img-id="1*2l2c8AKpzS8CgOGXellFSA.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/200/200/1*2l2c8AKpzS8CgOGXellFSA.jpeg);"></a><p name="36ed" id="36ed" class="graf graf--p graf-after--mixtapeEmbed">You can download images from Google by using <a href="https://github.com/hardikvasa/google-images-download" data-href="https://github.com/hardikvasa/google-images-download" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">this tool</a> and solve your own problems:</p><a href="https://towardsdatascience.com/fun-with-small-image-data-sets-part-2-54d683ca8c96" data-href="https://towardsdatascience.com/fun-with-small-image-data-sets-part-2-54d683ca8c96" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://towardsdatascience.com/fun-with-small-image-data-sets-part-2-54d683ca8c96" rel="nofollow"><strong class="markup--strong markup--mixtapeEmbed-strong">Fun with small image data-sets (Part 2)</strong><br><em class="markup--em markup--mixtapeEmbed-em">by Nikhil B</em>towardsdatascience.com</a><a href="https://towardsdatascience.com/fun-with-small-image-data-sets-part-2-54d683ca8c96" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="15b38aa4131a69d6356de536388dc605" data-thumbnail-img-id="1*vLYOU3WGHAWtsIJvrddS5Q.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/200/200/1*vLYOU3WGHAWtsIJvrddS5Q.png);"></a><p name="b5b1" id="b5b1" class="graf graf--p graf-after--mixtapeEmbed">Introduction on how to train Neural Net (a great technical writing):</p><a href="https://towardsdatascience.com/how-do-we-train-neural-networks-edd985562b73" data-href="https://towardsdatascience.com/how-do-we-train-neural-networks-edd985562b73" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://towardsdatascience.com/how-do-we-train-neural-networks-edd985562b73" rel="nofollow"><strong class="markup--strong markup--mixtapeEmbed-strong">How do we ‘train’ neural networks&nbsp;?</strong><br><em class="markup--em markup--mixtapeEmbed-em">by Vitaly Bushaev</em>towardsdatascience.com</a><a href="https://towardsdatascience.com/how-do-we-train-neural-networks-edd985562b73" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="d0c8cc12f177b8c42ecd591d1676484a" data-thumbnail-img-id="1*q_gynfVmn1xaPrMPvRLtRA.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/200/200/1*q_gynfVmn1xaPrMPvRLtRA.jpeg);"></a><p name="d93f" id="d93f" class="graf graf--p graf-after--mixtapeEmbed">Students are competing with Jeremy in <a href="https://www.kaggle.com/c/plant-seedlings-classification/leaderboard" data-href="https://www.kaggle.com/c/plant-seedlings-classification/leaderboard" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Kaggle seedling classification competition</a>.</p><h3 name="89bf" id="89bf" class="graf graf--h3 graf-after--p">II. Collaborative Filtering — using MovieLens dataset</h3><p name="7806" id="7806" class="graf graf--p graf-after--h3">The notebook discussed can be found <a href="https://github.com/fastai/fastai/blob/master/courses/dl1/lesson5-movielens.ipynb" data-href="https://github.com/fastai/fastai/blob/master/courses/dl1/lesson5-movielens.ipynb" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">here(lesson5-movielens.ipynb)</a>.</p><p name="5eb0" id="5eb0" class="graf graf--p graf-after--p">Let’s take a look at the data. We will use <code class="markup--code markup--p-code">userId</code> (categorical), <code class="markup--code markup--p-code">movieId</code>(categorical) and <code class="markup--code markup--p-code">rating</code> (dependent) for modeling.</p><pre name="2365" id="2365" class="graf graf--pre graf-after--p">ratings = pd.read_csv(path+'ratings.csv')<br>ratings.head()</pre><figure name="801b" id="801b" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_p-154IwDcs32F5_betEmEw.png"></figure><h4 name="d5aa" id="d5aa" class="graf graf--h4 graf-after--figure"><strong class="markup--strong markup--h4-strong">Create subset for&nbsp;Excel</strong></h4><p name="0a1c" id="0a1c" class="graf graf--p graf-after--h4">We create a crosstab of the most popular movies and most movie-addicted users which we will copy into Excel for visualization.</p><pre name="3430" id="3430" class="graf graf--pre graf-after--p">g=ratings.groupby('userId')['rating'].count()<br>topUsers=g.sort_values(ascending=False)[:15]</pre><pre name="0a04" id="0a04" class="graf graf--pre graf-after--pre">g=ratings.groupby('movieId')['rating'].count()<br>topMovies=g.sort_values(ascending=False)[:15]</pre><pre name="e75b" id="e75b" class="graf graf--pre graf-after--pre">top_r = ratings.join(topUsers, rsuffix='_r', how='inner', on='userId')<br>top_r = top_r.join(topMovies, rsuffix='_r', how='inner', on='movieId')</pre><pre name="254f" id="254f" class="graf graf--pre graf-after--pre">pd.crosstab(top_r.userId, top_r.movieId, top_r.rating, aggfunc=np.sum)</pre><figure name="8256" id="8256" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_QO-Doqw_0YGOU-vmI-R5CA.png"></figure><p name="9ffe" id="9ffe" class="graf graf--p graf-after--figure"><a href="https://github.com/fastai/fastai/blob/master/courses/dl1/excel/collab_filter.xlsx" data-href="https://github.com/fastai/fastai/blob/master/courses/dl1/excel/collab_filter.xlsx" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">This</a> is the excel file with above information. To begin with, we will use <strong class="markup--strong markup--p-strong">matrix factorization/decomposition</strong> instead of building a neural net.</p><figure name="9cb8" id="9cb8" class="graf graf--figure graf--layoutOutsetCenter graf-after--p" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_ps-Mq2y88JBT3JsKBh-sKQ.png"></figure><ul class="postList"><li name="ed68" id="ed68" class="graf graf--li graf-after--figure">Blue cells — the actual rating</li><li name="4e71" id="4e71" class="graf graf--li graf-after--li">Purple cells — our predictions</li><li name="0919" id="0919" class="graf graf--li graf-after--li">Red cell — our loss function i.e. Root Mean Squared Error (RMSE)</li><li name="6dcd" id="6dcd" class="graf graf--li graf-after--li">Green cells — movie embeddings (randomly initialized)</li><li name="ac19" id="ac19" class="graf graf--li graf-after--li">Orange cells — user embeddings (randomly initialized)</li></ul><p name="c483" id="c483" class="graf graf--p graf-after--li">Each prediction is a dot product of movie embedding vector and user embedding vector. In linear algebra term, it is equivalent of matrix product as one is a row and one is a column. If there is no actual rating, we set the prediction to zero (think of this as test data — not training data).</p><figure name="a111" id="a111" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_2SeWMcKe9VCLkVQVuCvU8g.png"></figure><p name="25a0" id="25a0" class="graf graf--p graf-after--figure">We then use Gradient Descent to minimize our loss. Microsoft excel has a “solver” in the add-ins that would minimize a variable by changing selected cells (<code class="markup--code markup--p-code">GRG Nonlinear</code> is the method you want to use).</p><p name="fa4a" id="fa4a" class="graf graf--p graf-after--p">This can be called “shallow learning” (as opposed to deep learning) as there is no nonlinear layer or a second linear layer. So what did we just do intuitively? The five numbers for each movie is called “embeddings” (latent factors) — the first number might represent how much it is sci-fi and fantasy, the second might be how much special effect is used for a movie, the third might be how dialog driven it is, etc. Similarly, each user also has 5 numbers representing, for example, how much does the user like sci-fi fantasy, special effects, and dialog-driven in movies. Our prediction is a cross product of these vectors. Since we do not have every movie review for every user, we are trying to figure out which movies are similar this movie and how other users who rated other movies similarly to this user rate this movie (hence the name “collaborative”).</p><p name="dacd" id="dacd" class="graf graf--p graf-after--p">What do we do with a new user or a new movie — do we have to retrain a model? We do not have a time to cover this now, but basically you need to have a new user model or a new movie model that you would use initially and over time you will need to re-train the model.</p><h4 name="a37f" id="a37f" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Simple Python version&nbsp;[</strong><a href="https://youtu.be/J99NV9Cr75I?t=26m3s" data-href="https://youtu.be/J99NV9Cr75I?t=26m3s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--h4-strong">26:03</strong></a><strong class="markup--strong markup--h4-strong">]</strong></h4><p name="88fd" id="88fd" class="graf graf--p graf-after--h4">This should look familiar by now. We create a validation set by picking random set of ID’s. <code class="markup--code markup--p-code">wd</code> is a weight decay for L2 regularization, and <code class="markup--code markup--p-code">n_factors</code> is how big an embedding matrix we want.</p><pre name="cfd0" id="cfd0" class="graf graf--pre graf-after--p">val_idxs = get_cv_idxs(len(ratings)) <br>wd = 2e-4 <br>n_factors = 50</pre><p name="bd0d" id="bd0d" class="graf graf--p graf-after--pre">We create a model data object from CSV file:</p><pre name="dc5b" id="dc5b" class="graf graf--pre graf-after--p">cf = CollabFilterDataset.from_csv(path, 'ratings.csv', 'userId', 'movieId', 'rating')</pre><p name="59da" id="59da" class="graf graf--p graf-after--pre">We then get a learner that is suitable for the model data, and fit the model:</p><pre name="761d" id="761d" class="graf graf--pre graf-after--p">learn = cf.get_learner(n_factors, val_idxs, 64, opt_fn=optim.Adam)</pre><pre name="10c1" id="10c1" class="graf graf--pre graf-after--pre">learn.fit(1e-2, 2, wds=wd, cycle_len=1, cycle_mult=2)</pre><figure name="8df7" id="8df7" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*Xl9If92kjaI5OEIxKyNLiw.png" data-width="360" data-height="86" src="../img/1_Xl9If92kjaI5OEIxKyNLiw.png"><figcaption class="imageCaption">Output MSE</figcaption></figure><p name="9923" id="9923" class="graf graf--p graf-after--figure">Since the output is Mean Squared Error, you can take RMSE by:</p><pre name="9707" id="9707" class="graf graf--pre graf-after--p">math.sqrt(0.765)</pre><p name="3761" id="3761" class="graf graf--p graf-after--pre">The output is about 0.88 which outperforms the bench mark of 0.91.</p><p name="1662" id="1662" class="graf graf--p graf-after--p">You can get a prediction in a usual way:</p><pre name="5fb1" id="5fb1" class="graf graf--pre graf-after--p">preds = learn.predict()</pre><p name="0f6a" id="0f6a" class="graf graf--p graf-after--pre">And you can also plot using seaborn <code class="markup--code markup--p-code">sns</code> (built on top of matplotlib):</p><pre name="5c95" id="5c95" class="graf graf--pre graf-after--p">y = learn.data.val_y<br>sns.jointplot(preds, y, kind='hex', stat_func=None)</pre><figure name="219f" id="219f" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_cXAU8huHFkxKbJjZUwwxIA.png"></figure><h4 name="1e43" id="1e43" class="graf graf--h4 graf-after--figure"><strong class="markup--strong markup--h4-strong">Dot product with&nbsp;Python</strong></h4><figure name="3c2b" id="3c2b" class="graf graf--figure graf--layoutOutsetRow is-partialWidth graf-after--h4" style="width: 51.518%;" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_kSUYsjtdLbyn2SqW9cKiHA.jpeg"></figure><figure name="81e2" id="81e2" class="graf graf--figure graf--layoutOutsetRowContinue is-partialWidth graf-after--figure" style="width: 48.482%;" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_H_VqypjqEku0QjLZ51rvKA.jpeg"></figure><p name="fab0" id="fab0" class="graf graf--p graf-after--figure"><code class="markup--code markup--p-code">T</code> is a tensor in Torch</p><pre name="e4b0" id="e4b0" class="graf graf--pre graf-after--p">a = T([[1., 2], [3, 4]])<br>b = T([[2., 2], [10, 10]])</pre><p name="0d10" id="0d10" class="graf graf--p graf-after--pre">When we have a mathematical operator between tensors in numpy or PyTorch, it will do element-wise assuming that they both have the same dimensionality. The below is how you would calculate the dot product of two vectors (e.g. (1, 2)⋅(2, 2) = 6 — the first rows of matrix a and b):</p><pre name="7227" id="7227" class="graf graf--pre graf-after--p">(a*b).sum(1)</pre><pre name="6265" id="6265" class="graf graf--pre graf-after--pre">6<br>70<br>[torch.FloatTensor of size 2]</pre><h4 name="2eea" id="2eea" class="graf graf--h4 graf-after--pre"><strong class="markup--strong markup--h4-strong">Building our first custom layer (i.e. PyTorch module)&nbsp;[</strong><a href="https://youtu.be/J99NV9Cr75I?t=33m55s" data-href="https://youtu.be/J99NV9Cr75I?t=33m55s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--h4-strong">33:55</strong></a><strong class="markup--strong markup--h4-strong">]</strong></h4><p name="a1e3" id="a1e3" class="graf graf--p graf-after--h4"><span class="markup--quote markup--p-quote is-other" name="anon_770cad27bb65" data-creator-ids="anon">We do this by creating a Python class that extends <code class="markup--code markup--p-code">nn.Module</code> and override<code class="markup--code markup--p-code">forward</code> function.</span></p><pre name="ba2e" id="ba2e" class="graf graf--pre graf-after--p">class DotProduct (nn.Module):<br>   def forward(self, u, m): return (u*m).sum(1)</pre><p name="dc07" id="dc07" class="graf graf--p graf-after--pre">Now we can call it and get the expected result (notice that we do not need to say <code class="markup--code markup--p-code">model.forward(a, b)</code> to call the <code class="markup--code markup--p-code">forward</code> function — it is a PyTorch magic.) [<a href="https://youtu.be/J99NV9Cr75I?t=40m14s" data-href="https://youtu.be/J99NV9Cr75I?t=40m14s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">40:14</a>]:</p><pre name="e82f" id="e82f" class="graf graf--pre graf-after--p">model = DotProduct()<br><strong class="markup--strong markup--pre-strong">model(a,b)</strong></pre><pre name="ca1b" id="ca1b" class="graf graf--pre graf-after--pre">6<br>70<br>[torch.FloatTensor of size 2]</pre><h4 name="b729" id="b729" class="graf graf--h4 graf-after--pre"><strong class="markup--strong markup--h4-strong">Building more complex module&nbsp;[</strong><a href="https://youtu.be/J99NV9Cr75I?t=41m31s" data-href="https://youtu.be/J99NV9Cr75I?t=41m31s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--h4-strong">41:31</strong></a><strong class="markup--strong markup--h4-strong">]</strong></h4><p name="f1ad" id="f1ad" class="graf graf--p graf-after--h4">This implementation has two additions to the <code class="markup--code markup--p-code">DotProduct</code> class:</p><ul class="postList"><li name="1ecf" id="1ecf" class="graf graf--li graf-after--p">Two <code class="markup--code markup--li-code">nn.Embedding</code> matrices</li><li name="0837" id="0837" class="graf graf--li graf-after--li">Look up our users and movies in above embedding matrices</li></ul><p name="5fb3" id="5fb3" class="graf graf--p graf-after--li">It is quite possible that user ID’s are not contiguous which makes it hard to use as an index of embedding matrix. So we will start by creating indexes that starts from zero and contiguous and replace <code class="markup--code markup--p-code">ratings.userId</code> column with the index by using Panda’s <code class="markup--code markup--p-code">apply</code> function with an anonymous function <code class="markup--code markup--p-code">lambda</code> and do the same for <code class="markup--code markup--p-code">ratings.movieId</code>&nbsp;.</p><pre name="6da2" id="6da2" class="graf graf--pre graf-after--p">u_uniq = ratings.userId.unique() <br>user2idx = {o:i <strong class="markup--strong markup--pre-strong">for</strong> i,o <strong class="markup--strong markup--pre-strong">in</strong> enumerate(u_uniq)} <br>ratings.userId = ratings.userId.apply(<strong class="markup--strong markup--pre-strong">lambda</strong> x: user2idx[x])  </pre><pre name="920b" id="920b" class="graf graf--pre graf-after--pre">m_uniq = ratings.movieId.unique() <br>movie2idx = {o:i <strong class="markup--strong markup--pre-strong">for</strong> i,o <strong class="markup--strong markup--pre-strong">in</strong> enumerate(m_uniq)} <br>ratings.movieId = ratings.movieId.apply(<strong class="markup--strong markup--pre-strong">lambda</strong> x: movie2idx[x])  </pre><pre name="4c44" id="4c44" class="graf graf--pre graf-after--pre">n_users=int(ratings.userId.nunique()) n_movies=int(ratings.movieId.nunique())</pre><p name="10bd" id="10bd" class="graf graf--p graf-after--pre"><em class="markup--em markup--p-em">Tip: </em><code class="markup--code markup--p-code">{o:i for i,o in enumerate(u_uniq)}</code> is a handy line of code to keep in your tool belt!</p><pre name="1438" id="1438" class="graf graf--pre graf-after--p">class EmbeddingDot(nn.Module):<br>    def __init__(self, n_users, n_movies):<br>        super().__init__()<br>        self.u = nn.Embedding(n_users, n_factors)<br>        self.m = nn.Embedding(n_movies, n_factors)<br>        self.u.weight.data.uniform_(0,0.05)<br>        self.m.weight.data.uniform_(0,0.05)<br>        <br>    def forward(self, cats, conts):<br>        users,movies = cats[:,0],cats[:,1]<br>        u,m = self.u(users),self.m(movies)<br>        return (u*m).sum(1)</pre><p name="db2f" id="db2f" class="graf graf--p graf-after--pre">Note that <code class="markup--code markup--p-code">__init__</code> is a constructor which is now needed because our class needs to keep track of “states” (how many movies, mow many users, how many factors, etc). We initialized the weights to random numbers between 0 and 0.05 and you can find more information about a standard algorithm for weight initialization, “Kaiming Initialization” <a href="http://www.jefkine.com/deep/2016/08/08/initialization-of-deep-networks-case-of-rectifiers/" data-href="http://www.jefkine.com/deep/2016/08/08/initialization-of-deep-networks-case-of-rectifiers/" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">here</a> (PyTorch has He initialization utility function but we are trying to do things from scratch here) [<a href="https://youtu.be/J99NV9Cr75I?t=46m58s" data-href="https://youtu.be/J99NV9Cr75I?t=46m58s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">46:58</a>].</p><p name="08d2" id="08d2" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">Embedding</code> is not a tensor but a <strong class="markup--strong markup--p-strong">variable</strong>. A variable does the exact same operations as a tensor but it also does automatic differentiation. To pull a tensor out of a variable, call <code class="markup--code markup--p-code">data</code> attribute. All the tensor functions have a variation with trailing underscore (e.g. <code class="markup--code markup--p-code">uniform_</code>) will do things in-place.</p><pre name="5310" id="5310" class="graf graf--pre graf-after--p">x = ratings.drop(['rating', 'timestamp'],axis=1)<br>y = ratings['rating'].astype(np.float32)<br>data = ColumnarModelData.from_data_frame(path, val_idxs, x, y, ['userId', 'movieId'], 64)</pre><p name="7c56" id="7c56" class="graf graf--p graf-after--pre">We are reusing <code class="markup--code markup--p-code">ColumnarModelData</code> (from fast.ai library) from Rossmann notebook, and that is the reason behind why there are both categorical and continuous variables in <code class="markup--code markup--p-code">def forward(self, cats, conts)</code> function in <code class="markup--code markup--p-code">EmbeddingDot</code> class [<a href="https://youtu.be/J99NV9Cr75I?t=50m20s" data-href="https://youtu.be/J99NV9Cr75I?t=50m20s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">50:20</a>]. Since we do not have continuous variable in this case, we will ignore <code class="markup--code markup--p-code">conts</code> and use the first and second columns of <code class="markup--code markup--p-code">cats</code> as <code class="markup--code markup--p-code">users</code> and <code class="markup--code markup--p-code">movies</code>&nbsp;. Note that they are mini-batches of users and movies. It is important not to manually loop through mini-batches because you will not get GPU acceleration, instead, process a whole mini-batch at a time as you see in line 3 and 4 of <code class="markup--code markup--p-code">forward</code> function above [<a href="https://youtu.be/J99NV9Cr75I?t=51m" data-href="https://youtu.be/J99NV9Cr75I?t=51m" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">51:00</a>–52:05].</p><pre name="18bb" id="18bb" class="graf graf--pre graf-after--p">wd=1e-5<br>model = EmbeddingDot(n_users, n_movies).cuda()<br>opt = optim.SGD(model.parameters(), 1e-1, weight_decay=wd, momentum=0.9)</pre><p name="5abf" id="5abf" class="graf graf--p graf-after--pre"><code class="markup--code markup--p-code">optim</code> is what gives us the optimizers in PyTorch. <code class="markup--code markup--p-code">model.parameters()</code> is one of the function inherited from <code class="markup--code markup--p-code">nn.Modules</code> that gives us all the weight to be updated/learned.</p><pre name="f785" id="f785" class="graf graf--pre graf-after--p">fit(model, data, 3, opt, F.mse_loss)</pre><p name="e820" id="e820" class="graf graf--p graf-after--pre">This function is from fast.ai library [<a href="https://youtu.be/J99NV9Cr75I?t=54m40s" data-href="https://youtu.be/J99NV9Cr75I?t=54m40s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">54:40</a>] and is closer to regular PyTorch approach compared to <code class="markup--code markup--p-code">learner.fit()</code> we have been using. It will not give you features like “stochastic gradient descent with restarts” or “differential learning rate” out of box.</p><h4 name="3414" id="3414" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Let’s improve our&nbsp;model</strong></h4><p name="f860" id="f860" class="graf graf--p graf-after--h4"><strong class="markup--strong markup--p-strong">Bias </strong>— to adjust to generally popular movies or generally enthusiastic users.</p><pre name="f593" id="f593" class="graf graf--pre graf-after--p">min_rating,max_rating = ratings.rating.min(),ratings.rating.max()<br>min_rating,max_rating</pre><pre name="d0dc" id="d0dc" class="graf graf--pre graf-after--pre">def get_emb(ni,nf):<br>    e = nn.Embedding(ni, nf)<br>    e.weight.data.uniform_(-0.01,0.01)<br>    return e</pre><pre name="a248" id="a248" class="graf graf--pre graf-after--pre">class EmbeddingDotBias(nn.Module):<br>    def __init__(self, n_users, n_movies):<br>        super().__init__()<br>        (self.u, self.m, <strong class="markup--strong markup--pre-strong">self.ub</strong>, <strong class="markup--strong markup--pre-strong">self.mb</strong>) = [get_emb(*o) for o in [<br>            (n_users, n_factors), (n_movies, n_factors), (n_users,1), (n_movies,1)<br>        ]]<br>        <br>    def forward(self, cats, conts):<br>        users,movies = cats[:,0],cats[:,1]<br>        um = (self.u(users)* self.m(movies)).sum(1)<br>        res = um +<strong class="markup--strong markup--pre-strong"> self.ub(users)</strong>.squeeze() + <strong class="markup--strong markup--pre-strong">self.mb(movies)</strong>.squeeze()<br>        res = F.sigmoid(res) * (max_rating-min_rating) + min_rating<br>        return res</pre><p name="9214" id="9214" class="graf graf--p graf-after--pre"><code class="markup--code markup--p-code">squeeze</code> is PyTorch version of <em class="markup--em markup--p-em">broadcasting</em> [<a href="https://youtu.be/J99NV9Cr75I?t=1h4m11s" data-href="https://youtu.be/J99NV9Cr75I?t=1h4m11s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:04:11</a>] for more information, see Machine Learning class or <a href="https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html" data-href="https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">numpy documentation</a>.</p><p name="60ce" id="60ce" class="graf graf--p graf-after--p">Can we squish the ratings so that it is between 1 and 5? Yes! By putting the prediction through sigmoid function will result in number between 1 and 0. So in our case, we can multiply that by 4 and add 1 — which will result in number between 1 and 5.</p><figure name="cb54" id="cb54" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_UYeXmpTtxA0pIkHJ8ETMUA.png"></figure><p name="79af" id="79af" class="graf graf--p graf-after--figure"><code class="markup--code markup--p-code">F</code> is a PyTorch functional (<code class="markup--code markup--p-code">torch.nn.functional</code>) that contains all functions for tensors, and is imported as <code class="markup--code markup--p-code">F</code> in most cases.</p><pre name="5f2d" id="5f2d" class="graf graf--pre graf-after--p">wd=2e-4<br>model = EmbeddingDotBias(cf.n_users, cf.n_items).cuda()<br>opt = optim.SGD(model.parameters(), 1e-1, weight_decay=wd, momentum=0.9)</pre><pre name="666f" id="666f" class="graf graf--pre graf-after--pre">fit(model, data, 3, opt, F.mse_loss)<br>[ 0.       0.85056  0.83742]                                     <br>[ 1.       0.79628  0.81775]                                     <br>[ 2.       0.8012   0.80994]</pre><p name="e3fc" id="e3fc" class="graf graf--p graf-after--pre">Let’s take a look at fast.ai code [<a href="https://youtu.be/J99NV9Cr75I?t=1h13m44s" data-href="https://youtu.be/J99NV9Cr75I?t=1h13m44s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:13:44</a>] we used in our <strong class="markup--strong markup--p-strong">Simple Python version. </strong>In <code class="markup--code markup--p-code">column_data.py</code> file, <code class="markup--code markup--p-code">CollabFilterDataSet.get_leaner</code> calls <code class="markup--code markup--p-code">get_model</code> function that creates <code class="markup--code markup--p-code">EmbeddingDotBias</code> class that is identical to what we created.</p><h4 name="f7f6" id="f7f6" class="graf graf--h4 graf-after--p">Neural Net Version [<a href="https://youtu.be/J99NV9Cr75I?t=1h17m21s" data-href="https://youtu.be/J99NV9Cr75I?t=1h17m21s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:17:21</a>]</h4><p name="bf9f" id="bf9f" class="graf graf--p graf-after--h4">We go back to excel sheet to understand the intuition. Notice that we create user_idx to look up Embeddings just like we did in the python code earlier. If we were to one-hot-encode the user_idx and multiply it by user embeddings, we will get the applicable row for the user. If it is just matrix multiplication, why do we need Embeddings? It is for computational performance optimization purposes.</p><figure name="adcd" id="adcd" class="graf graf--figure graf--layoutOutsetCenter graf-after--p" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_0CRZIBnNzw1lT_9EHOyd5g.png"></figure><p name="e467" id="e467" class="graf graf--p graf-after--figure">Rather than calculating the dot product of user embedding vector and movie embedding vector to get a prediction, we will concatenate the two and feed it through neural net.</p><pre name="15fc" id="15fc" class="graf graf--pre graf-after--p">class EmbeddingNet(nn.Module):<br>    def __init__(self, n_users, n_movies, <strong class="markup--strong markup--pre-strong">nh</strong>=10, p1=0.5, p2=0.5):<br>        super().__init__()<br>        (self.u, self.m) = [get_emb(*o) for o in [<br>            (n_users, n_factors), (n_movies, n_factors)]]<br>        self.lin1 = <strong class="markup--strong markup--pre-strong">nn.Linear</strong>(n_factors*2, nh)<br>        self.lin2 = nn.Linear(nh, 1)<br>        self.drop1 = nn.Dropout(p1)<br>        self.drop2 = nn.Dropout(p2)<br>        <br>    def forward(self, cats, conts):<br>        users,movies = cats[:,0],cats[:,1]<br>        x = self.drop1(torch.cat([self.u(users),self.m(movies)], dim=1))<br>        x = self.drop2(F.relu(self.lin1(x)))<br>        return F.sigmoid(self.lin2(x)) * (max_rating-min_rating+1) + min_rating-0.5</pre><p name="1d75" id="1d75" class="graf graf--p graf-after--pre">Notice that we no longer has bias terms since <code class="markup--code markup--p-code">Linear</code> layer in PyTorch already has a build in bias. <code class="markup--code markup--p-code">nh</code> is a number of activations a linear layer creates (Jeremy calls it “num hidden”).</p><figure name="b915" id="b915" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_EUxuR7ejeb1wJUib0GRr2g.jpeg"></figure><p name="abb8" id="abb8" class="graf graf--p graf-after--figure">It only has one hidden layer, so maybe not “deep”, but this is definitely a neural network.</p><pre name="cbd3" id="cbd3" class="graf graf--pre graf-after--p">wd=1e-5<br>model = EmbeddingNet(n_users, n_movies).cuda()<br>opt = optim.Adam(model.parameters(), 1e-3, weight_decay=wd)<br>fit(model, data, 3, opt, <strong class="markup--strong markup--pre-strong">F.mse_loss</strong>)</pre><pre name="3a8c" id="3a8c" class="graf graf--pre graf-after--pre">A Jupyter Widget</pre><pre name="98ef" id="98ef" class="graf graf--pre graf-after--pre">[ 0.       0.88043  0.82363]                                    <br>[ 1.       0.8941   0.81264]                                    <br>[ 2.       0.86179  0.80706]</pre><p name="8fae" id="8fae" class="graf graf--p graf-after--pre">Notice that the loss functions are also in <code class="markup--code markup--p-code">F</code> (here, it s mean squared loss).</p><p name="8be7" id="8be7" class="graf graf--p graf-after--p">Now that we have neural net, there are many things we can try:</p><ul class="postList"><li name="9f9d" id="9f9d" class="graf graf--li graf-after--p">Add dropouts</li><li name="2f53" id="2f53" class="graf graf--li graf-after--li">Use different embedding sizes for user embedding and movie embedding</li><li name="6ac6" id="6ac6" class="graf graf--li graf-after--li">Not only user and movie embeddings, but append movie genre embedding and/or timestamp from the original data.</li><li name="8de6" id="8de6" class="graf graf--li graf-after--li">Increase/decrease number of hidden layers and activations</li><li name="c0b4" id="c0b4" class="graf graf--li graf-after--li">Increase/decrease regularization</li></ul><h4 name="18c1" id="18c1" class="graf graf--h4 graf-after--li"><strong class="markup--strong markup--h4-strong">What is happening in the training loop?</strong> [<a href="https://youtu.be/J99NV9Cr75I?t=1h33m21s" data-href="https://youtu.be/J99NV9Cr75I?t=1h33m21s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:33:21</a>]</h4><p name="9061" id="9061" class="graf graf--p graf-after--h4">Currently, we are passing off the updating of weights to PyTorch’s optimizer. What does an optimizer do? and what is a <code class="markup--code markup--p-code">momentum</code>?</p><pre name="661c" id="661c" class="graf graf--pre graf-after--p">opt = optim.SGD(model.parameters(), 1e-1, weight_decay=wd, momentum=0.9)</pre><p name="8f01" id="8f01" class="graf graf--p graf-after--pre">We are going to implement gradient descent in an excel sheet (<a href="https://github.com/fastai/fastai/blob/master/courses/dl1/excel/graddesc.xlsm" data-href="https://github.com/fastai/fastai/blob/master/courses/dl1/excel/graddesc.xlsm" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">graddesc.xlsm</a>) — see worksheets right to left. First we create a random <em class="markup--em markup--p-em">x</em>’s, and <em class="markup--em markup--p-em">y</em>’s that are linearly correlated with the <em class="markup--em markup--p-em">x</em>’s (e.g. <em class="markup--em markup--p-em">y</em>= <em class="markup--em markup--p-em">a*x</em> + <em class="markup--em markup--p-em">b</em>). By using sets of <em class="markup--em markup--p-em">x</em>’s and <em class="markup--em markup--p-em">y</em>’s, we will try to learn <em class="markup--em markup--p-em">a</em> and <em class="markup--em markup--p-em">b.</em></p><figure name="a1cd" id="a1cd" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_EyHgeFUNArZ3xZRbY507QQ.jpeg"></figure><figure name="cfa9" id="cfa9" class="graf graf--figure graf-after--figure"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_D_qMGnGAmQYMwsuBpBhhlQ.jpeg"></figure><p name="b840" id="b840" class="graf graf--p graf-after--figure">To calculate the error, we first need a prediction, and square the difference:</p><figure name="9fff" id="9fff" class="graf graf--figure graf--layoutOutsetCenter graf-after--p" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_q7Fb4G2j2csZ7sS0tbi8zQ.png"></figure><p name="e9aa" id="e9aa" class="graf graf--p graf-after--figure">To reduce the error, we increase/decrease <em class="markup--em markup--p-em">a</em> and <em class="markup--em markup--p-em">b</em> a little bit and figure out what would make the error decrease. This is called finding the derivative through finite differencing.</p><figure name="2ae8" id="2ae8" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_Z2NHeXo8RFIOwhCyuQb7oQ.jpeg"></figure><p name="7569" id="7569" class="graf graf--p graf-after--figure">Finite differencing gets complicated in high dimensional spaces [<a href="https://youtu.be/J99NV9Cr75I?t=1h41m46s" data-href="https://youtu.be/J99NV9Cr75I?t=1h41m46s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:41:46</a>], and it becomes very memory intensive and takes a long time. So we want to find some way to do this more quickly. It is worthwhile to look up things like Jacobian and Hessian (Deep Learning book: <a href="http://www.deeplearningbook.org/contents/numerical.html" data-href="http://www.deeplearningbook.org/contents/numerical.html" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">section 4.3.1 page 84</a>).</p><h4 name="5d32" id="5d32" class="graf graf--h4 graf-after--p">Chain Rule and Backpropagation</h4><p name="03a7" id="03a7" class="graf graf--p graf-after--h4">The faster approach is to do this analytically [<a href="https://youtu.be/J99NV9Cr75I?t=1h45m27s" data-href="https://youtu.be/J99NV9Cr75I?t=1h45m27s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:45:27</a>]. For this, we need a chain rule:</p><figure name="9f7a" id="9f7a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*DS4ZfpUfsseOBayQMqS4Yw.png" data-width="381" data-height="76" src="../img/1_DS4ZfpUfsseOBayQMqS4Yw.png"><figcaption class="imageCaption">Overview of <a href="https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/differentiating-vector-valued-functions/a/multivariable-chain-rule-simple-version" data-href="https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/differentiating-vector-valued-functions/a/multivariable-chain-rule-simple-version" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">chain&nbsp;rule</a></figcaption></figure><p name="33b8" id="33b8" class="graf graf--p graf-after--figure">Here is a great article by Chris Olah on <a href="http://colah.github.io/posts/2015-08-Backprop/" data-href="http://colah.github.io/posts/2015-08-Backprop/" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Backpropagation as a chain rule</a>.</p><p name="1e9a" id="1e9a" class="graf graf--p graf-after--p">Now we replace the finite-difference with an actual derivative <a href="https://www.wolframalpha.com/" data-href="https://www.wolframalpha.com/" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">WolframAlpha </a>gave us (notice that finite-difference output is fairly close to the actual derivative and good way to do quick sanity check if you need to calculate your own derivative):</p><figure name="e67a" id="e67a" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_VHXoG1HpJxlR_y0yfrEz-g.jpeg"></figure><ul class="postList"><li name="cfcb" id="cfcb" class="graf graf--li graf--startsWithDoubleQuote graf-after--figure">“Online” training — mini-batch with size 1</li></ul><p name="292e" id="292e" class="graf graf--p graf-after--li">And this is how you do SGD with excel sheet. If you were to change the prediction value with the output from CNN spreadsheet, we can train CNN with SGD.</p><h4 name="2d4e" id="2d4e" class="graf graf--h4 graf-after--p">Momentum [<a href="https://youtu.be/J99NV9Cr75I?t=1h53m47s" data-href="https://youtu.be/J99NV9Cr75I?t=1h53m47s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:53:47</a>]</h4><blockquote name="77dd" id="77dd" class="graf graf--blockquote graf-after--h4">Come on, take a hint — that’s a good direction. Please keep doing that but more.</blockquote><p name="2838" id="2838" class="graf graf--p graf-after--blockquote">With this approach, we will use a linear interpolation between the current mini-batch’s derivative and the step (and direction) we took after the last mini-batch (cell K9):</p><figure name="7a84" id="7a84" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_zvTMttj6h4iwFcxnt8zKyg.png"></figure><p name="8414" id="8414" class="graf graf--p graf-after--figure">Compared to <em class="markup--em markup--p-em">de</em>/<em class="markup--em markup--p-em">db</em> whose sign (+/-) is random, the one with momentum will keep going the same direction a little bit faster up till certain point. This will reduce a number of epochs required for training.</p><h4 name="006f" id="006f" class="graf graf--h4 graf-after--p">Adam [<a href="https://youtu.be/J99NV9Cr75I?t=1h59m4s" data-href="https://youtu.be/J99NV9Cr75I?t=1h59m4s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:59:04</a>]</h4><p name="a87f" id="a87f" class="graf graf--p graf-after--h4">Adam is much faster but the issue has been that final predictions are not as good as as they are with SGD with momentum. It seems as though that it was due to the combined usage of Adam and weight decay. The new version that fixes this issue is called <strong class="markup--strong markup--p-strong">AdamW</strong>.</p><figure name="534e" id="534e" class="graf graf--figure graf--layoutOutsetCenter graf-after--p" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_0yZ9Hbn2BPSNY9L-5jL0Tg.png"></figure><ul class="postList"><li name="a199" id="a199" class="graf graf--li graf-after--figure"><code class="markup--code markup--li-code">cell J8</code>&nbsp;: a linear interpolation of derivative and previous direction (identical to what we had in momentum)</li><li name="9d18" id="9d18" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">cell L8</code>&nbsp;: a linear interpolation of derivative squared + derivative squared from last step ( <code class="markup--code markup--li-code">cell L7</code>)</li><li name="f7dc" id="f7dc" class="graf graf--li graf-after--li">The idea is called “exponentially weighted moving average” (in another words, average with previous values multiplicatively decreased)</li></ul><p name="cdff" id="cdff" class="graf graf--p graf-after--li">Learning rate is much higher than before because we are dividing it by square root of <code class="markup--code markup--p-code">L8</code>&nbsp;.</p><p name="66a9" id="66a9" class="graf graf--p graf-after--p">If you take a look at fast.ai library (model.py), you will notice that in <code class="markup--code markup--p-code">fit</code> function, it does not just calculate average loss, but it is calculating the <strong class="markup--strong markup--p-strong">exponentially weighted moving average of loss</strong>.</p><pre name="0c44" id="0c44" class="graf graf--pre graf-after--p">avg_loss = avg_loss * avg_mom + loss * (1-avg_mom)</pre><p name="3ee2" id="3ee2" class="graf graf--p graf-after--pre">Another helpful concept is whenever you see `α(…) + (1-α)(…)`, immediately think <strong class="markup--strong markup--p-strong">linear interpolation.</strong></p><h4 name="2c33" id="2c33" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Some intuitions</strong></h4><ul class="postList"><li name="16f1" id="16f1" class="graf graf--li graf-after--h4">We calculated exponentially weighted moving average of gradient squared, take a square root of that, and divided the learning rate by it.</li><li name="77a0" id="77a0" class="graf graf--li graf-after--li">Gradient squared is always positive.</li><li name="b6c4" id="b6c4" class="graf graf--li graf-after--li">When there is high variance in gradients, gradient squared will be large.</li><li name="ca37" id="ca37" class="graf graf--li graf-after--li">When the gradients are constant, gradient squared will be small.</li><li name="3645" id="3645" class="graf graf--li graf-after--li">If gradients are changing a lot, we want to be careful and divide the learning rate by a big number (slow down)</li><li name="a808" id="a808" class="graf graf--li graf-after--li">If gradients are not changing much, we will take a bigger step by dividing the learning rate with a small number</li><li name="a082" id="a082" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Adaptive learning rate</strong> — keep track of the average of the squares of the gradients and use that to adjust the learning rate. So there is just one learning rage, but effectively every parameter at every epoch is getting a bigger jump if the gradient is constant; smaller jump otherwise.</li><li name="0fac" id="0fac" class="graf graf--li graf-after--li">There are two momentums — one for gradient, and the other for gradient squared (in PyTorch, it is called a beta which is a tuple of two numbers)</li></ul><h4 name="18b1" id="18b1" class="graf graf--h4 graf-after--li">AdamW[<a href="https://youtu.be/J99NV9Cr75I?t=2h11m18s" data-href="https://youtu.be/J99NV9Cr75I?t=2h11m18s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">2:11:18</a>]</h4><p name="f1c6" id="f1c6" class="graf graf--p graf-after--h4">When there are much more parameters than data points, regularizations become important. We had seen dropout previously, and weight decay is another type of regularization. Weight decay (L2 regularization) penalizes large weights by adding squared weights (times weight decay multiplier) to the loss. Now the loss function wants to keep the weights small because increasing the weights will increase the loss; hence only doing so when the loss improves by more than the penalty.</p><p name="3170" id="3170" class="graf graf--p graf-after--p graf--trailing">The problem is that since we added the squared weights to the loss function, this affects the moving average of gradients and the moving average of the squared gradients for Adam. This result in decreasing the amount of weight decay when there is high variance in gradients, and increasing the amount of weight decay when there is little variation. In other words, “penalize large weights unless gradients varies a lot” which is not what we intended. AdamW removed the weight decay out of the loss function, and added it directly when updating the weights.</p><hr class="section-divider"><p name="0758" id="0758" class="graf graf--p graf--leading graf--trailing">Lessons: <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" class="markup--anchor markup--p-anchor" target="_blank">1</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" class="markup--anchor markup--p-anchor" target="_blank">2</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" class="markup--anchor markup--p-anchor" target="_blank">3</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" class="markup--anchor markup--p-anchor" target="_blank">4</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" class="markup--anchor markup--p-anchor" target="_blank"><strong class="markup--strong markup--p-strong">5</strong></a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" class="markup--anchor markup--p-anchor" target="_blank">6</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" class="markup--anchor markup--p-anchor" target="_blank">7</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" class="markup--anchor markup--p-anchor" target="_blank">8</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" class="markup--anchor markup--p-anchor" target="_blank">9</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" class="markup--anchor markup--p-anchor" target="_blank">10</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" class="markup--anchor markup--p-anchor" target="_blank">11</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" class="markup--anchor markup--p-anchor" target="_blank">12</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" class="markup--anchor markup--p-anchor" target="_blank">13</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" class="markup--anchor markup--p-anchor" target="_blank">14</a></p></body></html>