
<!-- saved from url=(0063)file:///C:/Users/asus/Desktop/fastai-ml-dl-notes-zh/en/dl7.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></head><body><hr class="section-divider"><h1 name="2f7f" id="2f7f" class="graf graf--h3 graf--leading graf--title">Deep Learning 2: Part 1 Lesson&nbsp;7</h1><p name="724a" id="724a" class="graf graf--p graf-after--h3"><em class="markup--em markup--p-em">My personal notes from </em><a href="http://www.fast.ai/" data-href="http://www.fast.ai/" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank"><em class="markup--em markup--p-em">fast.ai course</em></a><em class="markup--em markup--p-em">. These notes will continue to be updated and improved as I continue to review the course to “really” understand it. Much appreciation to </em><a href="https://twitter.com/jeremyphoward" data-href="https://twitter.com/jeremyphoward" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank"><em class="markup--em markup--p-em">Jeremy</em></a><em class="markup--em markup--p-em"> and </em><a href="https://twitter.com/math_rachel" data-href="https://twitter.com/math_rachel" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank"><em class="markup--em markup--p-em">Rachel</em></a><em class="markup--em markup--p-em"> who gave me this opportunity to learn.</em></p><p name="8a99" id="8a99" class="graf graf--p graf-after--p graf--trailing">Lessons: <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" class="markup--anchor markup--p-anchor" target="_blank">1</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" class="markup--anchor markup--p-anchor" target="_blank">2</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" class="markup--anchor markup--p-anchor" target="_blank">3</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" class="markup--anchor markup--p-anchor" target="_blank">4</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" class="markup--anchor markup--p-anchor" target="_blank">5</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" class="markup--anchor markup--p-anchor" target="_blank">6</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" class="markup--anchor markup--p-anchor" target="_blank"><strong class="markup--strong markup--p-strong">7</strong></a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" class="markup--anchor markup--p-anchor" target="_blank">8</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" class="markup--anchor markup--p-anchor" target="_blank">9</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" class="markup--anchor markup--p-anchor" target="_blank">10</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" class="markup--anchor markup--p-anchor" target="_blank">11</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" class="markup--anchor markup--p-anchor" target="_blank">12</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" class="markup--anchor markup--p-anchor" target="_blank">13</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" class="markup--anchor markup--p-anchor" target="_blank">14</a></p><hr class="section-divider"><h3 name="46b3" id="46b3" class="graf graf--h3 graf--leading"><a href="http://forums.fast.ai/t/wiki-lesson-7/9405" data-href="http://forums.fast.ai/t/wiki-lesson-7/9405" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">Lesson 7</a></h3><p name="dc17" id="dc17" class="graf graf--p graf-after--h3">The theme of Part 1 is:</p><ul class="postList"><li name="a260" id="a260" class="graf graf--li graf-after--p">classification and regression with deep learning</li><li name="239c" id="239c" class="graf graf--li graf-after--li">identifying and learning best and established practices</li><li name="1df8" id="1df8" class="graf graf--li graf-after--li">focus is on classification and regression which is predicting “a thing” (e.g. a number, a small number of labels)</li></ul><p name="ce50" id="ce50" class="graf graf--p graf-after--li">Part 2 of the course:</p><ul class="postList"><li name="bdb8" id="bdb8" class="graf graf--li graf-after--p">focus is on generative modeling which means predicting “lots of things” — for example, creating a sentence as in neural translation, image captioning, or question answering while creating an image such as in style transfer, super-resolution, segmentation and so forth.</li><li name="7f27" id="7f27" class="graf graf--li graf-after--li">not as much best practices but a little more speculative from recent papers that may not be fully tested.</li></ul><h4 name="687e" id="687e" class="graf graf--h4 graf-after--li">Review of Char3Model [<a href="https://youtu.be/H3g26EVADgY?t=2m49s" data-href="https://youtu.be/H3g26EVADgY?t=2m49s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">02:49</a>]</h4><p name="0fd0" id="0fd0" class="graf graf--p graf-after--h4">Reminder: RNN is not in any way different or unusual or magical — just a standard fully connected network.</p><figure name="9769" id="9769" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_9XXQ3J7G3rD92tFkusi4bA.png"><figcaption class="imageCaption">Standard fully connected network</figcaption></figure><ul class="postList"><li name="4a92" id="4a92" class="graf graf--li graf-after--figure">Arrows represent one or more layer operations — generally speaking a linear followed by a non-linear function, in this case matrix multiplications followed by <code class="markup--code markup--li-code">relu</code> or <code class="markup--code markup--li-code">tanh</code></li><li name="9313" id="9313" class="graf graf--li graf-after--li">Arrows of the same color represent exactly the same weight matrix being used.</li><li name="5393" id="5393" class="graf graf--li graf-after--li">One slight difference from previous is that there are inputs coming in at the second and third layers. We tried two approaches — concatenating and adding these inputs to the current activations.</li></ul><pre name="b1a6" id="b1a6" class="graf graf--pre graf-after--li"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">Char3Model</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, vocab_size, n_fac):<br>        super().__init__()<br>        self.e = nn.Embedding(vocab_size, n_fac)<br><br>        <em class="markup--em markup--pre-em"># The 'green arrow' from our diagram</em><br>        self.l_in = nn.Linear(n_fac, n_hidden)<br><br>        <em class="markup--em markup--pre-em"># The 'orange arrow' from our diagram</em><br>        self.l_hidden = nn.Linear(n_hidden, n_hidden)<br>        <br>        <em class="markup--em markup--pre-em"># The 'blue arrow' from our diagram</em><br>        self.l_out = nn.Linear(n_hidden, vocab_size)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, c1, c2, c3):<br>        in1 = F.relu(self.l_in(self.e(c1)))<br>        in2 = F.relu(self.l_in(self.e(c2)))<br>        in3 = F.relu(self.l_in(self.e(c3)))<br>        <br>        h = V(torch.zeros(in1.size()).cuda())<br>        h = F.tanh(self.l_hidden(h+in1))<br>        h = F.tanh(self.l_hidden(h+in2))<br>        h = F.tanh(self.l_hidden(h+in3))<br>        <br>        <strong class="markup--strong markup--pre-strong">return</strong> F.log_softmax(self.l_out(h))</pre><ul class="postList"><li name="844e" id="844e" class="graf graf--li graf-after--pre">By using <code class="markup--code markup--li-code">nn.Linear</code> we get both the weight matrix and the bias vector wrapped up for free for us.</li><li name="7795" id="7795" class="graf graf--li graf-after--li">To deal with the fact that there is no orange arrow coming in for the first ellipse&nbsp;, we invented an empty matrix</li></ul><pre name="ff33" id="ff33" class="graf graf--pre graf-after--li"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">CharLoopModel</strong>(nn.Module):<br>    <em class="markup--em markup--pre-em"># This is an RNN!</em><br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, vocab_size, n_fac):<br>        super().__init__()<br>        self.e = nn.Embedding(vocab_size, n_fac)<br>        self.l_in = nn.Linear(n_fac, n_hidden)<br>        self.l_hidden = nn.Linear(n_hidden, n_hidden)<br>        self.l_out = nn.Linear(n_hidden, vocab_size)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, *cs):<br>        bs = cs[0].size(0)<br>        h = V(torch.zeros(bs, n_hidden).cuda())<br>        <strong class="markup--strong markup--pre-strong">for</strong> c <strong class="markup--strong markup--pre-strong">in</strong> cs:<br>            inp = F.relu(self.l_in(self.e(c)))<br>            h = F.tanh(self.l_hidden(h+inp))<br>        <br>        <strong class="markup--strong markup--pre-strong">return</strong> F.log_softmax(self.l_out(h), dim=-1)</pre><ul class="postList"><li name="d5f9" id="d5f9" class="graf graf--li graf-after--pre">Almost identical except for the <code class="markup--code markup--li-code">for</code> loop</li></ul><pre name="399b" id="399b" class="graf graf--pre graf-after--li"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">CharRnn</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, vocab_size, n_fac):<br>        super().__init__()<br>        self.e = nn.Embedding(vocab_size, n_fac)<br>        self.rnn = nn.RNN(n_fac, n_hidden)<br>        self.l_out = nn.Linear(n_hidden, vocab_size)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, *cs):<br>        bs = cs[0].size(0)<br>        h = V(torch.zeros(1, bs, n_hidden))<br>        inp = self.e(torch.stack(cs))<br>        outp,h = self.rnn(inp, h)<br>        <br>        <strong class="markup--strong markup--pre-strong">return</strong> F.log_softmax(self.l_out(outp[-1]), dim=-1)</pre><ul class="postList"><li name="b74e" id="b74e" class="graf graf--li graf-after--pre">PyTorch version — <code class="markup--code markup--li-code">nn.RNN</code> will create the loop and keep track of <code class="markup--code markup--li-code">h</code> as it goes along.</li><li name="8ce5" id="8ce5" class="graf graf--li graf-after--li">We are using white section to predict the green character — which seems wasteful as the next section mostly overlaps with the current section.</li></ul><figure name="cc6c" id="cc6c" class="graf graf--figure graf-after--li"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_4v68iwTS32RHplB8c-egmg.png"></figure><ul class="postList"><li name="17ad" id="17ad" class="graf graf--li graf-after--figure">We then tried splitting it into non-overlapping pieces in multi-output model:</li></ul><figure name="0b91" id="0b91" class="graf graf--figure graf-after--li"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_5LY1Sdql1_VLHDfdd2e8lw.png"></figure><ul class="postList"><li name="83c2" id="83c2" class="graf graf--li graf-after--figure">In this approach, we are throwing away our <code class="markup--code markup--li-code">h</code> activation after processing each section and started a new one. In order to predict the second character using the first one in the next section, it has nothing to go on but a default activation. Let’s not throw away <code class="markup--code markup--li-code">h</code>&nbsp;.</li></ul><h4 name="3ed0" id="3ed0" class="graf graf--h4 graf-after--li">Stateful RNN&nbsp;[<a href="https://youtu.be/H3g26EVADgY?t=8m52s" data-href="https://youtu.be/H3g26EVADgY?t=8m52s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">08:52</a>]</h4><pre name="2fc5" id="2fc5" class="graf graf--pre graf-after--h4"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">CharSeqStatefulRnn</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, vocab_size, n_fac, bs):<br>        self.vocab_size = vocab_size<br>        super().__init__()<br>        self.e = nn.Embedding(vocab_size, n_fac)<br>        self.rnn = nn.RNN(n_fac, n_hidden)<br>        self.l_out = nn.Linear(n_hidden, vocab_size)<br>        <strong class="markup--strong markup--pre-strong">self.init_hidden(bs)</strong><br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, cs):<br>        bs = cs[0].size(0)<br>        <strong class="markup--strong markup--pre-strong">if</strong> self.h.size(1) != bs: self.init_hidden(bs)<br>        outp,h = self.rnn(self.e(cs), self.h)<br>        <strong class="markup--strong markup--pre-strong">self.h = repackage_var(h)</strong><br>        <strong class="markup--strong markup--pre-strong">return</strong> F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)<br>    <br>    <strong class="markup--strong markup--pre-strong">def</strong> init_hidden(self, bs): self.h = V(torch.zeros(1, bs, n_hidden))</pre><ul class="postList"><li name="1f50" id="1f50" class="graf graf--li graf-after--pre">One additional line in constructor. <code class="markup--code markup--li-code">self.init_hidden(bs)</code> sets <code class="markup--code markup--li-code">self.h</code> to bunch of zeros.</li><li name="f027" id="f027" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Wrinkle #1</strong> [<a href="https://youtu.be/H3g26EVADgY?t=10m51s" data-href="https://youtu.be/H3g26EVADgY?t=10m51s" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">10:51</a>] — if we were to simply do <code class="markup--code markup--li-code">self.h = h</code>&nbsp;, and we trained on a document that is a million characters long, then the size of unrolled version of the RNN has a million layers (ellipses). One million layer fully connected network is going to be very memory intensive because in order to do a chain rule, we have to multiply one million layers while remembering all one million gradients every batch.</li><li name="7a3c" id="7a3c" class="graf graf--li graf-after--li">To avoid this, we tell it to forget its history from time to time. We can still remember the state (the values in our hidden matrix) without remembering everything about how we got there.</li></ul><pre name="1c88" id="1c88" class="graf graf--pre graf-after--li">def repackage_var(h):<em class="markup--em markup--pre-em"><br>    </em>return Variable(h.data) if type(h) == Variable else tuple(repackage_var(v) for v in h)</pre><ul class="postList"><li name="297b" id="297b" class="graf graf--li graf-after--pre">Grab the tensor out of <code class="markup--code markup--li-code">Variable</code> <code class="markup--code markup--li-code">h</code> (remember, a tensor itself does not have any concept of history), and create a new <code class="markup--code markup--li-code">Variable</code> out of that. The new variable has the same value but no history of operations, therefore when it tries to back-propagate, it will stop there.</li><li name="abcd" id="abcd" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">forward</code> will process 8 characters, it then back propagate through eight layers, keep track of the values in out hidden state, but it will throw away its history of operations. This is called <strong class="markup--strong markup--li-strong">back-prop through time (bptt)</strong>.</li><li name="2296" id="2296" class="graf graf--li graf-after--li">In other words, after the <code class="markup--code markup--li-code">for</code> loop, just throw away the history of operations and start afresh. So we are keeping our hidden state but we are not keeping our hidden state history.</li><li name="bfe4" id="bfe4" class="graf graf--li graf-after--li">Another good reason not to back-propagate through too many layers is that if you have any kind of gradient instability (e.g. gradient explosion or gradient banishing), the more layers you have, the harder the network gets to train (slower and less resilient).</li><li name="bb28" id="bb28" class="graf graf--li graf-after--li">On the other hand, the longer <code class="markup--code markup--li-code">bptt</code> means that you are able to explicitly capture a longer memory and more state.</li><li name="8b4d" id="8b4d" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Wrinkle #2</strong> [<a href="https://youtu.be/H3g26EVADgY?t=16m" data-href="https://youtu.be/H3g26EVADgY?t=16m" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">16:00</a>] — how to create mini-batches. We do not want to process one section at a time, but a bunch in parallel at a time.</li><li name="1165" id="1165" class="graf graf--li graf-after--li">When we started looking at TorchText for the first time, we talked about how it creates these mini-batches.</li><li name="2ca6" id="2ca6" class="graf graf--li graf-after--li">Jeremy said we take a whole long document consisting of the entire works of Nietzsche or all of the IMDB reviews concatenated together, we split this into 64 equal sized chunks (NOT chunks of size 64).</li></ul><figure name="01a4" id="01a4" class="graf graf--figure graf-after--li"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_YOUoCz-p7semcNuDFZqp_w.png"></figure><ul class="postList"><li name="1066" id="1066" class="graf graf--li graf-after--figure">For a document that is 64 million characters long, each “chunk” will be 1 million characters. We stack them together and now split them by <code class="markup--code markup--li-code">bptt</code> — 1 mini-bach consists of 64 by <code class="markup--code markup--li-code">bptt</code> matrix.</li><li name="8a37" id="8a37" class="graf graf--li graf-after--li">The first character of the second chunk(1,000,001th character) is likely be in the middle of a sentence. But it is okay since it only happens once every million characters.</li></ul><h4 name="92e7" id="92e7" class="graf graf--h4 graf-after--li">Question: Data augmentation for this kind of dataset?&nbsp;[<a href="https://youtu.be/H3g26EVADgY?t=20m34s" data-href="https://youtu.be/H3g26EVADgY?t=20m34s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">20:34</a>]</h4><p name="f30b" id="f30b" class="graf graf--p graf-after--h4">There is no known good way. Somebody recently won a Kaggle competition by doing data augmentation which randomly inserted parts of different rows — something like that may be useful here. But there has not been any recent state-of-the-art NLP papers that are doing this kind of data augmentation.</p><h4 name="175d" id="175d" class="graf graf--h4 graf-after--p">Question: How do we choose the size of bptt?&nbsp;[<a href="https://youtu.be/H3g26EVADgY?t=21m36s" data-href="https://youtu.be/H3g26EVADgY?t=21m36s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">21:36</a>]</h4><p name="0182" id="0182" class="graf graf--p graf-after--h4">There are a couple things to think about:</p><ul class="postList"><li name="69fb" id="69fb" class="graf graf--li graf-after--p">the first is that mini-batch matrix has a size of <code class="markup--code markup--li-code">bs</code> (# of chunks) by <code class="markup--code markup--li-code">bptt</code> so your GPU RAM must be able to fit that by your embedding matrix. So if you get CUDA out of memory error, you need reduce one of these.</li><li name="3504" id="3504" class="graf graf--li graf-after--li">If your training is unstable (e.g. your loss is shooting off to NaN suddenly), then you could try decreasing your <code class="markup--code markup--li-code">bptt</code> because you have less layers to gradient explode through.</li><li name="ed23" id="ed23" class="graf graf--li graf-after--li">If it is too slow [<a href="https://youtu.be/H3g26EVADgY?t=22m44s" data-href="https://youtu.be/H3g26EVADgY?t=22m44s" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">22:44</a>], try decreasing your <code class="markup--code markup--li-code">bptt</code> because it will do one of those steps at a time. <code class="markup--code markup--li-code">for</code> loop cannot be parallelized (for the current version). There is a recent thing called QRNN (Quasi-Recurrent Neural Network) which does parallelize it and we hope to cover in part 2.</li><li name="e917" id="e917" class="graf graf--li graf-after--li">So pick the highest number that satisfies all these.</li></ul><h4 name="a9ad" id="a9ad" class="graf graf--h4 graf-after--li">Stateful RNN &amp; TorchText [<a href="https://youtu.be/H3g26EVADgY?t=23m23s" data-href="https://youtu.be/H3g26EVADgY?t=23m23s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">23:23</a>]</h4><p name="89bd" id="89bd" class="graf graf--p graf-after--h4">When using an existing API which expects data to be certain format, you can either change your data to fit that format or you can write your own dataset sub-class to handle the format that your data is already in. Either is fine, but in this case, we will put our data in the format TorchText already support. Fast.ai wrapper around TorchText already has something where you can have a training path and validation path, and one or more text files in each path containing bunch of text that are concatenated together for your language model.</p><pre name="0c59" id="0c59" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">torchtext</strong> <strong class="markup--strong markup--pre-strong">import</strong> vocab, data  </pre><pre name="1025" id="1025" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">fastai.nlp</strong> <strong class="markup--strong markup--pre-strong">import</strong> * <br><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">fastai.lm_rnn</strong> <strong class="markup--strong markup--pre-strong">import</strong> *  </pre><pre name="8a75" id="8a75" class="graf graf--pre graf-after--pre">PATH='data/nietzsche/'  </pre><pre name="ffd3" id="ffd3" class="graf graf--pre graf-after--pre">TRN_PATH = 'trn/' <br>VAL_PATH = 'val/' <br>TRN = f'<strong class="markup--strong markup--pre-strong">{PATH}{TRN_PATH}</strong>' <br>VAL = f'<strong class="markup--strong markup--pre-strong">{PATH}{VAL_PATH}</strong>'</pre><pre name="a49b" id="a49b" class="graf graf--pre graf-after--pre">%ls {PATH}<br><em class="markup--em markup--pre-em">models/  nietzsche.txt  trn/  val/</em></pre><pre name="f7d1" id="f7d1" class="graf graf--pre graf-after--pre">%ls {PATH}trn<br><em class="markup--em markup--pre-em">trn.txt</em></pre><ul class="postList"><li name="d4bc" id="d4bc" class="graf graf--li graf-after--pre">Made a copy of Nietzsche file, pasted into training and validation directory. Then deleted the last 20% of the rows from training set, and deleted everything but the last 20% from the validation set [<a href="https://youtu.be/H3g26EVADgY?t=25m15s" data-href="https://youtu.be/H3g26EVADgY?t=25m15s" class="markup--anchor markup--li-anchor" rel="noopener nofollow" target="_blank">25:15</a>].</li><li name="7a31" id="7a31" class="graf graf--li graf-after--li">The other benefit of doing it this way is that it seems like it is more realistic to have a validation set that was not a random shuffled set of rows of text, but it was totally separate part of the corpus.</li><li name="f4a0" id="f4a0" class="graf graf--li graf-after--li">When you are doing a language model, you do not really need separate files. You can have multiple files but they just get concatenated together anyway.</li></ul><pre name="678c" id="678c" class="graf graf--pre graf-after--li">TEXT = data.Field(lower=<strong class="markup--strong markup--pre-strong">True</strong>, tokenize=list)<br>bs=64; bptt=8; n_fac=42; n_hidden=256<br><br>FILES = dict(train=TRN_PATH, validation=VAL_PATH, test=VAL_PATH)<br>md = LanguageModelData.from_text_files(PATH, TEXT, **FILES, bs=bs, bptt=bptt, min_freq=3)<br><br>len(md.trn_dl), md.nt, len(md.trn_ds), len(md.trn_ds[0].text)<br><em class="markup--em markup--pre-em">(963, 56, 1, 493747)</em></pre><ul class="postList"><li name="83dc" id="83dc" class="graf graf--li graf-after--pre">In TorchText, we make this thing called <code class="markup--code markup--li-code">Field</code> and initially <code class="markup--code markup--li-code">Field</code> is just a description of how to go about pre-processing the text.</li><li name="2db6" id="2db6" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">lower</code> — we told it to lowercase the text</li><li name="609b" id="609b" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">tokenize</code> — Last time, we used a function that splits on whitespace that gave us a word model. This time, we want a character model, so use <code class="markup--code markup--li-code">list</code> function to tokenize strings. Remember, in Python, <code class="markup--code markup--li-code">list('abc')</code> will return <code class="markup--code markup--li-code">['a', 'b', 'c']</code>&nbsp;.</li><li name="17bc" id="17bc" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">bs</code>&nbsp;: batch size, <code class="markup--code markup--li-code">bptt</code>&nbsp;: we renamed <code class="markup--code markup--li-code">cs</code>&nbsp;, <code class="markup--code markup--li-code">n_fac</code>&nbsp;: size of embedding, <code class="markup--code markup--li-code">n_hidden</code>&nbsp;: size of our hidden state</li><li name="8935" id="8935" class="graf graf--li graf-after--li">We do not have a separate test set, so we’ll just use validation set for testing</li><li name="8014" id="8014" class="graf graf--li graf-after--li">TorchText randomize the length of <code class="markup--code markup--li-code">bptt</code> a little bit each time. It does not always give us exactly 8 characters; 5% of the time, it will cut it in half and add on a small standard deviation to make it slightly bigger or smaller than 8. We cannot shuffle the data since it needs to be contiguous, so this is a way to introduce some randomness.</li><li name="7ce8" id="7ce8" class="graf graf--li graf-after--li">Question [<a href="https://youtu.be/H3g26EVADgY?t=31m46s" data-href="https://youtu.be/H3g26EVADgY?t=31m46s" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">31:46</a>]: Does the size remain constant per mini-batch? Yes, we need to do matrix multiplication with <code class="markup--code markup--li-code">h</code> weight matrix, so mini-batch size must remain constant. But sequence length can change no problem.</li><li name="79b4" id="79b4" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">len(md.trn_dl)</code>&nbsp;: length of data loader (i.e. how many mini-batches), <code class="markup--code markup--li-code">md.nt</code>&nbsp;: number of tokens (i.e. how many unique things are in the vocabulary)</li><li name="c9d4" id="c9d4" class="graf graf--li graf-after--li">Once you run <code class="markup--code markup--li-code">LanguageModelData.from_text_files</code>&nbsp;, <code class="markup--code markup--li-code">TEXT</code> will contain an extra attribute called <code class="markup--code markup--li-code">vocab</code>. <code class="markup--code markup--li-code">TEXT.vocab.itos</code> list of unique items in the vocabulary, and <code class="markup--code markup--li-code">TEXT.vocab.stoi</code> is a reverse mapping from each item to number.</li></ul><pre name="e596" id="e596" class="graf graf--pre graf-after--li"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">CharSeqStatefulRnn</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, vocab_size, n_fac, bs):<br>        self.vocab_size = vocab_size<br>        super().__init__()<br>        self.e = nn.Embedding(vocab_size, n_fac)<br>        self.rnn = nn.RNN(n_fac, n_hidden)<br>        self.l_out = nn.Linear(n_hidden, vocab_size)<br>        self.init_hidden(bs)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, cs):<br>        bs = cs[0].size(0)<br>        <strong class="markup--strong markup--pre-strong">if self.h.size(1) != bs: self.init_hidden(bs)</strong><br>        outp,h = self.rnn(self.e(cs), self.h)<br>        self.h = repackage_var(h)<br>        <strong class="markup--strong markup--pre-strong">return</strong> <strong class="markup--strong markup--pre-strong">F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)</strong><br>    <br>    <strong class="markup--strong markup--pre-strong">def</strong> init_hidden(self, bs): self.h = V(torch.zeros(1, bs, n_hidden))</pre><ul class="postList"><li name="f663" id="f663" class="graf graf--li graf-after--pre"><strong class="markup--strong markup--li-strong">Wrinkle #3</strong> [<a href="https://youtu.be/H3g26EVADgY?t=33m51s" data-href="https://youtu.be/H3g26EVADgY?t=33m51s" class="markup--anchor markup--li-anchor" rel="noopener nofollow" target="_blank">33:51</a>]: Jeremy lied to us when he said that mini-batch size remains constant. It is very likely that the last mini-batch is shorter than the rest unless the dataset is exactly divisible by <code class="markup--code markup--li-code">bptt</code> times <code class="markup--code markup--li-code">bs</code>&nbsp;. That is why we check whether <code class="markup--code markup--li-code">self.h</code> ‘s second dimension is the same as <code class="markup--code markup--li-code">bs</code> of the input. If it is not the same, set it back to zero with the input’s <code class="markup--code markup--li-code">bs</code>&nbsp;. This happens at the end of the epoch and the beginning of the epoch (setting back to the full batch size).</li><li name="b2fc" id="b2fc" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Wrinkle #4 </strong>[<a href="https://youtu.be/H3g26EVADgY?t=35m44s" data-href="https://youtu.be/H3g26EVADgY?t=35m44s" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">35:44</a>]: The last wrinkle is something that slightly sucks about PyTorch and maybe somebody can be nice enough to try and fix it with a PR. Loss functions are not happy receiving a rank 3 tensor (i.e. three dimensional array). There is no particular reason they ought to not be happy receiving a rank 3 tensor (sequence length by batch size by results — so you can just calculate loss for each of the two initial axis). Works for rank 2 or 4, but not 3.</li><li name="235d" id="235d" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">.view</code> will reshape rank 3 tensor into rank 2 of <code class="markup--code markup--li-code">-1</code> (however big as necessary) by <code class="markup--code markup--li-code">vocab_size</code>. TorchText automatically changes the <strong class="markup--strong markup--li-strong">target</strong> to be flattened out, so we do not need to do that for actual values (when we looked at a mini-batch in lesson 4, we noticed that it was flattened. Jeremy said we will learn about why later, so later is now.)</li><li name="c881" id="c881" class="graf graf--li graf-after--li">PyTorch (as of 0.3), <code class="markup--code markup--li-code">log_softmax</code> requires us to specify which axis we want to do the softmax over (i.e. which axis we want to sum to one). In this case we want to do it over the last axis <code class="markup--code markup--li-code">dim = -1</code>.</li></ul><pre name="0378" id="0378" class="graf graf--pre graf-after--li">m = CharSeqStatefulRnn(md.nt, n_fac, 512).cuda() <br>opt = optim.Adam(m.parameters(), 1e-3)</pre><pre name="1896" id="1896" class="graf graf--pre graf-after--pre">fit(m, md, 4, opt, F.nll_loss)</pre><h4 name="0554" id="0554" class="graf graf--h4 graf-after--pre">Let’s gain more insight by unpacking RNN&nbsp;[<a href="https://youtu.be/H3g26EVADgY?t=42m48s" data-href="https://youtu.be/H3g26EVADgY?t=42m48s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">42:48</a>]</h4><p name="2c84" id="2c84" class="graf graf--p graf-after--h4">We remove the use of <code class="markup--code markup--p-code">nn.RNN</code> and replace it with <code class="markup--code markup--p-code">nn.RNNCell</code>&nbsp;. PyTorch source code looks like the following. You should be able to read and understand (Note: they do not concatenate the input and the hidden state, but they sum them together — which was our first approach):</p><pre name="7d7c" id="7d7c" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> RNNCell(input, hidden, w_ih, w_hh, b_ih, b_hh):<br>    <strong class="markup--strong markup--pre-strong">return</strong> F.tanh(F.linear(input, w_ih, b_ih) + F.linear(hidden, w_hh, b_hh))</pre><p name="38a2" id="38a2" class="graf graf--p graf-after--pre">Question about <code class="markup--code markup--p-code">tanh</code> [<a href="https://youtu.be/H3g26EVADgY?t=44m6s" data-href="https://youtu.be/H3g26EVADgY?t=44m6s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">44:06</a>]: As we have seen last week, <code class="markup--code markup--p-code">tanh</code> is forcing the value to be between -1 and 1. Since we are multiplying by this weight matrix again and again, we would worry that <code class="markup--code markup--p-code">relu</code> (since it is unbounded) might have more gradient explosion problem. Having said that, you can specify <code class="markup--code markup--p-code">RNNCell</code> to use different <code class="markup--code markup--p-code">nonlineality</code> whose default is <code class="markup--code markup--p-code">tanh</code> and ask it to use <code class="markup--code markup--p-code">relu</code> if you wanted to.</p><pre name="87cc" id="87cc" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">CharSeqStatefulRnn2</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, vocab_size, n_fac, bs):<br>        super().__init__()<br>        self.vocab_size = vocab_size<br>        self.e = nn.Embedding(vocab_size, n_fac)<br>        self.rnn = <strong class="markup--strong markup--pre-strong">nn.RNNCell</strong>(n_fac, n_hidden)<br>        self.l_out = nn.Linear(n_hidden, vocab_size)<br>        self.init_hidden(bs)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, cs):<br>        bs = cs[0].size(0)<br>        <strong class="markup--strong markup--pre-strong">if</strong> self.h.size(1) != bs: self.init_hidden(bs)<br>        outp = []<br>        o = self.h<br>        <strong class="markup--strong markup--pre-strong">for</strong> c <strong class="markup--strong markup--pre-strong">in</strong> cs: <br>            o = self.rnn(self.e(c), o)<br>            outp.append(o)<br>        outp = self.l_out(torch.stack(outp))<br>        self.h = repackage_var(o)<br>        <strong class="markup--strong markup--pre-strong">return</strong> F.log_softmax(outp, dim=-1).view(-1, self.vocab_size)<br>    <br>    <strong class="markup--strong markup--pre-strong">def</strong> init_hidden(self, bs): self.h = V(torch.zeros(1, bs, n_hidden))</pre><ul class="postList"><li name="1bfc" id="1bfc" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">for</code> loop is back and append the result of linear function to a list — which in end gets stacked up together.</li><li name="9cc0" id="9cc0" class="graf graf--li graf-after--li">fast.ai library actually does exactly this in order to use regularization approaches that are not supported by PyTorch.</li></ul><h4 name="8f68" id="8f68" class="graf graf--h4 graf-after--li">Gated Recurrent Unit (GRU)&nbsp;[<a href="https://youtu.be/H3g26EVADgY?t=46m44s" data-href="https://youtu.be/H3g26EVADgY?t=46m44s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">46:44</a>]</h4><p name="7cd3" id="7cd3" class="graf graf--p graf-after--h4">In practice, nobody really uses <code class="markup--code markup--p-code">RNNCell</code> since even with <code class="markup--code markup--p-code">tanh</code>&nbsp;, gradient explosions are still a problem and we need use low learning rate and small <code class="markup--code markup--p-code">bptt</code> to get them to train. So what we do is to replace <code class="markup--code markup--p-code">RNNCell</code> with something like <code class="markup--code markup--p-code">GRUCell</code>&nbsp;.</p><figure name="57e4" id="57e4" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1__29x3zNI1C0vM3fxiIpiVA.png"><figcaption class="imageCaption"><a href="http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/" data-href="http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/</a></figcaption></figure><ul class="postList"><li name="0be5" id="0be5" class="graf graf--li graf-after--figure">Normally, the input gets multiplied by a weight matrix to create new activations <code class="markup--code markup--li-code">h</code> and get added to the existing activations straight away. That is not wha happens here.</li><li name="78e7" id="78e7" class="graf graf--li graf-after--li">Input goes into <code class="markup--code markup--li-code">h˜</code> and it doesn’t just get added to the previous activations, but the previous activation gets multiplied by <code class="markup--code markup--li-code">r</code> (reset gate) which has a value of 0 or 1.</li><li name="97d7" id="97d7" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">r</code> is calculated as below — matrix multiplication of some weight matrix and the concatenation of our previous hidden state and new input. In other words, this is a little one hidden layer neural net. It gets put through the sigmoid function as well. This mini neural net learns to determine how much of the hidden states to remember (maybe forget it all when it sees a full-stop character — beginning of a new sentence).</li><li name="8d12" id="8d12" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">z</code> gate (update gate) determines what degree to use <code class="markup--code markup--li-code">h˜</code> (the new input version of hidden states) and what degree to leave the hidden state the same as before.</li></ul><figure name="3ba3" id="3ba3" class="graf graf--figure graf-after--li"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_qzfburCutJ3p-FYu1T6Q3Q.png"><figcaption class="imageCaption"><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" data-href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></figcaption></figure><figure name="08f3" id="08f3" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*M7ujxxzjQfL5e33BjJQViw.png" data-width="420" data-height="77" src="../img/1_M7ujxxzjQfL5e33BjJQViw.png"></figure><ul class="postList"><li name="f8f5" id="f8f5" class="graf graf--li graf-after--figure">Linear interpolation</li></ul><pre name="5f0e" id="5f0e" class="graf graf--pre graf-after--li"><strong class="markup--strong markup--pre-strong">def</strong> GRUCell(input, hidden, w_ih, w_hh, b_ih, b_hh):<br>    gi = F.linear(input, w_ih, b_ih)<br>    gh = F.linear(hidden, w_hh, b_hh)<br>    i_r, i_i, i_n = gi.chunk(3, 1)<br>    h_r, h_i, h_n = gh.chunk(3, 1)<br><br>    resetgate = F.sigmoid(i_r + h_r)<br>    inputgate = F.sigmoid(i_i + h_i)<br>    newgate = F.tanh(i_n + resetgate * h_n)<br>    <strong class="markup--strong markup--pre-strong">return</strong> newgate + inputgate * (hidden - newgate)</pre><p name="2734" id="2734" class="graf graf--p graf-after--pre">Above is what <code class="markup--code markup--p-code">GRUCell</code> code looks like, and our new model that utilize this is below:</p><pre name="5830" id="5830" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">CharSeqStatefulGRU</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, vocab_size, n_fac, bs):<br>        super().__init__()<br>        self.vocab_size = vocab_size<br>        self.e = nn.Embedding(vocab_size, n_fac)<br>        self.rnn = nn.GRU(n_fac, n_hidden)<br>        self.l_out = nn.Linear(n_hidden, vocab_size)<br>        self.init_hidden(bs)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, cs):<br>        bs = cs[0].size(0)<br>        <strong class="markup--strong markup--pre-strong">if</strong> self.h.size(1) != bs: self.init_hidden(bs)<br>        outp,h = self.rnn(self.e(cs), self.h)<br>        self.h = repackage_var(h)<br>        <strong class="markup--strong markup--pre-strong">return</strong> F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)<br>    <br>    <strong class="markup--strong markup--pre-strong">def</strong> init_hidden(self, bs): self.h = V(torch.zeros(1, bs, n_hidden))</pre><p name="10d3" id="10d3" class="graf graf--p graf-after--pre">As a result, we can lower the loss down to 1.36 (<code class="markup--code markup--p-code">RNNCell</code> one was 1.54). In practice, GRU and LSTM are what people uses.</p><h4 name="2c05" id="2c05" class="graf graf--h4 graf-after--p">Putting it all together: Long Short-Term Memory&nbsp;[<a href="https://youtu.be/H3g26EVADgY?t=54m9s" data-href="https://youtu.be/H3g26EVADgY?t=54m9s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">54:09</a>]</h4><p name="be04" id="be04" class="graf graf--p graf-after--h4">LSTM has one more piece of state in it called “cell state” (not just hidden state), so if you do use a LSTM, you have to return a tuple of matrices in <code class="markup--code markup--p-code">init_hidden</code> (exactly the same size as hidden state):</p><pre name="ba6b" id="ba6b" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">fastai</strong> <strong class="markup--strong markup--pre-strong">import</strong> sgdr<br><br>n_hidden=512</pre><pre name="d419" id="d419" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">CharSeqStatefulLSTM</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, vocab_size, n_fac, bs, nl):<br>        super().__init__()<br>        self.vocab_size,self.nl = vocab_size,nl<br>        self.e = nn.Embedding(vocab_size, n_fac)<br>        self.rnn = nn.LSTM(n_fac, n_hidden, nl, <strong class="markup--strong markup--pre-strong">dropout</strong>=0.5)<br>        self.l_out = nn.Linear(n_hidden, vocab_size)<br>        self.init_hidden(bs)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, cs):<br>        bs = cs[0].size(0)<br>        <strong class="markup--strong markup--pre-strong">if</strong> self.h[0].size(1) != bs: self.init_hidden(bs)<br>        outp,h = self.rnn(self.e(cs), self.h)<br>        self.h = repackage_var(h)<br>        <strong class="markup--strong markup--pre-strong">return</strong> F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)<br>    <br>    <strong class="markup--strong markup--pre-strong">def</strong> init_hidden(self, bs):<br><strong class="markup--strong markup--pre-strong">        self.h = (V(torch.zeros(self.nl, bs, n_hidden)),<br>                  V(torch.zeros(self.nl, bs, n_hidden)))</strong></pre><p name="663f" id="663f" class="graf graf--p graf-after--pre">The code is identical to GRU one. The one thing that was added was <code class="markup--code markup--p-code">dropout</code> which does dropout after each time step and doubled the hidden layer — in a hope that it will be able to learn more and be resilient as it does so.</p><h4 name="c653" id="c653" class="graf graf--h4 graf-after--p">Callbacks (specifically SGDR) without Learner class&nbsp;[<a href="https://youtu.be/H3g26EVADgY?t=55m23s" data-href="https://youtu.be/H3g26EVADgY?t=55m23s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">55:23</a>]</h4><pre name="40cf" id="40cf" class="graf graf--pre graf-after--h4">m = CharSeqStatefulLSTM(md.nt, n_fac, 512, 2).cuda()<br>lo = LayerOptimizer(optim.Adam, m, 1e-2, 1e-5)</pre><ul class="postList"><li name="198a" id="198a" class="graf graf--li graf-after--pre">After creating a standard PyTorch model, we usually do something like <code class="markup--code markup--li-code">opt = optim.Adam(m.parameters(), 1e-3)</code>. Instead, we will use fast.ai <code class="markup--code markup--li-code">LayerOptimizer</code> which takes an optimizer <code class="markup--code markup--li-code">optim.Adam</code>&nbsp;, our model <code class="markup--code markup--li-code">m</code>&nbsp;, learning rate <code class="markup--code markup--li-code">1e-2</code>&nbsp;, and optionally weight decay <code class="markup--code markup--li-code">1e-5</code>&nbsp;.</li><li name="0ddb" id="0ddb" class="graf graf--li graf-after--li">A key reason <code class="markup--code markup--li-code">LayerOptimizer</code> exists is to do differential learning rates and differential weight decay. The reason we need to use it is that all of the mechanics inside fast.ai assumes that you have one of these. If you want to use callbacks or SGDR in code you are not using the Learner class, you need to use this.</li><li name="07f2" id="07f2" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">lo.opt</code> returns the optimizer.</li></ul><pre name="4b38" id="4b38" class="graf graf--pre graf-after--li">on_end = <strong class="markup--strong markup--pre-strong">lambda</strong> sched, cycle: save_model(m, f'<strong class="markup--strong markup--pre-strong">{PATH}</strong>models/cyc_<strong class="markup--strong markup--pre-strong">{cycle}</strong>')</pre><pre name="8627" id="8627" class="graf graf--pre graf-after--pre">cb = [CosAnneal(lo, len(md.trn_dl), cycle_mult=2, on_cycle_end=on_end)]</pre><pre name="632c" id="632c" class="graf graf--pre graf-after--pre">fit(m, md, 2**4-1, lo.opt, F.nll_loss, callbacks=cb)</pre><ul class="postList"><li name="b163" id="b163" class="graf graf--li graf-after--pre">When we call <code class="markup--code markup--li-code">fit</code>, we can now pass the <code class="markup--code markup--li-code">LayerOptimizer</code> and also <code class="markup--code markup--li-code">callbacks</code>.</li><li name="890b" id="890b" class="graf graf--li graf-after--li">Here, we use cosine annealing callback — which requires a <code class="markup--code markup--li-code">LayerOptimizer</code> object. It does cosine annealing by changing learning rate in side the <code class="markup--code markup--li-code">lo</code> object.</li><li name="871a" id="871a" class="graf graf--li graf-after--li">Concept: Create a cosine annealing callback which is going to update the learning rates in the layer optimizer <code class="markup--code markup--li-code">lo</code>&nbsp;. The length of an epoch is equal to <code class="markup--code markup--li-code">len(md.trn_dl)</code> — how many mini-batches are there in an epoch is the length of the data loader. Since it is doing cosine annealing, it needs to know how often to reset. You can pass in <code class="markup--code markup--li-code">cycle_mult</code> in usual way. We can even save our model automatically just like we did with <code class="markup--code markup--li-code">cycle_save_name</code> in <code class="markup--code markup--li-code">Learner.fit</code>.</li><li name="75f8" id="75f8" class="graf graf--li graf-after--li">We can do callback at a start of a training, epoch or a batch, or at the end of a training, an epoch, or a batch.</li><li name="c3af" id="c3af" class="graf graf--li graf-after--li">It has been used for <code class="markup--code markup--li-code">CosAnneal</code> (SGDR), and decoupled weight decay (AdamW), loss-over-time graph, etc.</li></ul><h4 name="6061" id="6061" class="graf graf--h4 graf-after--li">Testing [<a href="https://youtu.be/H3g26EVADgY?t=59m55s" data-href="https://youtu.be/H3g26EVADgY?t=59m55s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">59:55</a>]</h4><pre name="1657" id="1657" class="graf graf--pre graf-after--h4"><strong class="markup--strong markup--pre-strong">def</strong> get_next(inp):<br>    idxs = TEXT.numericalize(inp)<br>    p = m(VV(idxs.transpose(0,1)))<br>    r = <strong class="markup--strong markup--pre-strong">torch.multinomial(p[-1].exp(), 1)</strong><br>    <strong class="markup--strong markup--pre-strong">return</strong> TEXT.vocab.itos[to_np(r)[0]]</pre><pre name="6c76" id="6c76" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> get_next_n(inp, n):<br>    res = inp<br>    <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(n):<br>        c = get_next(inp)<br>        res += c<br>        inp = inp[1:]+c<br>    <strong class="markup--strong markup--pre-strong">return</strong> res</pre><pre name="6fad" id="6fad" class="graf graf--pre graf-after--pre">print(get_next_n('for thos', 400))</pre><pre name="b8d1" id="b8d1" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">for those the skemps), or imaginates, though they deceives. it should so each ourselvess and new present, step absolutely for the science." the contradity and measuring,  the whole!  </em></pre><pre name="1600" id="1600" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">293. perhaps, that every life a values of blood of intercourse when it senses there is unscrupulus, his very rights, and still impulse, love? just after that thereby how made with the way anything, and set for harmless philos</em></pre><ul class="postList"><li name="3ade" id="3ade" class="graf graf--li graf-after--pre">In lesson 6, when we were testing <code class="markup--code markup--li-code">CharRnn</code> model, we noticed that it repeated itself over and over. <code class="markup--code markup--li-code">torch.multinomial</code> used in this new version deals with this problem. <code class="markup--code markup--li-code">p[-1]</code> to get the final output (the triangle), <code class="markup--code markup--li-code">exp</code> to convert log probability to probability. We then use <code class="markup--code markup--li-code">torch.multinomial</code> function which will give us a sample using the given probabilities. If probability is [0, 1, 0, 0] and ask it to give us a sample, it will always return the second item. If it was [0.5, 0, 0.5], it will give the first item 50% of the time, and second item&nbsp;. 50% of the time (<a href="http://onlinestatbook.com/2/probability/multinomial.html" data-href="http://onlinestatbook.com/2/probability/multinomial.html" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">review of multinomial distribution</a>)</li><li name="fbfc" id="fbfc" class="graf graf--li graf-after--li">To play around with training character based language models like this, try running <code class="markup--code markup--li-code">get_next_n</code> at different levels of loss to get a sense of what it looks like. The example above is at 1.25, but at 1.3, it looks like a total junk.</li><li name="bfb9" id="bfb9" class="graf graf--li graf-after--li">When you are playing around with NLP, particularly generative model like this, and the results are kind of okay but not great, do not be disheartened because that means you are actually very VERY nearly there!</li></ul><h3 name="48dd" id="48dd" class="graf graf--h3 graf-after--li"><a href="https://github.com/fastai/fastai/blob/master/courses/dl1/lesson7-cifar10.ipynb" data-href="https://github.com/fastai/fastai/blob/master/courses/dl1/lesson7-cifar10.ipynb" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">Back to computer vision: CIFAR 10</a> [<a href="https://youtu.be/H3g26EVADgY?t=1h1m58s" data-href="https://youtu.be/H3g26EVADgY?t=1h1m58s" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">1:01:58</a>]</h3><p name="da72" id="da72" class="graf graf--p graf-after--h3">CIFAR 10 is an old and well known dataset in academia — well before ImageNet, there was CIFAR 10. It is small both in terms of number of images and size of images which makes it interesting and challenging. You will likely be working with thousands of images rather than one and a half million images. Also a lot of the things we are looking at like in medical imaging, we are looking at a specific area where there is a lung nodule, you are probably looking at 32 by 32 pixels at most.</p><p name="04f9" id="04f9" class="graf graf--p graf-after--p">It also runs quickly, so it is much better to test our your algorithms. As Ali Rahini mentioned in NIPS 2017, Jeremy has the concern that many people are not doing carefully tuned and throught-about experiments in deep learning, but instead, they throw lots of GPUs and TPUs or lots of data and consider that a day. It is important to test many versions of your algorithm on dataset like CIFAR 10 rather than ImageNet that takes weeks. MNIST is also good for studies and experiments even though people tend to complain about it.</p><p name="d725" id="d725" class="graf graf--p graf-after--p">CIFAR 10 data in image format is available <a href="http://pjreddie.com/media/files/cifar.tgz" data-href="http://pjreddie.com/media/files/cifar.tgz" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">here</a></p><pre name="75a0" id="75a0" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">fastai.conv_learner</strong> <strong class="markup--strong markup--pre-strong">import</strong> *<br>PATH = "data/cifar10/"<br>os.makedirs(PATH,exist_ok=<strong class="markup--strong markup--pre-strong">True</strong>)</pre><pre name="9eb0" id="9eb0" class="graf graf--pre graf-after--pre">classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')<br>stats = (np.array([ 0.4914 ,  0.48216,  0.44653]), np.array([ 0.24703,  0.24349,  0.26159]))</pre><pre name="fb08" id="fb08" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> get_data(sz,bs):<br>     tfms = <strong class="markup--strong markup--pre-strong">tfms_from_stats</strong>(stats, sz, aug_tfms=[RandomFlipXY()], pad=sz//8)<br>     <strong class="markup--strong markup--pre-strong">return</strong> ImageClassifierData.from_paths(PATH, val_name='test', tfms=tfms, bs=bs)</pre><pre name="c93a" id="c93a" class="graf graf--pre graf-after--pre">bs=256</pre><ul class="postList"><li name="a707" id="a707" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">classes</code> — image labels</li><li name="6af1" id="6af1" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">stats</code> —When we use pre-trained models, you can call <code class="markup--code markup--li-code">tfms_from_model</code> which creates the necessary transforms to convert our data set into a normalized dataset based on the means and standard deviations of each channel in the original model that was trained in. Since we are training a model from scratch, we ned to tell it the mean and standard deviation of our data to normalize it. Make sure you can calculate the mean and the standard deviation for each channel.</li><li name="a90b" id="a90b" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">tfms</code> — For CIFAR 10 data augmentation, people typically do horizontal flip and black padding around the edge and randomly select 32 by 32 area within the padded image.</li></ul><pre name="a4dc" id="a4dc" class="graf graf--pre graf-after--li">data = get_data(32,bs)<br><br>lr=1e-2</pre><p name="8e48" id="8e48" class="graf graf--p graf-after--pre">From <a href="https://github.com/KeremTurgutlu/deeplearning/blob/master/Exploring%20Optimizers.ipynb" data-href="https://github.com/KeremTurgutlu/deeplearning/blob/master/Exploring%20Optimizers.ipynb" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">this notebook</a> by our student Kerem Turgutlu:</p><pre name="9fca" id="9fca" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">SimpleNet</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, layers):<br>        super().__init__()<br>        self.layers = <strong class="markup--strong markup--pre-strong">nn.ModuleList</strong>([<br>            nn.Linear(layers[i], layers[i + 1]) <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(len(layers) - 1)])<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x):<br>        x = x.view(x.size(0), -1)<br>        <strong class="markup--strong markup--pre-strong">for</strong> l <strong class="markup--strong markup--pre-strong">in</strong> self.layers:<br>            l_x = l(x)<br>            x = F.relu(l_x)<br>        <strong class="markup--strong markup--pre-strong">return</strong> F.log_softmax(l_x, dim=-1)</pre><ul class="postList"><li name="42f7" id="42f7" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">nn.ModuleList</code> — whenever you create a list of layers in PyTorch, you have to wrap it in <code class="markup--code markup--li-code">ModuleList</code> to register these as attributes.</li></ul><pre name="afaa" id="afaa" class="graf graf--pre graf-after--li">learn = ConvLearner.from_model_data(SimpleNet([32*32*3, 40,10]), data)</pre><ul class="postList"><li name="013f" id="013f" class="graf graf--li graf-after--pre">Now we step up one level of API higher — rather than calling <code class="markup--code markup--li-code">fit</code> function, we create a <code class="markup--code markup--li-code">learn</code> object <em class="markup--em markup--li-em">from a custom model</em>. <code class="markup--code markup--li-code">ConfLearner.from_model_data</code> takes standard PyTorch model and model data object.</li></ul><pre name="0fa9" id="0fa9" class="graf graf--pre graf-after--li">learn, [o.numel() <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> learn.model.parameters()]</pre><pre name="2012" id="2012" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(SimpleNet(<br>   (layers): ModuleList(<br>     (0): Linear(in_features=3072, out_features=40)<br>     (1): Linear(in_features=40, out_features=10)<br>   )<br> ), [122880, 40, 400, 10])</em></pre><pre name="85fd" id="85fd" class="graf graf--pre graf-after--pre">learn.summary()</pre><pre name="88f0" id="88f0" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">OrderedDict([('Linear-1',<br>              OrderedDict([('input_shape', [-1, 3072]),<br>                           ('output_shape', [-1, 40]),<br>                           ('trainable', True),<br>                           ('nb_params', 122920)])),<br>             ('Linear-2',<br>              OrderedDict([('input_shape', [-1, 40]),<br>                           ('output_shape', [-1, 10]),<br>                           ('trainable', True),<br>                           ('nb_params', 410)]))])</em></pre><pre name="3f7e" id="3f7e" class="graf graf--pre graf-after--pre">learn.lr_find()</pre><pre name="b7af" id="b7af" class="graf graf--pre graf-after--pre">learn.sched.plot()</pre><figure name="f8c3" id="f8c3" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1__5sTAdoWHTBQUzbaVrc4HA.png"></figure><pre name="15f5" id="15f5" class="graf graf--pre graf-after--figure">%time learn.fit(lr, 2)</pre><pre name="e001" id="e001" class="graf graf--pre graf-after--pre">A Jupyter Widget</pre><pre name="81cd" id="81cd" class="graf graf--pre graf-after--pre">[ 0.       1.7658   1.64148  0.42129]                       <br>[ 1.       1.68074  1.57897  0.44131]                       <br><br>CPU times: user 1min 11s, sys: 32.3 s, total: 1min 44s<br>Wall time: 55.1 s</pre><pre name="8ec0" id="8ec0" class="graf graf--pre graf-after--pre">%time learn.fit(lr, 2, cycle_len=1)</pre><pre name="a5d9" id="a5d9" class="graf graf--pre graf-after--pre">A Jupyter Widget</pre><pre name="4db5" id="4db5" class="graf graf--pre graf-after--pre">[ 0.       1.60857  1.51711  0.46631]                       <br>[ 1.       1.59361  1.50341  0.46924]                       <br><br>CPU times: user 1min 12s, sys: 31.8 s, total: 1min 44s<br>Wall time: 55.3 s</pre><p name="d12e" id="d12e" class="graf graf--p graf-after--pre">With a simple one hidden layer model with 122,880 parameters, we achieved 46.9% accuracy. Let’s improve this and gradually build up to a basic ResNet architecture.</p><h4 name="e35d" id="e35d" class="graf graf--h4 graf-after--p">CNN [<a href="https://youtu.be/H3g26EVADgY?t=1h12m30s" data-href="https://youtu.be/H3g26EVADgY?t=1h12m30s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">01:12:30</a>]</h4><ul class="postList"><li name="f56f" id="f56f" class="graf graf--li graf-after--h4">Let’s replace a fully connected model with a convolutional model. Fully connected layer is simply doing a dot product. That is why the weight matrix is big (3072 input * 40 = 122880). We are not using the parameters very efficiently because every single pixel in the input has a different weight. What we want to do is a group of 3 by 3 pixels that have particular patterns to them (i.e. convolution).</li><li name="c4c9" id="c4c9" class="graf graf--li graf-after--li">We will use a filter with three by three kernel. When there are multiple filters, the output will have additional dimension.</li></ul><pre name="956d" id="956d" class="graf graf--pre graf-after--li"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">ConvNet</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, layers, c):<br>        super().__init__()<br>        self.layers = nn.ModuleList([<br>            <strong class="markup--strong markup--pre-strong">nn.Conv2d(layers[i], layers[i + 1], kernel_size=3, stride=2)</strong><br>            <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(len(layers) - 1)])<br>        self.pool = nn.AdaptiveMaxPool2d(1)<br>        self.out = nn.Linear(layers[-1], c)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x):<br>        <strong class="markup--strong markup--pre-strong">for</strong> l <strong class="markup--strong markup--pre-strong">in</strong> self.layers: x = F.relu(l(x))<br>        x = self.pool(x)<br>        x = x.view(x.size(0), -1)<br>        <strong class="markup--strong markup--pre-strong">return</strong> F.log_softmax(self.out(x), dim=-1)</pre><ul class="postList"><li name="ae05" id="ae05" class="graf graf--li graf-after--pre">Replace <code class="markup--code markup--li-code">nn.Linear</code> with <code class="markup--code markup--li-code">nn.Conv2d</code></li><li name="b44e" id="b44e" class="graf graf--li graf-after--li">First two parameters are exactly the same as <code class="markup--code markup--li-code">nn.Linear</code> — the number of features coming in, and the number of features coming out</li><li name="7e41" id="7e41" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">kernel_size=3</code>&nbsp;, the size of the filter</li><li name="fd62" id="fd62" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">stride=2</code> will use every other 3 by 3 area which will halve the output resolution in each dimension (i.e. it has the same effect as 2 by 2 max-pooling)</li></ul><pre name="cf96" id="cf96" class="graf graf--pre graf-after--li">learn = ConvLearner.from_model_data(ConvNet([3, 20, 40, 80], 10), data)</pre><pre name="efa6" id="efa6" class="graf graf--pre graf-after--pre">learn.summary()</pre><pre name="6614" id="6614" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">OrderedDict([('Conv2d-1',<br>              OrderedDict([('input_shape', [-1, 3, 32, 32]),<br>                           ('output_shape', [-1, 20, 15, 15]),<br>                           ('trainable', True),<br>                           ('nb_params', 560)])),<br>             ('Conv2d-2',<br>              OrderedDict([('input_shape', [-1, 20, 15, 15]),<br>                           ('output_shape', [-1, 40, 7, 7]),<br>                           ('trainable', True),<br>                           ('nb_params', 7240)])),<br>             ('Conv2d-3',<br>              OrderedDict([('input_shape', [-1, 40, 7, 7]),<br>                           ('output_shape', [-1, 80, 3, 3]),<br>                           ('trainable', True),<br>                           ('nb_params', 28880)])),<br>             ('AdaptiveMaxPool2d-4',<br>              OrderedDict([('input_shape', [-1, 80, 3, 3]),<br>                           ('output_shape', [-1, 80, 1, 1]),<br>                           ('nb_params', 0)])),<br>             ('Linear-5',<br>              OrderedDict([('input_shape', [-1, 80]),<br>                           ('output_shape', [-1, 10]),<br>                           ('trainable', True),<br>                           ('nb_params', 810)]))])</em></pre><ul class="postList"><li name="0106" id="0106" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">ConvNet([3, 20, 40, 80], 10)</code> — It start with 3 RGB channels, 20, 40, 80 features, then 10 classes to predict.</li><li name="7f89" id="7f89" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">AdaptiveMaxPool2d</code> — This followed by a linear layer is how you get from 3 by 3 down to a prediction of one of 10 classes and is now a standard for state-of-the-art algorithms. The very last layer, we do a special kind of max-pooling for which you specify the output activation resolution rather than how big of an area to poll. In other words, here we do 3 by 3 max-pool which is equivalent of 1 by 1 <em class="markup--em markup--li-em">adaptive</em> max-pool.</li><li name="dcf2" id="dcf2" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">x = x.view(x.size(0), -1)</code> — <code class="markup--code markup--li-code">x</code> has a shape of # of the features by 1 by 1, so it will remove the last two layers.</li><li name="00b5" id="00b5" class="graf graf--li graf-after--li">This model is called “fully convolutional network” — where every layer is convolutional except for the very last.</li></ul><pre name="5c4c" id="5c4c" class="graf graf--pre graf-after--li">learn.lr_find(<strong class="markup--strong markup--pre-strong">end_lr=100</strong>)<br>learn.sched.plot()</pre><figure name="1e20" id="1e20" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_YuNvyUac9HvAv0XZn08-3g.png"></figure><ul class="postList"><li name="eb07" id="eb07" class="graf graf--li graf-after--figure">The default final learning rate <code class="markup--code markup--li-code">lr_find</code> tries is 10. If the loss is still getting better at that point, you can overwrite by specifying <code class="markup--code markup--li-code">end_lr</code>&nbsp;.</li></ul><pre name="1c6f" id="1c6f" class="graf graf--pre graf-after--li">%time learn.fit(1e-1, 2)</pre><pre name="5546" id="5546" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">A Jupyter Widget</em></pre><pre name="f992" id="f992" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[ 0.       1.72594  1.63399  0.41338]                       <br>[ 1.       1.51599  1.49687  0.45723]                       <br><br>CPU times: user 1min 14s, sys: 32.3 s, total: 1min 46s<br>Wall time: 56.5 s</em></pre><pre name="3b9b" id="3b9b" class="graf graf--pre graf-after--pre">%time learn.fit(1e-1, 4, cycle_len=1)</pre><pre name="d9f2" id="d9f2" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">A Jupyter Widget</em></pre><pre name="af51" id="af51" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[ 0.       1.36734  1.28901  0.53418]                       <br>[ 1.       1.28854  1.21991  0.56143]                       <br>[ 2.       1.22854  1.15514  0.58398]                       <br>[ 3.       1.17904  1.12523  0.59922]                       <br><br>CPU times: user 2min 21s, sys: 1min 3s, total: 3min 24s<br>Wall time: 1min 46s</em></pre><ul class="postList"><li name="bf9c" id="bf9c" class="graf graf--li graf-after--pre">It flattened out around 60% accuracy. Considering it uses about 30,000 parameters (compared to 47% with 122k parameters)</li><li name="49ed" id="49ed" class="graf graf--li graf-after--li">Time per epoch is about the same since their architectures are both simple and most of time is spent doing memory transfer.</li></ul><h4 name="cc39" id="cc39" class="graf graf--h4 graf-after--li">Refactored [<a href="https://youtu.be/H3g26EVADgY?t=1h21m57s" data-href="https://youtu.be/H3g26EVADgY?t=1h21m57s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">01:21:57</a>]</h4><p name="340e" id="340e" class="graf graf--p graf-after--h4">Simplify <code class="markup--code markup--p-code">forward</code> function by creating <code class="markup--code markup--p-code">ConvLayer</code> (our first custom layer!). In PyTorch, layer definition and neural network definitions are identical. Anytime you have a layer, you can use it as a neural net, when you have a neural net, you can use it as a layer.</p><pre name="41d6" id="41d6" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">ConvLayer</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, ni, nf):<br>        super().__init__()<br>        self.conv = nn.Conv2d(ni, nf, kernel_size=3, stride=2, padding=1)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x): <strong class="markup--strong markup--pre-strong">return</strong> F.relu(self.conv(x))</pre><ul class="postList"><li name="393b" id="393b" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">padding=1</code> — When you do convolution the image shrink by 1 pixel on each side. So it does not go from 32 by 32 to 16 by 16 but actually 15 by 15. <code class="markup--code markup--li-code">padding</code> will add a border so we can keep the edge pixel information. It is not as big of a deal for a big image, but when it’s down to 4 by 4, you really don’t want to throw away a whole piece.</li></ul><pre name="d8b6" id="d8b6" class="graf graf--pre graf-after--li"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">ConvNet2</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, layers, c):<br>        super().__init__()<br>        self.layers = nn.ModuleList([ConvLayer(layers[i], layers[i + 1])<br>            <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(len(layers) - 1)])<br>        self.out = nn.Linear(layers[-1], c)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x):<br>        <strong class="markup--strong markup--pre-strong">for</strong> l <strong class="markup--strong markup--pre-strong">in</strong> self.layers: x = l(x)<br>        x = <strong class="markup--strong markup--pre-strong">F.adaptive_max_pool2d(x, 1)</strong><br>        x = x.view(x.size(0), -1)<br>        <strong class="markup--strong markup--pre-strong">return</strong> F.log_softmax(self.out(x), dim=-1)</pre><ul class="postList"><li name="89d9" id="89d9" class="graf graf--li graf-after--pre">Another difference from the last model is that <code class="markup--code markup--li-code">nn.AdaptiveMaxPool2d</code> does not have any state (i.e. no weights). So we can just call it as a function <code class="markup--code markup--li-code">F.adaptive_max_pool2d</code>&nbsp;.</li></ul><h4 name="acdf" id="acdf" class="graf graf--h4 graf-after--li">BatchNorm [<a href="https://youtu.be/H3g26EVADgY?t=1h25m10s" data-href="https://youtu.be/H3g26EVADgY?t=1h25m10s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:25:10</a>]</h4><ul class="postList"><li name="6334" id="6334" class="graf graf--li graf-after--h4">The last model, when we tried to add more layers, we had trouble training. The reason we had trouble training was that if we used larger learning rates, it would go off to NaN and if we used smaller learning rate, it would take forever and doesn’t have a chance to explore properly — so it was not resilient.</li><li name="82ea" id="82ea" class="graf graf--li graf-after--li">To make it resilient, we will use something called batch normalization. BatchNorm came out about two years ago and it has been quite transformative since it suddenly makes it really easy to train deeper networks.</li><li name="e576" id="e576" class="graf graf--li graf-after--li">We can simply use <code class="markup--code markup--li-code">nn.BatchNorm</code> but to learn about it, we will write it from scratch.</li><li name="786a" id="786a" class="graf graf--li graf-after--li">It is unlikely that the weight matrices on average are not going to cause your activations to keep getting smaller and smaller or keep getting bigger and bigger. It is important to keep them at reasonable scale. So we start things off with zero-mean standard deviation one by normalizing the input. What we really want to do is to do this for all layers, not just the inputs.</li></ul><pre name="e8b6" id="e8b6" class="graf graf--pre graf-after--li"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">BnLayer</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, ni, nf, stride=2, kernel_size=3):<br>        super().__init__()<br>        self.conv = nn.Conv2d(ni, nf, kernel_size=kernel_size, <br>                              stride=stride, bias=<strong class="markup--strong markup--pre-strong">False</strong>, padding=1)<br>        self.a = nn.Parameter(torch.zeros(nf,1,1))<br>        self.m = nn.Parameter(torch.ones(nf,1,1))<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x):<br>        x = F.relu(self.conv(x))<br>        x_chan = x.transpose(0,1).contiguous().view(x.size(1), -1)<br>        <strong class="markup--strong markup--pre-strong">if</strong> self.training:<br>            <strong class="markup--strong markup--pre-strong">self.means = x_chan.mean(1)[:,None,None]</strong><br>           <strong class="markup--strong markup--pre-strong"> self.stds  = x_chan.std (1)[:,None,None]</strong><br>        <strong class="markup--strong markup--pre-strong">return</strong> <strong class="markup--strong markup--pre-strong">(x-self.means) / self.stds *self.m + self.a</strong></pre><ul class="postList"><li name="0b51" id="0b51" class="graf graf--li graf-after--pre">Calculate the mean of each channel or each filter and standard deviation of each channel or each filter. Then subtract the means and divide by the standard deviations.</li><li name="a619" id="a619" class="graf graf--li graf-after--li">We no longer need to normalize our input because it is normalizing it per channel or for later layers it is normalizing per filter.</li><li name="98bc" id="98bc" class="graf graf--li graf-after--li">Turns out this is not enough since SGD is bloody-minded [<a href="https://youtu.be/H3g26EVADgY?t=1h29m20s" data-href="https://youtu.be/H3g26EVADgY?t=1h29m20s" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">01:29:20</a>]. If SGD decided that it wants matrix to be bigger/smaller overall, doing <code class="markup--code markup--li-code">(x=self.means) / self.stds</code> is not enough because SGD will undo it and try to do it again in the next mini-batch. So we will add two parameters: <code class="markup--code markup--li-code">a</code> — adder (initial value zeros) and <code class="markup--code markup--li-code">m</code> — multiplier (initial value ones) for each channel.</li><li name="4345" id="4345" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">Parameter</code> tells PyTorch that it is allowed to learn these as weights.</li><li name="b2ad" id="b2ad" class="graf graf--li graf-after--li">Why does this work? If it wants to scale the layer up, it does not have to scale up every single value in the matrix. It can just scale up this single trio of numbers <code class="markup--code markup--li-code">self.m</code>&nbsp;, if it wants to shift it all up or down a bit, it does not have to shift the entire weight matrix, they can just shift this trio of numbers <code class="markup--code markup--li-code">self.a</code>. Intuition: We are normalizing the data and then we are saying you can then shift it and scale it using far fewer parameters than would have been necessary if it were to actually shift and scale the entire set of convolutional filters. In practice, it allows us to increase our learning rates, it increase the resilience of training, and it allows us to add more layers and still train effectively.</li><li name="5106" id="5106" class="graf graf--li graf-after--li">The other thing batch norm does is that it regularizes, in other words, you can often decrease or remove dropout or weight decay. The reason why is each mini-batch is going to have a different mean and a different standard deviation to the previous mini-batch. So they keep changing and it is changing the meaning of the filters in a subtle way acting as a noise (i.e. regularization).</li><li name="2794" id="2794" class="graf graf--li graf-after--li">In real version, it does not use this batch’s mean and standard deviation but takes an exponentially weighted moving average standard deviation and mean.</li><li name="a040" id="a040" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code u-paddingRight0 u-marginRight0"><strong class="markup--strong markup--li-strong">if</strong> self.training</code> — this is important because when you are going through the validation set, you do not want to be changing the meaning of the model. There are some types of layer that are actually sensitive to what the mode of the network is whether it is in training mode or evaluation/test mode. There was a bug when we implemented mini net for MovieLens that dropout was applied during the validation — which was fixed. In PyTorch, there are two such layer: dropout and batch norm. <code class="markup--code markup--li-code">nn.Dropout</code> already does the check.</li><li name="1474" id="1474" class="graf graf--li graf-after--li">[<a href="https://youtu.be/H3g26EVADgY?t=1h37m1s" data-href="https://youtu.be/H3g26EVADgY?t=1h37m1s" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">01:37:01</a>] The key difference in fast.ai which no other library does is that these means and standard deviations get updated in training mode in every other library as soon as you basically say I am training, regardless of whether that layer is set to trainable or not. With a pre-trained network, that is a terrible idea. If you have a pre-trained network for specific values of those means and standard deviations in batch norm, if you change them, it changes the meaning of those pre-trained layers. In fast.ai, always by default, it will not touch those means and standard deviations if your layer is frozen. As soon as you un-freeze it, it will start updating them unless you set <code class="markup--code markup--li-code">learn.bn_freeze=True</code>. In practice, this often seems to work a lot better for pre-trained models particularly if you are working with data that is quite similar to what the pre-trained model was trained with.</li><li name="89ad" id="89ad" class="graf graf--li graf-after--li">Where do you put batch-norm layer? We will talk more in a moment, but for now, after <code class="markup--code markup--li-code">relu</code></li></ul><h4 name="e607" id="e607" class="graf graf--h4 graf-after--li">Ablation Study [<a href="https://youtu.be/H3g26EVADgY?t=1h39m41s" data-href="https://youtu.be/H3g26EVADgY?t=1h39m41s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">01:39:41</a>]</h4><p name="9816" id="9816" class="graf graf--p graf-after--h4">It is something where you try turning on and off different pieces of your model to see which bits make which impacts, and one of the things that wasn’t done in the original batch norm paper was any kind of effective ablation. And one of the things therefore that was missing was this question which was just asked — where to put the batch norm. That oversight caused a lot of problems because it turned out the original paper did not actually put it in the best spot. Other people since then have now figured that out and when Jeremy show people code where it is actually in the spot that is better, people say his batch norm is in the wrong spot.</p><ul class="postList"><li name="9076" id="9076" class="graf graf--li graf-after--p">Try and always use batch norm on every layer if you can</li><li name="e256" id="e256" class="graf graf--li graf-after--li">Don’t stop normalizing your data so that people using your data will know how you normalized your data. Other libraries might not deal with batch norm for pre-trained models correctly, so when people start re-training, it might cause problems.</li></ul><pre name="97ea" id="97ea" class="graf graf--pre graf-after--li"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">ConvBnNet</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, layers, c):<br>        super().__init__()<br>        <strong class="markup--strong markup--pre-strong">self.conv1 = nn.Conv2d(3, 10, kernel_size=5, stride=1, padding=2)</strong><br>        self.layers = nn.ModuleList([<strong class="markup--strong markup--pre-strong">BnLayer</strong>(layers[i], layers[i + 1])<br>            <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(len(layers) - 1)])<br>        self.out = nn.Linear(layers[-1], c)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x):<br>        x = self.conv1(x)<br>        <strong class="markup--strong markup--pre-strong">for</strong> l <strong class="markup--strong markup--pre-strong">in</strong> self.layers: x = l(x)<br>        x = F.adaptive_max_pool2d(x, 1)<br>        x = x.view(x.size(0), -1)<br>        <strong class="markup--strong markup--pre-strong">return</strong> F.log_softmax(self.out(x), dim=-1)</pre><ul class="postList"><li name="cc26" id="cc26" class="graf graf--li graf-after--pre">Rest of the code is similar — Using <code class="markup--code markup--li-code">BnLayer</code> instead of <code class="markup--code markup--li-code">ConvLayer</code></li><li name="bae3" id="bae3" class="graf graf--li graf-after--li">A single convolutional layer was added at the start trying to get closer to the modern approaches. It has a bigger kernel size and a stride of 1. The basic idea is that we want the first layer to have a richer input. It does convolution using the 5 by 5 area which allows it to try and find more interesting richer features in that 5 by 5 area, then spit out bigger output (in this case, it’s 10 by 5 by 5 filters). Typically it is 5 by 5 or 7 by 7, or even 11 by 11 convolution with quite a few filters coming out (e.g. 32 filters).</li><li name="1226" id="1226" class="graf graf--li graf-after--li">Since <code class="markup--code markup--li-code">padding = kernel_size — 1 / 2</code> and <code class="markup--code markup--li-code">stride=1</code>&nbsp;, the input size is the same as the output size — just more filters.</li><li name="77f8" id="77f8" class="graf graf--li graf-after--li">It is a good way of trying to create a richer starting point.</li></ul><h4 name="5991" id="5991" class="graf graf--h4 graf-after--li">Deep BatchNorm [<a href="https://youtu.be/H3g26EVADgY?t=1h50m52s" data-href="https://youtu.be/H3g26EVADgY?t=1h50m52s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">01:50:52</a>]</h4><p name="3230" id="3230" class="graf graf--p graf-after--h4">Let’s increase the depth of the model. We cannot just add more of stride 2 layers since it halves the size of the image each time. Instead, after each stride 2 layer, we insert a stride 1 layer.</p><pre name="c821" id="c821" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">ConvBnNet2</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, layers, c):<br>        super().__init__()<br>        self.conv1 = nn.Conv2d(3, 10, kernel_size=5, stride=1, padding=2)<br>        self.layers = nn.ModuleList([BnLayer(layers[i], layers[i+1])<br>            <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(len(layers) - 1)])<br>        self.layers2 = nn.ModuleList([BnLayer(layers[i+1], layers[i + 1], 1)<br>            <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(len(layers) - 1)])<br>        self.out = nn.Linear(layers[-1], c)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x):<br>        x = self.conv1(x)<br>        <strong class="markup--strong markup--pre-strong">for</strong> l,l2 <strong class="markup--strong markup--pre-strong">in</strong> zip(self.layers, self.layers2):<br>            x = l(x)<br>            x = l2(x)<br>        x = F.adaptive_max_pool2d(x, 1)<br>        x = x.view(x.size(0), -1)<br>        <strong class="markup--strong markup--pre-strong">return</strong> F.log_softmax(self.out(x), dim=-1)</pre><pre name="0298" id="0298" class="graf graf--pre graf-after--pre">learn = ConvLearner.from_model_data((ConvBnNet2([10, 20, 40, 80, 160], 10), data)</pre><pre name="fe51" id="fe51" class="graf graf--pre graf-after--pre">%time learn.fit(1e-2, 2)</pre><pre name="e029" id="e029" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">A Jupyter Widget</em></pre><pre name="58f1" id="58f1" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[ 0.       1.53499  1.43782  0.47588]                       <br>[ 1.       1.28867  1.22616  0.55537]                       <br><br>CPU times: user 1min 22s, sys: 34.5 s, total: 1min 56s<br>Wall time: 58.2 s</em></pre><pre name="b483" id="b483" class="graf graf--pre graf-after--pre">%time learn.fit(1e-2, 2, cycle_len=1)</pre><pre name="16ed" id="16ed" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">A Jupyter Widget</em></pre><pre name="7b64" id="7b64" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[ 0.       1.10933  1.06439  0.61582]                       <br>[ 1.       1.04663  0.98608  0.64609]                       <br><br>CPU times: user 1min 21s, sys: 32.9 s, total: 1min 54s<br>Wall time: 57.6 s</em></pre><p name="bc77" id="bc77" class="graf graf--p graf-after--pre">The accuracy remained the same as before. This is now 12 layers deep, and it is too deep even for batch norm to handle. It is possible to train 12 layer deep conv net but it starts to get difficult. And it does not seem to be helping much if at all.</p><h4 name="9bfa" id="9bfa" class="graf graf--h4 graf-after--p">ResNet [<a href="https://youtu.be/H3g26EVADgY?t=1h52m43s" data-href="https://youtu.be/H3g26EVADgY?t=1h52m43s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">01:52:43</a>]</h4><pre name="6c2d" id="6c2d" class="graf graf--pre graf-after--h4"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">ResnetLayer</strong>(BnLayer):<br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x): <strong class="markup--strong markup--pre-strong">return</strong> <strong class="markup--strong markup--pre-strong">x + super().forward(x)</strong></pre><pre name="f565" id="f565" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">Resnet</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, layers, c):<br>        super().__init__()<br>        self.conv1 = nn.Conv2d(3, 10, kernel_size=5, stride=1, padding=2)<br>        self.layers = nn.ModuleList([BnLayer(layers[i], layers[i+1])<br>            <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(len(layers) - 1)])<br>        self.layers2 = nn.ModuleList([ResnetLayer(layers[i+1], layers[i + 1], 1)<br>            <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(len(layers) - 1)])<br>        self.layers3 = nn.ModuleList([ResnetLayer(layers[i+1], layers[i + 1], 1)<br>            <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(len(layers) - 1)])<br>        self.out = nn.Linear(layers[-1], c)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x):<br>        x = self.conv1(x)<br>        <strong class="markup--strong markup--pre-strong">for</strong> l,l2,l3 <strong class="markup--strong markup--pre-strong">in</strong> zip(self.layers, self.layers2, self.layers3):<br>            x = l3(l2(l(x)))<br>        x = F.adaptive_max_pool2d(x, 1)<br>        x = x.view(x.size(0), -1)<br>        <strong class="markup--strong markup--pre-strong">return</strong> F.log_softmax(self.out(x), dim=-1)</pre><ul class="postList"><li name="2181" id="2181" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">ResnetLayer</code> inherit from <code class="markup--code markup--li-code">BnLayer</code> and override <code class="markup--code markup--li-code">forward</code>.</li><li name="6706" id="6706" class="graf graf--li graf-after--li">Then add bunch of layers and make it 3 times deeper, ad it still trains beautifully just because of <code class="markup--code markup--li-code">x + super().forward(x)</code>&nbsp;.</li></ul><pre name="9932" id="9932" class="graf graf--pre graf-after--li">learn = ConvLearner.from_model_data(Resnet([10, 20, 40, 80, 160], 10), data)</pre><pre name="0db7" id="0db7" class="graf graf--pre graf-after--pre">wd=1e-5</pre><pre name="721c" id="721c" class="graf graf--pre graf-after--pre">%time learn.fit(1e-2, 2, wds=wd)</pre><pre name="fa4d" id="fa4d" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">A Jupyter Widget</em></pre><pre name="7ff9" id="7ff9" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[ 0.       1.58191  1.40258  0.49131]                       <br>[ 1.       1.33134  1.21739  0.55625]                       <br><br>CPU times: user 1min 27s, sys: 34.3 s, total: 2min 1s<br>Wall time: 1min 3s</em></pre><pre name="7057" id="7057" class="graf graf--pre graf-after--pre">%time learn.fit(1e-2, 3, cycle_len=1, cycle_mult=2, wds=wd)</pre><pre name="8b6a" id="8b6a" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">A Jupyter Widget</em></pre><pre name="42cc" id="42cc" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[ 0.       1.11534  1.05117  0.62549]                       <br>[ 1.       1.06272  0.97874  0.65185]                       <br>[ 2.       0.92913  0.90472  0.68154]                        <br>[ 3.       0.97932  0.94404  0.67227]                        <br>[ 4.       0.88057  0.84372  0.70654]                        <br>[ 5.       0.77817  0.77815  0.73018]                        <br>[ 6.       0.73235  0.76302  0.73633]                        <br><br>CPU times: user 5min 2s, sys: 1min 59s, total: 7min 1s<br>Wall time: 3min 39s</em></pre><pre name="3216" id="3216" class="graf graf--pre graf-after--pre">%time learn.fit(1e-2, 8, cycle_len=4, wds=wd)</pre><pre name="f593" id="f593" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">A Jupyter Widget</em></pre><pre name="41f5" id="41f5" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[ 0.       0.8307   0.83635  0.7126 ]                        <br>[ 1.       0.74295  0.73682  0.74189]                        <br>[ 2.       0.66492  0.69554  0.75996]                        <br>[ 3.       0.62392  0.67166  0.7625 ]                        <br>[ 4.       0.73479  0.80425  0.72861]                        <br>[ 5.       0.65423  0.68876  0.76318]                        <br>[ 6.       0.58608  0.64105  0.77783]                        <br>[ 7.       0.55738  0.62641  0.78721]                        <br>[ 8.       0.66163  0.74154  0.7501 ]                        <br>[ 9.       0.59444  0.64253  0.78106]                        <br>[ 10.        0.53      0.61772   0.79385]                    <br>[ 11.        0.49747   0.65968   0.77832]                    <br>[ 12.        0.59463   0.67915   0.77422]                    <br>[ 13.        0.55023   0.65815   0.78106]                    <br>[ 14.        0.48959   0.59035   0.80273]                    <br>[ 15.        0.4459    0.61823   0.79336]                    <br>[ 16.        0.55848   0.64115   0.78018]                    <br>[ 17.        0.50268   0.61795   0.79541]                    <br>[ 18.        0.45084   0.57577   0.80654]                    <br>[ 19.        0.40726   0.5708    0.80947]                    <br>[ 20.        0.51177   0.66771   0.78232]                    <br>[ 21.        0.46516   0.6116    0.79932]                    <br>[ 22.        0.40966   0.56865   0.81172]                    <br>[ 23.        0.3852    0.58161   0.80967]                    <br>[ 24.        0.48268   0.59944   0.79551]                    <br>[ 25.        0.43282   0.56429   0.81182]                    <br>[ 26.        0.37634   0.54724   0.81797]                    <br>[ 27.        0.34953   0.54169   0.82129]                    <br>[ 28.        0.46053   0.58128   0.80342]                    <br>[ 29.        0.4041    0.55185   0.82295]                    <br>[ 30.        0.3599    0.53953   0.82861]                    <br>[ 31.        0.32937   0.55605   0.82227]                    <br><br>CPU times: user 22min 52s, sys: 8min 58s, total: 31min 51s<br>Wall time: 16min 38s</em></pre><p name="0ed8" id="0ed8" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">ResNet block </strong>[<a href="https://youtu.be/H3g26EVADgY?t=1h53m18s" data-href="https://youtu.be/H3g26EVADgY?t=1h53m18s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">01:53:18</a>]</p><p name="a633" id="a633" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code u-paddingRight0 u-marginRight0"><strong class="markup--strong markup--p-strong">return</strong> <strong class="markup--strong markup--p-strong">x + super().forward(x)</strong></code></p><p name="d1d3" id="d1d3" class="graf graf--p graf-after--p"><em class="markup--em markup--p-em">y = x + f(x)</em></p><p name="a354" id="a354" class="graf graf--p graf-after--p">Where <em class="markup--em markup--p-em">x</em> is prediction from the previous layer, <em class="markup--em markup--p-em">y</em> is prediction from the current layer.Shuffle around the formula and we get:formula shuffle</p><p name="a4eb" id="a4eb" class="graf graf--p graf-after--p"><em class="markup--em markup--p-em">f(x) = y − x</em></p><p name="36d4" id="36d4" class="graf graf--p graf-after--p">The difference <em class="markup--em markup--p-em">y − x </em>is <strong class="markup--strong markup--p-strong">residual</strong>. The residual is the error in terms of what we have calculated so far. What this is saying is that try to find a set of convolutional weights that attempts to fill in the amount we were off by. So in other words, we have an input, and we have a function which tries to predict the error (i.e. how much we are off by). Then we add a prediction of how much we were wrong by to the input, then add another prediction of how much we were wrong by that time, and repeat that layer after layer — zooming into the correct answer. This is based on a theory called <strong class="markup--strong markup--p-strong">boosting</strong>.</p><ul class="postList"><li name="0b95" id="0b95" class="graf graf--li graf-after--p">The full ResNet does two convolutions before it gets added back to the original input (we did just one here).</li><li name="5177" id="5177" class="graf graf--li graf-after--li">In every block <code class="markup--code markup--li-code">x = l3(l2(l(x)))</code>&nbsp;, one of the layers is not a <span class="markup--quote markup--li-quote is-other" name="anon_c28637c34a43" data-creator-ids="anon"><code class="markup--code markup--li-code">ResnetLayer</code></span> but a standard convolution with <code class="markup--code markup--li-code">stride=2</code> — this is called a “bottleneck layer”. ResNet does not convolutional layer but a different form of bottleneck block which we will cover in Part 2.</li></ul><figure name="20af" id="20af" class="graf graf--figure graf-after--li"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_0_0J8BFYOTK4Mupk94Izrw.png"></figure><h4 name="efe6" id="efe6" class="graf graf--h4 graf-after--figure">ResNet 2 [<a href="https://youtu.be/H3g26EVADgY?t=1h59m33s" data-href="https://youtu.be/H3g26EVADgY?t=1h59m33s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">01:59:33</a>]</h4><p name="3b17" id="3b17" class="graf graf--p graf-after--h4">Here, we increased the size of features and added dropout.</p><pre name="57e5" id="57e5" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">Resnet2</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, layers, c, p=0.5):<br>        super().__init__()<br>        self.conv1 = BnLayer(3, 16, stride=1, kernel_size=7)<br>        self.layers = nn.ModuleList([BnLayer(layers[i], layers[i+1])<br>            <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(len(layers) - 1)])<br>        self.layers2 = nn.ModuleList([ResnetLayer(layers[i+1], layers[i + 1], 1)<br>            <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(len(layers) - 1)])<br>        self.layers3 = nn.ModuleList([ResnetLayer(layers[i+1], layers[i + 1], 1)<br>            <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(len(layers) - 1)])<br>        self.out = nn.Linear(layers[-1], c)<br>        self.drop = nn.Dropout(p)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x):<br>        x = self.conv1(x)<br>        <strong class="markup--strong markup--pre-strong">for</strong> l,l2,l3 <strong class="markup--strong markup--pre-strong">in</strong> zip(self.layers, self.layers2, self.layers3):<br>            x = l3(l2(l(x)))<br>        x = F.adaptive_max_pool2d(x, 1)<br>        x = x.view(x.size(0), -1)<br>        x = self.drop(x)<br>        <strong class="markup--strong markup--pre-strong">return</strong> F.log_softmax(self.out(x), dim=-1)</pre><pre name="2c9e" id="2c9e" class="graf graf--pre graf-after--pre">learn = ConvLearner.from_model_data(Resnet2([<strong class="markup--strong markup--pre-strong">16, 32, 64, 128, 256</strong>], 10, 0.2), data)</pre><pre name="19ca" id="19ca" class="graf graf--pre graf-after--pre">wd=1e-6</pre><pre name="0392" id="0392" class="graf graf--pre graf-after--pre">%time learn.fit(1e-2, 2, wds=wd)<br>%time learn.fit(1e-2, 3, cycle_len=1, cycle_mult=2, wds=wd)<br>%time learn.fit(1e-2, 8, cycle_len=4, wds=wd)</pre><pre name="b438" id="b438" class="graf graf--pre graf-after--pre">log_preds,y = learn.TTA()<br>preds = np.mean(np.exp(log_preds),0)</pre><pre name="ccc8" id="ccc8" class="graf graf--pre graf-after--pre">metrics.log_loss(y,preds), accuracy(preds,y)<br><em class="markup--em markup--pre-em">(0.44507397166057938, 0.84909999999999997)</em></pre><p name="43f2" id="43f2" class="graf graf--p graf-after--pre">85% was a state-of-the-art back in 2012 or 2013 for CIFAR 10. Nowadays, it is up to 97% so there is a room for improvement but all based on these tecniques:</p><ul class="postList"><li name="1c58" id="1c58" class="graf graf--li graf-after--p">Better approaches to data augmentation</li><li name="3a64" id="3a64" class="graf graf--li graf-after--li">Better approaches to regularization</li><li name="6632" id="6632" class="graf graf--li graf-after--li">Some tweaks on ResNet</li></ul><p name="9561" id="9561" class="graf graf--p graf-after--li">Question [<a href="https://youtu.be/H3g26EVADgY?t=2h1m7s" data-href="https://youtu.be/H3g26EVADgY?t=2h1m7s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">02:01:07</a>]: Can we apply “training on the residual” approach for non-image problem? Yes! But it has been ignored everywhere else. In NLP, “transformer architecture” recently appeared and was shown to be the state of the art for translation, and it has a simple ResNet structure in it. This general approach is called “skip connection” (i.e. the idea of skipping over a layer) and appears a lot in computer vision, but nobody else much seems to be using it even through there is nothing computer vision specific about it. Good opportunity!</p><h3 name="6bed" id="6bed" class="graf graf--h3 graf-after--p"><a href="https://github.com/fastai/fastai/blob/master/courses/dl1/lesson7-CAM.ipynb" data-href="https://github.com/fastai/fastai/blob/master/courses/dl1/lesson7-CAM.ipynb" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">Dogs vs. Cats</a> [<a href="https://youtu.be/H3g26EVADgY?t=2h2m3s" data-href="https://youtu.be/H3g26EVADgY?t=2h2m3s" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">02:02:03</a>]</h3><p name="27ad" id="27ad" class="graf graf--p graf-after--h3">Going back dogs and cats. We will create resnet34 (if you are interested in what the trailing number means, <a href="https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py" data-href="https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">see here</a> — just different parameters).</p><pre name="89bb" id="89bb" class="graf graf--pre graf-after--p">PATH = "data/dogscats/"<br>sz = 224<br>arch = resnet34  # &lt;-- Name of the function <br>bs = 64</pre><pre name="27df" id="27df" class="graf graf--pre graf-after--pre">m = arch(pretrained=True) # Get a model w/ pre-trained weight loaded<br>m</pre><pre name="885d" id="885d" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">ResNet(<br>  (conv1): Conv2d (3, 64, </em><strong class="markup--strong markup--pre-strong"><em class="markup--em markup--pre-em">kernel_size=(7, 7)</em></strong><em class="markup--em markup--pre-em">, stride=(2, 2), padding=(3, 3), bias=False)<br>  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)<br>  (relu): ReLU(inplace)<br>  (maxpool): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=(1, 1))<br>  (</em><strong class="markup--strong markup--pre-strong"><em class="markup--em markup--pre-em">layer1</em></strong><em class="markup--em markup--pre-em">): Sequential(<br>    (0): BasicBlock(<br>      (conv1): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br>      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)<br>      (relu): ReLU(inplace)<br>      (conv2): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br>      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)<br>    )<br>    (1): BasicBlock(<br>      (conv1): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br>      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)<br>      (relu): ReLU(inplace)<br>      (conv2): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br>      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)<br>    )<br>    (2): BasicBlock(<br>      (conv1): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br>      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)<br>      (relu): ReLU(inplace)<br>      (conv2): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br>      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)<br>    )<br>  )<br>  (</em><strong class="markup--strong markup--pre-strong"><em class="markup--em markup--pre-em">layer2</em></strong><em class="markup--em markup--pre-em">): Sequential(<br>    (0): BasicBlock(<br>      (conv1): Conv2d (64, 128, kernel_size=(3, 3), </em><strong class="markup--strong markup--pre-strong"><em class="markup--em markup--pre-em">stride=(2, 2)</em></strong><em class="markup--em markup--pre-em">, padding=(1, 1), bias=False)<br>      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)<br>      (relu): ReLU(inplace)<br>      (conv2): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br>      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)<br>      (downsample): Sequential(<br>        (0): Conv2d (64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)<br>        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)<br>      )<br>    )<br>    (1): BasicBlock(<br>      (conv1): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br>      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)<br>      (relu): ReLU(inplace)<br>      (conv2): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br>      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)<br>    )<br>    (2): BasicBlock(<br>      (conv1): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br>      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)<br>      (relu): ReLU(inplace)<br>      (conv2): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br>      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)<br>    )<br>    (3): BasicBlock(<br>      (conv1): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br>      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)<br>      (relu): ReLU(inplace)<br>      (conv2): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br>      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)<br>    )<br>  )</em></pre><pre name="96b4" id="96b4" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">  ...</em></pre><pre name="c627" id="c627" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">  (avgpool): AvgPool2d(kernel_size=7, stride=7, padding=0, ceil_mode=False, count_include_pad=True)<br>  (fc): Linear(in_features=512, out_features=1000)<br>)</em></pre><p name="b9af" id="b9af" class="graf graf--p graf-after--pre">Our ResNet model had Relu → BatchNorm. TorchVision does BatchNorm →Relu. There are three different versions of ResNet floating around, and the best one is PreAct (<a href="https://arxiv.org/pdf/1603.05027.pdf" data-href="https://arxiv.org/pdf/1603.05027.pdf" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">https://arxiv.org/pdf/1603.05027.pdf</a>).</p><ul class="postList"><li name="3c9a" id="3c9a" class="graf graf--li graf-after--p">Currently, the final layer has a thousands features because ImageNet has 1000 features, so we need to get rid of it.</li><li name="011b" id="011b" class="graf graf--li graf-after--li">When you use fast.ai’s <code class="markup--code markup--li-code">ConvLearner</code>&nbsp;, it deletes the last two layers for you. fast.ai replaces <code class="markup--code markup--li-code">AvgPool2d</code> with Adaptive Average Pooling and Adaptive Max Pooling and concatenate the two together.</li><li name="4ccc" id="4ccc" class="graf graf--li graf-after--li">For this exercise, we will do a simple version.</li></ul><pre name="7af8" id="7af8" class="graf graf--pre graf-after--li">m = nn.Sequential(*children(m)[:-2], <br>                  nn.Conv2d(512, 2, 3, padding=1), <br>                  nn.AdaptiveAvgPool2d(1), Flatten(), <br>                  nn.LogSoftmax())</pre><ul class="postList"><li name="9f05" id="9f05" class="graf graf--li graf-after--pre">Remove the last two layers</li><li name="810e" id="810e" class="graf graf--li graf-after--li">Add a convolution which just has 2 outputs.</li><li name="af87" id="af87" class="graf graf--li graf-after--li">Do average pooling then softmax</li><li name="c02d" id="c02d" class="graf graf--li graf-after--li">There is no linear layer at the end. This is a different way of producing just two numbers — which allows us to do CAM!</li></ul><pre name="a6db" id="a6db" class="graf graf--pre graf-after--li">tfms = tfms_from_model(arch, sz, aug_tfms=transforms_side_on, max_zoom=1.1)<br>data = ImageClassifierData.from_paths(PATH, tfms=tfms, bs=bs)</pre><pre name="b24a" id="b24a" class="graf graf--pre graf-after--pre">learn = <strong class="markup--strong markup--pre-strong">ConvLearner.from_model_data</strong>(m, data)</pre><pre name="a856" id="a856" class="graf graf--pre graf-after--pre">learn.freeze_to(-4)</pre><pre name="a331" id="a331" class="graf graf--pre graf-after--pre">learn.fit(0.01, 1)<br>learn.fit(0.01, 1, cycle_len=1)</pre><ul class="postList"><li name="0e57" id="0e57" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">ConvLearner.from_model</code> is what we learned about earlier — allows us to create a Learner object with custom model.</li><li name="05e1" id="05e1" class="graf graf--li graf-after--li">Then freeze the layer except the ones we just added.</li></ul><h4 name="dc54" id="dc54" class="graf graf--h4 graf-after--li">Class Activation Maps (CAM) [<a href="https://youtu.be/H3g26EVADgY?t=2h8m55s" data-href="https://youtu.be/H3g26EVADgY?t=2h8m55s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">02:08:55</a>]</h4><p name="4f34" id="4f34" class="graf graf--p graf-after--h4">We pick a specific image, and use a technique called CAM where we take a model and we ask it which parts of the image turned out to be important.</p><figure name="281d" id="281d" class="graf graf--figure graf--layoutOutsetRow is-partialWidth graf-after--p" style="width: 50.866%;" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_BrMBBupbny4CFsqBVjgcfA.png"></figure><figure name="d92f" id="d92f" class="graf graf--figure graf--layoutOutsetRowContinue is-partialWidth graf-after--figure" style="width: 49.134%;" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_zayLvr0jvnUXe-G27odldQ.png"></figure><p name="9782" id="9782" class="graf graf--p graf-after--figure">How did it do this? Let’s work backwards. The way it did it was by producing this matrix:</p><figure name="182a" id="182a" class="graf graf--figure graf--layoutOutsetCenter graf-after--p" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_DPIlEiNjJOeAbiIQUubNLg.png"></figure><p name="8f4f" id="8f4f" class="graf graf--p graf-after--figure">Big numbers correspond to the cat. So what is this matrix? This matrix simply equals to the value of feature matrix <code class="markup--code markup--p-code">feat</code> times <code class="markup--code markup--p-code">py</code> vector:</p><pre name="0f8a" id="0f8a" class="graf graf--pre graf-after--p">f2=np.dot(np.rollaxis(<strong class="markup--strong markup--pre-strong">feat</strong>,0,3), <strong class="markup--strong markup--pre-strong">py</strong>)<br>f2-=f2.min()<br>f2/=f2.max()<br>f2</pre><p name="8d04" id="8d04" class="graf graf--p graf-after--pre"><code class="markup--code markup--p-code">py</code> vector is the predictions that says “I am 100% confident it’s a cat.” <code class="markup--code markup--p-code">feat</code> is the values (2×7×7) coming out of the final convolutional layer (the <code class="markup--code markup--p-code">Conv2d</code> layer we added). If we multiply <code class="markup--code markup--p-code">feat</code> by <code class="markup--code markup--p-code">py</code>&nbsp;, we get all of the first channel and none of the second channel. Therefore, it is going to return the value of the last convolutional layers for the section which lines up with being a cat. In other words, if we multiply <code class="markup--code markup--p-code">feat</code> by <code class="markup--code markup--p-code">[0, 1]</code>&nbsp;, it will line up with being a dog.</p><pre name="388c" id="388c" class="graf graf--pre graf-after--p">sf = SaveFeatures(m[-4])<br>py = m(Variable(x.cuda()))<br>sf.remove()<br><br>py = np.exp(to_np(py)[0]); py</pre><pre name="0669" id="0669" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">array([ 1.,  0.], dtype=float32)</em></pre><pre name="2e1e" id="2e1e" class="graf graf--pre graf-after--pre">feat = np.maximum(0, sf.features[0])<br>feat.shape</pre><p name="89c7" id="89c7" class="graf graf--p graf-after--pre">Put it in another way, in the model, the only thing that happened after the convolutional layer was an average pooling layer. The average pooling layer took took the 7 by 7 grid and averaged out how much each part is “cat-like”. We then took the “cattyness” matrix, resized it to be the same size as the original cat image, and overlaid it on top, then you get the heat map.</p><p name="41b3" id="41b3" class="graf graf--p graf-after--p">The way you can use this technique at home is</p><ol class="postList"><li name="c79e" id="c79e" class="graf graf--li graf-after--p">when you have a large image, you can calculate this matrix on a quick small little convolutional net</li><li name="f177" id="f177" class="graf graf--li graf-after--li">zoom into the area that has the highest value</li><li name="deef" id="deef" class="graf graf--li graf-after--li">re-run it just on that part</li></ol><p name="7737" id="7737" class="graf graf--p graf-after--li">We skipped this over quickly as we ran out of time, but we will learn more about these kind of approaches in Part 2.</p><p name="3ce1" id="3ce1" class="graf graf--p graf--startsWithDoubleQuote graf-after--p">“Hook” is the mechanism that lets us ask the model to return the matrix. <code class="markup--code markup--p-code">register_forward_hook</code> asks PyTorch that every time it calculates a layer it runs the function given — sort of like a callback that happens every time it calculates a layer. In the following case, it saves the value of the particular layer we were interested in:</p><pre name="75b8" id="75b8" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">SaveFeatures</strong>():<br>    features=<strong class="markup--strong markup--pre-strong">None</strong><br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, m): <br>        self.hook = m.register_forward_hook(self.hook_fn)<br>    <strong class="markup--strong markup--pre-strong">def</strong> hook_fn(self, module, input, output): <br>        self.features = to_np(output)<br>    <strong class="markup--strong markup--pre-strong">def</strong> remove(self): self.hook.remove()</pre><h4 name="b432" id="b432" class="graf graf--h4 graf-after--pre">Questions to Jeremy [<a href="https://youtu.be/H3g26EVADgY?t=2h14m27s" data-href="https://youtu.be/H3g26EVADgY?t=2h14m27s" class="markup--anchor markup--h4-anchor" rel="noopener nofollow" target="_blank">02:14:27</a>]: “Your journey into Deep Learning” and “How to keep up with important research for practitioners”</h4><p name="cb8a" id="cb8a" class="graf graf--p graf--startsWithDoubleQuote graf-after--h4">“If you intend to come to Part 2, you are expected to master all the techniques er have learned in Part 1”. Here are something you can do:</p><ol class="postList"><li name="90cf" id="90cf" class="graf graf--li graf-after--p">Watch each of the video at least 3 times.</li><li name="171b" id="171b" class="graf graf--li graf-after--li">Make sure you can re-create the notebooks without watching the videos — maybe do so with different datasets to make it more interesting.</li><li name="0834" id="0834" class="graf graf--li graf-after--li">Keep an eye on the forum for recent papers, recent advances.</li><li name="961d" id="961d" class="graf graf--li graf-after--li graf--trailing">Be tenacious and keep working at it!</li></ol><hr class="section-divider"><p name="3c17" id="3c17" class="graf graf--p graf--leading graf--trailing">Lessons: <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" class="markup--anchor markup--p-anchor" target="_blank">1</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" class="markup--anchor markup--p-anchor" target="_blank">2</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" class="markup--anchor markup--p-anchor" target="_blank">3</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" class="markup--anchor markup--p-anchor" target="_blank">4</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" class="markup--anchor markup--p-anchor" target="_blank">5</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" class="markup--anchor markup--p-anchor" target="_blank">6</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" class="markup--anchor markup--p-anchor" target="_blank"><strong class="markup--strong markup--p-strong">7</strong></a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" class="markup--anchor markup--p-anchor" target="_blank">8</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" class="markup--anchor markup--p-anchor" target="_blank">9</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" class="markup--anchor markup--p-anchor" target="_blank">10</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" class="markup--anchor markup--p-anchor" target="_blank">11</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" class="markup--anchor markup--p-anchor" target="_blank">12</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" class="markup--anchor markup--p-anchor" target="_blank">13</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" class="markup--anchor markup--p-anchor" target="_blank">14</a></p></body></html>