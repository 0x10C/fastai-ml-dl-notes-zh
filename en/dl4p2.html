<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></head><body><<p name="dcf8" id="dcf8" class="graf graf--p graf-after--pre">A language model can be incredibly deep and subtle, so we are going to try and build that — not because we care about this at all, but because we are trying to create a pre-trained model which is used to do some other tasks. For example, given an IMDB movie review, we will figure out whether they are positive or negative. It is a lot like cats vs. dogs — a classification problem. So we would really like to use a pre-trained network which at least knows how to read English. So we will train a model that predicts a next word of a sentence (i.e. language model), and just like in computer vision, stick some new layers on the end and ask it to predict whether something is positive or negative.</p><h4 name="37a4" id="37a4" class="graf graf--h4 graf-after--p">IMDB [<a href="https://youtu.be/gbceqO8PpBg?t=1h31m11s" data-href="https://youtu.be/gbceqO8PpBg?t=1h31m11s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:31:11</a>]</h4><p name="d23c" id="d23c" class="graf graf--p graf-after--h4"><a href="https://github.com/fastai/fastai/blob/master/courses/dl1/lesson4-imdb.ipynb" data-href="https://github.com/fastai/fastai/blob/master/courses/dl1/lesson4-imdb.ipynb" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Notebook</a></p><p name="a321" id="a321" class="graf graf--p graf-after--p">What we are going to do is to train a language model, making that the pre-trained model for a classification model. In other words, we are trying to leverage exactly what we learned in our computer vision which is how to do fine-tuning to create powerful classification models.</p><p name="7e98" id="7e98" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: why would doing directly what you want to do not work? [<a href="https://youtu.be/gbceqO8PpBg?t=1h31m34s" data-href="https://youtu.be/gbceqO8PpBg?t=1h31m34s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">01:31:34</a>] It just turns out it doesn’t empirically. There are several reasons. First of all, we know fine-tuning a pre-trained network is really powerful. So if we can get it to learn some related tasks first, then we can use all that information to try and help it on the second task. The other is IMDB movie reviews are up to a thousands words long. So after reading a thousands words knowing nothing about how English is structured or concept of a word or punctuation, all you get is a 1 or a 0 (positive or negative). Trying to learn the entire structure of English and then how it expresses positive and negative sentiments from a single number is just too much to expect.</p><p name="a1ff" id="a1ff" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: Is this similar to Char-RNN by Karpathy? [<a href="https://youtu.be/gbceqO8PpBg?t=1h33m9s" data-href="https://youtu.be/gbceqO8PpBg?t=1h33m9s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">01:33:09</a>] This is somewhat similar to Char-RNN which predicts the next letter given a number of previous letters. Language model generally work at a word level (but they do not have to), and we will focus on word level modeling in this course.</p><p name="f4e0" id="f4e0" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: To what extent are these generated words/sentences actual copies of what it found in the training set? [<a href="https://youtu.be/gbceqO8PpBg?t=1h33m44s" data-href="https://youtu.be/gbceqO8PpBg?t=1h33m44s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">01:33:44</a>] Words are definitely words it has seen before because it is not a character level so it can only give us the word it has seen before. Sentences, there are rigorous ways of doing it but the easiest would be by looking at examples like above, you get a sense of it. Most importantly, when we train the language model, we will have a validation set so that we are trying to predict the next word of something that has never seen before. There are tricks to using language models to generate text like <a href="http://forums.fast.ai/t/tricks-for-using-language-models-to-generate-text/8127/2" data-href="http://forums.fast.ai/t/tricks-for-using-language-models-to-generate-text/8127/2" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">beam search</a>.</p><p name="fbc5" id="fbc5" class="graf graf--p graf-after--p">Use cases of text classification:</p><ul class="postList"><li name="9532" id="9532" class="graf graf--li graf-after--p">For hedge fund, identify things in articles or Twitter that caused massive market drops in the past.</li><li name="a5f0" id="a5f0" class="graf graf--li graf-after--li">Identify customer service queries which tend to be associated with people who cancel their contracts in the next month</li><li name="dc0a" id="dc0a" class="graf graf--li graf-after--li">Organize documents into whether they are part of legal discovery or not.</li></ul><pre name="4786" id="4786" class="graf graf--pre graf-after--li">from fastai.learner import *</pre><pre name="1ed4" id="1ed4" class="graf graf--pre graf-after--pre">import torchtext<br>from torchtext import vocab, data<br>from torchtext.datasets import language_modeling</pre><pre name="a3ee" id="a3ee" class="graf graf--pre graf-after--pre">from fastai.rnn_reg import *<br>from fastai.rnn_train import *<br>from fastai.nlp import *<br>from fastai.lm_rnn import *</pre><pre name="43ba" id="43ba" class="graf graf--pre graf-after--pre">import dill as pickle</pre><ul class="postList"><li name="e7ff" id="e7ff" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">torchtext</code> — PyTorch’s NLP library</li></ul><h4 name="d296" id="d296" class="graf graf--h4 graf-after--li">Data [<a href="https://youtu.be/gbceqO8PpBg?t=1h37m5s" data-href="https://youtu.be/gbceqO8PpBg?t=1h37m5s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">01:37:05</a>]</h4><p name="65b8" id="65b8" class="graf graf--p graf-after--h4">IMDB <a href="http://ai.stanford.edu/~amaas/data/sentiment/" data-href="http://ai.stanford.edu/~amaas/data/sentiment/" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Large Movie Review Dataset</a></p><pre name="38bb" id="38bb" class="graf graf--pre graf-after--p">PATH = 'data/aclImdb/'</pre><pre name="06b1" id="06b1" class="graf graf--pre graf-after--pre">TRN_PATH = 'train/all/'<br>VAL_PATH = 'test/all/'<br>TRN = f'{PATH}{TRN_PATH}'<br>VAL = f'{PATH}{VAL_PATH}'</pre><pre name="16ed" id="16ed" class="graf graf--pre graf-after--pre">%ls {PATH}</pre><pre name="9b87" id="9b87" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">imdbEr.txt  imdb.vocab  models/  README  test/  tmp/  train/</em></pre><p name="cd42" id="cd42" class="graf graf--p graf-after--pre">We do not have separate test and validation in this case. Just like in vision, the training directory has bunch of files in it:</p><pre name="f3e7" id="f3e7" class="graf graf--pre graf-after--p">trn_files = !ls {TRN}<br>trn_files[:10]<br><br><em class="markup--em markup--pre-em">['0_0.txt',<br> '0_3.txt',<br> '0_9.txt',<br> '10000_0.txt',<br> '10000_4.txt',<br> '10000_8.txt',<br> '1000_0.txt',<br> '10001_0.txt',<br> '10001_10.txt',<br> '10001_4.txt']</em></pre><pre name="f8bc" id="f8bc" class="graf graf--pre graf-after--pre">review = !cat {TRN}{trn_files[6]}<br>review[0]</pre><pre name="c71d" id="c71d" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">"I have to say when a name like Zombiegeddon and an atom bomb on the front cover I was expecting a flat out chop-socky fung-ku, but what I got instead was a comedy. So, it wasn't quite was I was expecting, but I really liked it anyway! The best scene ever was the main cop dude pulling those kids over and pulling a Bad Lieutenant on them!! I was laughing my ass off. I mean, the cops were just so bad! And when I say bad, I mean The Shield Vic Macky bad. But unlike that show I was laughing when they shot people and smoked dope.&lt;br /&gt;&lt;br /&gt;Felissa Rose...man, oh man. What can you say about that hottie. She was great and put those other actresses to shame. She should work more often!!!!! I also really liked the fight scene outside of the building. That was done really well. Lots of fighting and people getting their heads banged up. FUN! Last, but not least Joe Estevez and William Smith were great as the...well, I wasn't sure what they were, but they seemed to be having fun and throwing out lines. I mean, some of it didn't make sense with the rest of the flick, but who cares when you're laughing so hard! All in all the film wasn't the greatest thing since sliced bread, but I wasn't expecting that. It was a Troma flick so I figured it would totally suck. It's nice when something surprises you but not totally sucking.&lt;br /&gt;&lt;br /&gt;Rent it if you want to get stoned on a Friday night and laugh with your buddies. Don't rent it if you are an uptight weenie or want a zombie movie with lots of flesh eating.&lt;br /&gt;&lt;br /&gt;P.S. Uwe Boil was a nice touch."</em></pre><p name="3247" id="3247" class="graf graf--p graf-after--pre">Now we will check how many words are in the dataset:</p><pre name="e0ce" id="e0ce" class="graf graf--pre graf-after--p">!find {TRN} -name '*.txt' | xargs cat | wc -w</pre><pre name="33e6" id="33e6" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">17486581</em></pre><pre name="1b43" id="1b43" class="graf graf--pre graf-after--pre">!find {VAL} -name '*.txt' | xargs cat | wc -w</pre><pre name="be76" id="be76" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">5686719</em></pre><p name="d17f" id="d17f" class="graf graf--p graf-after--pre">Before we can do anything with text, we have to turn it into a list of tokens. Token is basically like a word. Eventually we will turn them into a list of numbers, but the first step is to turn it into a list of words — this is called “tokenization” in NLP. A good tokenizer will do a good job of recognizing pieces in your sentence. Each separated piece of punctuation will be separated, and each part of multi-part word will be separated as appropriate. Spacy does a lot of NLP stuff, and it has the best tokenizer Jeremy knows. So Fast.ai library is designed to work well with the Spacey tokenizer as with torchtext.</p><h4 name="0337" id="0337" class="graf graf--h4 graf-after--p">Creating a field [<a href="https://youtu.be/gbceqO8PpBg?t=1h41m1s" data-href="https://youtu.be/gbceqO8PpBg?t=1h41m1s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">01:41:01</a>]</h4><p name="376a" id="376a" class="graf graf--p graf-after--h4">A field is a definition of how to pre-process some text.</p><pre name="031d" id="031d" class="graf graf--pre graf-after--p">TEXT = data.Field(lower=<strong class="markup--strong markup--pre-strong">True</strong>, tokenize=spacy_tok)</pre><ul class="postList"><li name="a65f" id="a65f" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">lower=True</code> — lowercase the text</li><li name="7186" id="7186" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">tokenize=spacy_tok</code> — tokenize with <code class="markup--code markup--li-code">spacy_tok</code></li></ul><p name="44a9" id="44a9" class="graf graf--p graf-after--li">Now we create the usual Fast.ai model data object:</p><pre name="3b18" id="3b18" class="graf graf--pre graf-after--p">bs=64; bptt=70</pre><pre name="cc5e" id="cc5e" class="graf graf--pre graf-after--pre">FILES = dict(train=TRN_PATH, validation=VAL_PATH, test=VAL_PATH)<br>md = LanguageModelData.from_text_files(PATH, TEXT, **FILES, bs=bs, <br>                                       bptt=bptt, min_freq=10)</pre><ul class="postList"><li name="9fce" id="9fce" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">PATH</code>&nbsp;: as per usual where the data is, where to save models, etc</li><li name="aa16" id="aa16" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">TEXT</code>&nbsp;: torchtext’s Field definition</li><li name="37ba" id="37ba" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">**FILES</code>&nbsp;: list of all of the files we have: training, validation, and test (to keep things simple, we do not have a separate validation and test set, so both points to validation folder)</li><li name="b549" id="b549" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">bs</code>&nbsp;: batch size</li><li name="e5c8" id="e5c8" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">bptt</code>&nbsp;: Back Prop Through Time. It means how long a sentence we will stick on the GPU at once</li><li name="bd05" id="bd05" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">min_freq=10</code>&nbsp;: In a moment, we are going to be replacing words with integers (a unique index for every word). If there are any words that occur less than 10 times, just call it unknown.</li></ul><p name="c436" id="c436" class="graf graf--p graf-after--li">After building our <code class="markup--code markup--p-code">ModelData</code> object, it automatically fills the <code class="markup--code markup--p-code">TEXT</code> object with a very important attribute: <code class="markup--code markup--p-code">TEXT.vocab</code>. This is a <em class="markup--em markup--p-em">vocabulary</em>, which stores which unique words (or <em class="markup--em markup--p-em">tokens</em>) have been seen in the text, and how each word will be mapped to a unique integer id.</p><pre name="26f8" id="26f8" class="graf graf--pre graf-after--p"><em class="markup--em markup--pre-em"># 'itos': 'int-to-string'</em> <br>TEXT.vocab.itos[:12]</pre><pre name="7900" id="7900" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">['&lt;unk&gt;', '&lt;pad&gt;', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is', 'it', 'in']</em></pre><pre name="2eca" id="2eca" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em"># 'stoi': 'string to int'</em><br>TEXT.vocab.stoi['the']</pre><pre name="6e72" id="6e72" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">2</em></pre><p name="7b6d" id="7b6d" class="graf graf--p graf-after--pre"><code class="markup--code markup--p-code">itos</code> is sorted by frequency except for the first two special ones. Using <code class="markup--code markup--p-code">vocab</code>, torchtext will turn words into integer IDs for us&nbsp;:</p><pre name="3abc" id="3abc" class="graf graf--pre graf-after--p">md.trn_ds[0].text[:12]</pre><pre name="2c48" id="2c48" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">['i',<br> 'have',<br> 'always',<br> 'loved',<br> 'this',<br> 'story',<br> '-',<br> 'the',<br> 'hopeful',<br> 'theme',<br> ',',<br> 'the']</em></pre><pre name="8a44" id="8a44" class="graf graf--pre graf-after--pre">TEXT.numericalize([md.trn_ds[0].text[:12]])</pre><pre name="7e86" id="7e86" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">Variable containing:<br>   12<br>   35<br>  227<br>  480<br>   13<br>   76<br>   17<br>    2<br> 7319<br>  769<br>    3<br>    2<br>[torch.cuda.LongTensor of size 12x1 (GPU 0)]</em></pre><p name="9659" id="9659" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">Question</strong>: Is it common to do any stemming or lemma-tizing? [<a href="https://youtu.be/gbceqO8PpBg?t=1h45m47s" data-href="https://youtu.be/gbceqO8PpBg?t=1h45m47s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">01:45:47</a>] Not really, no. Generally tokenization is what we want. To keep it as general as possible, we want to know what is coming next so whether it is future tense or past tense or plural or singular, we don’t really know which things are going to be interesting and which are not, so it seems that it is generally best to leave it alone as much as possible.</p><p name="941b" id="941b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: When dealing with natural language, isn’t context important? Why are we tokenizing and looking at individual word? [<a href="https://youtu.be/gbceqO8PpBg?t=1h46m38s" data-href="https://youtu.be/gbceqO8PpBg?t=1h46m38s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">01:46:38</a>] No, we are not looking at individual word — they are still in order. Just because we replaced I with a number 12, they are still in that order. There is a different way of dealing with natural language called “bag of words” and they do throw away the order and context. In the Machine Learning course, we will be learning about working with bag of words representations but my belief is that they are no longer useful or in the verge of becoming no longer useful. We are starting to learn how to use deep learning to use context properly.</p><h4 name="2eff" id="2eff" class="graf graf--h4 graf-after--p">Batch size and BPTT [<a href="https://youtu.be/gbceqO8PpBg?t=1h47m40s" data-href="https://youtu.be/gbceqO8PpBg?t=1h47m40s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">01:47:40</a>]</h4><p name="077f" id="077f" class="graf graf--p graf-after--h4">What happens in a language model is even though we have lots of movie reviews, they all get concatenated together into one big block of text. So we predict the next word in this huge long thing which is all of the IMDB movie reviews concatenated together.</p><figure name="5fdb" id="5fdb" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_O-Kq1qtgZmrShbKhaN3fTg.png"></figure><ul class="postList"><li name="c2ec" id="c2ec" class="graf graf--li graf-after--figure">We split up the concatenated reviews into batches. In this case, we will split it to 64 sections</li><li name="289d" id="289d" class="graf graf--li graf-after--li">We then move each section underneath the previous one, and transpose it.</li><li name="80f9" id="80f9" class="graf graf--li graf-after--li">We end up with a matrix which is 1 million by 64.</li><li name="09d8" id="09d8" class="graf graf--li graf-after--li">We then grab a little chunk at time and those chunk lengths are <strong class="markup--strong markup--li-strong">approximately</strong> equal to BPTT. Here, we grab a little 70 long section and that is the first thing we chuck into our GPU (i.e. the batch).</li></ul><pre name="caa0" id="caa0" class="graf graf--pre graf-after--li">next(iter(md.trn_dl))</pre><pre name="883b" id="883b" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(Variable containing:<br>     12    567      3  ...    2118      4   2399<br>    </em><strong class="markup--strong markup--pre-strong"><em class="markup--em markup--pre-em"> 35      7     33</em></strong><em class="markup--em markup--pre-em">  ...       6    148     55<br>    227    103    533  ...    4892     31     10<br>         ...            ⋱           ...         <br>     19   8879     33  ...      41     24    733<br>    552   8250     57  ...     219     57   1777<br>      5     19      2  ...    3099      8     48<br> [torch.cuda.LongTensor of size 75x64 (GPU 0)], Variable containing:<br>     </em><strong class="markup--strong markup--pre-strong"><em class="markup--em markup--pre-em">35</em></strong><em class="markup--em markup--pre-em"><br>      </em><strong class="markup--strong markup--pre-strong"><em class="markup--em markup--pre-em">7</em></strong><em class="markup--em markup--pre-em"><br>     </em><strong class="markup--strong markup--pre-strong"><em class="markup--em markup--pre-em">33</em></strong><em class="markup--em markup--pre-em"><br>   ⋮   <br>     22<br>   3885<br>  21587<br> [torch.cuda.LongTensor of size 4800 (GPU 0)])</em></pre><ul class="postList"><li name="e5af" id="e5af" class="graf graf--li graf-after--pre">We grab our first training batch by wrapping data loader with <code class="markup--code markup--li-code">iter</code> then calling <code class="markup--code markup--li-code">next</code>.</li><li name="21d9" id="21d9" class="graf graf--li graf-after--li">We got back a 75 by 64 tensor (approximately 70 rows but not exactly)</li><li name="6f03" id="6f03" class="graf graf--li graf-after--li">A neat trick torchtext does is to randomly change the <code class="markup--code markup--li-code">bptt</code> number every time so each epoch it is getting slightly different bits of text — similar to shuffling images in computer vision. We cannot randomly shuffle the words because they need to be in the right order, so instead, we randomly move their breakpoints a little bit.</li><li name="49f1" id="49f1" class="graf graf--li graf-after--li">The target value is also 75 by 64 but for minor technical reasons it is flattened out into a single vector.</li></ul><p name="4554" id="4554" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">Question</strong>: Why not split by a sentence? [<a href="https://youtu.be/gbceqO8PpBg?t=1h53m40s" data-href="https://youtu.be/gbceqO8PpBg?t=1h53m40s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">01:53:40</a>] Not really. Remember, we are using columns. So each of our column is of length about 1 million, so although it is true that those columns are not always exactly finishing on a full stop, they are so darn long we do not care. Each column contains multiple sentences.</p><p name="5544" id="5544" class="graf graf--p graf-after--p">Pertaining to this question, Jeremy found what is in this language model matrix a little mind-bending for quite a while, so do not worry if it takes a while and you have to ask a thousands questions.</p><h4 name="a9e8" id="a9e8" class="graf graf--h4 graf-after--p">Create a model [<a href="https://youtu.be/gbceqO8PpBg?t=1h55m46s" data-href="https://youtu.be/gbceqO8PpBg?t=1h55m46s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">01:55:46</a>]</h4><p name="fdb6" id="fdb6" class="graf graf--p graf-after--h4">Now that we have a model data object that can fee d us batches, we can create a model. First, we are going to create an embedding matrix.</p><p name="7398" id="7398" class="graf graf--p graf-after--p">Here are the: # batches; # unique tokens in the vocab; length of the dataset; # of words</p><pre name="f3a8" id="f3a8" class="graf graf--pre graf-after--p">len(md.trn_dl), md.nt, len(md.trn_ds), len(md.trn_ds[0].text)</pre><pre name="3a40" id="3a40" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(4602, 34945, 1, 20621966)</em></pre><p name="3bf7" id="3bf7" class="graf graf--p graf-after--pre">This is our embedding matrix looks like:</p><figure name="4cab" id="4cab" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_6EHxqeSYMioiLEQ5ufrf_g.png"></figure><ul class="postList"><li name="1f12" id="1f12" class="graf graf--li graf-after--figure">It is a high cardinality categorical variable and furthermore, it is the only variable — this is typical in NLP</li><li name="aed9" id="aed9" class="graf graf--li graf-after--li">The embedding size is 200 which is much bigger than our previous embedding vectors. Not surprising because a word has a lot more nuance to it than the concept of Sunday. <strong class="markup--strong markup--li-strong">Generally, an embedding size for a word will be somewhere between 50 and 600.</strong></li></ul><pre name="c445" id="c445" class="graf graf--pre graf-after--li">em_sz = 200  <em class="markup--em markup--pre-em"># size of each embedding vector</em><br>nh = 500     <em class="markup--em markup--pre-em"># number of hidden activations per layer</em><br>nl = 3       <em class="markup--em markup--pre-em"># number of layers</em></pre><p name="fab7" id="fab7" class="graf graf--p graf-after--pre">Researchers have found that large amounts of <em class="markup--em markup--p-em">momentum</em> (which we’ll learn about later) don’t work well with these kinds of <em class="markup--em markup--p-em">RNN </em>models, so we create a version of the <em class="markup--em markup--p-em">Adam</em> optimizer with less momentum than its default of <code class="markup--code markup--p-code">0.9</code>. Any time you are doing NLP, you should probably include this line:</p><pre name="deb5" id="deb5" class="graf graf--pre graf-after--p">opt_fn = partial(optim.Adam, betas=(0.7, 0.99))</pre><p name="8836" id="8836" class="graf graf--p graf-after--pre">Fast.ai uses a variant of the state of the art <a href="https://arxiv.org/abs/1708.02182" data-href="https://arxiv.org/abs/1708.02182" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">AWD LSTM Language Model</a> developed by Stephen Merity. A key feature of this model is that it provides excellent regularization through <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network#Dropout" data-href="https://en.wikipedia.org/wiki/Convolutional_neural_network#Dropout" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Dropout</a>. There is no simple way known (yet!) to find the best values of the dropout parameters below — you just have to experiment…</p><p name="789e" id="789e" class="graf graf--p graf-after--p">However, the other parameters (<code class="markup--code markup--p-code">alpha</code>, <code class="markup--code markup--p-code">beta</code>, and <code class="markup--code markup--p-code">clip</code>) shouldn't generally need tuning.</p><pre name="912a" id="912a" class="graf graf--pre graf-after--p">learner = md.get_model(opt_fn, em_sz, nh, nl, dropouti=0.05,<br>                       dropout=0.05, wdrop=0.1, dropoute=0.02, <br>                       dropouth=0.05)<br>learner.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)<br>learner.clip=0.3</pre><ul class="postList"><li name="7e45" id="7e45" class="graf graf--li graf-after--pre">In the last lecture, we will learn what the architecture is and what all these dropouts are. For now, just know it is the same as per usual, if you try to build an NLP model and you are under-fitting, then decrease all these dropouts, if overfitting, then increase all these dropouts in roughly this ratio. Since this is such a recent paper so there is not a lot of guidance but these ratios worked well — it is what Stephen has been using as well.</li><li name="aaeb" id="aaeb" class="graf graf--li graf-after--li">There is another kind of way we can avoid overfitting that we will talk about in the last class. For now, <code class="markup--code markup--li-code">learner.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)</code> works reliably so all of your NLP models probably want this particular line.</li><li name="d1c1" id="d1c1" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">learner.clip=0.3</code>&nbsp;: when you look at your gradients and you multiply them by the learning rate to decide how much to update your weights by, this will not allow them be more than 0.3. This is a cool little trick to prevent us from taking too big of a step.</li><li name="a5b3" id="a5b3" class="graf graf--li graf-after--li">Details do not matter too much right now, so you can use them as they are.</li></ul><p name="a174" id="a174" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">Question</strong>: There are word embedding out there such as Word2vec or GloVe. How are they different from this? And why not initialize the weights with those initially? [<a href="https://youtu.be/gbceqO8PpBg?t=2h2m29s" data-href="https://youtu.be/gbceqO8PpBg?t=2h2m29s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">02:02:29</a>] People have pre-trained these embedding matrices before to do various other tasks. They are not called pre-trained models; they are just a pre-trained embedding matrix and you can download them. There is no reason we could not download them. I found that building a whole pre-trained model in this way did not seem to benefit much if at all from using pre-trained word vectors; where else using a whole pre-trained language model made a much bigger difference. Maybe we can combine both to make them a little better still.</p><p name="5c7d" id="5c7d" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question:</strong> What is the architecture of the model? [<a href="https://youtu.be/gbceqO8PpBg?t=2h3m55s" data-href="https://youtu.be/gbceqO8PpBg?t=2h3m55s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">02:03:55</a>] We will be learning about the model architecture in the last lesson but for now, it is a recurrent neural network using something called LSTM (Long Short Term Memory).</p><h4 name="7f08" id="7f08" class="graf graf--h4 graf-after--p">Fitting [<a href="https://youtu.be/gbceqO8PpBg?t=2h4m24s" data-href="https://youtu.be/gbceqO8PpBg?t=2h4m24s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">02:04:24</a>]</h4><pre name="9718" id="9718" class="graf graf--pre graf-after--h4">learner.fit(3e-3, 4, wds=1e-6, cycle_len=1, cycle_mult=2)</pre><pre name="d224" id="d224" class="graf graf--pre graf-after--pre">learner.save_encoder('adam1_enc')</pre><pre name="b966" id="b966" class="graf graf--pre graf-after--pre">learner.fit(3e-3, 4, wds=1e-6, cycle_len=10, <br>            cycle_save_name='adam3_10')</pre><pre name="adb9" id="adb9" class="graf graf--pre graf-after--pre">learner.save_encoder('adam3_10_enc')</pre><pre name="7db7" id="7db7" class="graf graf--pre graf-after--pre">learner.fit(3e-3, 1, wds=1e-6, cycle_len=20, <br>            cycle_save_name='adam3_20')</pre><pre name="d5e7" id="d5e7" class="graf graf--pre graf-after--pre">learner.load_cycle('adam3_20',0)</pre><p name="02dc" id="02dc" class="graf graf--p graf-after--pre">In the sentiment analysis section, we'll just need half of the language model - the <em class="markup--em markup--p-em">encoder</em>, so we save that part.</p><pre name="a328" id="a328" class="graf graf--pre graf-after--p">learner.save_encoder('adam3_20_enc')</pre><pre name="1b2b" id="1b2b" class="graf graf--pre graf-after--pre">learner.load_encoder('adam3_20_enc')</pre><p name="6316" id="6316" class="graf graf--p graf-after--pre">Language modeling accuracy is generally measured using the metric <em class="markup--em markup--p-em">perplexity</em>, which is simply <code class="markup--code markup--p-code">exp()</code> of the loss function we used.</p><pre name="e107" id="e107" class="graf graf--pre graf-after--p">math.exp(4.165)</pre><pre name="55e8" id="55e8" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">64.3926824434624</em></pre><pre name="8445" id="8445" class="graf graf--pre graf-after--pre">pickle.dump(TEXT, open(f'<strong class="markup--strong markup--pre-strong">{PATH}</strong>models/TEXT.pkl','wb'))</pre><h4 name="867e" id="867e" class="graf graf--h4 graf-after--pre">Testing [<a href="https://youtu.be/gbceqO8PpBg?t=2h4m53s" data-href="https://youtu.be/gbceqO8PpBg?t=2h4m53s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">02:04:53</a>]</h4><p name="d755" id="d755" class="graf graf--p graf-after--h4">We can play around with our language model a bit to check it seems to be working OK. First, let’s create a short bit of text to ‘prime’ a set of predictions. We’ll use our torchtext field to numericalize it so we can feed it to our language model.</p><pre name="bad0" id="bad0" class="graf graf--pre graf-after--p">m=learner.model<br>ss=""". So, it wasn't quite was I was expecting, but I really liked it anyway! The best"""</pre><pre name="ae3a" id="ae3a" class="graf graf--pre graf-after--pre">s = [spacy_tok(ss)]<br>t=TEXT.numericalize(s)<br>' '.join(s[0])</pre><pre name="fda1" id="fda1" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">". So , it was n't quite was I was expecting , but I really liked it anyway ! The best"</em></pre><p name="51e7" id="51e7" class="graf graf--p graf-after--pre">We haven’t yet added methods to make it easy to test a language model, so we’ll need to manually go through the steps.</p><pre name="a9d0" id="a9d0" class="graf graf--pre graf-after--p"><em class="markup--em markup--pre-em"># Set batch size to 1</em><br>m[0].bs=1<br><em class="markup--em markup--pre-em"># Turn off dropout</em><br>m.eval()<br><em class="markup--em markup--pre-em"># Reset hidden state</em><br>m.reset()<br><em class="markup--em markup--pre-em"># Get predictions from model</em><br>res,*_ = m(t)<br><em class="markup--em markup--pre-em"># Put the batch size back to what it was</em><br>m[0].bs=bs</pre><p name="0307" id="0307" class="graf graf--p graf-after--pre">Let’s see what the top 10 predictions were for the next word after our short text:</p><pre name="f112" id="f112" class="graf graf--pre graf-after--p">nexts = torch.topk(res[-1], 10)[1]<br>[TEXT.vocab.itos[o] <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> to_np(nexts)]</pre><pre name="a4fb" id="a4fb" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">['film',<br> 'movie',<br> 'of',<br> 'thing',<br> 'part',<br> '&lt;unk&gt;',<br> 'performance',<br> 'scene',<br> ',',<br> 'actor']</em></pre><p name="0dcb" id="0dcb" class="graf graf--p graf-after--pre">…and let’s see if our model can generate a bit more text all by itself!</p><pre name="bad2" id="bad2" class="graf graf--pre graf-after--p">print(ss,"<strong class="markup--strong markup--pre-strong">\n</strong>")<br><strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(50):<br>    n=res[-1].topk(2)[1]<br>    n = n[1] <strong class="markup--strong markup--pre-strong">if</strong> n.data[0]==0 <strong class="markup--strong markup--pre-strong">else</strong> n[0]<br>    print(TEXT.vocab.itos[n.data[0]], end=' ')<br>    res,*_ = m(n[0].unsqueeze(0))<br>print('...')</pre><pre name="d83c" id="d83c" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">. So, it wasn't quite was I was expecting, but I really liked it anyway! The best </em></pre><pre name="96fa" id="96fa" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">film ever ! &lt;eos&gt; i saw this movie at the toronto international film festival . i was very impressed . i was very impressed with the acting . i was very impressed with the acting . i was surprised to see that the actors were not in the movie . ...</em></pre><h4 name="4421" id="4421" class="graf graf--h4 graf-after--pre">Sentiment [<a href="https://youtu.be/gbceqO8PpBg?t=2h5m9s" data-href="https://youtu.be/gbceqO8PpBg?t=2h5m9s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">02:05:09</a>]</h4><p name="ec85" id="ec85" class="graf graf--p graf-after--h4">So we had pre-trained a language model and now we want to fine-tune it to do sentiment classification.</p><p name="5055" id="5055" class="graf graf--p graf-after--p">To use a pre-trained model, we will need to the saved vocab from the language model, since we need to ensure the same words map to the same IDs.</p><pre name="8a2c" id="8a2c" class="graf graf--pre graf-after--p">TEXT = pickle.load(open(f'<strong class="markup--strong markup--pre-strong">{PATH}</strong>models/TEXT.pkl','rb'))</pre><p name="bef2" id="bef2" class="graf graf--p graf-after--pre"><code class="markup--code markup--p-code">sequential=False</code> tells torchtext that a text field should be tokenized (in this case, we just want to store the 'positive' or 'negative' single label).</p><pre name="d5a9" id="d5a9" class="graf graf--pre graf-after--p">IMDB_LABEL = data.Field(sequential=<strong class="markup--strong markup--pre-strong">False</strong>)</pre><p name="54d5" id="54d5" class="graf graf--p graf-after--pre">This time, we need to not treat the whole thing as one big piece of text but every review is separate because each one has a different sentiment attached to it.</p><p name="9bcf" id="9bcf" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">splits</code> is a torchtext method that creates train, test, and validation sets. The IMDB dataset is built into torchtext, so we can take advantage of that. Take a look at <code class="markup--code markup--p-code">lang_model-arxiv.ipynb</code> to see how to define your own fastai/torchtext datasets.</p><pre name="357e" id="357e" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">splits</strong> = torchtext.datasets.IMDB.splits(TEXT, IMDB_LABEL, 'data/')</pre><pre name="2c7f" id="2c7f" class="graf graf--pre graf-after--pre">t = splits[0].examples[0]</pre><pre name="df60" id="df60" class="graf graf--pre graf-after--pre">t.label, ' '.join(t.text[:16])</pre><pre name="8db4" id="8db4" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">('pos', 'ashanti is a very 70s sort of film ( 1979 , to be precise ) .')</em></pre><p name="23d1" id="23d1" class="graf graf--p graf-after--pre">fastai can create a <code class="markup--code markup--p-code">ModelData</code> object directly from torchtext <code class="markup--code markup--p-code">splits</code>.</p><pre name="38e8" id="38e8" class="graf graf--pre graf-after--p">md2 = TextData.from_splits(PATH, splits, bs)</pre><p name="411b" id="411b" class="graf graf--p graf-after--pre">Now you can go ahead and call <code class="markup--code markup--p-code">get_model</code> that gets us our learner. Then we can load into it the pre-trained language model (<code class="markup--code markup--p-code">load_encoder</code>).</p><pre name="91ae" id="91ae" class="graf graf--pre graf-after--p">m3 = md2.get_model(opt_fn, 1500, bptt, emb_sz=em_sz, n_hid=nh, <br>                   n_layers=nl, dropout=0.1, dropouti=0.4,<br>                   wdrop=0.5, dropoute=0.05, dropouth=0.3)</pre><pre name="b3d4" id="b3d4" class="graf graf--pre graf-after--pre">m3.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)</pre><pre name="7618" id="7618" class="graf graf--pre graf-after--pre">m3.<strong class="markup--strong markup--pre-strong">load_encoder</strong>(f'adam3_20_enc')</pre><p name="4323" id="4323" class="graf graf--p graf-after--pre">Because we’re fine-tuning a pretrained model, we’ll use differential learning rates, and also increase the max gradient for clipping, to allow the SGDR to work better.</p><pre name="ae86" id="ae86" class="graf graf--pre graf-after--p">m3.clip=25.<br>lrs=np.array([1e-4,1e-3,1e-2])</pre><pre name="9193" id="9193" class="graf graf--pre graf-after--pre">m3.freeze_to(-1)<br>m3.fit(lrs/2, 1, metrics=[accuracy])<br>m3.unfreeze()<br>m3.fit(lrs, 1, metrics=[accuracy], cycle_len=1)</pre><pre name="e294" id="e294" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[ 0.       0.45074  0.28424  0.88458]</em></pre><pre name="9e68" id="9e68" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[ 0.       0.29202  0.19023  0.92768]</em></pre><p name="88c9" id="88c9" class="graf graf--p graf-after--pre">We make sure all except the last layer is frozen. Then we train a bit, unfreeze it, train it a bit. The nice thing is once you have got a pre-trained language model, it actually trains really fast.</p><pre name="7e3c" id="7e3c" class="graf graf--pre graf-after--p">m3.fit(lrs, 7, metrics=[accuracy], cycle_len=2, <br>       cycle_save_name='imdb2')</pre><pre name="3647" id="3647" class="graf graf--pre graf-after--pre">[ 0.       0.29053  0.18292  0.93241]                        <br>[ 1.       0.24058  0.18233  0.93313]                        <br>[ 2.       0.24244  0.17261  0.93714]                        <br>[ 3.       0.21166  0.17143  0.93866]                        <br>[ 4.       0.2062   0.17143  0.94042]                        <br>[ 5.       0.18951  0.16591  0.94083]                        <br>[ 6.       0.20527  0.16631  0.9393 ]                        <br>[ 7.       0.17372  0.16162  0.94159]                        <br>[ 8.       0.17434  0.17213  0.94063]                        <br>[ 9.       0.16285  0.16073  0.94311]                        <br>[ 10.        0.16327   0.17851   0.93998]                    <br>[ 11.        0.15795   0.16042   0.94267]                    <br>[ 12.        0.1602    0.16015   0.94199]                    <br>[ 13.        0.15503   0.1624    0.94171]</pre><pre name="d3c3" id="d3c3" class="graf graf--pre graf-after--pre">m3.load_cycle('imdb2', 4)</pre><pre name="1fd7" id="1fd7" class="graf graf--pre graf-after--pre">accuracy(*m3.predict_with_targs())</pre><pre name="319f" id="319f" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">0.94310897435897434</em></pre><p name="d74f" id="d74f" class="graf graf--p graf-after--pre">A recent paper from Bradbury et al, <a href="https://einstein.ai/research/learned-in-translation-contextualized-word-vectors" data-href="https://einstein.ai/research/learned-in-translation-contextualized-word-vectors" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Learned in translation: contextualized word vectors</a>, has a handy summary of the latest academic research in solving this IMDB sentiment analysis problem. Many of the latest algorithms shown are tuned for this specific problem.</p><figure name="1038" id="1038" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_PotEPJjvS-R4C5OCMbw7Vw.png"></figure><p name="5664" id="5664" class="graf graf--p graf-after--figure">As you see, we just got a new state of the art result in sentiment analysis, decreasing the error from 5.9% to 5.5%! You should be able to get similarly world-class results on other NLP classification problems using the same basic steps.</p><p name="465d" id="465d" class="graf graf--p graf-after--p">There are many opportunities to further improve this, although we won’t be able to get to them until part 2 of this course.</p><ul class="postList"><li name="35e2" id="35e2" class="graf graf--li graf-after--p">For example we could start training language models that look at lots of medical journals and then we could make a downloadable medical language model that then anybody could use to fine-tune on a prostate cancer subset of medical literature.</li><li name="b0f3" id="b0f3" class="graf graf--li graf-after--li">We could also combine this with pre-trained word vectors</li><li name="c752" id="c752" class="graf graf--li graf-after--li">We could have pre-trained a Wikipedia corpus language model and then fine-tuned it into an IMDB language model, and then fine-tune that into an IMDB sentiment analysis model and we would have gotten something better than this.</li></ul><p name="fa6b" id="fa6b" class="graf graf--p graf-after--li">There is a really fantastic researcher called Sebastian Ruder who is the only NLP researcher who has been really writing a lot about pre-training, fine-tuning, and transfer learning in NLP. Jeremy was asking him why this is not happening more, and his view was it is because there is not a software to make it easy. Hopefully Fast.ai will change that.</p><h4 name="96c3" id="96c3" class="graf graf--h4 graf-after--p">Collaborative Filtering Introduction [<a href="https://youtu.be/gbceqO8PpBg?t=2h11m38s" data-href="https://youtu.be/gbceqO8PpBg?t=2h11m38s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">02:11:38</a>]</h4><p name="36ff" id="36ff" class="graf graf--p graf-after--h4"><a href="https://github.com/fastai/fastai/blob/master/courses/dl1/lesson5-movielens.ipynb" data-href="https://github.com/fastai/fastai/blob/master/courses/dl1/lesson5-movielens.ipynb" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Notebook</a></p><p name="cd71" id="cd71" class="graf graf--p graf-after--p">Data available from <a href="http://files.grouplens.org/datasets/movielens/ml-latest-small.zip" data-href="http://files.grouplens.org/datasets/movielens/ml-latest-small.zip" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">http://files.grouplens.org/datasets/movielens/ml-latest-small.zip</a></p><pre name="a104" id="a104" class="graf graf--pre graf-after--p">path='data/ml-latest-small/'</pre><pre name="7106" id="7106" class="graf graf--pre graf-after--pre">ratings = pd.read_csv(path+'ratings.csv')<br>ratings.head()</pre><p name="9437" id="9437" class="graf graf--p graf-after--pre">The dataset looks like this:</p><figure name="9a3c" id="9a3c" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_Ev47i52AF-qIRHtYTOYm2Q.png"></figure><p name="7a15" id="7a15" class="graf graf--p graf-after--figure">It contains ratings by users. Our goal will be for some user-movie combination we have not seen before, we have to predict a rating.</p><pre name="73e0" id="73e0" class="graf graf--pre graf-after--p">movies = pd.read_csv(path+'movies.csv')<br>movies.head()</pre><figure name="8386" id="8386" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_cl9JWMSKPsrYf4hHsxNq-Q.png"></figure><p name="e3ee" id="e3ee" class="graf graf--p graf-after--figure">To make it more interesting, we will also actually download a list of movies so that we can interpret what is actually in these embedding matrices.</p><pre name="d248" id="d248" class="graf graf--pre graf-after--p">g=ratings.groupby('userId')['rating'].count()<br>topUsers=g.sort_values(ascending=False)[:15]</pre><pre name="d961" id="d961" class="graf graf--pre graf-after--pre">g=ratings.groupby('movieId')['rating'].count()<br>topMovies=g.sort_values(ascending=False)[:15]</pre><pre name="7360" id="7360" class="graf graf--pre graf-after--pre">top_r = ratings.join(topUsers, rsuffix='_r', how='inner', <br>                     on='userId')<br>top_r = top_r.join(topMovies, rsuffix='_r', how='inner', <br>                   on='movieId')</pre><pre name="9a6b" id="9a6b" class="graf graf--pre graf-after--pre">pd.crosstab(top_r.userId, top_r.movieId, top_r.rating, <br>            aggfunc=np.sum)</pre><figure name="6161" id="6161" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_f50pUlwGbsu85fVI-n9-MA.png"></figure><p name="15ce" id="15ce" class="graf graf--p graf-after--figure">This is what we are creating — this kind of cross tab of users by movies.</p><p name="c90d" id="c90d" class="graf graf--p graf-after--p graf--trailing">Feel free to look ahead and you will find that most of the steps are familiar to you already.</p><hr class="section-divider"><p name="ecc7" id="ecc7" class="graf graf--p graf--leading graf--trailing">Lessons: <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" class="markup--anchor markup--p-anchor" target="_blank">1</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" class="markup--anchor markup--p-anchor" target="_blank">2</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" class="markup--anchor markup--p-anchor" target="_blank">3</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" class="markup--anchor markup--p-anchor" target="_blank"><strong class="markup--strong markup--p-strong">4</strong></a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" class="markup--anchor markup--p-anchor" target="_blank">5</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" class="markup--anchor markup--p-anchor" target="_blank">6</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" class="markup--anchor markup--p-anchor" target="_blank">7</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" class="markup--anchor markup--p-anchor" target="_blank">8</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" class="markup--anchor markup--p-anchor" target="_blank">9</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" class="markup--anchor markup--p-anchor" target="_blank">10</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" class="markup--anchor markup--p-anchor" target="_blank">11</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" class="markup--anchor markup--p-anchor" target="_blank">12</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" class="markup--anchor markup--p-anchor" target="_blank">13</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" class="markup--anchor markup--p-anchor" target="_blank">14</a></p></body></html>