<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></head><body><pre name="757d" id="757d" class="graf graf--pre graf-after--pre">learn.model.eval()<br>preds = learn.model(VV(x[<strong class="markup--strong markup--pre-strong">None</strong>]))<br>x.shape,y.shape,preds.shape</pre><pre name="9293" id="9293" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">((3, 256, 256), (3, 256, 256), torch.Size([1, 3, 256, 256]))</em></pre><p name="597a" id="597a" class="graf graf--p graf-after--pre">At the end, I have my <code class="markup--code markup--p-code">sum_layers=False</code> so I can see what each part looks like and see they are balanced. And I can finally pop it out</p><pre name="31b9" id="31b9" class="graf graf--pre graf-after--p">learn.crit(preds, VV(y[<strong class="markup--strong markup--pre-strong">None</strong>]), sum_layers=<strong class="markup--strong markup--pre-strong">False</strong>)</pre><pre name="a7e5" id="a7e5" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[Variable containing:<br>  53.2221<br> [torch.cuda.FloatTensor of size 1 (GPU 0)], Variable containing:<br>  3.8336<br> [torch.cuda.FloatTensor of size 1 (GPU 0)], Variable containing:<br>  4.0612<br> [torch.cuda.FloatTensor of size 1 (GPU 0)], Variable containing:<br>  5.0639<br> [torch.cuda.FloatTensor of size 1 (GPU 0)], Variable containing:<br>  53.0019<br> [torch.cuda.FloatTensor of size 1 (GPU 0)]]</em></pre><pre name="c3c5" id="c3c5" class="graf graf--pre graf-after--pre">learn.crit.close()</pre><pre name="d2cd" id="d2cd" class="graf graf--pre graf-after--pre">_,axes=plt.subplots(1,2,figsize=(14,7))<br>show_img(x[<strong class="markup--strong markup--pre-strong">None</strong>], 0, ax=axes[0])<br>show_img(preds, 0, ax=axes[1])</pre><figure name="a174" id="a174" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_Hlb0cHXu_IdLZmfBxJIgJA.png"></figure><p name="232c" id="232c" class="graf graf--p graf-after--figure">So I mentioned that should be pretty easy and yet it took me about 4 days because I just found this incredibly fiddly to actually get it to work [<a href="https://youtu.be/nG3tT31nPmQ?t=1h24m26s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h24m26s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:24:26</a>]. So when I finally got up in the morning I said to Rachel “guess what, it trained correctly.” Rachel said “I never thought that was going to happen.” It just looked awful all the time and it’s really about getting the exact right mix of content loss and a style loss and the mix of the layers of the style loss. The worst part was it takes a really long time to train the darn CNN and I didn’t really know how long to train it before I decided it wasn’t doing well. Should I just train it for longer? And I don’t know all the little details didn’t seem to slightly change it but just it would totally fall apart all the time. So I kind of mentioned this partly to say just remember the final answer you see here is after me driving myself crazy all week of nearly always not working until finally the last minute it finally does. Even for things which just seemed like they couldn’t possibly be difficult because that is combining two things we already have working. The other is to be careful about how we interpret what authors claim.</p><figure name="e2ae" id="e2ae" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_6AvLQSM40JcTWE26C4GafA.png"></figure><p name="9e75" id="9e75" class="graf graf--p graf-after--figure">It was so fiddly getting this style transfer to work [<a href="https://youtu.be/nG3tT31nPmQ?t=1h26m10s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h26m10s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:26:10</a>]. After doing it, it left me thinking why did I bother because now I’ve got something that takes hours to create a network that can turn any kind of photo into one specific style. It just seems very unlikely I would want that for anything. The only reason I could think that being useful would be to do some art-y stuff on a video where I wanted to turn every frame into some style. It’s incredibly niche thing to want to do. But when I looked at the paper, the table is saying “oh, we are a thousand times faster than the Gatys’ approach which is just such an obviously meaningless thing to say. Such an incredibly misleading thing to say because it ignores all the hours of training for each individual style and I find this frustrating because groups like this Stanford group clearly know better or ought to know better, but still I guess the academic community encourages people to make these ridiculously grand claims. It also completely ignores this incredibly sensitive fiddly training process so this paper was just so well accepted when it came out. I remember everybody getting on Twitter and saying “wow, you know these Stanford people have found this way of doing style transfer a thousand times faster.” And clearly people saying this were top researchers in the field, clearly none of them actually understood it because nobody said “I don’t see why this is remotely useful, and also I tried it and it was incredibly fiddly to get it all to work.” It’s not until 18 months later I finally coming back to it and kind of thinking like “wait a minute, this is kind of stupid.” So this is the answer, I think, to the question of why haven’t people done follow ups on this to create really amazing best practices and better approaches like with a super resolution part of the paper. And I think the answer is because it’s dumb. So I think super resolution part of the paper is clearly not dumb. And it’s been improved and improved and now we have great super resolution. And I think we can derive from that great noise reduction, great colorization, great slant removal, great interactive artifact removal, etc. So I think there’s a lot of really cool techniques here. It’s also leveraging a lot of stuff that we’ve been learning and getting better and better at.</p><h3 name="90f0" id="90f0" class="graf graf--h3 graf-after--p">Segmentation [<a href="https://youtu.be/nG3tT31nPmQ?t=1h29m13s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h29m13s" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">1:29:13</a>]</h3><figure name="e8d3" id="e8d3" class="graf graf--figure graf-after--h3"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_6iq4f7DtfqsWxsE9ib0SPg.png"></figure><p name="2144" id="2144" class="graf graf--p graf-after--figure">Finally, let’s talk about segmentation. This is from the famous CamVid dataset which is a classic example of an academic segmentation dataset. Basically you can see what we do is we start with a picture (they are actually video frames in this dataset) and we have some labels where they are not actually colors — each one has an ID and the IDs are mapped to colors. So red might be 1, purple might be 2, light pink might be 3 and so all the buildings are one class, all the cars are another class, all the people are another class, all the road is another class, and so on. So what we are actually doing here is multi-class classification for every pixel. You can see, sometimes that multi-class classification really is quite tricky — like these branches. Although, sometimes the labels are really not that great. This is very coarse as you can see. So that’s what we are going to do.</p><p name="6e73" id="6e73" class="graf graf--p graf-after--p">We are going to do segmentation and so it’s a lot like bounding boxes. But rather than just finding a box around each thing, we are actually going to label every single pixel with its class. Really, it’s actually a lot easier because it fits our CNN style so nicely that we can create any CNN where the output is an N by M grid containing the integers from 0 to C where there are C categories. And then we can use cross-entropy loss with a softmax activation and we are done. I could actually stop the class there and you can go and use exactly the same approaches you’ve learnt in lesson 1 and 2 and you’ll get a perfectly okay result. So the first thing to say is this is not actually a terribly hard thing to do. But we are going to try and do it really well.</p><h4 name="6328" id="6328" class="graf graf--h4 graf-after--p">Doing it the simple way [<a href="https://youtu.be/nG3tT31nPmQ?t=1h31m26s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h31m26s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:31:26</a>]</h4><p name="58b7" id="58b7" class="graf graf--p graf-after--h4"><a href="https://github.com/fastai/fastai/blob/master/courses/dl2/carvana.ipynb" data-href="https://github.com/fastai/fastai/blob/master/courses/dl2/carvana.ipynb" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Notebook</a></p><p name="5576" id="5576" class="graf graf--p graf-after--p">Let’s start by doing it the really simple way. And we are going to use Kaggle <a href="https://www.kaggle.com/c/carvana-image-masking-challenge" data-href="https://www.kaggle.com/c/carvana-image-masking-challenge" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Carvana</a> competition and you can download it with Kaggle API as usual.</p><pre name="10c5" id="10c5" class="graf graf--pre graf-after--p">%matplotlib inline<br>%reload_ext autoreload<br>%autoreload 2</pre><pre name="c2dc" id="c2dc" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">fastai.conv_learner</strong> <strong class="markup--strong markup--pre-strong">import</strong> *<br><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">fastai.dataset</strong> <strong class="markup--strong markup--pre-strong">import</strong> *<br><br><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">pathlib</strong> <strong class="markup--strong markup--pre-strong">import</strong> Path<br><strong class="markup--strong markup--pre-strong">import</strong> <strong class="markup--strong markup--pre-strong">json</strong></pre><h4 name="381e" id="381e" class="graf graf--h4 graf-after--pre">Setup</h4><p name="f902" id="f902" class="graf graf--p graf-after--h4">There is a train folder containing bunch of images which is the independent variable and a train_masks folder there’s the dependent variable and they look like below.</p><figure name="84f3" id="84f3" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_3lO9olOrpojAt5C22B5CoQ.png"></figure><p name="78b2" id="78b2" class="graf graf--p graf-after--figure">In this case, just like cats and dogs, we are going simple rather than doing multi-class classification, we are going to do binary classification. But of course multi-class is just the more general version — categorical cross entropy or binary class entropy. There is no differences conceptually, so the dependent variable is just zeros and ones, where else the independent variable is a regular image.</p><p name="da00" id="da00" class="graf graf--p graf-after--p">In order to do this well, it would really help to know what cars look like. Because really what we want to do is to figure out this is a car and its orientation and put white pixels where we expect the car to be based on the picture and their understanding of what cars look like.</p><pre name="7a38" id="7a38" class="graf graf--pre graf-after--p">PATH = Path('data/carvana')<br>list(PATH.iterdir())</pre><pre name="38eb" id="38eb" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[PosixPath('data/carvana/train_masks.csv'),<br> PosixPath('data/carvana/train_masks-128'),<br> PosixPath('data/carvana/sample_submission.csv'),<br> PosixPath('data/carvana/train_masks_png'),<br> PosixPath('data/carvana/train.csv'),<br> PosixPath('data/carvana/train-128'),<br> PosixPath('data/carvana/train'),<br> PosixPath('data/carvana/metadata.csv'),<br> PosixPath('data/carvana/tmp'),<br> PosixPath('data/carvana/models'),<br> PosixPath('data/carvana/train_masks')]</em></pre><pre name="826c" id="826c" class="graf graf--pre graf-after--pre">MASKS_FN = 'train_masks.csv'<br>META_FN = 'metadata.csv'<br>TRAIN_DN = 'train'<br>MASKS_DN = 'train_masks'</pre><pre name="6d64" id="6d64" class="graf graf--pre graf-after--pre">masks_csv = pd.read_csv(PATH/MASKS_FN)<br>masks_csv.head()</pre><figure name="0315" id="0315" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_gSfi6-hcJG8YP3knRDQUAg.png"></figure><p name="174c" id="174c" class="graf graf--p graf-after--figure">The original dataset came with these CSV files as well [<a href="https://youtu.be/nG3tT31nPmQ?t=1h32m44s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h32m44s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:32:44</a>]. I don’t really use them for very much other than getting the list of images from them.</p><pre name="f2cd" id="f2cd" class="graf graf--pre graf-after--p">meta_csv = pd.read_csv(PATH/META_FN)<br>meta_csv.head()</pre><figure name="8173" id="8173" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_yBm5uRoK_sHQrS-ddATg6w.png"></figure><pre name="e876" id="e876" class="graf graf--pre graf-after--figure"><strong class="markup--strong markup--pre-strong">def</strong> show_img(im, figsize=<strong class="markup--strong markup--pre-strong">None</strong>, ax=<strong class="markup--strong markup--pre-strong">None</strong>, alpha=<strong class="markup--strong markup--pre-strong">None</strong>):<br>    <strong class="markup--strong markup--pre-strong">if</strong> <strong class="markup--strong markup--pre-strong">not</strong> ax: fig,ax = plt.subplots(figsize=figsize)<br>    ax.imshow(im, alpha=alpha)<br>    ax.set_axis_off()<br>    <strong class="markup--strong markup--pre-strong">return</strong> ax</pre><pre name="98c4" id="98c4" class="graf graf--pre graf-after--pre">CAR_ID = '00087a6bd4dc'</pre><pre name="b065" id="b065" class="graf graf--pre graf-after--pre">list((PATH/TRAIN_DN).iterdir())[:5]</pre><pre name="38b6" id="38b6" class="graf graf--pre graf-after--pre">[PosixPath('data/carvana/train/5ab34f0e3ea5_15.jpg'),<br> PosixPath('data/carvana/train/de3ca5ec1e59_07.jpg'),<br> PosixPath('data/carvana/train/28d9a149cb02_13.jpg'),<br> PosixPath('data/carvana/train/36a3f7f77e85_12.jpg'),<br> PosixPath('data/carvana/train/843763f47895_08.jpg')]</pre><pre name="1eb0" id="1eb0" class="graf graf--pre graf-after--pre">Image.open(PATH/TRAIN_DN/f'<strong class="markup--strong markup--pre-strong">{CAR_ID}</strong>_01.jpg').resize((300,200))</pre><figure name="b51b" id="b51b" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_KwDMh55dYxo5-ychRK2srQ.png"></figure><pre name="f946" id="f946" class="graf graf--pre graf-after--figure">list((PATH/MASKS_DN).iterdir())[:5]</pre><pre name="0341" id="0341" class="graf graf--pre graf-after--pre">[PosixPath('data/carvana/train_masks/6c0cd487abcd_03_mask.gif'),<br> PosixPath('data/carvana/train_masks/351c583eabd6_01_mask.gif'),<br> PosixPath('data/carvana/train_masks/90fdd8932877_02_mask.gif'),<br> PosixPath('data/carvana/train_masks/28d9a149cb02_10_mask.gif'),<br> PosixPath('data/carvana/train_masks/88bc32b9e1d9_14_mask.gif')]</pre><pre name="3c07" id="3c07" class="graf graf--pre graf-after--pre">Image.open(PATH/MASKS_DN/f'<strong class="markup--strong markup--pre-strong">{CAR_ID}</strong>_01_mask.gif').resize((300,200))</pre><figure name="1e7c" id="1e7c" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_zz4dhkhhF00W9VIQQiiGcg.png"></figure><p name="388e" id="388e" class="graf graf--p graf-after--figure">Each image after the car ID has a 01, 02, etc of which I’ve printed out all 16 of them for one car and as you can see basically those numbers are the 16 orientations of one car [<a href="https://youtu.be/nG3tT31nPmQ?t=1h32m58s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h32m58s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:32:58</a>]. I don’t think anybody in this competition actually used these orientation information. I believe they all kept the car’s images just treated them separately.</p><pre name="d3fa" id="d3fa" class="graf graf--pre graf-after--p">ims = [open_image(PATH/TRAIN_DN/f'<strong class="markup--strong markup--pre-strong">{CAR_ID}</strong>_{i+1:02d}.jpg') <br>          <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(16)]</pre><pre name="58e5" id="58e5" class="graf graf--pre graf-after--pre">fig, axes = plt.subplots(4, 4, figsize=(9, 6))<br><strong class="markup--strong markup--pre-strong">for</strong> i,ax <strong class="markup--strong markup--pre-strong">in</strong> enumerate(axes.flat): show_img(ims[i], ax=ax)<br>plt.tight_layout(pad=0.1)</pre><figure name="c9fd" id="c9fd" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_ran3w5qDWsvfVaoPileaxw.png"></figure><h4 name="b9a4" id="b9a4" class="graf graf--h4 graf-after--figure">Resize and convert [<a href="https://youtu.be/nG3tT31nPmQ?t=1h33m27s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h33m27s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:33:27</a>]</h4><p name="9975" id="9975" class="graf graf--p graf-after--h4">These images are pretty big — over 1000 by 1000 in size and just opening the JPEGs and resizing them is slow. So I processed them all. Also OpenCV can’t handle GIF files so I converted them.</p><p name="f033" id="f033" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: How would somebody get these masks for training initially? <a href="https://www.mturk.com/" data-href="https://www.mturk.com/" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Mechanical turk</a> or something [<a href="https://youtu.be/nG3tT31nPmQ?t=1h33m48s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h33m48s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:33:48</a>]? Yeah, just a lot of boring work. Probably there are some tools that help you with a bit of edge snapping so that the human can do it roughly and then just fine tune the bits it gets wrong. These kinds of labels are expensive. So one of the things I really want to work on is deep learning enhanced interactive labeling tools because that’s clearly something that would help a lot of people.</p><p name="e0f5" id="e0f5" class="graf graf--p graf-after--p">I’ve got a little section here that you can run if you want to. You probably want to. It converts the GIFs into PNGs so just open int up with PIL and then save it as PNG because OpenCV doesn’t have GIF support. As per usual for this kind of stuff, I do it with a ThreadPool so I can take advantage of parallel processing. And then also create a separate directory <code class="markup--code markup--p-code">train-128</code> and <code class="markup--code markup--p-code">train_masks-128</code> which contains the 128 by 128 resized versions of them.</p><p name="ca7c" id="ca7c" class="graf graf--p graf-after--p">This is the kind of stuff that keeps you sane if you do it early in the process. So anytime you get a new dataset, seriously think about creating a smaller version to make life fast. Anytime you find yourself waiting on your computer, try and think of a way to create a smaller version.</p><pre name="d7eb" id="d7eb" class="graf graf--pre graf-after--p">(PATH/'train_masks_png').mkdir(exist_ok=<strong class="markup--strong markup--pre-strong">True</strong>)</pre><pre name="49bf" id="49bf" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> convert_img(fn):<br>    fn = fn.name<br>    Image.open(PATH/'train_masks'/fn).save(PATH/'train_masks_png'/<br>                     f'<strong class="markup--strong markup--pre-strong">{fn[:-4]}</strong>.png')</pre><pre name="6a95" id="6a95" class="graf graf--pre graf-after--pre">files = list((PATH/'train_masks').iterdir())<br><strong class="markup--strong markup--pre-strong">with</strong> ThreadPoolExecutor(8) <strong class="markup--strong markup--pre-strong">as</strong> e: e.map(convert_img, files)</pre><pre name="42f4" id="42f4" class="graf graf--pre graf-after--pre">(PATH/'train_masks-128').mkdir(exist_ok=<strong class="markup--strong markup--pre-strong">True</strong>)</pre><pre name="9b6b" id="9b6b" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> resize_mask(fn):<br>    Image.open(fn).resize((128,128)).save((fn.parent.parent)<br>        /'train_masks-128'/fn.name)<br><br>files = list((PATH/'train_masks_png').iterdir())<br><strong class="markup--strong markup--pre-strong">with</strong> ThreadPoolExecutor(8) <strong class="markup--strong markup--pre-strong">as</strong> e: e.map(resize_img, files)</pre><pre name="0b2b" id="0b2b" class="graf graf--pre graf-after--pre">(PATH/'train-128').mkdir(exist_ok=<strong class="markup--strong markup--pre-strong">True</strong>)</pre><pre name="eb5f" id="eb5f" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> resize_img(fn):<br>    Image.open(fn).resize((128,128)).save((fn.parent.parent)<br>         /'train-128'/fn.name)<br><br>files = list((PATH/'train').iterdir())<br><strong class="markup--strong markup--pre-strong">with</strong> ThreadPoolExecutor(8) <strong class="markup--strong markup--pre-strong">as</strong> e: e.map(resize_img, files)</pre><p name="c665" id="c665" class="graf graf--p graf-after--pre">So after you grab it from Kaggle, you probably want to run this stuff, go away, have lunch, come back and when you are done, you’ll have these smaller directories which we are going to use below 128 by 128 to start with.</p><h4 name="b90f" id="b90f" class="graf graf--h4 graf-after--p">Dataset [<a href="https://youtu.be/nG3tT31nPmQ?t=1h35m33s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h35m33s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:35:33</a>]</h4><pre name="30b1" id="30b1" class="graf graf--pre graf-after--h4">TRAIN_DN = 'train-128'<br>MASKS_DN = 'train_masks-128'<br>sz = 128<br>bs = 64</pre><pre name="6925" id="6925" class="graf graf--pre graf-after--pre">ims = [open_image(PATH/TRAIN_DN<br>            /f'<strong class="markup--strong markup--pre-strong">{CAR_ID}</strong>_{i+1:02d}.jpg') <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(16)]<br>im_masks = [open_image(PATH/MASKS_DN<br>            /f'<strong class="markup--strong markup--pre-strong">{CAR_ID}</strong>_{i+1:02d}_mask.png') <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(16)]</pre><p name="d0be" id="d0be" class="graf graf--p graf-after--pre">So here is a cool trick. If you use the same axis object (<code class="markup--code markup--p-code">ax</code>) to plot an image twice and the second time you use alpha which you might know means transparency in the computer vision world, then you can actually plot the mask over the top of the photo. So here is a nice way to see all the masks on top of the photos for all of the cars in one group.</p><pre name="9454" id="9454" class="graf graf--pre graf-after--p">fig, axes = plt.subplots(4, 4, figsize=(9, 6))<br><strong class="markup--strong markup--pre-strong">for</strong> i,ax <strong class="markup--strong markup--pre-strong">in</strong> enumerate(axes.flat):<br>    ax = show_img(ims[i], ax=ax)<br>    show_img(im_masks[i][...,0], ax=ax, alpha=0.5)<br>plt.tight_layout(pad=0.1)</pre><figure name="a8ff" id="a8ff" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_KuD6ZEQEbZH8Ka3u8tP5Ig.png"></figure><p name="e562" id="e562" class="graf graf--p graf-after--figure">This is the same MatchedFilesDataset we’ve seen twice already. This is all the same code. Here is something important though. If we had something that was in the training set the one on the left, and then the validation had the image on the right, that would be kind of cheating because it’s the same car.</p><figure name="3af9" id="3af9" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*vUDf60cZZxP3gezhcP36-w.png" data-width="260" data-height="99" src="../img/1_vUDf60cZZxP3gezhcP36-w.png"></figure><pre name="74e2" id="74e2" class="graf graf--pre graf-after--figure"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">MatchedFilesDataset</strong>(FilesDataset):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, fnames, y, transform, path):<br>        self.y=y<br>        <strong class="markup--strong markup--pre-strong">assert</strong>(len(fnames)==len(y))<br>        super().__init__(fnames, transform, path)<br>    <strong class="markup--strong markup--pre-strong">def</strong> get_y(self, i): <br>        <strong class="markup--strong markup--pre-strong">return</strong> open_image(os.path.join(self.path, self.y[i]))<br>    <strong class="markup--strong markup--pre-strong">def</strong> get_c(self): <strong class="markup--strong markup--pre-strong">return</strong> 0</pre><pre name="882e" id="882e" class="graf graf--pre graf-after--pre">x_names = np.array([Path(TRAIN_DN)/o <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> masks_csv['img']])<br>y_names = np.array([Path(MASKS_DN)/f'<strong class="markup--strong markup--pre-strong">{o[:-4]}</strong>_mask.png' <br>                       <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> masks_csv['img']])</pre><pre name="4b02" id="4b02" class="graf graf--pre graf-after--pre">len(x_names)//16//5*16</pre><pre name="4e18" id="4e18" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">1008</em></pre><p name="1159" id="1159" class="graf graf--p graf-after--pre">So we use a continuous set of car IDs and since each set is a set of 16, we make sure that’s evenly divisible by 16. So we make sure that our validation set contains different car IDs to our training set. This is the kind of stuff which you’ve got to be careful of. On Kaggle, it’s not so bad — you’ll know about it because you’ll submit your result and you’ll get a very different result on your leaderboard compared to your validation set. But in the real world. you won’t know until you put it in production and send your company bankrupt and lose your job. So you might want to think carefully about your validation set in that case.</p><pre name="c79e" id="c79e" class="graf graf--pre graf-after--p">val_idxs = list(range(1008))<br>((val_x,trn_x),(val_y,trn_y)) = split_by_idx(val_idxs, x_names, <br>                                              y_names)<br>len(val_x),len(trn_x)</pre><pre name="45fc" id="45fc" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(1008, 4080)</em></pre><p name="ed3d" id="ed3d" class="graf graf--p graf-after--pre">Here we are going to use transform type classification (<code class="markup--code markup--p-code">TfmType.CLASS</code>) [<a href="https://youtu.be/nG3tT31nPmQ?t=1h37m3s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h37m3s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:37:03</a>]. It’s basically the same as transform type pixel (<code class="markup--code markup--p-code">TfmType.PIXEL</code>) but if you think about it, with a pixel version if we rotate a little bit then we probably want to average the pixels in between the two, but the classification, obviously we don’t. We use nearest neighbor. So there’s slight difference there. Also for classification, lighting doesn’t kick in, normalization doesn’t kick in to the dependent variable.</p><pre name="42cd" id="42cd" class="graf graf--pre graf-after--p">aug_tfms = [RandomRotate(4, tfm_y=TfmType.CLASS),<br>            RandomFlip(tfm_y=TfmType.CLASS),<br>            RandomLighting(0.05, 0.05)]<br><em class="markup--em markup--pre-em"># aug_tfms = []</em></pre><p name="14f9" id="14f9" class="graf graf--p graf-after--pre">They are already square images, so we don’t have to do any cropping.</p><pre name="6e78" id="6e78" class="graf graf--pre graf-after--p">tfms = tfms_from_model(resnet34, sz, crop_type=CropType.NO, tfm_y=TfmType.CLASS, aug_tfms=aug_tfms)<br>datasets = ImageData.get_ds(MatchedFilesDataset, (trn_x,trn_y), (val_x,val_y), tfms, path=PATH)<br>md = ImageData(PATH, datasets, bs, num_workers=8, classes=<strong class="markup--strong markup--pre-strong">None</strong>)</pre><pre name="f384" id="f384" class="graf graf--pre graf-after--pre">denorm = md.trn_ds.denorm<br>x,y = next(iter(md.aug_dl))<br>x = denorm(x)</pre><p name="8f3d" id="8f3d" class="graf graf--p graf-after--pre">So here you can see different versions of the augmented images — they are moving around a bit, and they are rotating a bit, and so forth.</p><pre name="6cf7" id="6cf7" class="graf graf--pre graf-after--p">fig, axes = plt.subplots(5, 6, figsize=(12, 10))<br><strong class="markup--strong markup--pre-strong">for</strong> i,ax <strong class="markup--strong markup--pre-strong">in</strong> enumerate(axes.flat):<br>    ax=show_img(x[i], ax=ax)<br>    show_img(y[i], ax=ax, alpha=0.5)<br>plt.tight_layout(pad=0.1)</pre><figure name="0886" id="0886" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_ak5NPATO_ayUjsc7UGfEdQ.png"></figure><p name="d0df" id="d0df" class="graf graf--p graf-after--figure">I get a lot of questions during our study group about how do I debug things and fix things that aren’t working. I never have a great answer other than every time I fix a problem is because of stuff like this that I do all the time. I just always print out everything as I go and then the one thing that I screw up always turns out to be the one thing that I forgot to check along the way. The more of this kind of thing you can do the better. If you are not looking at all of your intermediate results, you are going to have troubles.</p><h4 name="e7bd" id="e7bd" class="graf graf--h4 graf-after--p">Model [<a href="https://youtu.be/nG3tT31nPmQ?t=1h38m30s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h38m30s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:38:30</a>]</h4><pre name="c864" id="c864" class="graf graf--pre graf-after--h4"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">Empty</strong>(nn.Module): <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self,x): <strong class="markup--strong markup--pre-strong">return</strong> x<br><br>models = ConvnetBuilder(resnet34, 0, 0, 0, custom_head=Empty())<br>learn = ConvLearner(md, models)<br>learn.summary()</pre><pre name="7b45" id="7b45" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">StdUpsample</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, nin, nout):<br>        super().__init__()<br>        self.conv = nn.ConvTranspose2d(nin, nout, 2, stride=2)<br>        self.bn = nn.BatchNorm2d(nout)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x): <strong class="markup--strong markup--pre-strong">return</strong> self.bn(F.relu(self.conv(x)))</pre><pre name="93cc" id="93cc" class="graf graf--pre graf-after--pre">flatten_channel = Lambda(<strong class="markup--strong markup--pre-strong">lambda</strong> x: x[:,0])</pre><pre name="913a" id="913a" class="graf graf--pre graf-after--pre">simple_up = nn.Sequential(<br>    nn.ReLU(),<br>    StdUpsample(512,256),<br>    StdUpsample(256,256),<br>    StdUpsample(256,256),<br>    StdUpsample(256,256),<br>    nn.ConvTranspose2d(256, 1, 2, stride=2),<br>    flatten_channel<br>)</pre><p name="1815" id="1815" class="graf graf--p graf-after--pre">Given that we want something that knows what cars look like, we probably want to start with a pre-trained ImageNet network. So we are going to start with ResNet34. With <code class="markup--code markup--p-code">ConvnetBuilder</code>, we can grab our ResNet34 and we can add a custom head. The custom head is going to be something that upsamples a bunch of times and we are going to do things really dumb for now which is we’re just going to do a ConvTranspose2d, batch norm, ReLU.</p><p name="6fbe" id="6fbe" class="graf graf--p graf-after--p">This is what I am saying — any of you could have built this without looking at any of this notebook or at least you have the information from previous classes. There is nothing new at all. So at the very end, we have a single filter. Now that’s going to give us something which is batch size by 1 by 128 by 128. But we want something which is batch size by 128 by 128. So we have to remove that unit axis so I’ve got a lambda layer here. Lambda layers are incredibly helpful because without the lambda layer here, which is simply removing that unit axis by just indexing it with a 0, without a lambda layer, I would have to have created a custom class with a custom forward method and so forth. But by creating a lambda layer that does the one custom bit, I can now just chuck it in the Sequential and so that makes life easier.</p><p name="e580" id="e580" class="graf graf--p graf-after--p">PyTorch people are kind of snooty about this approach. Lambda layer is actually something that’s a part of the fastai library not part of the PyTorch library. And literally people on PyTorch discussion board say “yes, we could give people this”, “yes it is only a single line of code” but they never encourage them to use sequential too often. So there you go.</p><p name="8d82" id="8d82" class="graf graf--p graf-after--p">So this is our custom head [<a href="https://youtu.be/nG3tT31nPmQ?t=1h40m36s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h40m36s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:40:36</a>]. So we are going to have a ResNet 34 that goes downsample and then a really simple custom head that very quickly upsamples, and that hopefully will do something. And we are going to use accuracy with a threshold of 0.5 and print out metrics.</p><pre name="6a44" id="6a44" class="graf graf--pre graf-after--p">models = ConvnetBuilder(resnet34, 0, 0, 0, custom_head=simple_up)<br>learn = ConvLearner(md, models)<br>learn.opt_fn=optim.Adam<br>learn.crit=nn.BCEWithLogitsLoss()<br>learn.metrics=[accuracy_thresh(0.5)]</pre><pre name="71c6" id="71c6" class="graf graf--pre graf-after--pre">learn.lr_find()<br>learn.sched.plot()</pre><pre name="774d" id="774d" class="graf graf--pre graf-after--pre">94%|█████████▍| 30/32 [00:05&lt;00:00,  5.48it/s, loss=10.6]</pre><figure name="04ea" id="04ea" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_0RoKSchCdIyFHGVjb7PXHA.png"></figure><pre name="dba8" id="dba8" class="graf graf--pre graf-after--figure">lr=4e-2</pre><pre name="3d50" id="3d50" class="graf graf--pre graf-after--pre">learn.fit(lr,1,cycle_len=5,use_clr=(20,5))</pre><pre name="e8fb" id="e8fb" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss   &lt;lambda&gt;                  <br>    0      0.124078   0.133566   0.945951  <br>    1      0.111241   0.112318   0.954912                  <br>    2      0.099743   0.09817    0.957507                   <br>    3      0.090651   0.092375   0.958117                   <br>    4      0.084031   0.086026   0.963243</em></pre><pre name="b542" id="b542" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[0.086025625, 0.96324310824275017]</em></pre><p name="1fbe" id="1fbe" class="graf graf--p graf-after--pre">After a few epochs, we’ve got 96 percent accurate. Is that good [<a href="https://youtu.be/nG3tT31nPmQ?t=1h40m56s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h40m56s" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">1:40:56</a>]? Is 96% accurate good? And hopefully the answer to that question is it depends. What’s it for? The answer is Carvana wanted this because they wanted to be able to take their car image and cut them out and paste them on exotic Monte Carlo backgrounds or whatever (that’s Monte Carlo the place and not the simulation). To do that, you you need a really good mask. You don’t want to leave the rearview mirrors behind, have one wheel missing, or include a little bit of background or something. That would look stupid. So you would need something very good. So only having 96% of the pixels correct doesn’t sound great. But we won’t really know until we look at it. So let’s look at it.</p><pre name="e43b" id="e43b" class="graf graf--pre graf-after--p">learn.save('tmp')</pre><pre name="8a62" id="8a62" class="graf graf--pre graf-after--pre">learn.load('tmp')</pre><pre name="9fd7" id="9fd7" class="graf graf--pre graf-after--pre">py,ay = learn.predict_with_targs()</pre><pre name="bae3" id="bae3" class="graf graf--pre graf-after--pre">ay.shape</pre><pre name="ec1b" id="ec1b" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(1008, 128, 128)</em></pre><p name="cd51" id="cd51" class="graf graf--p graf-after--pre">So there is the correct version that we want to cut out [<a href="https://youtu.be/nG3tT31nPmQ?t=1h41m54s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h41m54s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:41:54</a>]</p><pre name="0abd" id="0abd" class="graf graf--pre graf-after--p">show_img(ay[0]);</pre><figure name="1711" id="1711" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_ZuW4s04Ubneh3fFjQuQ3rQ.png"></figure><p name="6fbf" id="6fbf" class="graf graf--p graf-after--figure">That’s the 96% accurate version. So when you look at it you realize “oh yeah, getting 96% of the pixel accurate is actually easy because all the outside bit is not car, and all the inside bit is a car, and really interesting bit is the edge. So we need to do better.</p><pre name="5078" id="5078" class="graf graf--pre graf-after--p">show_img(py[0]&gt;0);</pre><figure name="3117" id="3117" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_cp-SvDXQPGdN6k-8JL2CMw.png"></figure><p name="11f5" id="11f5" class="graf graf--p graf-after--figure">Let’s unfreeze because all we’ve done so far is train the custom head. Let’s do more.</p><pre name="3d63" id="3d63" class="graf graf--pre graf-after--p">learn.unfreeze()</pre><pre name="d43c" id="d43c" class="graf graf--pre graf-after--pre">learn.bn_freeze(<strong class="markup--strong markup--pre-strong">True</strong>)</pre><pre name="3e0b" id="3e0b" class="graf graf--pre graf-after--pre">lrs = np.array([lr/100,lr/10,lr])/4</pre><pre name="4cf0" id="4cf0" class="graf graf--pre graf-after--pre">learn.fit(lrs,1,cycle_len=20,use_clr=(20,10))</pre><pre name="09a7" id="09a7" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss   &lt;lambda&gt;                   <br>    0      0.06577    0.053292   0.972977  <br>    1      0.049475   0.043025   0.982559                   <br>    2      0.039146   0.035927   0.98337                    <br>    3      0.03405    0.031903   0.986982                   <br>    4      0.029788   0.029065   0.987944                   <br>    5      0.027374   0.027752   0.988029                   <br>    6      0.026041   0.026718   0.988226                   <br>    7      0.024302   0.025927   0.989512                   <br>    8      0.022921   0.026102   0.988276                   <br>    9      0.021944   0.024714   0.989537                   <br>    10     0.021135   0.0241     0.990628                   <br>    11     0.020494   0.023367   0.990652                   <br>    12     0.01988    0.022961   0.990989                   <br>    13     0.019241   0.022498   0.991014                   <br>    14     0.018697   0.022492   0.990571                   <br>    15     0.01812    0.021771   0.99105                    <br>    16     0.017597   0.02183    0.991365                   <br>    17     0.017192   0.021434   0.991364                   <br>    18     0.016768   0.021383   0.991643                   <br>    19     0.016418   0.021114   0.99173</em></pre><pre name="7898" id="7898" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[0.021113895, 0.99172959849238396]</em></pre><p name="62e3" id="62e3" class="graf graf--p graf-after--pre">After a bit more, we’ve got 99.1%. Is that good? I don’t know. Let’s take a look.</p><pre name="c1cd" id="c1cd" class="graf graf--pre graf-after--p">learn.save('0')</pre><pre name="a8c0" id="a8c0" class="graf graf--pre graf-after--pre">x,y = next(iter(md.val_dl))<br>py = to_np(learn.model(V(x)))</pre><p name="49a7" id="49a7" class="graf graf--p graf-after--pre">Actually no. It’s totally missed the rearview vision mirror on the left and missed a lot of it on the right. And it’s clearly got an edge wrong on the bottom. And these things are totally going to matter when we try to cut it out, so it’s still not good enough.</p><pre name="a270" id="a270" class="graf graf--pre graf-after--p">ax = show_img(denorm(x)[0])<br>show_img(py[0]&gt;0, ax=ax, alpha=0.5);</pre><figure name="db2a" id="db2a" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_b4NbyzWojBS_6peHalw3tQ.png"></figure><pre name="5221" id="5221" class="graf graf--pre graf-after--figure">ax = show_img(denorm(x)[0])<br>show_img(y[0], ax=ax, alpha=0.5);</pre><figure name="b23b" id="b23b" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_nh7F97XxSE1ZOcleTfoPeA.png"></figure><h4 name="3fed" id="3fed" class="graf graf--h4 graf-after--figure">512x512 [<a href="https://youtu.be/nG3tT31nPmQ?t=1h42m50s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h42m50s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:42:50</a>]</h4><p name="c002" id="c002" class="graf graf--p graf-after--h4">Let’s try upscaling. And the nice thing is that when we upscale to 512 by 512, (make sure you decrease the batch size because you’ll run out of memory), it’s quite a lot more information there for it to go on so our accuracy increases to 99.4% and things keep getting better.</p><pre name="406f" id="406f" class="graf graf--pre graf-after--p">TRAIN_DN = 'train'<br>MASKS_DN = 'train_masks_png'<br>sz = 512<br>bs = 16</pre><pre name="bae2" id="bae2" class="graf graf--pre graf-after--pre">x_names = np.array([Path(TRAIN_DN)/o <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> masks_csv['img']])<br>y_names = np.array([Path(MASKS_DN)/f'<strong class="markup--strong markup--pre-strong">{o[:-4]}</strong>_mask.png' <br>                      <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> masks_csv['img']])</pre><pre name="821b" id="821b" class="graf graf--pre graf-after--pre">((val_x,trn_x),(val_y,trn_y)) = split_by_idx(val_idxs, x_names, <br>                                      y_names)<br>len(val_x),len(trn_x)</pre><pre name="7772" id="7772" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(1008, 4080)</em></pre><pre name="25db" id="25db" class="graf graf--pre graf-after--pre">tfms = tfms_from_model(resnet34, sz, crop_type=CropType.NO,<br>                         tfm_y=TfmType.CLASS, aug_tfms=aug_tfms)<br>datasets = ImageData.get_ds(MatchedFilesDataset, (trn_x,trn_y),<br>                      (val_x,val_y), tfms, path=PATH)<br>md = ImageData(PATH, datasets, bs, num_workers=8, classes=<strong class="markup--strong markup--pre-strong">None</strong>)</pre><pre name="859d" id="859d" class="graf graf--pre graf-after--pre">denorm = md.trn_ds.denorm<br>x,y = next(iter(md.aug_dl))<br>x = denorm(x)</pre><p name="9176" id="9176" class="graf graf--p graf-after--pre">Here is the true ones.</p><pre name="6fee" id="6fee" class="graf graf--pre graf-after--p">fig, axes = plt.subplots(4, 4, figsize=(10, 10))<br><strong class="markup--strong markup--pre-strong">for</strong> i,ax <strong class="markup--strong markup--pre-strong">in</strong> enumerate(axes.flat):<br>    ax=show_img(x[i], ax=ax)<br>    show_img(y[i], ax=ax, alpha=0.5)<br>plt.tight_layout(pad=0.1)</pre><figure name="45ad" id="45ad" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_viBgn7WA9biBQ6BkzSEEnw.png"></figure><pre name="afad" id="afad" class="graf graf--pre graf-after--figure">simple_up = nn.Sequential(<br>    nn.ReLU(),<br>    StdUpsample(512,256),<br>    StdUpsample(256,256),<br>    StdUpsample(256,256),<br>    StdUpsample(256,256),<br>    nn.ConvTranspose2d(256, 1, 2, stride=2),<br>    flatten_channel<br>)</pre><pre name="d90a" id="d90a" class="graf graf--pre graf-after--pre">models = ConvnetBuilder(resnet34, 0, 0, 0, custom_head=simple_up)<br>learn = ConvLearner(md, models)<br>learn.opt_fn=optim.Adam<br>learn.crit=nn.BCEWithLogitsLoss()<br>learn.metrics=[accuracy_thresh(0.5)]</pre><pre name="8f27" id="8f27" class="graf graf--pre graf-after--pre">learn.load('0')</pre><pre name="a3e0" id="a3e0" class="graf graf--pre graf-after--pre">learn.lr_find()<br>learn.sched.plot()</pre><pre name="9973" id="9973" class="graf graf--pre graf-after--pre">85%|████████▌ | 218/255 [02:12&lt;00:22,  1.64it/s, loss=8.91]</pre><figure name="4133" id="4133" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_hjhVP2TyYd8FZMvyGevPgA.png"></figure><pre name="4148" id="4148" class="graf graf--pre graf-after--figure">lr=4e-2</pre><pre name="b2b4" id="b2b4" class="graf graf--pre graf-after--pre">learn.fit(lr,1,cycle_len=5,use_clr=(20,5))</pre><pre name="655f" id="655f" class="graf graf--pre graf-after--pre">epoch      trn_loss   val_loss   &lt;lambda&gt;                     <br>    0      0.02178    0.020653   0.991708  <br>    1      0.017927   0.020653   0.990241                     <br>    2      0.015958   0.016115   0.993394                     <br>    3      0.015172   0.015143   0.993696                     <br>    4      0.014315   0.014679   0.99388</pre><pre name="4064" id="4064" class="graf graf--pre graf-after--pre">[0.014679321, 0.99388032489352751]</pre><pre name="53e2" id="53e2" class="graf graf--pre graf-after--pre">learn.save('tmp')</pre><pre name="6543" id="6543" class="graf graf--pre graf-after--pre">learn.load('tmp')</pre><pre name="3a85" id="3a85" class="graf graf--pre graf-after--pre">learn.unfreeze()<br>learn.bn_freeze(<strong class="markup--strong markup--pre-strong">True</strong>)</pre><pre name="15e9" id="15e9" class="graf graf--pre graf-after--pre">lrs = np.array([lr/100,lr/10,lr])/4</pre><pre name="660f" id="660f" class="graf graf--pre graf-after--pre">learn.fit(lrs,1,cycle_len=8,use_clr=(20,8))</pre><pre name="1a7e" id="1a7e" class="graf graf--pre graf-after--pre">epoch      trn_loss   val_loss   mask_acc                     <br>    0      0.038687   0.018685   0.992782  <br>    1      0.024906   0.014355   0.994933                     <br>    2      0.025055   0.014737   0.995526                     <br>    3      0.024155   0.014083   0.995708                     <br>    4      0.013446   0.010564   0.996166                     <br>    5      0.01607    0.010555   0.996096                     <br>    6      0.019197   0.010883   0.99621                      <br>    7      0.016157   0.00998    0.996393</pre><pre name="1ccb" id="1ccb" class="graf graf--pre graf-after--pre">[0.0099797687, 0.99639255659920833]</pre><pre name="a070" id="a070" class="graf graf--pre graf-after--pre">learn.save('512')</pre><pre name="9ed6" id="9ed6" class="graf graf--pre graf-after--pre">x,y = next(iter(md.val_dl))<br>py = to_np(learn.model(V(x)))</pre><pre name="e57e" id="e57e" class="graf graf--pre graf-after--pre">ax = show_img(denorm(x)[0])<br>show_img(py[0]&gt;0, ax=ax, alpha=0.5);</pre><figure name="03dc" id="03dc" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1__nnK8pvyBueihmtg6JhqPA.png"></figure><pre name="c120" id="c120" class="graf graf--pre graf-after--figure">ax = show_img(denorm(x)[0])<br>show_img(y[0], ax=ax, alpha=0.5);</pre><figure name="fd4b" id="fd4b" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_G5zNxkOplWvUIbGlSPs86Q.png"></figure><p name="81d1" id="81d1" class="graf graf--p graf-after--figure">Things keep getting better but we’ve still got quite a few little black blocky bits. so let’s go to 1024 by 1024.</p><h4 name="d0a6" id="d0a6" class="graf graf--h4 graf-after--p">1024x1024 [<a href="https://youtu.be/nG3tT31nPmQ?t=1h43m17s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h43m17s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:43:17</a>]</h4><p name="5b87" id="5b87" class="graf graf--p graf-after--h4">So let’s go to 1024 by 1024, batch size down to 4. This is pretty high res now, and train a bit more, 99.6, 99.8%!</p><pre name="87d3" id="87d3" class="graf graf--pre graf-after--p">sz = 1024<br>bs = 4</pre><pre name="cffb" id="cffb" class="graf graf--pre graf-after--pre">tfms = tfms_from_model(resnet34, sz, crop_type=CropType.NO,<br>                         tfm_y=TfmType.CLASS, aug_tfms=aug_tfms)<br>datasets = ImageData.get_ds(MatchedFilesDataset, (trn_x,trn_y), <br>                            (val_x,val_y), tfms, path=PATH)<br>md = ImageData(PATH, datasets, bs, num_workers=8, classes=<strong class="markup--strong markup--pre-strong">None</strong>)</pre><pre name="8c28" id="8c28" class="graf graf--pre graf-after--pre">denorm = md.trn_ds.denorm<br>x,y = next(iter(md.aug_dl))<br>x = denorm(x)<br>y = to_np(y)</pre><pre name="b915" id="b915" class="graf graf--pre graf-after--pre">fig, axes = plt.subplots(2, 2, figsize=(8, 8))<br><strong class="markup--strong markup--pre-strong">for</strong> i,ax <strong class="markup--strong markup--pre-strong">in</strong> enumerate(axes.flat):<br>    show_img(x[i], ax=ax)<br>    show_img(y[i], ax=ax, alpha=0.5)<br>plt.tight_layout(pad=0.1)</pre><figure name="a61e" id="a61e" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_4PrOwKZEYtXv7xdf9rPkhg.png"></figure><pre name="bb11" id="bb11" class="graf graf--pre graf-after--figure">simple_up = nn.Sequential(<br>    nn.ReLU(),<br>    StdUpsample(512,256),<br>    StdUpsample(256,256),<br>    StdUpsample(256,256),<br>    StdUpsample(256,256),<br>    nn.ConvTranspose2d(256, 1, 2, stride=2),<br>    flatten_channel,<br>)</pre><pre name="09d6" id="09d6" class="graf graf--pre graf-after--pre">models = ConvnetBuilder(resnet34, 0, 0, 0, custom_head=simple_up)<br>learn = ConvLearner(md, models)<br>learn.opt_fn=optim.Adam<br>learn.crit=nn.BCEWithLogitsLoss()<br>learn.metrics=[accuracy_thresh(0.5)]</pre><pre name="15e5" id="15e5" class="graf graf--pre graf-after--pre">learn.load('512')</pre><pre name="16bb" id="16bb" class="graf graf--pre graf-after--pre">learn.lr_find()<br>learn.sched.plot()</pre><pre name="652c" id="652c" class="graf graf--pre graf-after--pre">85%|████████▌ | 218/255 [02:12&lt;00:22,  1.64it/s, loss=8.91]</pre><figure name="511e" id="511e" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_hjhVP2TyYd8FZMvyGevPgA.png"></figure><pre name="ed98" id="ed98" class="graf graf--pre graf-after--figure">lr=4e-2</pre><pre name="9540" id="9540" class="graf graf--pre graf-after--pre">learn.fit(lr,1,cycle_len=2,use_clr=(20,4))</pre><pre name="6b85" id="6b85" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss   &lt;lambda&gt;                       <br>    0      0.01066    0.011119   0.996227  <br>    1      0.009357   0.009696   0.996553</em></pre><pre name="65c0" id="65c0" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[0.0096957013, 0.99655332546385511]</em></pre><pre name="dfde" id="dfde" class="graf graf--pre graf-after--pre">learn.save('tmp')</pre><pre name="a038" id="a038" class="graf graf--pre graf-after--pre">learn.load('tmp')</pre><pre name="d025" id="d025" class="graf graf--pre graf-after--pre">learn.unfreeze()<br>learn.bn_freeze(<strong class="markup--strong markup--pre-strong">True</strong>)</pre><pre name="de3d" id="de3d" class="graf graf--pre graf-after--pre">lrs = np.array([lr/100,lr/10,lr])/8</pre><pre name="16cb" id="16cb" class="graf graf--pre graf-after--pre">learn.fit(lrs,1,cycle_len=40,use_clr=(20,10))</pre><pre name="0c6c" id="0c6c" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss   mask_acc                       <br>    0      0.015565   0.007449   0.997661  <br>    1      0.01979    0.008376   0.997542                       <br>    2      0.014874   0.007826   0.997736                       <br>    3      0.016104   0.007854   0.997347                       <br>    4      0.023386   0.009745   0.997218                       <br>    5      0.018972   0.008453   0.997588                       <br>    6      0.013184   0.007612   0.997588                       <br>    7      0.010686   0.006775   0.997688                       <br>    8      0.0293     0.015299   0.995782                       <br>    9      0.018713   0.00763    0.997638                       <br>    10     0.015432   0.006575   0.9978                         <br>    11     0.110205   0.060062   0.979043                      <br>    12     0.014374   0.007753   0.997451                       <br>    13     0.022286   0.010282   0.997587                       <br>    14     0.015645   0.00739    0.997776                       <br>    15     0.013821   0.00692    0.997869                       <br>    16     0.022389   0.008632   0.997696                       <br>    17     0.014607   0.00677    0.997837                       <br>    18     0.018748   0.008194   0.997657                       <br>    19     0.016447   0.007237   0.997899                       <br>    20     0.023596   0.008211   0.997918                       <br>    21     0.015721   0.00674    0.997848                       <br>    22     0.01572    0.006415   0.998006                       <br>    23     0.019519   0.007591   0.997876                       <br>    24     0.011159   0.005998   0.998053                       <br>    25     0.010291   0.005806   0.998012                       <br>    26     0.010893   0.005755   0.998046                       <br>    27     0.014534   0.006313   0.997901                       <br>    28     0.020971   0.006855   0.998018                       <br>    29     0.014074   0.006107   0.998053                       <br>    30     0.01782    0.006561   0.998114                       <br>    31     0.01742    0.006414   0.997942                       <br>    32     0.016829   0.006514   0.9981                         <br>    33     0.013148   0.005819   0.998033                       <br>    34     0.023495   0.006261   0.997856                       <br>    35     0.010931   0.005516   0.99812                        <br>    36     0.015798   0.006176   0.998126                       <br>    37     0.021636   0.005931   0.998067                       <br>    38     0.012133   0.005496   0.998158                       <br>    39     0.012562   0.005678   0.998172</em></pre><pre name="9653" id="9653" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[0.0056782686, 0.99817223208291195]</em></pre><pre name="8864" id="8864" class="graf graf--pre graf-after--pre">learn.save('1024')</pre><pre name="295a" id="295a" class="graf graf--pre graf-after--pre">x,y = next(iter(md.val_dl))<br>py = to_np(learn.model(V(x)))</pre><pre name="32e6" id="32e6" class="graf graf--pre graf-after--pre">ax = show_img(denorm(x)[0])<br>show_img(py[0][0]&gt;0, ax=ax, alpha=0.5);</pre><figure name="eb5e" id="eb5e" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_8kgJWpP6-nxlDfWT8N25_g.png"></figure><pre name="0d44" id="0d44" class="graf graf--pre graf-after--figure">ax = show_img(denorm(x)[0])<br>show_img(y[0,...,-1], ax=ax, alpha=0.5);</pre><figure name="31d6" id="31d6" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1__-Kx9dC5aTgBSf_aSrBrNQ.png"></figure><pre name="9662" id="9662" class="graf graf--pre graf-after--figure">show_img(py[0][0]&gt;0);</pre><figure name="e0e8" id="e0e8" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_G3C8DPyOB4BC3VLPGjbwcA.png"></figure><pre name="6f0f" id="6f0f" class="graf graf--pre graf-after--figure">show_img(y[0,...,-1]);</pre><figure name="3dcd" id="3dcd" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_fJrWvuzyX0cG5ATWvqaDPg.png"></figure><p name="5df3" id="5df3" class="graf graf--p graf-after--figure">Now if we look at the masks,&nbsp;, they are actually looking not bad. That’s looking pretty good. So can we do better? And the answer is yes, we can.</p><h3 name="0ffa" id="0ffa" class="graf graf--h3 graf-after--p">U-Net [<a href="https://youtu.be/nG3tT31nPmQ?t=1h43m45s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h43m45s" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">1:43:45</a>]</h3><p name="e5cb" id="e5cb" class="graf graf--p graf-after--h3"><a href="https://github.com/fastai/fastai/blob/master/courses/dl2/carvana-unet.ipynb" data-href="https://github.com/fastai/fastai/blob/master/courses/dl2/carvana-unet.ipynb" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Notebook</a> / <a href="https://arxiv.org/abs/1505.04597" data-href="https://arxiv.org/abs/1505.04597" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Paper</a></p><p name="b5e6" id="b5e6" class="graf graf--p graf-after--p">U-Net network is quite magnificent. With that previous approach, our pre-trained ImageNet network was being squished down all the way down to 7x7 and then expand it out all the way back up to 224x224 (1024 gets squished down to quite a bit bigger than 7x7). And then expanded out again all this way which means it has to somehow store all the information about the much bigger version in the small version. And actually most of the information about the bigger version was really in the original picture anyway. So it doesn’t seem like a great approach — this squishing and un-squishing.</p><figure name="c410" id="c410" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_PvXW__XxRQIMoFoVFJq-Zw.png"></figure><p name="2d7d" id="2d7d" class="graf graf--p graf-after--figure">So the U-Net idea comes from this fantastic paper where it was literally invented in this very domain-specific area of biomedical image segmentation. But in fact, basically every Kaggle winner in anything even vaguely related to segmentation has end up using U-Net. It’s one of these things that everybody in Kaggle knows it is the best practice, but in more of academic circles, this has been around for a couple of years at least, a lot of people still don’t realize this is by far the best approach.</p><figure name="5547" id="5547" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_9nxe-lIVxXawNsLzItvqcg.png"></figure><p name="09c5" id="09c5" class="graf graf--p graf-after--figure">Here is the basic idea [<a href="https://youtu.be/nG3tT31nPmQ?t=1h45m10s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h45m10s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:45:10</a>]. On the left is the downward path where we start at 572x572 in this case then halve the grid size 4 times, then on the right is the upward path where we double the grid size 4 times. But the thing that we also do is, at every point where we halve the grid size, we actually copy those activations over to the upward path and concatenate them together.</p><p name="ba90" id="ba90" class="graf graf--p graf-after--p">You can see on the bottom right, these red arrows are max pooling operation, these green arrows are upward sampling, and then these gray arrows are copying. So we copy and concat. In other words, the input image after a couple of convs is copied over to the output, concatenated together, and so now we get to use all of the informations gone through all of the informations gone through all the down and all the up, plus also a slightly modified version of the input pixels. And slightly modified version of one thing down from the input pixels because they came up through here. So we have all of the richness of going all the way down and up, but also a slightly less coarse version and a slightly less coarse version and then the really simple version, and they can all be combined together. So that’s U-Net. It’s such a cool idea.</p><p name="0c3f" id="0c3f" class="graf graf--p graf-after--p">Here we are in the carvana-unet notebook. All this is the same code as before.</p><pre name="f105" id="f105" class="graf graf--pre graf-after--p">%matplotlib inline<br>%reload_ext autoreload<br>%autoreload 2</pre><pre name="b9dd" id="b9dd" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">fastai.conv_learner</strong> <strong class="markup--strong markup--pre-strong">import</strong> *<br><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">fastai.dataset</strong> <strong class="markup--strong markup--pre-strong">import</strong> *<br><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">fastai.models.resnet</strong> <strong class="markup--strong markup--pre-strong">import</strong> vgg_resnet50<br><br><strong class="markup--strong markup--pre-strong">import</strong> <strong class="markup--strong markup--pre-strong">json</strong></pre><pre name="c574" id="c574" class="graf graf--pre graf-after--pre">torch.backends.cudnn.benchmark=<strong class="markup--strong markup--pre-strong">True</strong></pre><h3 name="a68a" id="a68a" class="graf graf--h3 graf-after--pre">Data</h3><pre name="ac3e" id="ac3e" class="graf graf--pre graf-after--h3">PATH = Path('data/carvana')<br>MASKS_FN = 'train_masks.csv'<br>META_FN = 'metadata.csv'<br>masks_csv = pd.read_csv(PATH/MASKS_FN)<br>meta_csv = pd.read_csv(PATH/META_FN)</pre><pre name="4c94" id="4c94" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> show_img(im, figsize=<strong class="markup--strong markup--pre-strong">None</strong>, ax=<strong class="markup--strong markup--pre-strong">None</strong>, alpha=<strong class="markup--strong markup--pre-strong">None</strong>):<br>    <strong class="markup--strong markup--pre-strong">if</strong> <strong class="markup--strong markup--pre-strong">not</strong> ax: fig,ax = plt.subplots(figsize=figsize)<br>    ax.imshow(im, alpha=alpha)<br>    ax.set_axis_off()<br>    <strong class="markup--strong markup--pre-strong">return</strong> ax</pre><pre name="1f89" id="1f89" class="graf graf--pre graf-after--pre">TRAIN_DN = 'train-128'<br>MASKS_DN = 'train_masks-128'<br>sz = 128<br>bs = 64<br>nw = 16</pre><pre name="9e98" id="9e98" class="graf graf--pre graf-after--pre">TRAIN_DN = 'train'<br>MASKS_DN = 'train_masks_png'<br>sz = 128<br>bs = 64<br>nw = 16</pre><pre name="6d6b" id="6d6b" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">MatchedFilesDataset</strong>(FilesDataset):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, fnames, y, transform, path):<br>        self.y=y<br>        <strong class="markup--strong markup--pre-strong">assert</strong>(len(fnames)==len(y))<br>        super().__init__(fnames, transform, path)<br>    <strong class="markup--strong markup--pre-strong">def</strong> get_y(self, i): <br>        <strong class="markup--strong markup--pre-strong">return</strong> open_image(os.path.join(self.path, self.y[i]))<br>    <strong class="markup--strong markup--pre-strong">def</strong> get_c(self): <strong class="markup--strong markup--pre-strong">return</strong> 0</pre><pre name="18ff" id="18ff" class="graf graf--pre graf-after--pre">x_names = np.array([Path(TRAIN_DN)/o <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> masks_csv['img']])<br>y_names = np.array([Path(MASKS_DN)/f'<strong class="markup--strong markup--pre-strong">{o[:-4]}</strong>_mask.png' <br>                        <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> masks_csv['img']])</pre><pre name="ca81" id="ca81" class="graf graf--pre graf-after--pre">val_idxs = list(range(1008))<br>((val_x,trn_x),(val_y,trn_y)) = split_by_idx(val_idxs, x_names, <br>                                             y_names)</pre><pre name="4f6d" id="4f6d" class="graf graf--pre graf-after--pre">aug_tfms = [RandomRotate(4, tfm_y=TfmType.CLASS),<br>            RandomFlip(tfm_y=TfmType.CLASS),<br>            RandomLighting(0.05, 0.05, tfm_y=TfmType.CLASS)]</pre><pre name="947a" id="947a" class="graf graf--pre graf-after--pre">tfms = tfms_from_model(resnet34, sz, crop_type=CropType.NO, <br>                        tfm_y=TfmType.CLASS, aug_tfms=aug_tfms)<br>datasets = ImageData.get_ds(MatchedFilesDataset, (trn_x,trn_y), <br>                             (val_x,val_y), tfms, path=PATH)<br>md = ImageData(PATH, datasets, bs, num_workers=16, classes=<strong class="markup--strong markup--pre-strong">None</strong>)<br>denorm = md.trn_ds.denorm</pre><pre name="d275" id="d275" class="graf graf--pre graf-after--pre">x,y = next(iter(md.trn_dl))</pre><pre name="c095" id="c095" class="graf graf--pre graf-after--pre">x.shape,y.shape</pre><pre name="91d0" id="91d0" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(torch.Size([64, 3, 128, 128]), torch.Size([64, 128, 128]))</em></pre><h3 name="c5a9" id="c5a9" class="graf graf--h3 graf-after--pre">Simple upsample</h3><p name="6874" id="6874" class="graf graf--p graf-after--h3">And at the start, I’ve got a simple upsample version just to show you again the non U-net version. This time, I’m going to add in something called the dice metric. Dice is very similar, as you see, to Jaccard or I over U. It’s just a minor difference. It’s basically intersection over union with a minor tweak. The reason we are going to use dice is that’s the metric that Kaggle competition used and it’s a little bit harder to get a high dice score than a high accuracy because it’s really looking at what the overlap of the correct pixels are with your pixels. But it’s pretty similar.</p><p name="933f" id="933f" class="graf graf--p graf-after--p">So in the Kaggle competition, people that were doing okay were getting about 99.6 dice and the winners were about 99.7 dice.</p><pre name="62eb" id="62eb" class="graf graf--pre graf-after--p">f = resnet34<br>cut,lr_cut = model_meta[f]</pre><pre name="7461" id="7461" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> get_base():<br>    layers = cut_model(f(<strong class="markup--strong markup--pre-strong">True</strong>), cut)<br>    <strong class="markup--strong markup--pre-strong">return</strong> nn.Sequential(*layers)</pre><pre name="dc8d" id="dc8d" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> dice(pred, targs):<br>    pred = (pred&gt;0).float()<br>    <strong class="markup--strong markup--pre-strong">return</strong> 2. * (pred*targs).sum() / (pred+targs).sum()</pre><p name="8d29" id="8d29" class="graf graf--p graf-after--pre">Here is our standard upsample.</p><pre name="a159" id="a159" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">StdUpsample</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, nin, nout):<br>        super().__init__()<br>        self.conv = nn.ConvTranspose2d(nin, nout, 2, stride=2)<br>        self.bn = nn.BatchNorm2d(nout)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x): <strong class="markup--strong markup--pre-strong">return</strong> self.bn(F.relu(self.conv(x)))</pre><p name="27b2" id="27b2" class="graf graf--p graf-after--pre">This all as before.</p><pre name="4f5a" id="4f5a" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">Upsample34</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, rn):<br>        super().__init__()<br>        self.rn = rn<br>        self.features = nn.Sequential(<br>            rn, nn.ReLU(),<br>            StdUpsample(512,256),<br>            StdUpsample(256,256),<br>            StdUpsample(256,256),<br>            StdUpsample(256,256),<br>            nn.ConvTranspose2d(256, 1, 2, stride=2))<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self,x): <strong class="markup--strong markup--pre-strong">return</strong> self.features(x)[:,0]</pre><pre name="32ac" id="32ac" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">UpsampleModel</strong>():<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self,model,name='upsample'):<br>        self.model,self.name = model,name<br><br>    <strong class="markup--strong markup--pre-strong">def</strong> get_layer_groups(self, precompute):<br>        lgs = list(split_by_idxs(children(self.model.rn), [lr_cut]))<br>        <strong class="markup--strong markup--pre-strong">return</strong> lgs + [children(self.model.features)[1:]]</pre><pre name="c218" id="c218" class="graf graf--pre graf-after--pre">m_base = get_base() </pre><pre name="b10e" id="b10e" class="graf graf--pre graf-after--pre">m = to_gpu(Upsample34(m_base))<br>models = UpsampleModel(m)</pre><pre name="bfc2" id="bfc2" class="graf graf--pre graf-after--pre">learn = ConvLearner(md, models)<br>learn.opt_fn=optim.Adam<br>learn.crit=nn.BCEWithLogitsLoss()<br>learn.metrics=[accuracy_thresh(0.5),dice]</pre><pre name="57f0" id="57f0" class="graf graf--pre graf-after--pre">learn.freeze_to(1)</pre><pre name="c11f" id="c11f" class="graf graf--pre graf-after--pre">learn.lr_find()<br>learn.sched.plot()</pre><pre name="ba37" id="ba37" class="graf graf--pre graf-after--pre">86%|█████████████████████████████████████████████████████████████          | 55/64 [00:22&lt;00:03,  2.46it/s, loss=3.21]</pre><figure name="40ce" id="40ce" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_X_dHSL-SZqkKw31hZgughg.png"></figure><pre name="deaa" id="deaa" class="graf graf--pre graf-after--figure">lr=4e-2<br>wd=1e-7<br>lrs = np.array([lr/100,lr/10,lr])/2</pre><pre name="66ec" id="66ec" class="graf graf--pre graf-after--pre">learn.fit(lr,1, wds=wd, cycle_len=4,use_clr=(20,8))</pre><pre name="b940" id="b940" class="graf graf--pre graf-after--pre">0%|          | 0/64 [00:00&lt;?, ?it/s]<br>epoch      trn_loss   val_loss   &lt;lambda&gt;   dice           <br>    0      0.216882   0.133512   0.938017   0.855221  <br>    1      0.169544   0.115158   0.946518   0.878381       <br>    2      0.153114   0.099104   0.957748   0.903353       <br>    3      0.144105   0.093337   0.964404   0.915084</pre><pre name="0723" id="0723" class="graf graf--pre graf-after--pre">[0.09333742126112893, 0.9644036065964472, 0.9150839788573129]</pre><pre name="8b4c" id="8b4c" class="graf graf--pre graf-after--pre">learn.save('tmp')</pre><pre name="cd9c" id="cd9c" class="graf graf--pre graf-after--pre">learn.load('tmp')</pre><pre name="3ffa" id="3ffa" class="graf graf--pre graf-after--pre">learn.unfreeze()<br>learn.bn_freeze(<strong class="markup--strong markup--pre-strong">True</strong>)</pre><pre name="9578" id="9578" class="graf graf--pre graf-after--pre">learn.fit(lrs,1,cycle_len=4,use_clr=(20,8))</pre><pre name="add2" id="add2" class="graf graf--pre graf-after--pre">epoch      trn_loss   val_loss   &lt;lambda&gt;   dice           <br>    0      0.174897   0.061603   0.976321   0.94382   <br>    1      0.122911   0.053625   0.982206   0.957624       <br>    2      0.106837   0.046653   0.985577   0.965792       <br>    3      0.099075   0.042291   0.986519   0.968925</pre><pre name="0294" id="0294" class="graf graf--pre graf-after--pre">[0.042291240323157536, 0.986519161670927, 0.9689251193924556]</pre><p name="f747" id="f747" class="graf graf--p graf-after--pre">Now we can check our dice metric [<a href="https://youtu.be/nG3tT31nPmQ?t=1h48m" data-href="https://youtu.be/nG3tT31nPmQ?t=1h48m" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">1:48:00</a>]. So you can see on dice metric, we are getting around 96.8 at 128x128. So that’s not great.</p><pre name="b497" id="b497" class="graf graf--pre graf-after--p">learn.save('128')</pre><pre name="55ee" id="55ee" class="graf graf--pre graf-after--pre">x,y = next(iter(md.val_dl))<br>py = to_np(learn.model(V(x)))</pre><pre name="508f" id="508f" class="graf graf--pre graf-after--pre">show_img(py[0]&gt;0);</pre><figure name="1980" id="1980" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_w6f-XvZMeLKt4Fc7O_S4EQ.png"></figure><pre name="a981" id="a981" class="graf graf--pre graf-after--figure">show_img(y[0]);</pre><figure name="8eda" id="8eda" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_SHntdwiyRvupP9SQO5BD5g.png"></figure><h4 name="5e27" id="5e27" class="graf graf--h4 graf-after--figure">U-net (ish) [<a href="https://youtu.be/nG3tT31nPmQ?t=1h48m16s" data-href="https://youtu.be/nG3tT31nPmQ?t=1h48m16s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:48:16</a>]</h4><p name="340c" id="340c" class="graf graf--p graf-after--h4">So let’s try U-Net. I’m calling it U-net(ish) because as per usual I’m creating my own somewhat hacky version — trying to keep things as similar to what you’re used to as possible and doing things that I think makes sense. So there should be plenty of opportunity for you to at least make this more authentically U-net by looking at the exact grid sizes and see how here (the top left convs) the size is going down a little bit. So they are obviously not adding any padding and then there are some cropping going on — there’s a few differences. But one of the things is because I want to take advantage of transfer learning — that means I can’t quite use U-Net.</p><p name="e4c3" id="e4c3" class="graf graf--p graf-after--p">So here is another big opportunity is what if you create the U-Net down path and then add a classifier on the end and then train that on ImageNet. You’ve now got an ImageNet trained classifier which is specifically designed to be a good backbone for U-Net. Then you should be able to now come back and get pretty closed to winning this old competition (it’s actually not that old — it’s fairly recent competition). Because that pre-trained network didn’t exist before. But if you think about what YOLO v3 did, it’s basically that. They created a DarkNet, they pre-trained it on ImageNet, and then they used it as the basis for their bounding boxes. So again, this idea of pre-training things which are designed not just for classification but designed for other things — it’s just something that nobody has done yet. But as we’ve shown, you can train ImageNet for $25 in three hours now. And if people in the community are interested in doing this, hopefully I’ll have credits I can help you with as well so if you do, the work to get it set up and give me a script, I can probably run it for you. For now though, we don’t have that yet. So we are going to use ResNet.</p></body></html>