
<!-- saved from url=(0064)file:///C:/Users/asus/Desktop/fastai-ml-dl-notes-zh/en/dl11.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></head><body><hr class="section-divider"><h1 name="a794" id="a794" class="graf graf--h3 graf--leading graf--title">Deep Learning 2: Part 2 Lesson&nbsp;11</h1><p name="2560" id="2560" class="graf graf--p graf-after--h3"><em class="markup--em markup--p-em">My personal notes from </em><a href="http://www.fast.ai/" data-href="http://www.fast.ai/" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener noopener noopener nofollow noopener nofollow noopener noopener nofollow noopener" target="_blank"><em class="markup--em markup--p-em">fast.ai course</em></a><em class="markup--em markup--p-em">. These notes will continue to be updated and improved as I continue to review the course to “really” understand it. Much appreciation to </em><a href="https://twitter.com/jeremyphoward" data-href="https://twitter.com/jeremyphoward" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener noopener noopener nofollow noopener nofollow noopener noopener nofollow noopener" target="_blank"><em class="markup--em markup--p-em">Jeremy</em></a><em class="markup--em markup--p-em"> and </em><a href="https://twitter.com/math_rachel" data-href="https://twitter.com/math_rachel" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener noopener noopener nofollow noopener nofollow noopener noopener nofollow noopener" target="_blank"><em class="markup--em markup--p-em">Rachel</em></a><em class="markup--em markup--p-em"> who gave me this opportunity to learn.</em></p><p name="dfbd" id="dfbd" class="graf graf--p graf-after--p graf--trailing">Lessons: <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" class="markup--anchor markup--p-anchor" target="_blank">1</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" class="markup--anchor markup--p-anchor" target="_blank">2</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" class="markup--anchor markup--p-anchor" target="_blank">3</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" class="markup--anchor markup--p-anchor" target="_blank">4</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" class="markup--anchor markup--p-anchor" target="_blank">5</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" class="markup--anchor markup--p-anchor" target="_blank">6</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" class="markup--anchor markup--p-anchor" target="_blank">7</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" class="markup--anchor markup--p-anchor" target="_blank">8</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" class="markup--anchor markup--p-anchor" target="_blank">9</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" class="markup--anchor markup--p-anchor" target="_blank">10</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" class="markup--anchor markup--p-anchor" target="_blank"><strong class="markup--strong markup--p-strong">11</strong></a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" class="markup--anchor markup--p-anchor" target="_blank">12</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" class="markup--anchor markup--p-anchor" target="_blank">13</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" class="markup--anchor markup--p-anchor" target="_blank">14</a></p><hr class="section-divider"><h3 name="0687" id="0687" class="graf graf--h3 graf--leading">Links</h3><p name="a8dc" id="a8dc" class="graf graf--p graf-after--h3"><a href="http://forums.fast.ai/t/part-2-lesson-11-in-class/14699/1" data-href="http://forums.fast.ai/t/part-2-lesson-11-in-class/14699/1" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank"><strong class="markup--strong markup--p-strong">Forum</strong></a><strong class="markup--strong markup--p-strong"> / </strong><a href="https://youtu.be/tY0n9OT5_nA" data-href="https://youtu.be/tY0n9OT5_nA" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong">Video</strong></a></p><h3 name="3522" id="3522" class="graf graf--h3 graf-after--p">Before getting&nbsp;started:</h3><ul class="postList"><li name="b947" id="b947" class="graf graf--li graf-after--h3"><a href="https://sgugger.github.io/the-1cycle-policy.html#the-1cycle-policy" data-href="https://sgugger.github.io/the-1cycle-policy.html#the-1cycle-policy" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">The 1cycle policy</a> by Sylvain Gugger. Based on Leslie Smith’s new paper which takes the previous two key papers (cyclical learning rate and super convergence) and built on them with a number of experiments to show how you can achieve super convergence. Super convergence lets you train models five times faster than the previous stepwise approach (and faster than CLR, although it is less than five times). Super convergence lets you get up to massively high learning rates by somewhere between 1 and 3. The interesting thing about super convergence is that you train at those very high learning rates for quite a large percentage of your epochs and during that time, the loss doesn’t really improve very much. But the trick is it’s doing a lot of searching through the space to find really generalizable areas it seems. Sylvain implemented it in fastai by flushing out the pieces that were missing then confirmed that he actually achieved super convergence on training on CIFAR10. It is currently called <code class="markup--code markup--li-code">use_clr_beta</code> but will be renamed in future. He also added cyclical momentum to fastai library.</li><li name="cc4e" id="cc4e" class="graf graf--li graf-after--li"><a href="https://towardsdatascience.com/how-to-create-data-products-that-are-magical-using-sequence-to-sequence-models-703f86a231f8" data-href="https://towardsdatascience.com/how-to-create-data-products-that-are-magical-using-sequence-to-sequence-models-703f86a231f8" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">How To Create Data Products That Are Magical Using Sequence-to-Sequence Model</a>s by Hamel Husain. He blogged about training a model to summarize GitHub issues. Here is the <a href="http://gh-demo.kubeflow.org/" data-href="http://gh-demo.kubeflow.org/" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">demo</a> Kubeflow team created based on his blog.</li></ul><h3 name="6a51" id="6a51" class="graf graf--h3 graf-after--li">Neural Machine Translation [<a href="https://youtu.be/tY0n9OT5_nA?t=5m36s" data-href="https://youtu.be/tY0n9OT5_nA?t=5m36s" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">5:36</a>]</h3><p name="764a" id="764a" class="graf graf--p graf-after--h3">Let’s build a sequence-to-sequence model! We are going to be working on machine translation. Machine translation is something that’s been around for a long time, but we are going to look at an approach called neural translation which uses neural networks for translation. Neural machine translation appeared a couple years ago and it was not as good as the statistical machine translation approaches that use classic feature engineering and standard NLP approaches like stemming, fiddling around with word frequencies, n-grams, etc. By a year later, it was better than everything else. It is based on a metric called BLEU — we are not going to discuss the metric because it is not a very good metric and it is not interesting, but everybody uses it.</p><figure name="e600" id="e600" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_f0hoBLrTuevFPgAl-lFIfQ.png"></figure><p name="2ea6" id="2ea6" class="graf graf--p graf-after--figure">We are seeing machine translation starting down the path that we saw starting computer vision object classification in 2012 which just surpassed the state-of-the-art and now zipping past it at great rate. It is unlikely that anybody watching this is actually going to build a machine translation model because <a href="https://translate.google.com/" data-href="https://translate.google.com/" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">https://translate.google.com/</a> works quite well. So why are we learning about machine translation? The reason we are learning about machine translation is that the general idea of taking some kind of input like a sentence in French and transforming it into some other kind of output with arbitrary length such as a sentence in English is a really useful thing to do. For example, as we just saw, Hamel took GitHub issues and turn them into summaries. Another example is taking videos and turning them into descriptions, or basically anything where you are spitting out an arbitrary sized output which is very often a sentence. Maybe taking a CT scan and spitting out a radiology report — this is where you can use sequence to sequence learning.</p><h4 name="d052" id="d052" class="graf graf--h4 graf-after--p">Four big wins of Neural Machine Translation [<a href="https://youtu.be/tY0n9OT5_nA?t=8m36s" data-href="https://youtu.be/tY0n9OT5_nA?t=8m36s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">8:36</a>]</h4><figure name="bf3a" id="bf3a" class="graf graf--figure graf-after--h4"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_c2kAArVl9mF_VaeqnXafBw.png"></figure><ul class="postList"><li name="5fa6" id="5fa6" class="graf graf--li graf-after--figure">End-to-end training: No fussing around with heuristics and hacky feature engineering.</li><li name="b2f3" id="b2f3" class="graf graf--li graf-after--li">We are able to build these distributed representations which are shared by lots of concepts within a single network.</li><li name="41a0" id="41a0" class="graf graf--li graf-after--li">We are able to use long term state in the RNN so it uses a lot more context than n-gram type approaches.</li><li name="0c46" id="0c46" class="graf graf--li graf-after--li">In the end, text we are generating uses RNN as well so we can build something that is more fluid.</li></ul><h4 name="5340" id="5340" class="graf graf--h4 graf-after--li">BiLSTMs(+Attn) not just for neural MT&nbsp;[<a href="https://youtu.be/tY0n9OT5_nA?t=9m20s" data-href="https://youtu.be/tY0n9OT5_nA?t=9m20s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">9:20</a>]</h4><figure name="4991" id="4991" class="graf graf--figure graf-after--h4"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_LrzmE8Xi5-mwsUrqzEQA6Q.png"></figure><p name="8fec" id="8fec" class="graf graf--p graf-after--figure">We are going to use bi-directional GRU (basically the same as LSTM) with attention — these general ideas can also be used for lots of other things as you see above.</p><h4 name="9a67" id="9a67" class="graf graf--h4 graf-after--p">Let’s jump into the code&nbsp;[<a href="https://youtu.be/tY0n9OT5_nA?t=9m47s" data-href="https://youtu.be/tY0n9OT5_nA?t=9m47s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">9:47</a>]</h4><p name="9e24" id="9e24" class="graf graf--p graf-after--h4"><a href="https://github.com/fastai/fastai/blob/master/courses/dl2/translate.ipynb" data-href="https://github.com/fastai/fastai/blob/master/courses/dl2/translate.ipynb" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Notebook</a></p><p name="f726" id="f726" class="graf graf--p graf-after--p">We are going to try to translate French into English by following the standard neural network approach:</p><ol class="postList"><li name="711f" id="711f" class="graf graf--li graf-after--p">Data</li><li name="7d68" id="7d68" class="graf graf--li graf-after--li">Architecture</li><li name="b242" id="b242" class="graf graf--li graf-after--li">Loss Function</li></ol><h4 name="e838" id="e838" class="graf graf--h4 graf-after--li">1. Data</h4><p name="3fb2" id="3fb2" class="graf graf--p graf-after--h4">As usual, we need <code class="markup--code markup--p-code">(x, y)</code> pair. In this case, x: French sentence, y: English sentence which you will compare your prediction against. We need lots of these tuples of French sentences with their equivalent English sentence — that is called “parallel corpus” and harder to find than a corpus for a language model. For a language model, we just need text in some language. For any living language, there will be a few gigabytes at least of text floating around the internet for you to grab. For translation, there are some pretty good parallel corpus available for European languages. The European Parliament has every sentence in every European language. Anything that goes to the UN is translated to lots of languages. For French to English, we have particularly nice thing which is pretty much any semi official Canadian website will have a French version and an English version[<a href="https://youtu.be/tY0n9OT5_nA?t=12m13s" data-href="https://youtu.be/tY0n9OT5_nA?t=12m13s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">12:13</a>].</p><h4 name="0b1d" id="0b1d" class="graf graf--h4 graf-after--p">Translation files</h4><pre name="36f6" id="36f6" class="graf graf--pre graf-after--h4"><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">fastai.text</strong> <strong class="markup--strong markup--pre-strong">import</strong> *</pre><p name="c210" id="c210" class="graf graf--p graf-after--pre">French/English parallel texts from <a href="http://www.statmt.org/wmt15/translation-task.html" data-href="http://www.statmt.org/wmt15/translation-task.html" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">http://www.statmt.org/wmt15/translation-task.html</a>&nbsp;. It was created by Chris Callison-Burch, who crawled millions of web pages and then used <em class="markup--em markup--p-em">a set of simple heuristics to transform French URLs onto English URLs (i.e. replacing “fr” with “en” and about 40 other hand-written rules), and assume that these documents are translations of each other</em>.</p><pre name="35d6" id="35d6" class="graf graf--pre graf-after--p">PATH = Path('data/translate')<br>TMP_PATH = PATH/'tmp'<br>TMP_PATH.mkdir(exist_ok=<strong class="markup--strong markup--pre-strong">True</strong>)<br>fname='giga-fren.release2.fixed'<br>en_fname = PATH/f'<strong class="markup--strong markup--pre-strong">{fname}</strong>.en'<br>fr_fname = PATH/f'<strong class="markup--strong markup--pre-strong">{fname}</strong>.fr'</pre><p name="a906" id="a906" class="graf graf--p graf-after--pre">For bounding boxes, all of the interesting stuff was in the loss function, but for neural translation, all of the interesting stuff is going to be int he architecture [<a href="https://youtu.be/tY0n9OT5_nA?t=13m1s" data-href="https://youtu.be/tY0n9OT5_nA?t=13m1s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">13:01</a>]. Let’s zip through this pretty quickly and one of the things Jeremy wants you to think about particularly is what are the relationships or the similarities in terms of the tasks we are doing and how we do it between language modeling vs. neural translation.</p><figure name="160a" id="160a" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_458KhA7uSET5eH3fe4EhDw.png"></figure><p name="8546" id="8546" class="graf graf--p graf-after--figure">The first step is to do the exact same thing we do in a language model which is to take a sentence and chuck it through an RNN[<a href="https://youtu.be/tY0n9OT5_nA?t=13m35s" data-href="https://youtu.be/tY0n9OT5_nA?t=13m35s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">13:35</a>].</p><figure name="6dac" id="6dac" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_ujJzFdJfk2jP0466peyyrQ.png"></figure><p name="a4b6" id="a4b6" class="graf graf--p graf-after--figure">Now with the classification model, we had a decoder which took the RNN output and grabbed three things: <code class="markup--code markup--p-code">maxpool</code> and <code class="markup--code markup--p-code">meanpool</code> over all of the time steps, and the value of the RNN at the last time step, stack all those together and put it through a linear layer [<a href="https://youtu.be/tY0n9OT5_nA?t=14m24s" data-href="https://youtu.be/tY0n9OT5_nA?t=14m24s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">14:24</a>]. Most people do not do that and just use the last time step, so all the things we will be talking about today uses the last time step.</p><p name="6cad" id="6cad" class="graf graf--p graf-after--p">We start out by chucking the input sentence through an RNN and out of it comes some “hidden state” (i.e. some vector that represents the output of an RNN that has encoded the sentence).</p><h4 name="eaf5" id="eaf5" class="graf graf--h4 graf-after--p">Encoder ≈ Backbone&nbsp;[<a href="https://youtu.be/tY0n9OT5_nA?t=15m18s" data-href="https://youtu.be/tY0n9OT5_nA?t=15m18s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">15:18</a>]</h4><p name="c8b4" id="c8b4" class="graf graf--p graf-after--h4">Stephen used the word “encoder”, but we tend to use the word “backbone”. Like when we talked about adding a custom head to an existing model, the existing pre-trained ImageNet model, for example, we say that is our backbone and then we stick on top of it some head that does the task we want. In sequence to sequence learning, they use the word encoder, but it basically is the same thing — it is some piece of a neural network architecture that takes the input and turns it into some representation which we can then stick a few more layers on top to grab something out of it such as we did for the classifier where we stack a linear layer on top of it to turn int into a sentiment. This time though, we have something that’s a little bit harder than just creating sentiment [<a href="https://youtu.be/tY0n9OT5_nA?t=16m12s" data-href="https://youtu.be/tY0n9OT5_nA?t=16m12s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">16:12</a>]. Instead of turning the hidden state into a positive or negative sentiment, we want to turn it into a sequence of tokens where that sequence of token is the German sentence in Stephen’s example.</p><p name="cb0e" id="cb0e" class="graf graf--p graf-after--p">This is sounding more like the language model than the classifier because the language had multiple tokens (for every input word, there was an output word). But the language model was also much easier because the number of tokens in the language model output was the same length as the number of tokens in the language model input. Not only they were the same length, but they exactly matched up (e.g. after word one comes word two, after word two comes word three, and so forth). For translating language, you don’t necessarily know that the word “he” will be translated as the first word in the output (unfortunately, it is in this particular case). Very often, the subject object order will be different or there will be some extra words inserted, or some pronouns we will need to add some gendered article, etc. This is the key issue we are going to have to deal with is the fact that we have an arbitrary length output where the tokens in the output do not correspond to the same order or the specific tokens in the input [<a href="https://youtu.be/tY0n9OT5_nA?t=17m31s" data-href="https://youtu.be/tY0n9OT5_nA?t=17m31s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">17:31</a>]. But the general idea is the same. Here is an RNN to encode the input, turns it into some hidden state, then this is the new thing we are going to learn is generating a sequence output.</p><h4 name="2737" id="2737" class="graf graf--h4 graf-after--p">Sequence output&nbsp;[<a href="https://youtu.be/tY0n9OT5_nA?t=17m47s" data-href="https://youtu.be/tY0n9OT5_nA?t=17m47s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">17:47</a>]</h4><p name="4228" id="4228" class="graf graf--p graf-after--h4">We already know:</p><ul class="postList"><li name="f4c5" id="f4c5" class="graf graf--li graf-after--p">Sequence to class (IMDB classifier)</li><li name="09ce" id="09ce" class="graf graf--li graf-after--li">Sequence to equal length sequence (Language model)</li></ul><p name="dd37" id="dd37" class="graf graf--p graf-after--li">But we do not know yet how to do a general purpose sequence to sequence, so that’s the new thing today. Very little of this will make sense unless you really understand lesson 6 how an RNN works.</p><h4 name="52a2" id="52a2" class="graf graf--h4 graf-after--p">Quick review of <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" class="markup--anchor markup--h4-anchor" target="_blank">Lesson 6</a>&nbsp;[<a href="https://youtu.be/tY0n9OT5_nA?t=18m20s" data-href="https://youtu.be/tY0n9OT5_nA?t=18m20s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">18:20</a>]</h4><p name="ed65" id="ed65" class="graf graf--p graf-after--h4">We learnt that an RNN at its heart is a standard fully connected network. Below is one with 4 layers — takes an input and puts it through four layers, but at the second layer, it concatenates in the second input, third layer concatenated in the third input, but we actually wrote this in Python as just a four layer neural network. There was nothing else we used other than linear layers and ReLUs. We used the same weight matrix every time when an input came in, we used the same matrix every time when we went from one of the hidden states to the next — that is why these arrows are the same color.</p><figure name="8eb7" id="8eb7" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_ZTtw8vtjy-K2CptW_xpLqw.png"></figure><p name="3e36" id="3e36" class="graf graf--p graf-after--figure">We can redraw the above diagram like the below [<a href="https://youtu.be/tY0n9OT5_nA?t=19m29s" data-href="https://youtu.be/tY0n9OT5_nA?t=19m29s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">19:29</a>].</p><figure name="9924" id="9924" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_QE70fqvLLDhq8fq_ctxpZQ.png"></figure><p name="eb44" id="eb44" class="graf graf--p graf-after--figure">Not only did we redraw it but we took the four lines of linear linear linear linear code in PyTorch and we replaced it with a for loop. Remember, we had something that did exactly the same thing as below, but it just had four lines of code saying <code class="markup--code markup--p-code">self.l_in(input)</code> and we replaced it with a for loop because that’s nice to refactor. The refactoring which does not change any of the math, any of the ideas, or any of the outputs is an RNN. It’s turning a bunch of separate lines in the code into a Python for loop.</p><figure name="3122" id="3122" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_ewV_N6jZBjStFNpSCMndxg.png"></figure><p name="4822" id="4822" class="graf graf--p graf-after--figure">We could take the output so that it is not outside the loop and put it inside the loop [<a href="https://youtu.be/tY0n9OT5_nA?t=20m25s" data-href="https://youtu.be/tY0n9OT5_nA?t=20m25s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">20:25</a>]. If we do that, we are now going to generate a separate output for every input. The code above, the hidden state gets replaced each time and we end up just spitting out the final hidden state. But if instead, we had something that said <code class="markup--code markup--p-code">hs.append(h)</code> and returned <code class="markup--code markup--p-code">hs</code> at the end, that would be the picture below.</p><figure name="fa1e" id="fa1e" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_CX45skUFZZO6uHsR8IndzA.png"></figure><p name="b36b" id="b36b" class="graf graf--p graf-after--figure">The main thing to remember is when we say hidden state, we are referring to a vector — technically a vector for each thing in the mini-batch so it’s a matrix, but generally when Jeremy speaks about these things, he ignores the mini-batch piece and treat it for just a single item.</p><figure name="8259" id="8259" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_ch4De-RThVp-fthGpqsaWw.png"></figure><p name="c658" id="c658" class="graf graf--p graf-after--figure">We also learned that you can stack these layers on top of each other [<a href="https://youtu.be/tY0n9OT5_nA?t=21m41s" data-href="https://youtu.be/tY0n9OT5_nA?t=21m41s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">21:41</a>]. So rather than the left RNN (in the diagram above) spitting out output, they could just spit out inputs into a second RNN. If you are thinking at this point “I think I understand this but I am not quite sure” that means you don’t understand this. The only way you know that you actually understand it is to go and write this from scratch in PyTorch or Numpy. If you can’t do that, then you know you don’t understand it and you can go back and re-watch lesson 6 and check out the notebook and copy some of the ideas until you can. It is really important that you can write that from scratch — it’s less than a screen of code. So you want to make sure you can create a 2 layer RNN. Below is what it looks like if you unroll it.</p><figure name="9272" id="9272" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_2GKBK9P_zpUieQF6JFyChw.png"></figure><p name="95cf" id="95cf" class="graf graf--p graf-after--figure">To get to a point that we have (x, y) pairs of sentences, we will start by downloading the dataset [<a href="https://youtu.be/tY0n9OT5_nA?t=22m39s" data-href="https://youtu.be/tY0n9OT5_nA?t=22m39s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">22:39</a>]. Training a translation model takes a long time. Google’s translation model has eight layers of RNN stacked on top of each other. There is no conceptual difference between eight layers and two layers. If you are Google and you have more GPUs or TPUs than you know what to do with, then you are fine doing that. Where else, in our case, it’s pretty likely that the kind of sequence to sequence models we are building are not going to require that level of computation. So to keep things simple [<a href="https://youtu.be/tY0n9OT5_nA?t=23m22s" data-href="https://youtu.be/tY0n9OT5_nA?t=23m22s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">23:22</a>], let’s do a cut-down thing where rather than learning how to translate French into English for any sentence, let’s learn to translate French questions into English questions — specifically questions that start with what/where/which/when. So here is a regex which looks for things that start with “wh” and end with a question mark.</p><pre name="2ba2" id="2ba2" class="graf graf--pre graf-after--p">re_eq = re.compile('^(Wh[^?.!]+\?)')<br>re_fq = re.compile('^([^?.!]+\?)')</pre><pre name="06a3" id="06a3" class="graf graf--pre graf-after--pre">lines = ((re_eq.search(eq), re_fq.search(fq)) <br>         <strong class="markup--strong markup--pre-strong">for</strong> eq, fq <strong class="markup--strong markup--pre-strong">in</strong> zip(open(en_fname, encoding='utf-8'), <br>                           open(fr_fname, encoding='utf-8')))</pre><pre name="9147" id="9147" class="graf graf--pre graf-after--pre">qs = [(e.group(), f.group()) <strong class="markup--strong markup--pre-strong">for</strong> e,f <strong class="markup--strong markup--pre-strong">in</strong> lines <strong class="markup--strong markup--pre-strong">if</strong> e <strong class="markup--strong markup--pre-strong">and</strong> f]</pre><p name="179f" id="179f" class="graf graf--p graf-after--pre">We go through the corpus [<a href="https://youtu.be/tY0n9OT5_nA?t=23m43s" data-href="https://youtu.be/tY0n9OT5_nA?t=23m43s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">23:43</a>], open up each of the two files, each line is one parallel text, zip them together, grab the English question and the French question, and check whether they match the regular expressions.</p><pre name="2aef" id="2aef" class="graf graf--pre graf-after--p">pickle.dump(qs, (PATH/'fr-en-qs.pkl').open('wb'))<br>qs = pickle.load((PATH/'fr-en-qs.pkl').open('rb'))</pre><p name="c656" id="c656" class="graf graf--p graf-after--pre">Dump that out as a pickle so we don’t have to do it again and so now we have 52,000 sentence pairs and here are some examples:</p><pre name="794f" id="794f" class="graf graf--pre graf-after--p">qs[:5], len(qs)</pre><pre name="34dc" id="34dc" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">([('What is light ?', 'Qu’est-ce que la lumière?'),<br>  ('Who are we?', 'Où sommes-nous?'),<br>  ('Where did we come from?', "D'où venons-nous?"),<br>  ('What would we do without it?', 'Que ferions-nous sans elle ?'),<br>  ('What is the absolute location (latitude and longitude) of Badger, Newfoundland and Labrador?',<br>   'Quelle sont les coordonnées (latitude et longitude) de Badger, à Terre-Neuve-etLabrador?')],<br> 52331)</em></pre><p name="3a70" id="3a70" class="graf graf--p graf-after--pre">One nice thing about this is that what/who/where type questions tend to be fairly short [<a href="https://youtu.be/tY0n9OT5_nA?t=24m8s" data-href="https://youtu.be/tY0n9OT5_nA?t=24m8s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">24:08</a>]. But the idea that we could learn from scratch with no previous understanding of the idea of language let alone of English or French that we could create something that can translate one to the other for any arbitrary question with only 50k sentences sounds like a ludicrously difficult thing to ask this to do. So it would be impressive if we can make any progress what so ever. This is very little data to do a very complex exercise.</p><p name="d8ad" id="d8ad" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">qs</code> contains the tuples of French and English [<a href="https://youtu.be/tY0n9OT5_nA?t=24m48s" data-href="https://youtu.be/tY0n9OT5_nA?t=24m48s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">24:48</a>]. You can use this handy idiom to split them apart into a list of English questions and a list of French questions.</p><pre name="2bb5" id="2bb5" class="graf graf--pre graf-after--p">en_qs,fr_qs = zip(*qs)</pre><p name="bd0a" id="bd0a" class="graf graf--p graf-after--pre">Then we tokenize the English questions and tokenize the French questions. So remember that just means splitting them up into separate words or word-like things. By default [<a href="https://youtu.be/tY0n9OT5_nA?t=25m11s" data-href="https://youtu.be/tY0n9OT5_nA?t=25m11s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">25:11</a>], the tokenizer that we have here (remember this is a wrapper around the spaCy tokenizer which is a fantastic tokenizer) assumes English. So to ask for French, you just add an extra parameter <code class="markup--code markup--p-code">'fr'</code>. The first time you do this, you will get an error saying you don’t have the spaCy French model installed so you can run <code class="markup--code markup--p-code">python -m spacy download fr</code> to grab the French model.</p><pre name="5efd" id="5efd" class="graf graf--pre graf-after--p">en_tok = Tokenizer.proc_all_mp(partition_by_cores(en_qs))</pre><pre name="aea8" id="aea8" class="graf graf--pre graf-after--pre">fr_tok = Tokenizer.proc_all_mp(partition_by_cores(fr_qs), 'fr')</pre><p name="d690" id="d690" class="graf graf--p graf-after--pre">It is unlikely that any of you are going to have RAM problems here because this is not particularly big corpus but some of the students were trying to train a new language models during the week and were having RAM problems. If you do, it’s worth knowing what these functions (<code class="markup--code markup--p-code">proc_all_mp</code>) are actually doing. <code class="markup--code markup--p-code">proc_all_mp</code> is processing every sentence across multiple processes [<a href="https://youtu.be/tY0n9OT5_nA?t=25m59s" data-href="https://youtu.be/tY0n9OT5_nA?t=25m59s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">25:59</a>]:</p><figure name="a07d" id="a07d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*3dijGYRXl1Vf9MFLD5AUOA.png" data-width="513" data-height="80" src="../img/1_3dijGYRXl1Vf9MFLD5AUOA.png"></figure><p name="e011" id="e011" class="graf graf--p graf-after--figure">The function above finds out how many CPUs you have, divide it by two (because normally with hyper-threading they don’t actually all work in parallel), then in parallel run this <code class="markup--code markup--p-code">proc_all</code> function. So that is going to spit out a whole separate Python processes for every CPU you have. If you have a lot of cores, that is a lot of Python processes — everyone is going to load all this data in and that can potentially use up all your RAM. So you could replace that with just <code class="markup--code markup--p-code">proc_all</code> rather than <code class="markup--code markup--p-code">proc_all_mp</code> to use less RAM. Or you could just use less cores. At the moment, we are calling <code class="markup--code markup--p-code">partition_by_cores</code> which calls <code class="markup--code markup--p-code">partition</code> on a list and asks to split it into a number of equal length things according to how many CPUs you have. So you could replace that to split into a smaller list and run it on less things.</p><figure name="1c92" id="1c92" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*9_D6dkXM4mR8fPf0E2eLcg.png" data-width="358" data-height="39" src="../img/1_9_D6dkXM4mR8fPf0E2eLcg.png"></figure><p name="ab7b" id="ab7b" class="graf graf--p graf-after--figure">Having tokenized the English and French, you can see how it gets split up [<a href="https://youtu.be/tY0n9OT5_nA?t=28m4s" data-href="https://youtu.be/tY0n9OT5_nA?t=28m4s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">28:04</a>]:</p><pre name="b154" id="b154" class="graf graf--pre graf-after--p">en_tok[0], fr_tok[0]</pre><pre name="8205" id="8205" class="graf graf--pre graf-after--pre">(['what', 'is', 'light', '?'],<br> ['qu’', 'est', '-ce', 'que', 'la', 'lumière', '?'])</pre><p name="7ae1" id="7ae1" class="graf graf--p graf-after--pre">You can see the tokenization for French is quite different looking because French loves their apostrophes and their hyphens. So if you try to use an English tokenizer for a French sentence, you’re going to get a pretty crappy outcome. You don’t need to know heaps of NLP ideas to use deep learning for NLP, but just some basic stuff like use the right tokenizer for your language is important [<a href="https://youtu.be/tY0n9OT5_nA?t=28m23s" data-href="https://youtu.be/tY0n9OT5_nA?t=28m23s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">28:23</a>]. Some of the students this week in our study group have been trying to build language models for Chinese instance which of course doesn’t really have the concept of a tokenizer in the same way, so we’ve been starting to look at <a href="https://github.com/google/sentencepiece" data-href="https://github.com/google/sentencepiece" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">sentence piece</a> which splits things into arbitrary sub-word units and so when Jeremy says tokenize, if you are using a language that doesn’t have spaces in, you should probably be checking out sentence piece or some other similar sub-word unit thing instead. Hopefully in the next week or two, we will be able to report back with some early results of these experiments with Chinese.</p><pre name="0e86" id="0e86" class="graf graf--pre graf-after--p">np.percentile([len(o) <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> en_tok], 90), <br>    np.percentile([len(o) <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> fr_tok], 90)</pre><pre name="2c02" id="2c02" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(23.0, 28.0)</em></pre><pre name="7002" id="7002" class="graf graf--pre graf-after--pre">keep = np.array([len(o)&lt;30 <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> en_tok])</pre><pre name="4773" id="4773" class="graf graf--pre graf-after--pre">en_tok = np.array(en_tok)[keep]<br>fr_tok = np.array(fr_tok)[keep]</pre><pre name="20e4" id="20e4" class="graf graf--pre graf-after--pre">pickle.dump(en_tok, (PATH/'en_tok.pkl').open('wb'))<br>pickle.dump(fr_tok, (PATH/'fr_tok.pkl').open('wb'))</pre><pre name="2cfa" id="2cfa" class="graf graf--pre graf-after--pre">en_tok = pickle.load((PATH/'en_tok.pkl').open('rb'))<br>fr_tok = pickle.load((PATH/'fr_tok.pkl').open('rb'))</pre><p name="6c7d" id="6c7d" class="graf graf--p graf-after--pre">So having tokenized it [<a href="https://youtu.be/tY0n9OT5_nA?t=29m25s" data-href="https://youtu.be/tY0n9OT5_nA?t=29m25s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">29:25</a>], we will save that to disk. Then remember, the next step after we create tokens is to turn them into numbers&nbsp;. To do that, we have two steps — the first is to get a list of all of the words that appear and then we turn every word into the index. If there are more than 40,000 words that appear, then let’s cut it off there so it doesn’t go too crazy. We insert a few extra tokens for beginning of stream (<code class="markup--code markup--p-code">_bos_</code>), padding (<code class="markup--code markup--p-code">_pad_</code>), end of stream (<code class="markup--code markup--p-code">_eos_</code>), and unknown (<code class="markup--code markup--p-code">_unk</code>). So if we try to look up something that wasn’t in the 40,000 most common, then we use a <code class="markup--code markup--p-code">deraultdict</code> to return 3 which is unknown.</p><pre name="b4cc" id="b4cc" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> toks2ids(tok,pre):<br>    freq = Counter(p <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> tok <strong class="markup--strong markup--pre-strong">for</strong> p <strong class="markup--strong markup--pre-strong">in</strong> o)<br>    itos = [o <strong class="markup--strong markup--pre-strong">for</strong> o,c <strong class="markup--strong markup--pre-strong">in</strong> freq.most_common(40000)]<br>    itos.insert(0, '_bos_')<br>    itos.insert(1, '_pad_')<br>    itos.insert(2, '_eos_')<br>    itos.insert(3, '_unk')<br>    stoi = collections.defaultdict(<strong class="markup--strong markup--pre-strong">lambda</strong>: 3, <br>                                   {v:k <strong class="markup--strong markup--pre-strong">for</strong> k,v <strong class="markup--strong markup--pre-strong">in</strong> enumerate(itos)})<br>    ids = np.array([([stoi[o] <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> p] + [2]) <strong class="markup--strong markup--pre-strong">for</strong> p <strong class="markup--strong markup--pre-strong">in</strong> tok])<br>    np.save(TMP_PATH/f'<strong class="markup--strong markup--pre-strong">{pre}</strong>_ids.npy', ids)<br>    pickle.dump(itos, open(TMP_PATH/f'<strong class="markup--strong markup--pre-strong">{pre}</strong>_itos.pkl', 'wb'))<br>    <strong class="markup--strong markup--pre-strong">return</strong> ids,itos,stoi</pre><p name="af75" id="af75" class="graf graf--p graf-after--pre">Now we can go ahead and turn every token into an ID by putting it through the string to integer dictionary (<code class="markup--code markup--p-code">stoi</code>) we just created and then at the end of that let’s add the number 2 which is the end of stream. The code you see here is the code Jeremy writes when he is iterating and experimenting [<a href="https://youtu.be/tY0n9OT5_nA?t=30m25s" data-href="https://youtu.be/tY0n9OT5_nA?t=30m25s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">30:25</a>]. Because 99% of the code he writes while iterating and experimenting turns out to be totally wrong or stupid or embarrassing and you don’t get to see it. But there is not point refactoring that and making it beautiful when he’s writing it so he wanted you to see all the little shortcuts he has. Rather than having some constant for <code class="markup--code markup--p-code">_eos_</code> marker and using that, when he is prototyping he just does the easy stuff. Not so much that he ends up with broken code but he tries to find some middle ground between beautiful code and code that works.</p><p name="fb04" id="fb04" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: Just heard him mention that we divide the number of CPUs by 2 because with hyper-threading, we don’t get a speed-up using all the hyper threaded cores. Is this based on practical experience or is there some underlying reason why we wouldn’t get additional speedup [<a href="https://youtu.be/tY0n9OT5_nA?t=31m18s" data-href="https://youtu.be/tY0n9OT5_nA?t=31m18s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">31:18</a>]? Yes, it’s just practical experience and it’s not all things seemed like this, but I definitely noticed with tokenization — hyper-threading seemed to slow things down a little bit. Also if I use all the cores, often I want to do something else at the same time (like running some interactive notebook) and I don’t have any spare room to do that.</p><p name="e5c2" id="e5c2" class="graf graf--p graf-after--p">Now for our English and French, we can grab a list of IDs <code class="markup--code markup--p-code">en_ids</code> [<a href="https://youtu.be/tY0n9OT5_nA?t=32m1s" data-href="https://youtu.be/tY0n9OT5_nA?t=32m1s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">32:01</a>]. When we do that, of course, we need to make sure that we also store the vocabulary. There is no point having IDs if we don’t know what a number 5 represents, there is no point having a number 5. So that’s our vocabulary <code class="markup--code markup--p-code">en_itos</code> and reverse mapping <code class="markup--code markup--p-code">en_stoi</code> that we can use to convert more corpuses in the future.</p><pre name="1d04" id="1d04" class="graf graf--pre graf-after--p">en_ids,en_itos,en_stoi = toks2ids(en_tok,'en')<br>fr_ids,fr_itos,fr_stoi = toks2ids(fr_tok,'fr')</pre><p name="900a" id="900a" class="graf graf--p graf-after--pre">Just to confirm it’s working, we can go through each ID, convert the int to a string, and spit that out — there we have our sentence back now with an end of stream marker at the end. Our English vocab is 17,000 and our French vocab is 25,000, so that’s not too big and not too complex vocab that we are dealing with.</p><pre name="9976" id="9976" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> load_ids(pre):<br>    ids = np.load(TMP_PATH/f'<strong class="markup--strong markup--pre-strong">{pre}</strong>_ids.npy')<br>    itos = pickle.load(open(TMP_PATH/f'<strong class="markup--strong markup--pre-strong">{pre}</strong>_itos.pkl', 'rb'))<br>    stoi = collections.defaultdict(<strong class="markup--strong markup--pre-strong">lambda</strong>: 3, <br>                                   {v:k <strong class="markup--strong markup--pre-strong">for</strong> k,v <strong class="markup--strong markup--pre-strong">in</strong> enumerate(itos)})<br>    <strong class="markup--strong markup--pre-strong">return</strong> ids,itos,stoi</pre><pre name="dbb5" id="dbb5" class="graf graf--pre graf-after--pre">en_ids,en_itos,en_stoi = load_ids('en')<br>fr_ids,fr_itos,fr_stoi = load_ids('fr')</pre><pre name="da07" id="da07" class="graf graf--pre graf-after--pre">[fr_itos[o] <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> fr_ids[0]], len(en_itos), len(fr_itos)</pre><pre name="1c35" id="1c35" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(['qu’', 'est', '-ce', 'que', 'la', 'lumière', '?', '_eos_'], 17573, 24793)</em></pre><h4 name="c016" id="c016" class="graf graf--h4 graf-after--pre">Word vectors&nbsp;[<a href="https://youtu.be/tY0n9OT5_nA?t=32m53s" data-href="https://youtu.be/tY0n9OT5_nA?t=32m53s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">32:53</a>]</h4><p name="fef2" id="fef2" class="graf graf--p graf-after--h4">We spent a lot of time on the forum during the week discussing how pointless word vectors are and how you should stop getting so excited about them — and now we are going to use them. Why? All the stuff we’ve been learning about using language models and pre-trained proper models rather than pre-trained linear single layers which is what word vectors are, applies equally well to sequence to sequence. But Jeremy and Sebastian are starting to look at that. There is a whole thing for anybody interested in creating some genuinely new highly publishable results, the entire area of sequence to sequence with pre-trained language models has not been touched yet. Jeremy believes it is going to be just as good as classifications. If you work on this and you get to the point where you have something that is looking exciting and you want help publishing it, Jeremy is very happy to help co-author papers. So feel free to reach out when you have some interesting results.</p><p name="7a41" id="7a41" class="graf graf--p graf-after--p">At this stage, we do not have any of that, so we are going to use very little fastai [<a href="https://youtu.be/tY0n9OT5_nA?t=34m14s" data-href="https://youtu.be/tY0n9OT5_nA?t=34m14s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">34:14</a>]. All we have is word vectors — so let’s at least use decent word vectors. Word2vec is very old word word vectors. There are better word vectors now and fast.text is a pretty good source of word vectors. There is hundreds of languages available for them, and your language is likely to be represented.</p><p name="684c" id="684c" class="graf graf--p graf-after--p">fasttext word vectors available from <a href="https://fasttext.cc/docs/en/english-vectors.html" data-href="https://fasttext.cc/docs/en/english-vectors.html" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">https://fasttext.cc/docs/en/english-vectors.html</a></p><p name="617b" id="617b" class="graf graf--p graf-after--p">fasttext Python library is not available in PyPI but here is a handy trick [<a href="https://youtu.be/tY0n9OT5_nA?t=35m3s" data-href="https://youtu.be/tY0n9OT5_nA?t=35m3s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">35:03</a>]. If there is a GitHub repo that has a setup.py and reqirements.txt in it, you can just chuck <code class="markup--code markup--p-code">git+</code> at the start then stick that in your <code class="markup--code markup--p-code">pip install</code> and it works. Hardly anybody seems to know this and if you go to the fasttext repo, they won’t tell you this — they’ll say you have to download it and <code class="markup--code markup--p-code">cd</code> into it and blah but you don’t. You can just run this:</p><pre name="df82" id="df82" class="graf graf--pre graf-after--p"><em class="markup--em markup--pre-em"># ! pip install git+https://github.com/facebookresearch/fastText.git</em></pre><pre name="1e85" id="1e85" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">import</strong> <strong class="markup--strong markup--pre-strong">fastText</strong> <strong class="markup--strong markup--pre-strong">as</strong> <strong class="markup--strong markup--pre-strong">ft</strong></pre><p name="7e13" id="7e13" class="graf graf--p graf-after--pre">To use the fastText library, you’ll need to download <a href="https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md" data-href="https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">fasttext word vectors</a> for your language (download the ‘bin plus text’ ones).</p><pre name="d8b2" id="d8b2" class="graf graf--pre graf-after--p">en_vecs = ft.load_model(str((PATH/'wiki.en.bin')))</pre><pre name="10c6" id="10c6" class="graf graf--pre graf-after--pre">fr_vecs = ft.load_model(str((PATH/'wiki.fr.bin')))</pre><p name="e549" id="e549" class="graf graf--p graf-after--pre">Above are our English and French models. There are a text version and a binary version. The binary version is faster, so we will use that. The text version is also a bit buggy. We are going to convert it into a standard Python dictionary to make it a bit easier to work with [<a href="https://youtu.be/tY0n9OT5_nA?t=35m55s" data-href="https://youtu.be/tY0n9OT5_nA?t=35m55s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">35:55</a>]. This is just going through each word with a dictionary comprehension and save it as a pickle dictionary:</p><pre name="66d1" id="66d1" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> get_vecs(lang, ft_vecs):<br>    vecd = {w:ft_vecs.get_word_vector(w) <br>                <strong class="markup--strong markup--pre-strong">for</strong> w <strong class="markup--strong markup--pre-strong">in</strong> ft_vecs.get_words()}<br>    pickle.dump(vecd, open(PATH/f'wiki.<strong class="markup--strong markup--pre-strong">{lang}</strong>.pkl','wb'))<br>    <strong class="markup--strong markup--pre-strong">return</strong> vecd</pre><pre name="e154" id="e154" class="graf graf--pre graf-after--pre">en_vecd = get_vecs('en', en_vecs)<br>fr_vecd = get_vecs('fr', fr_vecs)</pre><pre name="8971" id="8971" class="graf graf--pre graf-after--pre">en_vecd = pickle.load(open(PATH/'wiki.en.pkl','rb'))<br>fr_vecd = pickle.load(open(PATH/'wiki.fr.pkl','rb'))</pre><pre name="7632" id="7632" class="graf graf--pre graf-after--pre">ft_words = ft_vecs.get_words(include_freq=<strong class="markup--strong markup--pre-strong">True</strong>)<br>ft_word_dict = {k:v <strong class="markup--strong markup--pre-strong">for</strong> k,v <strong class="markup--strong markup--pre-strong">in</strong> zip(*ft_words)}<br>ft_words = sorted(ft_word_dict.keys(), <br>                     key=<strong class="markup--strong markup--pre-strong">lambda</strong> x: ft_word_dict[x])</pre><p name="e671" id="e671" class="graf graf--p graf-after--pre">Now we have our pickle dictionary, we can go ahead and look up a word, for example, a comma [<a href="https://youtu.be/tY0n9OT5_nA?t=36m7s" data-href="https://youtu.be/tY0n9OT5_nA?t=36m7s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">36:07</a>]. That will return a vector. The length of the vector is the dimensionality of this set of word vectors. In this case, we have 300 dimensional English and French word vectors.</p><pre name="955a" id="955a" class="graf graf--pre graf-after--p">dim_en_vec = len(en_vecd[','])<br>dim_fr_vec = len(fr_vecd[','])<br>dim_en_vec,dim_fr_vec</pre><pre name="23a0" id="23a0" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(300, 300)</em></pre><p name="9fa0" id="9fa0" class="graf graf--p graf-after--pre">For reasons you will see in a moment, we also want to find out what the mean and standard deviation of our vectors are. So the mean is about zero and standard deviation is about 0.3.</p><pre name="c268" id="c268" class="graf graf--pre graf-after--p">en_vecs = np.stack(list(en_vecd.values()))<br>en_vecs.mean(),en_vecs.std()</pre><pre name="8d1d" id="8d1d" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(0.0075652334, 0.29283327)</em></pre><h4 name="9944" id="9944" class="graf graf--h4 graf-after--pre">Model data&nbsp;[<a href="https://youtu.be/tY0n9OT5_nA?t=36m48s" data-href="https://youtu.be/tY0n9OT5_nA?t=36m48s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">36:48</a>]</h4><p name="92a6" id="92a6" class="graf graf--p graf-after--h4">Often corpuses have a pretty long tailed distribution of sequence length and it’s the longest sequences that tend to overwhelm how long things take, how much memory is used, etc. So in this case, we are going to grab 99th to 97th percentile of the English and French and truncate them to that amount. Originally Jeremy was using 90 percentiles (hence the variable name):</p><pre name="f946" id="f946" class="graf graf--pre graf-after--p">enlen_90 = int(np.percentile([len(o) <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> en_ids], 99))<br>frlen_90 = int(np.percentile([len(o) <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> fr_ids], 97))<br>enlen_90,frlen_90</pre><pre name="9295" id="9295" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(29, 33)</em></pre><p name="abbc" id="abbc" class="graf graf--p graf-after--pre">We are nearly there [<a href="https://youtu.be/tY0n9OT5_nA?t=37m24s" data-href="https://youtu.be/tY0n9OT5_nA?t=37m24s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">37:24</a>]. We’ve got our tokenized, numerixalized English and French dataset. We’ve got some word vectors. So now we need to get it ready for PyTorch. PyTorch expects a <code class="markup--code markup--p-code">Dataset</code> object and hopefully by now you can say that a Dataset object requires two things — a length (<code class="markup--code markup--p-code">__len__</code>)and an indexer (<code class="markup--code markup--p-code">__getitem__</code>). Jeremy started out writing <code class="markup--code markup--p-code">Seq2SeqDataset</code> which turned out to be just a generic <code class="markup--code markup--p-code">Dataset</code> [<a href="https://youtu.be/tY0n9OT5_nA?t=37m52s" data-href="https://youtu.be/tY0n9OT5_nA?t=37m52s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">37:52</a>].</p><pre name="c9ec" id="c9ec" class="graf graf--pre graf-after--p">en_ids_tr = np.array([o[:enlen_90] <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> en_ids])<br>fr_ids_tr = np.array([o[:frlen_90] <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> fr_ids])</pre><pre name="7fca" id="7fca" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">Seq2SeqDataset</strong>(Dataset):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, x, y): self.x,self.y = x,y<br>    <strong class="markup--strong markup--pre-strong">def</strong> __getitem__(self, idx): <strong class="markup--strong markup--pre-strong">return</strong> A(self.x[idx], self.y[idx])<br>    <strong class="markup--strong markup--pre-strong">def</strong> __len__(self): <strong class="markup--strong markup--pre-strong">return</strong> len(self.x)</pre><ul class="postList"><li name="d8d7" id="d8d7" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">A</code>&nbsp;: Arrays. It will go through each of the thing you pass it, if it is not already a numpy array, it converts into a numpy array and returns back a tuple of all of the things you passed it which are now guaranteed to be numpy arrays [<a href="https://youtu.be/tY0n9OT5_nA?t=38m32s" data-href="https://youtu.be/tY0n9OT5_nA?t=38m32s" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">38:32</a>].</li><li name="b87a" id="b87a" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">V</code>&nbsp;: Variables</li><li name="bffc" id="bffc" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">T</code>&nbsp;: Tensors</li></ul><h4 name="19e1" id="19e1" class="graf graf--h4 graf-after--li">Training set and validation set&nbsp;[<a href="https://youtu.be/tY0n9OT5_nA?t=39m3s" data-href="https://youtu.be/tY0n9OT5_nA?t=39m3s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">39:03</a>]</h4><p name="0c7c" id="0c7c" class="graf graf--p graf-after--h4">Now we need to grab our English and French IDs and get a training set and a validation set. One of the things which is pretty disappointing about a lot of code out there on the internet is that they don’t follow some simple best practices. For example, if you go to PyTorch website, they have an example section for sequence to sequence translation. Their example does not have a separate validation set. Jeremy tried training according to their settings and tested it with a validation set and it turned out that it overfit massively. So this is not just a theoretical problem — the actual PyTorch repo has the actual official sequence to sequence translation example which does not check for overfitting and overfits horribly [<a href="https://youtu.be/tY0n9OT5_nA?t=39m41s" data-href="https://youtu.be/tY0n9OT5_nA?t=39m41s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">39:41</a>]. Also it fails to use mini-batches so it actually fails to utilize any of the efficiency of PyTorch whatsoever. Even if you find code in the official PyTorch repo, don’t assume it’s any good at all. The other thing you’ll notice is that pretty much every other sequence to sequence model Jeremy found in PyTorch anywhere on the internet has clearly copied from that crappy PyTorch repo because all has the same variable names, it has the same problems, it has the same mistakes.</p><p name="04a7" id="04a7" class="graf graf--p graf-after--p">Another example is that nearly every PyTorch convolutional neural network Jeremy found does not use an adaptive pooling layer [<a href="https://youtu.be/tY0n9OT5_nA?t=40m27s" data-href="https://youtu.be/tY0n9OT5_nA?t=40m27s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">40:27</a>]. So in other words, the final layer is always average pool (7,7). They assume that the previous layer is 7 by 7 and if you use any other size input, you get an exception, and therefore nearly everybody Jeremy has spoken who uses PyTorch thinks that there is a fundamental limitation of CNNs that they are tied to the input size and that has not been true since VGG. So every time Jeremy grabs a new model and stick it in the fastai repo, he has to go and search for “pool” and add “adaptive” to the start and replace the 7 with a 1and now it works on any sized object. So just be careful. It’s still early days and believe it or not, even though most of you have only started in the last year your deep learning journey, you know quite a lot more about a lot of the more important practical aspects than the vast majority of people that have publishing and writing stuff in official repos. So you need to have a little more self-confidence than you might expect when it comes to reading other people’s code. If you find yourself thinking “that looks odd”, it’s not necessarily you.</p><p name="eb9f" id="eb9f" class="graf graf--p graf-after--p">If the repo you are looking at doesn’t have a section on it saying here is the test we did where we got the same results as the paper that’s supposed to be implementing, that almost certainly means they haven’t got the same results of the paper they’re implementing, but probably haven’t even checked [<a href="https://youtu.be/tY0n9OT5_nA?t=42m13s" data-href="https://youtu.be/tY0n9OT5_nA?t=42m13s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">42:13</a>]. If you run it, definitely won’t get those results because it’s hard to get things right the first time — it takes Jeremy 12 goes. If they haven’t tested it once, it’s almost certainly won’t work.</p><p name="8a8a" id="8a8a" class="graf graf--p graf-after--p">Here is an easy way to get training and validation sets [<a href="https://youtu.be/tY0n9OT5_nA?t=42m45s" data-href="https://youtu.be/tY0n9OT5_nA?t=42m45s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">42:45</a>]. Grab a bunch of random numbers — one for each row of your data, and see if they are bigger than 0.1 or not. That gets you a list of booleans. Index into your array with that list of booleans to grab a training set, index into that array with the opposite of that list of booleans to get your validation set.</p><pre name="35c2" id="35c2" class="graf graf--pre graf-after--p">np.random.seed(42)<br>trn_keep = np.random.rand(len(en_ids_tr))&gt;0.1<br>en_trn,fr_trn = en_ids_tr[trn_keep],fr_ids_tr[trn_keep]<br>en_val,fr_val = en_ids_tr[~trn_keep],fr_ids_tr[~trn_keep]<br>len(en_trn),len(en_val)</pre><pre name="dc57" id="dc57" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(45219, 5041)</em></pre><p name="61fb" id="61fb" class="graf graf--p graf-after--pre">Now we can create our dataset with our X’s and Y’s (i.e. French and English)[<a href="https://youtu.be/tY0n9OT5_nA?t=43m12s" data-href="https://youtu.be/tY0n9OT5_nA?t=43m12s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">43:12</a>]. If you want to translate instead English to French, switch these two around and you’re done.</p><pre name="b69e" id="b69e" class="graf graf--pre graf-after--p">trn_ds = Seq2SeqDataset(fr_trn,en_trn)<br>val_ds = Seq2SeqDataset(fr_val,en_val)</pre><p name="26ca" id="26ca" class="graf graf--p graf-after--pre">Now we need to create DataLoaders [<a href="https://youtu.be/tY0n9OT5_nA?t=43m22s" data-href="https://youtu.be/tY0n9OT5_nA?t=43m22s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">43:22</a>]. We can just grab our data loader and pass in our dataset and batch size. We actually have to transpose the arrays — we won’t go into the details about why, but we can talk about it during the week if you’re interested but have a think about why we might need to transpose their orientation. Since we’ve already done all the pre-processing, there is no point spawning off multiple workers to do augmentation, etc because there is no work to do. So <code class="markup--code markup--p-code">making num_workers=1</code> will save you some time. We have to tell it what our padding index is — that is pretty important because what’s going to happen is that we’ve got different length sentences and fastai will automatically stick them together and pad the shorter ones so that they are all equal length. Remember a tensor has to be rectangular.</p><pre name="cd3c" id="cd3c" class="graf graf--pre graf-after--p">bs=125</pre><pre name="9180" id="9180" class="graf graf--pre graf-after--pre">trn_samp = SortishSampler(en_trn, key=<strong class="markup--strong markup--pre-strong">lambda</strong> x: len(en_trn[x]), <br>                          bs=bs)<br>val_samp = SortSampler(en_val, key=<strong class="markup--strong markup--pre-strong">lambda</strong> x: len(en_val[x]))</pre><pre name="45ef" id="45ef" class="graf graf--pre graf-after--pre">trn_dl = DataLoader(trn_ds, bs, transpose=<strong class="markup--strong markup--pre-strong">True</strong>, transpose_y=<strong class="markup--strong markup--pre-strong">True</strong>, <br>                    num_workers=1, pad_idx=1, pre_pad=<strong class="markup--strong markup--pre-strong">False</strong>, <br>                    sampler=trn_samp)<br>val_dl = DataLoader(val_ds, int(bs*1.6), transpose=<strong class="markup--strong markup--pre-strong">True</strong>, <br>                    transpose_y=<strong class="markup--strong markup--pre-strong">True</strong>, num_workers=1, pad_idx=1,<br>                    pre_pad=<strong class="markup--strong markup--pre-strong">False</strong>, sampler=val_samp)<br>md = ModelData(PATH, trn_dl, val_dl)</pre><p name="d151" id="d151" class="graf graf--p graf-after--pre">In the decoder in particular, we want our padding to be at the end, not at the start [<a href="https://youtu.be/tY0n9OT5_nA?t=44m29s" data-href="https://youtu.be/tY0n9OT5_nA?t=44m29s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">44:29</a>]:</p><ul class="postList"><li name="98e3" id="98e3" class="graf graf--li graf-after--p">Classifier → padding in the beginning. Because we want that final token to represent the last word of the movie review.</li><li name="77d7" id="77d7" class="graf graf--li graf-after--li">Decoder → padding at the end. As you will see, it actually is going to work out a bit better to have the padding at the end.</li></ul><p name="4280" id="4280" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">Sampler [</strong><a href="https://youtu.be/tY0n9OT5_nA?t=44m54s" data-href="https://youtu.be/tY0n9OT5_nA?t=44m54s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong">44:54</strong></a><strong class="markup--strong markup--p-strong">]</strong> Finally, since we’ve got sentences of different lengths coming in and they all have to be put together in a mini-batch to be the same size by padding, we would much prefer that the sentences in a mini-batch are of similar sizes already. Otherwise it is going to be as long as the longest sentence and that is going to end up wasting time and memory. Therefore, we are going to use the sampler tricks that we learnt last time which is the validation set, we are going to ask it to sort everything by length first. Then for the training set, we are going to randomize the order of things but to roughly make it so that things of similar length are about in the same spot.</p><p name="0919" id="0919" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Model Data [</strong><a href="https://youtu.be/tY0n9OT5_nA?t=45m40s" data-href="https://youtu.be/tY0n9OT5_nA?t=45m40s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong">45:40</strong></a><strong class="markup--strong markup--p-strong">]</strong> At this point, we can create a model data object — remember a model data object really does one thing which is it says “I have a training set and a validation set, and an optional test set” and sticks them into a single object. We also has a path so that it has somewhere to store temporary files, models, stuff like that.</p><p name="d364" id="d364" class="graf graf--p graf-after--p">We are not using fastai for very much at all in this example. We used PyTorch compatible Dataset and and DataLoader — behind the scene it is actually using the fastai version because we need it to do the automatic padding for convenience, so there is a few tweaks in fastai version that are a bit faster and a bit more convenient. We are also using fastai’s Samplers, but there is not too much going on here.</p><h4 name="294d" id="294d" class="graf graf--h4 graf-after--p">Architecture [<a href="https://youtu.be/tY0n9OT5_nA?t=46m59s" data-href="https://youtu.be/tY0n9OT5_nA?t=46m59s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">46:59</a>]</h4><figure name="eedd" id="eedd" class="graf graf--figure graf-after--h4"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_IMBl2Aiclyt6PCrg1IQg5A.png"></figure><ul class="postList"><li name="da7d" id="da7d" class="graf graf--li graf-after--figure">The architecture is going to take our sequence of tokens.</li><li name="14c0" id="14c0" class="graf graf--li graf-after--li">It is going to spit them into an encoder (a.k.a. backbone).</li><li name="7d9d" id="7d9d" class="graf graf--li graf-after--li">That is going to spit out the final hidden state which for each sentence, it’s just a single vector.</li></ul><p name="2bf4" id="2bf4" class="graf graf--p graf-after--li">None of this is going to be new [<a href="https://youtu.be/tY0n9OT5_nA?t=47m41s" data-href="https://youtu.be/tY0n9OT5_nA?t=47m41s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">47:41</a>]. That is all going to be using very direct simple techniques that we’ve already learned.</p><ul class="postList"><li name="bef9" id="bef9" class="graf graf--li graf-after--p">Then we are going to take that, and we will spit it into a different RNN which is a decoder. That’s going to have some new stuff because we need something that can go through one word at a time. And it keeps going until it thinks it’s finished the sentence. It doesn’t know how long the sentence is going to be ahead of time. It keeps going until it thinks it’s finished the sentence and then it stops and returns a sentence.</li></ul><pre name="8f62" id="8f62" class="graf graf--pre graf-after--li"><strong class="markup--strong markup--pre-strong">def</strong> create_emb(vecs, itos, em_sz):<br>    emb = nn.Embedding(len(itos), em_sz, padding_idx=1)<br>    wgts = emb.weight.data<br>    miss = []<br>    <strong class="markup--strong markup--pre-strong">for</strong> i,w <strong class="markup--strong markup--pre-strong">in</strong> enumerate(itos):<br>        <strong class="markup--strong markup--pre-strong">try</strong>: wgts[i] = torch.from_numpy(vecs[w]*3)<br>        <strong class="markup--strong markup--pre-strong">except</strong>: miss.append(w)<br>    print(len(miss),miss[5:10])<br>    <strong class="markup--strong markup--pre-strong">return</strong> emb</pre><pre name="32ba" id="32ba" class="graf graf--pre graf-after--pre">nh,nl = 256,2</pre><p name="6d69" id="6d69" class="graf graf--p graf-after--pre">Let’s start with the encoder [<a href="https://youtu.be/tY0n9OT5_nA?t=48m15s" data-href="https://youtu.be/tY0n9OT5_nA?t=48m15s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">48:15</a>]. In terms of the variable naming here, there is identical attributes for encoder and decoder. The encoder version has <code class="markup--code markup--p-code">enc</code> the decoder version has <code class="markup--code markup--p-code">dec</code>.</p><ul class="postList"><li name="d68c" id="d68c" class="graf graf--li graf-after--p"><code class="markup--code markup--li-code">emb_enc</code>: Embeddings for the encoder</li><li name="82eb" id="82eb" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">gru</code>&nbsp;: RNN. GRU and LSTM are nearly the same thing.</li></ul><p name="7106" id="7106" class="graf graf--p graf-after--li">We need to create an embedding layer because remember — what we are being passed is the index of the words into a vocabulary. And we want to grab their fast.text embedding. Then over time, we might want to also fine tune to train that embedding end-to-end.</p><p name="f12b" id="f12b" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">create_emb</code> [<a href="https://youtu.be/tY0n9OT5_nA?t=49m37s" data-href="https://youtu.be/tY0n9OT5_nA?t=49m37s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">49:37</a>]: It is important that you know now how to set the rows and columns for your embedding so the number of rows has to be equal to your vocabulary size — so each vocabulary has a word vector. The size of the embedding is determined by fast.text and fast.text embeddings are size 300. So we have to use size 300 as well otherwise we can’t start out by using their embeddings.</p><p name="69df" id="69df" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">nn.Embedding </code>will initially going to give us a random set of embeddings [<a href="https://youtu.be/tY0n9OT5_nA?t=50m12s" data-href="https://youtu.be/tY0n9OT5_nA?t=50m12s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">50:12</a>]. So we will go through each one of these and if we find it in fast.text, we will replace it with the fast.text embedding. Again, something you should already know is that (<code class="markup--code markup--p-code">emb.weight.data</code>):</p><ul class="postList"><li name="b08a" id="b08a" class="graf graf--li graf-after--p">A PyTorch module that is learnable has <code class="markup--code markup--li-code">weight</code> attribute</li><li name="c64d" id="c64d" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">weight</code> attribute is a <code class="markup--code markup--li-code">Variable</code> that has <code class="markup--code markup--li-code">data</code> attribute</li><li name="76b2" id="76b2" class="graf graf--li graf-after--li">The <code class="markup--code markup--li-code">data</code> attribute is a tensor</li></ul><p name="7e8b" id="7e8b" class="graf graf--p graf-after--li">Now that we’ve got our weight tensor, we can just go through our vocabulary and we can look up the word in our pre-trained vectors and if we find it, we will replace the random weights with that pre-trained vector [<a href="https://youtu.be/tY0n9OT5_nA?t=52m35s" data-href="https://youtu.be/tY0n9OT5_nA?t=52m35s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">52:35</a>]. The random weights have a standard deviation of 1. Our pre-trained vectors has a standard deviation of about 0.3. So again, this is the kind of hacky thing Jeremy does when he is prototyping stuff, he just multiplied it by 3. By the time you see the video of this, we may able to put all this sequence to sequence stuff into the fastai library, you won’t find horrible hacks like that in there (sure hope). But hack away when you are prototyping. Some things won’t be in fast.text in which case, we’ll just keep track of it [<a href="https://youtu.be/tY0n9OT5_nA?t=53m22s" data-href="https://youtu.be/tY0n9OT5_nA?t=53m22s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">53:22</a>]. The print statement is there so that we can see what’s going on (i.e. why are we missing stuff?). Remember we had about 30,000 so we are not missing too many.</p><pre name="f790" id="f790" class="graf graf--pre graf-after--p"><em class="markup--em markup--pre-em">3097 ['l’', "d'", 't_up', 'd’', "qu'"]<br>1285 ["'s", '’s', "n't", 'n’t', ':']</em></pre><p name="0f76" id="0f76" class="graf graf--p graf-after--pre">Jeremy has started doing some stuff around incorporating large vocabulary handling into fastai — it’s not finished yet but hopefully by the time we get here, this kind of stuff will be possible [<a href="https://youtu.be/tY0n9OT5_nA?t=56m50s" data-href="https://youtu.be/tY0n9OT5_nA?t=56m50s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">56:50</a>].</p><pre name="53e4" id="53e4" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">Seq2SeqRNN</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, vecs_enc, itos_enc, em_sz_enc, vecs_dec, <br>                 itos_dec, em_sz_dec, nh, out_sl, nl=2):<br>        super().__init__()<br>        self.nl,self.nh,self.out_sl = nl,nh,out_sl<br>        self.emb_enc = create_emb(vecs_enc, itos_enc, em_sz_enc)<br>        self.emb_enc_drop = nn.Dropout(0.15)<br>        self.gru_enc = nn.GRU(em_sz_enc, nh, num_layers=nl, <br>                              dropout=0.25)<br>        self.out_enc = nn.Linear(nh, em_sz_dec, bias=<strong class="markup--strong markup--pre-strong">False</strong>)<br>        <br>        self.emb_dec = create_emb(vecs_dec, itos_dec, em_sz_dec)<br>        self.gru_dec = nn.GRU(em_sz_dec, em_sz_dec, num_layers=nl, <br>                              dropout=0.1)<br>        self.out_drop = nn.Dropout(0.35)<br>        self.out = nn.Linear(em_sz_dec, len(itos_dec))<br>        self.out.weight.data = self.emb_dec.weight.data<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, inp):<br>        sl,bs = inp.size()<br>        h = self.initHidden(bs)<br>        emb = self.emb_enc_drop(self.emb_enc(inp))<br>        enc_out, h = self.gru_enc(emb, h)<br>        h = self.out_enc(h)<br><br>        dec_inp = V(torch.zeros(bs).long())<br>        res = []<br>        <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(self.out_sl):<br>            emb = self.emb_dec(dec_inp).unsqueeze(0)<br>            outp, h = self.gru_dec(emb, h)<br>            outp = self.out(self.out_drop(outp[0]))<br>            res.append(outp)<br>            dec_inp = V(outp.data.max(1)[1])<br>            <strong class="markup--strong markup--pre-strong">if</strong> (dec_inp==1).all(): <strong class="markup--strong markup--pre-strong">break</strong><br>        <strong class="markup--strong markup--pre-strong">return</strong> torch.stack(res)<br>    <br>    <strong class="markup--strong markup--pre-strong">def</strong> initHidden(self, bs): <br>        <strong class="markup--strong markup--pre-strong">return</strong> V(torch.zeros(self.nl, bs, self.nh))</pre><p name="06f9" id="06f9" class="graf graf--p graf-after--pre">The key thing to know is that encoder takes our inputs and spits out a hidden vector that hopefully will learn to contain all of the information about what that sentence says and how it sets it [<a href="https://youtu.be/tY0n9OT5_nA?t=58m49s" data-href="https://youtu.be/tY0n9OT5_nA?t=58m49s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">58:49</a>]. If it can’t do that, we can’t feed it into a decoder and hope it to spit our our sentence in a different language. So that’s what we want it to learn to do. We are not going to do anything special to make it learn to do that — we are just going to do the three things (data, architecture, loss function) and cross our fingers.</p><p name="d81c" id="d81c" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Decoder [</strong><a href="https://youtu.be/tY0n9OT5_nA?t=59m58s" data-href="https://youtu.be/tY0n9OT5_nA?t=59m58s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong">59:58</strong></a><strong class="markup--strong markup--p-strong">]</strong>: How do we now do the new bit? The basic idea of the new bit is the same. We are going to do exactly the same thing, but we are going to write our own for loop. The for loop is going to do exactly what the for loop inside PyTorch does for encoder, but we are going to do it manually. How big is the for loop? It’s an output sequence length (<code class="markup--code markup--p-code">out_sl</code>) which was something passed to the constructor which is equal to the length of the largest English sentence. Since we are translating into English, so it can’t possibly be longer than that at least in this corpus. If we then used it on some different corpus that was longer, this is going to fail — you could always pass in a different parameter, of course. So the basic idea is the same [<a href="https://youtu.be/tY0n9OT5_nA?t=1h1m6s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h1m6s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:01:06</a>].</p><ul class="postList"><li name="6087" id="6087" class="graf graf--li graf-after--p">We are going to go through and put it through the embedding.</li><li name="3354" id="3354" class="graf graf--li graf-after--li">We are going to stick it through the RNN, dropout, and a linear layer.</li><li name="06f1" id="06f1" class="graf graf--li graf-after--li">We will then append the output to a list which will be stacked into a single tensor and get returned.</li></ul><p name="72a6" id="72a6" class="graf graf--p graf-after--li">Normally, a recurrent neural network works on a whole sequence at a time, but we have a for loop to go through each part of the sequence separately [<a href="https://youtu.be/tY0n9OT5_nA?t=1h1m37s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h1m37s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:01:37</a>]. Wo we have to add a leading unit axis to the start (<code class="markup--code markup--p-code">.unsqueeze(0)</code>) to basicaly say this is a sequence of length one. We are not really taking advantage of the recurrent net much at all — we could easily re-write this with a linear layer.</p><p name="e3e0" id="e3e0" class="graf graf--p graf-after--p">One thing to be aware of is <code class="markup--code markup--p-code">dec_inp</code>[<a href="https://youtu.be/tY0n9OT5_nA?t=1h2m34s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h2m34s" class="markup--anchor markup--p-anchor" rel="nofollow noopener noopener" target="_blank">1:02:34</a>]: <span class="markup--quote markup--p-quote is-other" name="anon_525be9aa639a" data-creator-ids="anon">What is the input to the embedding? The answer is it is the previous word that we translated.</span> The basic idea is if you are trying to translate the 4th word of the new sentence but you don’t know what the third word you just said was, that is going to be really hard. So we are going to feed that in at each time step. What was the previous word at the start? There was none. Specifically, we are going to start out with a beginning of stream token (<code class="markup--code markup--p-code">_bos_</code>) which is zero.</p><p name="a98d" id="a98d" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">outp</code> [<a href="https://youtu.be/tY0n9OT5_nA?t=1h5m24s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h5m24s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:05:24</a>]: it is a tensor whose length is equal to the number of words in our English vocabulary and it contains the probability for every one of those words that it is that word.</p><p name="7115" id="7115" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">outp.data.max</code>&nbsp;: It looks in its tensor to find out which word has the highest probability. <code class="markup--code markup--p-code">max</code> in PyTorch returns two things: the first thing is what is that max probability and the second is what is the index into the array of that max probability. So we want that second item which is the word index with the largest thing.</p><p name="9875" id="9875" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">dec_inp</code>&nbsp;: It contains the word index into the vocabulary of the word. If it’s one (i.e. padding), that means we are done — we reached the end because we finished with a bunch of padding. If it’s not one, let’s go back and continue.</p><p name="2dd1" id="2dd1" class="graf graf--p graf-after--p">Each time, we appended our outputs (not the word but the probabilities) to the list [<a href="https://youtu.be/tY0n9OT5_nA?t=1h6m48s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h6m48s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:06:48</a>] which we stack up into a tensor and we can now go ahead and feed that to a loss function.</p><h4 name="0bd6" id="0bd6" class="graf graf--h4 graf-after--p">Loss function [<a href="https://youtu.be/tY0n9OT5_nA?t=1h7m13s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h7m13s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:07:13</a>]</h4><p name="e70a" id="e70a" class="graf graf--p graf-after--h4">The loss function is categorical cross entropy loss. We have a list of probabilities for each of our classes where the classes are all the words in our English vocab and we have a target which is the correct class (i.e. which is the correct word at this location). There are two tweaks which is why we need to write our own loss function but you can see basically it is going to be cross entropy loss.</p><pre name="15b8" id="15b8" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> seq2seq_loss(input, target):<br>    sl,bs = target.size()<br>    sl_in,bs_in,nc = input.size()<br>    <strong class="markup--strong markup--pre-strong">if</strong> sl&gt;sl_in: input = F.pad(input, (0,0,0,0,0,sl-sl_in))<br>    input = input[:sl]<br>    <strong class="markup--strong markup--pre-strong">return</strong> F.cross_entropy(input.view(-1,nc), target.view(-1))</pre><p name="4a8b" id="4a8b" class="graf graf--p graf-after--pre">Tweaks [<a href="https://youtu.be/tY0n9OT5_nA?t=1h7m40s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h7m40s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:07:40</a>]:</p><ol class="postList"><li name="27b9" id="27b9" class="graf graf--li graf-after--p">If the generated sequence length is shorter than the sequence length of the target, we need to add some padding. PyTorch padding function requires a tuple of 6 to pad a rank 3 tensor (sequence length, batch size, by number of words in the vocab). Each pair represents padding before and after that dimension.</li></ol><p name="f5fd" id="f5fd" class="graf graf--p graf-after--li">2. <code class="markup--code markup--p-code">F.cross_entropy</code> expects a rank 2 tensor, but we have sequence length by batch size, so let’s just flatten out. That is what <code class="markup--code markup--p-code">view(-1,&nbsp;...)</code> does.</p><pre name="6d30" id="6d30" class="graf graf--pre graf-after--p">opt_fn = partial(optim.Adam, betas=(0.8, 0.99))</pre><p name="37be" id="37be" class="graf graf--p graf-after--pre">The difference between&nbsp;<code class="markup--code markup--p-code">.cuda()</code> and <code class="markup--code markup--p-code">to_gpu()</code>&nbsp;: <code class="markup--code markup--p-code">to_gpu</code> will not put to in the GPU if you do not have one. You can also set <code class="markup--code markup--p-code">fastai.core.USE_GPU</code> to <code class="markup--code markup--p-code">false</code> to force it to not use GPU that can be handy for debugging.</p><pre name="cb79" id="cb79" class="graf graf--pre graf-after--p">rnn = Seq2SeqRNN(fr_vecd, fr_itos, dim_fr_vec, en_vecd, en_itos, <br>                 dim_en_vec, nh, enlen_90)<br>learn = RNN_Learner(md, SingleModel(to_gpu(rnn)), opt_fn=opt_fn)<br>learn.crit = seq2seq_loss</pre><pre name="dfe5" id="dfe5" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">3097 ['l’', "d'", 't_up', 'd’', "qu'"]<br>1285 ["'s", '’s', "n't", 'n’t', ':']</em></pre><p name="6e9c" id="6e9c" class="graf graf--p graf-after--pre">We then need something that tells it how to handle learning rate groups so there is a thing called <code class="markup--code markup--p-code">SingleModel</code> that you can pass it to which treats the whole thing as a single learning rate group [<a href="https://youtu.be/tY0n9OT5_nA?t=1h9m40s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h9m40s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:09:40</a>]. So this is the easiest way to turn a PyTorch module into a fastai model.</p><figure name="74f4" id="74f4" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_NW1_lYHLm8R0ML_BWq0QRA.png"></figure><p name="0816" id="0816" class="graf graf--p graf-after--figure">We could just call Learner to turn that into a learner, but if we call RNN_Learner, it does add in <code class="markup--code markup--p-code">save_encoder</code> and <code class="markup--code markup--p-code">load_encoder</code> that can be handy sometimes. In this case, we really could have said <code class="markup--code markup--p-code">Leaner</code> but <code class="markup--code markup--p-code">RNN_Learner</code> also works.</p><pre name="23c2" id="23c2" class="graf graf--pre graf-after--p">learn.lr_find()<br>learn.sched.plot()</pre><figure name="1547" id="1547" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_Fwxxo1lXoIqdM5v24sWfIA.png"></figure><pre name="7b69" id="7b69" class="graf graf--pre graf-after--figure">lr=3e-3<br>learn.fit(lr, 1, cycle_len=12, use_clr=(20,10))</pre><pre name="b0ba" id="b0ba" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss                              <br>    0      5.48978    5.462648  <br>    1      4.616437   4.770539                              <br>    2      4.345884   4.37726                               <br>    3      3.857125   4.136014                              <br>    4      3.612306   3.941867                              <br>    5      3.375064   3.839872                              <br>    6      3.383987   3.708972                              <br>    7      3.224772   3.664173                              <br>    8      3.238523   3.604765                              <br>    9      2.962041   3.587814                              <br>    10     2.96163    3.574888                              <br>    11     2.866477   3.581224</em></pre><pre name="55b4" id="55b4" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[3.5812237]</em></pre><pre name="d550" id="d550" class="graf graf--pre graf-after--pre"><br>learn.save('initial')<br>learn.load('initial')</pre><h4 name="f1e5" id="f1e5" class="graf graf--h4 graf-after--pre">Test [<a href="https://youtu.be/tY0n9OT5_nA?t=1h11m1s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h11m1s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:11:01</a>]</h4><p name="1281" id="1281" class="graf graf--p graf-after--h4">Remember the model attribute of a learner is a standard PyTorch model so we can pass some <code class="markup--code markup--p-code">x</code> which we can grab out of our validation set or you could <code class="markup--code markup--p-code">learn.predict_array</code> or whatever you like to get some predictions. Then we convert those predictions into words by going&nbsp;<code class="markup--code markup--p-code">.max()[1]</code> to grab the index of the highest probability words to get some predictions. Then we can go through a few examples and print out the French, the correct English, and the predicted English for things that are not padding.</p><pre name="ed82" id="ed82" class="graf graf--pre graf-after--p">x,y = next(iter(val_dl))<br>probs = learn.model(V(x))<br>preds = to_np(probs.max(2)[1])<br><br><strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(180,190):<br>    print(' '.join([fr_itos[o] <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> x[:,i] <strong class="markup--strong markup--pre-strong">if</strong> o != 1]))<br>    print(' '.join([en_itos[o] <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> y[:,i] <strong class="markup--strong markup--pre-strong">if</strong> o != 1]))<br>    print(' '.join([en_itos[o] <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> preds[:,i] <strong class="markup--strong markup--pre-strong">if</strong> o!=1]))<br>    print()</pre><pre name="a65c" id="a65c" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">quels facteurs pourraient influer sur le choix de leur emplacement ? _eos_<br>what factors influencetheir location ? _eos_<br>what factors might might influence on the their ? ? _eos_<br><br>qu’ est -ce qui ne peut pas changer ? _eos_<br>what can not change ? _eos_<br>what not change change ? _eos_<br><br>que faites - vous ? _eos_<br>what do you do ? _eos_<br>what do you do ? _eos_<br><br>qui réglemente les pylônes d' antennes ? _eos_<br>who regulates antenna towers ? _eos_<br>who regulates the doors doors ? _eos_<br><br>où sont - ils situés ? _eos_<br>where are they located ? _eos_<br>where are the located ? _eos_<br><br>quelles sont leurs compétences ? _eos_<br>what are their qualifications ? _eos_<br>what are their skills ? _eos_<br><br>qui est victime de harcèlement sexuel ? _eos_<br>who experiences sexual harassment ? _eos_<br>who is victim sexual sexual ? ? _eos_<br><br>quelles sont les personnes qui visitent les communautés autochtones ? _eos_<br>who visits indigenous communities ? _eos_<br>who are people people aboriginal aboriginal ? _eos_<br><br>pourquoi ces trois points en particulier ? _eos_<br>why these specific three ? _eos_<br>why are these two different ? ? _eos_<br><br>pourquoi ou pourquoi pas ? _eos_<br>why or why not ? _eos_<br>why or why not _eos_</em></pre><p name="7a65" id="7a65" class="graf graf--p graf-after--pre">Amazingly enough, this kind of simplest possible written largely from scratch PyTorch module on only fifty thousand sentences is sometimes capable, on validation set, of giving you exactly the right answer. Sometimes the right answer is in slightly different wording, and sometimes sentences that really aren’t grammatically sensible or even have too many question marks. So we are well on the right track. We think you would agree even the simplest possible seq-to-seq trained for a very small number of epochs without any pre-training other than the use of word embeddings is surprisingly good. We are going to improve this later but the message here is even sequence to sequence models you think is simpler than they could possibly work even with less data than you think you could learn from can be surprisingly effective and in certain situations this may be enough for your needs.</p><p name="b22e" id="b22e" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: Would it help to normalize punctuation (e.g. <code class="markup--code markup--p-code">’</code> vs. <code class="markup--code markup--p-code">'</code>)? [<a href="https://youtu.be/tY0n9OT5_nA?t=1h13m10s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h13m10s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:13:10</a>] The answer to this particular case is probably yes — the difference between curly quotes and straight quotes is really semantic. You do have to be very careful though because it may turn out that people using beautiful curly quotes like using more formal language and they are writing in a different way. So if you are going to do some kind of pre-processing like punctuation normalization, you should definitely check your results with and without because nearly always that kind of pre-processing make things worse even when you’re sure it won’t.</p><p name="ecd4" id="ecd4" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: What might be some ways of regularizing these seq2seq models besides dropout and weight decay? [<a href="https://youtu.be/tY0n9OT5_nA?t=1h14m17s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h14m17s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:14:17</a>] Let me think about that during the week. AWD-LSTM which we have been relying a lot has dropouts of many different kinds and there is also a kind of a regularization based on activations and on changes. Jeremy has not seen anybody put anything like that amount of work into regularizing sequence to sequence model and there is a huge opportunity for somebody to do like the AWD-LSTM of seq-to-seq which might be as simple as stealing all the ideas from AWD-LSTM and using them directly in seq-to-seq that would be pretty easy to try. There’s been an interesting paper that Stephen Merity added in the last couple weeks where he used an idea which take all of these different AWD-LSTM hyper parameters and train a bunch of different models and then use a random forest to find out the feature importance — which ones actually matter the most and then figure out how to set them. You could totally use this approach to figure out for sequence to sequence regularization approaches which one is the best and optimize them and that would be amazing. But at the moment, we don’t know if there are additional ideas to sequence to sequence regularization beyond what is in that paper for regular language model.</p><h3 name="aea6" id="aea6" class="graf graf--h3 graf-after--p">Tricks [<a href="https://youtu.be/tY0n9OT5_nA?t=1h16m28s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h16m28s" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">1:16:28</a>]</h3><h4 name="d6c4" id="d6c4" class="graf graf--h4 graf-after--h3"><strong class="markup--strong markup--h4-strong">Trick #1&nbsp;: Go bi-directional</strong></h4><p name="1896" id="1896" class="graf graf--p graf-after--h4">For classification, the approach to bi-directional Jeremy suggested to use is take all of your token sequences, spin them around, train a new language model, and train a new classifier. He also mentioned that wikitext pre-trained model if you replace <code class="markup--code markup--p-code">fwd</code> with <code class="markup--code markup--p-code">bwd</code> in the name, you will get the pre-trained backward model he created for you. Get a set of predictions and then average the predictions just like a normal ensemble. That is how we do bi-dir for that kind of classification. There may be ways to do it end-to-end, but Jeremy hasn’t quite figured them out yet and they are not in fastai yet. So if you figure it out, that’s an interesting line of research. But because we are not doing massive documents where we have to chunk it into separate bits and then pool over them, we can do bi-dir very easily in this case. It is literally as simple as adding <code class="markup--code markup--p-code">bidirectional=True</code> to our encoder. People tend not to do bi-directional for the decoder partly because it is kind of considered cheating but maybe it can work in some situations although it might need to be more of an ensemble approach in the decoder because it’s a bit less obvious. But encoder it’s very simple — <code class="markup--code markup--p-code">bidirectional=True</code> and we now have a second RNN that is going the opposite direction. The second RNN is visiting each token in the opposing order so when we get to the final hidden state, it is the first (i.e. left most) token&nbsp;. But the hidden state is the same size, so the final result is that we end up with a tensor with an extra axis of length 2. Depending on what library you use, often that will be then combined with the number of layers, so if you have 2 layers and bi-directional — that tensor dimension is now length 4. With PyTorch it depends which bit of the process you are looking at as to whether you get a separate result for each layer and/or for each bidirectional bit. You have to look up the documentation and it will tell you input’s output’s tensor sizes appropriate for the number of layers and whether you have <code class="markup--code markup--p-code">bidirectional=True</code>.</p><p name="1fab" id="1fab" class="graf graf--p graf-after--p">In this particular case, you will see all the changes that had to be made [<a href="https://youtu.be/tY0n9OT5_nA?t=1h19m38s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h19m38s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:19:38</a>]. For example&nbsp;,when we added <code class="markup--code markup--p-code">bidirectional=True</code>, the <code class="markup--code markup--p-code">Linear</code> layer now needs number of hidden times 2 (i.e. <code class="markup--code markup--p-code">nh*2</code>) to reflect the fact that we have that second direction in our hidden state. Also in <code class="markup--code markup--p-code">initHidden</code> it’s now <code class="markup--code markup--p-code">self.nl*2</code>.</p><pre name="c1bd" id="c1bd" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">Seq2SeqRNN_Bidir</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, vecs_enc, itos_enc, em_sz_enc, vecs_dec, <br>                 itos_dec, em_sz_dec, nh, out_sl, nl=2):<br>        super().__init__()<br>        self.emb_enc = create_emb(vecs_enc, itos_enc, em_sz_enc)<br>        self.nl,self.nh,self.out_sl = nl,nh,out_sl<br>        self.gru_enc = nn.GRU(em_sz_enc, nh, num_layers=nl,<br>                              dropout=0.25, bidirectional=<strong class="markup--strong markup--pre-strong">True</strong>)<br>        self.out_enc = nn.Linear(nh<strong class="markup--strong markup--pre-strong">*2</strong>, em_sz_dec, bias=<strong class="markup--strong markup--pre-strong">False</strong>)<br>        self.drop_enc = nn.Dropout(0.05)<br>        self.emb_dec = create_emb(vecs_dec, itos_dec, em_sz_dec)<br>        self.gru_dec = nn.GRU(em_sz_dec, em_sz_dec, num_layers=nl,<br>                              dropout=0.1)<br>        self.emb_enc_drop = nn.Dropout(0.15)<br>        self.out_drop = nn.Dropout(0.35)<br>        self.out = nn.Linear(em_sz_dec, len(itos_dec))<br>        self.out.weight.data = self.emb_dec.weight.data<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, inp):<br>        sl,bs = inp.size()<br>        h = self.initHidden(bs)<br>        emb = self.emb_enc_drop(self.emb_enc(inp))<br>        enc_out, h = self.gru_enc(emb, h)<br>        h = h.view(2,2,bs,-1).permute(0,2,1,3)<br>                .contiguous().view(2,bs,-1)<br>        h = self.out_enc(self.drop_enc(h))</pre><pre name="2136" id="2136" class="graf graf--pre graf-after--pre">        dec_inp = V(torch.zeros(bs).long())<br>        res = []<br>        <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(self.out_sl):<br>            emb = self.emb_dec(dec_inp).unsqueeze(0)<br>            outp, h = self.gru_dec(emb, h)<br>            outp = self.out(self.out_drop(outp[0]))<br>            res.append(outp)<br>            dec_inp = V(outp.data.max(1)[1])<br>            <strong class="markup--strong markup--pre-strong">if</strong> (dec_inp==1).all(): <strong class="markup--strong markup--pre-strong">break</strong><br>        <strong class="markup--strong markup--pre-strong">return</strong> torch.stack(res)<br>    <br>    <strong class="markup--strong markup--pre-strong">def</strong> initHidden(self, bs): <br>        <strong class="markup--strong markup--pre-strong">return</strong> V(torch.zeros(self.nl<strong class="markup--strong markup--pre-strong">*2</strong>, bs, self.nh))</pre><p name="2244" id="2244" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">Question</strong>: Why is making the decoder bi-directional considered cheating? [<a href="https://youtu.be/tY0n9OT5_nA?t=1h20m13s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h20m13s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:20:13</a>] It’s not just cheating but we have this loop going on so it is not as simple as having two tensors. Then how do you turn those two separate loops into a final result? After talking about it during the break, Jeremy has gone from “everybody knows it doesn’t work” to “maybe it could work”, but it requires more thought. It is quite possible during the week, he’ll realize it’s a dumb idea, but we’ll think about it.</p><p name="6d51" id="6d51" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: Why do you need to set a range to the loop? [<a href="https://youtu.be/tY0n9OT5_nA?t=1h20m58s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h20m58s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:20:58</a>] Because when we start training, everything is random so <code class="markup--code markup--p-code">if (dec_inp==1).all(): break</code> will probably never be true. Later on, it will pretty much always break out eventually but basically we are going to go forever. It’s really important to remember when you are designing an architecture that when you start, the model knows nothing about anything. So you want to make sure if it’s going to do something at least it’s vaguely sensible.</p><p name="4c6e" id="4c6e" class="graf graf--p graf-after--p">We got 3.58 cross entropy loss with single direction [<a href="https://youtu.be/tY0n9OT5_nA?t=1h21m46s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h21m46s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:21:46</a>]. With bi-direction, we got down to 3.51, so that improved a little. It shouldn’t really slow things down too much. Bi-directional does mean there is a little bit more sequential processing have to happen, but it is generally a good win. In the Google translation model, of the 8 layers, only the first layer is bi-directional because it allows it to do more in parallel so if you create really deep models you may need to think about which ones are bi-directional otherwise we have performance issues.</p><pre name="ebba" id="ebba" class="graf graf--pre graf-after--p">rnn = Seq2SeqRNN_Bidir(fr_vecd, fr_itos, dim_fr_vec, en_vecd,<br>                       en_itos, dim_en_vec, nh, enlen_90)<br>learn = RNN_Learner(md, SingleModel(to_gpu(rnn)), opt_fn=opt_fn)<br>learn.crit = seq2seq_loss</pre><pre name="aee3" id="aee3" class="graf graf--pre graf-after--pre">learn.fit(lr, 1, cycle_len=12, use_clr=(20,10))</pre><pre name="5cac" id="5cac" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss                              <br>    0      4.896942   4.761351  <br>    1      4.323335   4.260878                              <br>    2      3.962747   4.06161                               <br>    3      3.596254   3.940087                              <br>    4      3.432788   3.944787                              <br>    5      3.310895   3.686629                              <br>    6      3.454976   3.638168                              <br>    7      3.093827   3.588456                              <br>    8      3.257495   3.610536                              <br>    9      3.033345   3.540344                              <br>    10     2.967694   3.516766                              <br>    11     2.718945   3.513977</em></pre><pre name="a27d" id="a27d" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[3.5139771]</em></pre><h4 name="0ad2" id="0ad2" class="graf graf--h4 graf-after--pre">Trick #2 Teacher Forcing [<a href="https://youtu.be/tY0n9OT5_nA?t=1h22m39s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h22m39s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:22:39</a>]</h4><p name="76cb" id="76cb" class="graf graf--p graf-after--h4">Now let’s talk about teacher forcing. When a model starts learning, it knows nothing about nothing. So when the model starts learning, it is not going to spit out “Er” at the first step, it is going to spit out some random meaningless word because it doesn’t know anything about German or about English or about the idea of language. And it is going to feed it to the next process as an input and be totally unhelpful. That means, early learning is going to be very difficult because it is feeding in an input that is stupid into a model that knows nothing and somehow it’s going to get better. So it is not asking too much eventually it gets there, but it’s definitely not as helpful as we can be. So what if instead of feeing in the thing I predicted just now, what if we instead we feed in the actual correct word was meant to be. We can’t do that at inference time because by definition we don’t know the correct word - it has to translate it. We can’t require the correct translation in order to do translation.</p><figure name="af01" id="af01" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_DU776SGr1rhYeU7ilIKX9w.png"></figure><p name="9052" id="9052" class="graf graf--p graf-after--figure">So the way it’s set up is we have this thing called <code class="markup--code markup--p-code">pr_force</code> which is probability of forcing [<a href="https://youtu.be/tY0n9OT5_nA?t=1h24m1s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h24m1s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:24:01</a>]. If some random number is less than that probability then we are going to replace our decoder input with the actual correct thing. If we have already gone too far and if it is already longer than the target sequence, we are just going to stop because obviously we can’t give it the correct thing. So you can see how beautiful PyTorch is for this. The key reasons that we switched to PyTorch at this exact point in last year’s class was because Jeremy tried to implement teacher forcing in Keras and TensorFlow and went even more insane than he started. It was weeks of getting nowhere then he saw on Twitter Andrej Karpathy said something about this thing called PyTorch that just came out and it’s really cool. He tried it that day, by the next day, he had teacher forcing. All this stuff of trying to debug things was suddenly so much easier and and this kind of dynamic thing is so much easier. So this is a great example of “hey, I get to use random numbers and if statements”.</p><pre name="8552" id="8552" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">Seq2SeqStepper</strong>(Stepper):<br>    <strong class="markup--strong markup--pre-strong">def</strong> step(self, xs, y, epoch):<br>        self.m.pr_force = (10-epoch)*0.1 <strong class="markup--strong markup--pre-strong">if</strong> epoch&lt;10 <strong class="markup--strong markup--pre-strong">else</strong> 0<br>        xtra = []<br>        output = self.m(*xs, y)<br>        <strong class="markup--strong markup--pre-strong">if</strong> isinstance(output,tuple): output,*xtra = output<br>        self.opt.zero_grad()<br>        loss = raw_loss = self.crit(output, y)<br>        <strong class="markup--strong markup--pre-strong">if</strong> self.reg_fn: loss = self.reg_fn(output, xtra, raw_loss)<br>        loss.backward()<br>        <strong class="markup--strong markup--pre-strong">if</strong> self.clip:   <em class="markup--em markup--pre-em"># Gradient clipping</em><br>            nn.utils.clip_grad_norm(trainable_params_(self.m), <br>                                    self.clip)<br>        self.opt.step()<br>        <strong class="markup--strong markup--pre-strong">return</strong> raw_loss.data[0]</pre><p name="6a94" id="6a94" class="graf graf--p graf-after--pre">Here is the basic idea [<a href="https://youtu.be/tY0n9OT5_nA?t=1h25m29s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h25m29s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:25:29</a>]. At the start of training, let’s set <code class="markup--code markup--p-code">pr_force</code> really high so that nearly always it gets the actual correct previous word and so it has a useful input. Then as we trained a bit more, let’s decrease <code class="markup--code markup--p-code">pr_force</code> so that by the end <code class="markup--code markup--p-code">pr_force</code> is zero and it has to learn properly which is fine because it is now actually feeding in sensible inputs most of the time anyway.</p><pre name="e419" id="e419" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">Seq2SeqRNN_TeacherForcing</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, vecs_enc, itos_enc, em_sz_enc, vecs_dec,<br>                 itos_dec, em_sz_dec, nh, out_sl, nl=2):<br>        super().__init__()<br>        self.emb_enc = create_emb(vecs_enc, itos_enc, em_sz_enc)<br>        self.nl,self.nh,self.out_sl = nl,nh,out_sl<br>        self.gru_enc = nn.GRU(em_sz_enc, nh, num_layers=nl, <br>                              dropout=0.25)<br>        self.out_enc = nn.Linear(nh, em_sz_dec, bias=<strong class="markup--strong markup--pre-strong">False</strong>)<br>        self.emb_dec = create_emb(vecs_dec, itos_dec, em_sz_dec)<br>        self.gru_dec = nn.GRU(em_sz_dec, em_sz_dec, num_layers=nl, <br>                              dropout=0.1)<br>        self.emb_enc_drop = nn.Dropout(0.15)<br>        self.out_drop = nn.Dropout(0.35)<br>        self.out = nn.Linear(em_sz_dec, len(itos_dec))<br>        self.out.weight.data = self.emb_dec.weight.data<br>        self.pr_force = 1.<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, inp, y=<strong class="markup--strong markup--pre-strong">None</strong>):<br>        sl,bs = inp.size()<br>        h = self.initHidden(bs)<br>        emb = self.emb_enc_drop(self.emb_enc(inp))<br>        enc_out, h = self.gru_enc(emb, h)<br>        h = self.out_enc(h)</pre><pre name="07ab" id="07ab" class="graf graf--pre graf-after--pre">        dec_inp = V(torch.zeros(bs).long())<br>        res = []<br>        <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(self.out_sl):<br>            emb = self.emb_dec(dec_inp).unsqueeze(0)<br>            outp, h = self.gru_dec(emb, h)<br>            outp = self.out(self.out_drop(outp[0]))<br>            res.append(outp)<br>            dec_inp = V(outp.data.max(1)[1])<br>            <strong class="markup--strong markup--pre-strong">if</strong> (dec_inp==1).all(): <strong class="markup--strong markup--pre-strong">break</strong><br>            <strong class="markup--strong markup--pre-strong">if</strong> (y <strong class="markup--strong markup--pre-strong">is</strong> <strong class="markup--strong markup--pre-strong">not</strong> <strong class="markup--strong markup--pre-strong">None</strong>) <strong class="markup--strong markup--pre-strong">and</strong> (random.random()&lt;self.pr_force):<br>                <strong class="markup--strong markup--pre-strong">if</strong> i&gt;=len(y): <strong class="markup--strong markup--pre-strong">break</strong><br>                dec_inp = y[i]<br>        <strong class="markup--strong markup--pre-strong">return</strong> torch.stack(res)<br>    <br>    <strong class="markup--strong markup--pre-strong">def</strong> initHidden(self, bs): <br>        <strong class="markup--strong markup--pre-strong">return</strong> V(torch.zeros(self.nl, bs, self.nh))</pre><p name="d6ad" id="d6ad" class="graf graf--p graf-after--pre"><code class="markup--code markup--p-code">pr_force</code>: “probability of forcing”. High in the beginning zero by the end.</p><p name="672d" id="672d" class="graf graf--p graf-after--p">Let’s now write something such that in the training loop, it gradually decreases <code class="markup--code markup--p-code">pr_force</code> [<a href="https://youtu.be/tY0n9OT5_nA?t=1h26m1s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h26m1s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:26:01</a>]. How do we do that? One approach would be to write our own training loop but let’s not do that because we already have a training loop that has progress bars, uses exponential weighted averages to smooth out the losses, keeps track of metrics, and does bunch of things. They also keep track of calling the reset for RNN at the start of the epoch to make sure the hidden state is set to zeros. What we’ve tended to find is that as we start to write some new thing and we need to replace some part of the code, we then add some little hook so that we can all use that hook to make things easier. In this particular case, there is a hook that Jeremy has ended up using all the time which is the hook called the stepper. If you look at the source code, model.py is where our fit function lives which is the lowest level thing that does not require learner or anything much at all — just requires a standard PyTorch model and a model data object. You just need to know how many epochs, a standard PyTorch optimizer, and a standard PyTorch loss function. We hardly ever used in the class, we normally call <code class="markup--code markup--p-code">learn.fit</code>, but <code class="markup--code markup--p-code">learn.fit</code> calls this.</p><figure name="5d07" id="5d07" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_hhksba0Jh8iyWmuC_tPtqg.png"></figure><p name="a435" id="a435" class="graf graf--p graf-after--figure">We have looked at the source code sometime [<a href="https://youtu.be/tY0n9OT5_nA?t=1h27m49s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h27m49s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:27:49</a>]. We’ve seen how it loos through each epoch and that loops through each thing in our batch and calls <code class="markup--code markup--p-code">stepper.step</code>. <code class="markup--code markup--p-code">stepper.step</code> is the thing that is responsible for:</p><ul class="postList"><li name="63a0" id="63a0" class="graf graf--li graf-after--p">calling the model</li><li name="9cff" id="9cff" class="graf graf--li graf-after--li">getting the loss</li><li name="11bb" id="11bb" class="graf graf--li graf-after--li">finding the loss function</li><li name="eeca" id="eeca" class="graf graf--li graf-after--li">calling the optimizer</li></ul><figure name="bf12" id="bf12" class="graf graf--figure graf-after--li"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_dlBOu68q6RyNuQ0opzvxMg.png"></figure><p name="b9d2" id="b9d2" class="graf graf--p graf-after--figure">So by default, <code class="markup--code markup--p-code">stepper.step</code> uses a particular class called <code class="markup--code markup--p-code">Stepper</code> which basically calls the model, zeros the gradient, calls the loss function, calls <code class="markup--code markup--p-code">backward</code>, does gradient clipping if necessary, then calls the optimizer. They are basic steps that back when we looked at “PyTorch from scratch” we had to do. The nice thing is, we can replace that with something else rather than replacing the training loop. If you inherit from <code class="markup--code markup--p-code">Stepper</code>, then write your own version of <code class="markup--code markup--p-code">step</code>&nbsp;, you can just copy and paste the contents of step and add whatever you like. Or if it’s something that you’re going to do before or afterwards, you could even call <code class="markup--code markup--p-code">super.step</code>. In this case, Jeremy rather suspects he has been unnecessarily complicated [<a href="https://youtu.be/tY0n9OT5_nA?t=1h29m12s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h29m12s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:29:12</a>] — he probably could have done something like:</p><pre name="4062" id="4062" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">Seq2SeqStepper</strong>(Stepper):<br>    <strong class="markup--strong markup--pre-strong">def</strong> step(self, xs, y, epoch):<br>        self.m.pr_force = (10-epoch)*0.1 <strong class="markup--strong markup--pre-strong">if</strong> epoch&lt;10 <strong class="markup--strong markup--pre-strong">else</strong> 0<br>        <strong class="markup--strong markup--pre-strong">return</strong> super.step(xs, y, epoch)</pre><p name="16e1" id="16e1" class="graf graf--p graf-after--pre">But as he said, when he is prototyping, he doesn’t think carefully about how to minimize his code — he copied and pasted the contents of the <code class="markup--code markup--p-code">step</code> and he added a single line to the top which was to replace <code class="markup--code markup--p-code">pr_force</code> in the module with something that gradually decreased linearly for the first 10 epochs, and after 10 epochs, it is zero. So total hack but good enough to try it out. The nice thing is that everything else is the same except for the addition of these three lines:</p><pre name="38fe" id="38fe" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">            if</strong> (y <strong class="markup--strong markup--pre-strong">is</strong> <strong class="markup--strong markup--pre-strong">not</strong> <strong class="markup--strong markup--pre-strong">None</strong>) <strong class="markup--strong markup--pre-strong">and</strong> (random.random()&lt;self.pr_force):<br>                <strong class="markup--strong markup--pre-strong">if</strong> i&gt;=len(y): <strong class="markup--strong markup--pre-strong">break</strong><br>                dec_inp = y[i]</pre><p name="2d7c" id="2d7c" class="graf graf--p graf-after--pre">And the only thing we need to do differently is when we call <code class="markup--code markup--p-code">fit</code>&nbsp;, we pass in our customized stepper class.</p><pre name="54ec" id="54ec" class="graf graf--pre graf-after--p">rnn = Seq2SeqRNN_TeacherForcing(fr_vecd, fr_itos, dim_fr_vec, <br>                         en_vecd, en_itos, dim_en_vec, nh, enlen_90)<br>learn = RNN_Learner(md, SingleModel(to_gpu(rnn)), opt_fn=opt_fn)<br>learn.crit = seq2seq_loss</pre><pre name="8b1e" id="8b1e" class="graf graf--pre graf-after--pre">learn.fit(lr, 1, cycle_len=12, use_clr=(20,10), <br>          stepper=Seq2SeqStepper)</pre><pre name="82e9" id="82e9" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss                              <br>    0      4.460622   12.661013 <br>    1      3.468132   7.138729                              <br>    2      3.235244   6.202878                              <br>    3      3.101616   5.454283                              <br>    4      3.135989   4.823736                              <br>    5      2.980696   4.933402                              <br>    6      2.91562    4.287475                              <br>    7      3.032661   3.975346                              <br>    8      3.103834   3.790773                              <br>    9      3.121457   3.578682                              <br>    10     2.917534   3.532427                              <br>    11     3.326946   3.490643</em></pre><pre name="2c7d" id="2c7d" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[3.490643]</em></pre><p name="42a1" id="42a1" class="graf graf--p graf-after--pre">And now our loss is down to 3.49. We needed to make sure at least do 10 epochs because before that, it was cheating by using the teacher forcing.</p><h4 name="92f0" id="92f0" class="graf graf--h4 graf-after--p">Trick #3 Attentional model [<a href="https://youtu.be/tY0n9OT5_nA?t=1h31m" data-href="https://youtu.be/tY0n9OT5_nA?t=1h31m" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:31:00</a>]</h4><p name="682b" id="682b" class="graf graf--p graf-after--h4">This next trick is a bigger and pretty cool trick. It’s called “attention.” The basic idea of attention is this — expecting the entirety of the sentence to be summarized into this single hidden vector is asking a lot. It has to know what was said, how it was said, and everything necessary to create the sentence in German. The idea of attention is basically maybe we are asking too much. Particularly because we could use this form of model (below) where we output every step of the loop to not just have a hidden state at the end but to have a hidden state after every single word. Why not try and use that information? It’s already there but so far we’ve just been throwing it away. Not only that but bi-directional, we got two vectors of state every step that we can use. How can we do this?</p><figure name="5fee" id="5fee" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_CX45skUFZZO6uHsR8IndzA.png"></figure><p name="3a76" id="3a76" class="graf graf--p graf-after--figure">Let’s say we are translating a word “liebte” right now [<a href="https://youtu.be/tY0n9OT5_nA?t=1h32m34s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h32m34s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:32:34</a>]. Which of previous 5 pieces of state do we want? We clearly want “love” because it is the word. How about “zu”? We probably need “eat” and “to” and loved” to make sure we have gotten the tense right and know that I actually need this part of the verb and so forth. So depending on which bit we are translating, we would need one or more bits of these various hidden states. In fact, we probably want some weighting of them. In other words, for these five pieces of hidden state, we want a weighted average [<a href="https://youtu.be/tY0n9OT5_nA?t=1h33m47s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h33m47s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:33:47</a>]. We want it weighted by something that can figure out which bits of the sentence is the most important right now. How do we figure out something like which bits of the sentence are important right now? We create a neural net and we train the neural net to figure it out. When do we train that neural net? End to end. So let’s now train two neural nets [<a href="https://youtu.be/tY0n9OT5_nA?t=1h34m18s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h34m18s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:34:18</a>]. Well, we’ve already got a bunch — RNN encoder, RNN decoder, a couple of linear layers, what the heck, let’s add another neural net into the mix. This neural net is going to spit out a weight for every one of these states and we will take the weighted average at every step, and it’s just another set of parameters that we learn all at the same time. So that is called “attention”.</p><figure name="5bd1" id="5bd1" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_JTCoNaf3I5LQVz2SrYCz0A.png"></figure><p name="77fb" id="77fb" class="graf graf--p graf-after--figure">The idea is that once that attention has been learned, each word is going to take a weighted average as you can see in this terrific demo from Chris Olah and Shan Carter [<a href="https://youtu.be/tY0n9OT5_nA?t=1h34m50s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h34m50s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:34:50</a>]. Check out this <a href="https://distill.pub/2016/augmented-rnns/" data-href="https://distill.pub/2016/augmented-rnns/" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">distill.pub article</a> — these things are interactive diagrams that shows you how the attention works and what the actual attention looks like in a trained translation model.</p><figure name="f04b" id="f04b" class="graf graf--figure graf-after--p"><a href="https://distill.pub/2016/augmented-rnns/" data-href="https://distill.pub/2016/augmented-rnns/" class="graf-imageAnchor" data-action="image-link" data-action-observe-only="true" rel="nofollow"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_fkL30nxS54fKVmyC2jtMrw.png"></a></figure><p name="d5f8" id="d5f8" class="graf graf--p graf-after--figure">Let’s try and implement attention [<a href="https://youtu.be/tY0n9OT5_nA?t=1h35m47s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h35m47s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:35:47</a>]:</p><pre name="2142" id="2142" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> rand_t(*sz): <strong class="markup--strong markup--pre-strong">return</strong> torch.randn(sz)/math.sqrt(sz[0])<br><strong class="markup--strong markup--pre-strong">def</strong> rand_p(*sz): <strong class="markup--strong markup--pre-strong">return</strong> nn.Parameter(rand_t(*sz))</pre><pre name="9b88" id="9b88" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">Seq2SeqAttnRNN</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, vecs_enc, itos_enc, em_sz_enc, vecs_dec, <br>                 itos_dec, em_sz_dec, nh, out_sl, nl=2):<br>        super().__init__()<br>        self.emb_enc = create_emb(vecs_enc, itos_enc, em_sz_enc)<br>        self.nl,self.nh,self.out_sl = nl,nh,out_sl<br>        self.gru_enc = nn.GRU(em_sz_enc, nh, num_layers=nl, <br>                              dropout=0.25)<br>        self.out_enc = nn.Linear(nh, em_sz_dec, bias=<strong class="markup--strong markup--pre-strong">False</strong>)<br>        self.emb_dec = create_emb(vecs_dec, itos_dec, em_sz_dec)<br>        self.gru_dec = nn.GRU(em_sz_dec, em_sz_dec, num_layers=nl, <br>                              dropout=0.1)<br>        self.emb_enc_drop = nn.Dropout(0.15)<br>        self.out_drop = nn.Dropout(0.35)<br>        self.out = nn.Linear(em_sz_dec*2, len(itos_dec))<br>        self.out.weight.data = self.emb_dec.weight.data</pre><pre name="81dc" id="81dc" class="graf graf--pre graf-after--pre">        self.W1 = rand_p(nh, em_sz_dec)<br>        self.l2 = nn.Linear(em_sz_dec, em_sz_dec)<br>        self.l3 = nn.Linear(em_sz_dec+nh, em_sz_dec)<br>        self.V = rand_p(em_sz_dec)</pre><pre name="c634" id="c634" class="graf graf--pre graf-after--pre">    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, inp, y=<strong class="markup--strong markup--pre-strong">None</strong>, ret_attn=<strong class="markup--strong markup--pre-strong">False</strong>):<br>        sl,bs = inp.size()<br>        h = self.initHidden(bs)<br>        emb = self.emb_enc_drop(self.emb_enc(inp))<br>        enc_out, h = self.gru_enc(emb, h)<br>        h = self.out_enc(h)</pre><pre name="e2fd" id="e2fd" class="graf graf--pre graf-after--pre">        dec_inp = V(torch.zeros(bs).long())<br>        res,attns = [],[]<br>        w1e = enc_out @ self.W1<br>        <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(self.out_sl):<br>            w2h = self.l2(h[-1])<br>            u = F.tanh(w1e + w2h)<br>            a = F.softmax(u @ self.V, 0)<br>            attns.append(a)<br>            Xa = (a.unsqueeze(2) * enc_out).sum(0)<br>            emb = self.emb_dec(dec_inp)<br>            wgt_enc = self.l3(torch.cat([emb, Xa], 1))<br>            <br>            outp, h = self.gru_dec(wgt_enc.unsqueeze(0), h)<br>            outp = self.out(self.out_drop(outp[0]))<br>            res.append(outp)<br>            dec_inp = V(outp.data.max(1)[1])<br>            <strong class="markup--strong markup--pre-strong">if</strong> (dec_inp==1).all(): <strong class="markup--strong markup--pre-strong">break</strong><br>            <strong class="markup--strong markup--pre-strong">if</strong> (y <strong class="markup--strong markup--pre-strong">is</strong> <strong class="markup--strong markup--pre-strong">not</strong> <strong class="markup--strong markup--pre-strong">None</strong>) <strong class="markup--strong markup--pre-strong">and</strong> (random.random()&lt;self.pr_force):<br>                <strong class="markup--strong markup--pre-strong">if</strong> i&gt;=len(y): <strong class="markup--strong markup--pre-strong">break</strong><br>                dec_inp = y[i]</pre><pre name="ccc7" id="ccc7" class="graf graf--pre graf-after--pre">        res = torch.stack(res)<br>        <strong class="markup--strong markup--pre-strong">if</strong> ret_attn: res = res,torch.stack(attns)<br>        <strong class="markup--strong markup--pre-strong">return</strong> res</pre><pre name="7764" id="7764" class="graf graf--pre graf-after--pre">    <strong class="markup--strong markup--pre-strong">def</strong> initHidden(self, bs): <br>        <strong class="markup--strong markup--pre-strong">return</strong> V(torch.zeros(self.nl, bs, self.nh))</pre><p name="dd4d" id="dd4d" class="graf graf--p graf-after--pre">With attention, most of the code is identical. The one major difference is this line: <code class="markup--code markup--p-code">Xa = (a.unsqueeze(2) * enc_out).sum(0)</code>&nbsp;. We are going to take a weighted average and the way we are going to do the weighted average is we create a little neural net which we are going to see here:</p><pre name="c09d" id="c09d" class="graf graf--pre graf-after--p">w2h = self.l2(h[-1])<br>u = F.tanh(w1e + w2h)<br>a = F.softmax(u @ self.V, 0)</pre><p name="460a" id="460a" class="graf graf--p graf-after--pre">We use softmax because the nice thing about softmax is that we want to ensure all of the weights that we are using add up to 1 and we also expect that one of those weights should probably be higher than the other ones [<a href="https://youtu.be/tY0n9OT5_nA?t=1h36m38s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h36m38s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:36:38</a>]. Softmax gives us the guarantee that they add up to 1 and because it has <code class="markup--code markup--p-code">e^</code> in it, it tends to encourage one of the weights to be higher than the other ones.</p><p name="9d2d" id="9d2d" class="graf graf--p graf-after--p">Let’s see how this works [<a href="https://youtu.be/tY0n9OT5_nA?t=1h37m9s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h37m9s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:37:09</a>]. We are going to take the last layer’s hidden state and we are going to stick it into a linear layer. Then we are going to stick it into a nonlinear activation, then we are going to do a matrix multiply. So if you think about it — a linear layer, nonlinear activation, matrix multiple — it’s a neural net. It is a neural net with one hidden layer. Stick it into a softmax and then we can use that to weight our encoder outputs. Now rather than just taking the last encoder output, we have the whole tensor of all of the encoder outputs which we just weight by this neural net we created.</p><p name="1183" id="1183" class="graf graf--p graf-after--p">In Python, <code class="markup--code markup--p-code">A @ B</code> is the matrix product, <code class="markup--code markup--p-code">A * B</code> the element-wise product</p><h4 name="ccd3" id="ccd3" class="graf graf--h4 graf-after--p">Papers [<a href="https://youtu.be/tY0n9OT5_nA?t=1h38m18s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h38m18s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:38:18</a>]</h4><ul class="postList"><li name="a4b0" id="a4b0" class="graf graf--li graf-after--h4"><a href="https://arxiv.org/abs/1409.0473" data-href="https://arxiv.org/abs/1409.0473" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">Neural Machine Translation by Jointly Learning to Align and Translate</a> — One amazing paper that originally introduced this idea of attention as well as a couple of key things which have really changed how people work in this field. They say area of attention has been used not just for text but for things like reading text out of pictures or doing various things with computer vi sion.</li><li name="d991" id="d991" class="graf graf--li graf-after--li"><a href="https://arxiv.org/abs/1412.7449" data-href="https://arxiv.org/abs/1412.7449" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">Grammar as a Foreign Language</a> — The second paper which Geoffrey Hinton was involved in that used this idea of RNN with attention to try to replace rules based grammar with an RNN which automatically tagged each word based on the grammar. It turned out to do it better than any rules based system which today seems obvious but at that time it was considered really surprising. They are summary of how attention works which is really nice and concise.</li></ul><p name="d35b" id="d35b" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">Question</strong>: Could you please explain attention again? [<a href="https://youtu.be/tY0n9OT5_nA?t=1h39m46s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h39m46s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:39:46</a>] Sure! Let’s go back and look at our original encoder.</p><figure name="8d45" id="8d45" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_NsX_t2WEEjsPcVyWOrcXFA.png"></figure><p name="bec9" id="bec9" class="graf graf--p graf-after--figure">The RNN spits out two things: it spits out a list of the state after every time step (<code class="markup--code markup--p-code">enc_out</code>), and it also tells you the state at the last time step (<code class="markup--code markup--p-code">h</code>)and we used the state at the last time step to create the input state for our decoder which is one vector <code class="markup--code markup--p-code">s</code> below:</p><figure name="b726" id="b726" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_QQiYoum-_J9Rm7DEQCElxA.png"></figure><p name="51f9" id="51f9" class="graf graf--p graf-after--figure">But we know that it’s creating a vector at every time steps (orange arrows), so wouldn’t it be nice to use them all? But wouldn’t it be nice to use the one or ones that’s most relevant to translating the word we are translating now? So wouldn’t it be nice to be able to take a weighted average of the hidden state at each time step weighted by whatever is the appropriate weight right now. For example, “liebte” would definitely be time step #2 is what it’s all about because that is the word I’m translating. So how do we get a list of weights that is suitable fore the word we are training right now? The answer is by training a neural net to figure out the list of weights. So anytime we want to figure out how to train a little neural net that does any task, the easiest way, normally always to do that is to include it in your module and train it in line with everything else. The minimal possible neural net is something that contains two layers and one nonlinear activation function, so <code class="markup--code markup--p-code">self.l2</code> is one linear layer.</p><figure name="4759" id="4759" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_fnTtr-UiW5JtNy8M9q1mkg.png"></figure><p name="8986" id="8986" class="graf graf--p graf-after--figure">In fact, instead of a linear layer, we can even just grab a random matrix if we do not care about bias [<a href="https://youtu.be/tY0n9OT5_nA?t=1h42m18s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h42m18s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:42:18</a>]. <code class="markup--code markup--p-code">self.W1</code> is a random tensor wrapped up in a <code class="markup--code markup--p-code">Parameter</code>.</p><p name="8015" id="8015" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">Parameter</code>&nbsp;: Remember, a <code class="markup--code markup--p-code">Parameter</code> is identical to PyTorch <code class="markup--code markup--p-code">Variable</code> but it just tells PyTorch “I want you to learn the weights for this please.” [<a href="https://youtu.be/tY0n9OT5_nA?t=1h42m35s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h42m35s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:42:35</a>]</p><p name="d5b1" id="d5b1" class="graf graf--p graf-after--p">So when we start out our decoder, let’s take the current hidden state of the decoder, put that into a linear layer (<code class="markup--code markup--p-code">self.l2</code>) because what is the information we use to decide what words we should focus on next — the only information we have to go on is what the decoder’s hidden state is now. So let’s grab that:</p><ul class="postList"><li name="32a8" id="32a8" class="graf graf--li graf-after--p">put it into the linear layer (<code class="markup--code markup--li-code">self.l2</code>)</li><li name="b54a" id="b54a" class="graf graf--li graf-after--li">put it through a non-linearity (<code class="markup--code markup--li-code">F.tanh</code>)</li><li name="0783" id="0783" class="graf graf--li graf-after--li">put it through one more nonlinear layer (<code class="markup--code markup--li-code">u @ self.V</code> doesn’t have a bias in it so it’s just matrix multiply)</li><li name="f948" id="f948" class="graf graf--li graf-after--li">put that through softmax</li></ul><p name="5710" id="5710" class="graf graf--p graf-after--li">That’s it — a little neural net. It doesn’t do anything. It’s just a neural net and no neural nets do anything they are just linear layers with nonlinear activations with random weights. But it starts to do something if we give it a job to do. In this case, the job we give it to do is to say don’t just take the final state but now let’s use all of the encoder states and let’s take all of them and multiply them by the output of that little neural net. So given that the things in this little neural net are learnable weights, hopefully it’s going to learn to weight those encoder hidden states by something useful. That is all neural net ever does is we give it some random weights to start with and a job to do, and hope that it learns to do the job. It turns out, it does.</p><figure name="9371" id="9371" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_jOVVKAFwMxGt9v6WEqgLXw.png"></figure><p name="2e00" id="2e00" class="graf graf--p graf-after--figure">Everything else in here is identical to what it was before. We have teacher forcing, it’s not bi-directional, so we can see how this goes.</p><pre name="cab4" id="cab4" class="graf graf--pre graf-after--p">rnn = Seq2SeqAttnRNN(fr_vecd, fr_itos, dim_fr_vec, en_vecd, en_itos, dim_en_vec, nh, enlen_90)<br>learn = RNN_Learner(md, SingleModel(to_gpu(rnn)), opt_fn=opt_fn)<br>learn.crit = seq2seq_loss<br>lr=2e-3</pre><pre name="baa5" id="baa5" class="graf graf--pre graf-after--pre">learn.fit(lr, 1, cycle_len=15, use_clr=(20,10), <br>          stepper=Seq2SeqStepper)</pre><pre name="19fc" id="19fc" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss                              <br>    0      3.882168   11.125291 <br>    1      3.599992   6.667136                              <br>    2      3.236066   5.552943                              <br>    3      3.050283   4.919096                              <br>    4      2.99024    4.500383                              <br>    5      3.07999    4.000295                              <br>    6      2.891087   4.024115                              <br>    7      2.854725   3.673913                              <br>    8      2.979285   3.590668                              <br>    9      3.109851   3.459867                              <br>    10     2.92878    3.517598                              <br>    11     2.778292   3.390253                              <br>    12     2.795427   3.388423                              <br>    13     2.809757   3.353334                              <br>    14     2.6723     3.368584</em></pre><pre name="5460" id="5460" class="graf graf--pre graf-after--pre">[3.3685837]</pre><p name="b4e4" id="b4e4" class="graf graf--p graf-after--pre">Teacher forcing had 3.49 and now with nearly exactly the same thing but we’ve got this little minimal neural net figuring out what weightings to give our inputs and we are down to 3.37. Remember, these loss are logs, so <code class="markup--code markup--p-code">e^3.37</code> is quite a significant change.</p><pre name="cc25" id="cc25" class="graf graf--pre graf-after--p">learn.save('attn')</pre><h4 name="1cca" id="1cca" class="graf graf--h4 graf-after--pre">Test [<a href="https://youtu.be/tY0n9OT5_nA?t=1h45m37s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h45m37s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:45:37</a>]</h4><pre name="8d2a" id="8d2a" class="graf graf--pre graf-after--h4">x,y = next(iter(val_dl))<br>probs,attns = learn.model(V(x),ret_attn=<strong class="markup--strong markup--pre-strong">True</strong>)<br>preds = to_np(probs.max(2)[1])</pre><pre name="f2fe" id="f2fe" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(180,190):<br>    print(' '.join([fr_itos[o] <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> x[:,i] <strong class="markup--strong markup--pre-strong">if</strong> o != 1]))<br>    print(' '.join([en_itos[o] <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> y[:,i] <strong class="markup--strong markup--pre-strong">if</strong> o != 1]))<br>    print(' '.join([en_itos[o] <strong class="markup--strong markup--pre-strong">for</strong> o <strong class="markup--strong markup--pre-strong">in</strong> preds[:,i] <strong class="markup--strong markup--pre-strong">if</strong> o!=1]))<br>    print()</pre><pre name="b7a3" id="b7a3" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">quels facteurs pourraient influer sur le choix de leur emplacement ? _eos_<br>what factors influencetheir location ? _eos_<br>what factors might influence the their their their ? _eos_</em></pre><pre name="1168" id="1168" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">qu’ est -ce qui ne peut pas changer ? _eos_<br>what can not change ? _eos_<br>what can not change change ? _eos_</em></pre><pre name="4f6f" id="4f6f" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">que faites - vous ? _eos_<br>what do you do ? _eos_<br>what do you do ? _eos_</em></pre><pre name="2284" id="2284" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">qui réglemente les pylônes d' antennes ? _eos_<br>who regulates antenna towers ? _eos_<br>who regulates the lights ? ? _eos_</em></pre><pre name="5a9a" id="5a9a" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">où sont - ils situés ? _eos_<br>where are they located ? _eos_<br>where are they located ? _eos_</em></pre><pre name="dcfb" id="dcfb" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">quelles sont leurs compétences ? _eos_<br>what are their qualifications ? _eos_<br>what are their skills ? _eos_</em></pre><pre name="4807" id="4807" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">qui est victime de harcèlement sexuel ? _eos_<br>who experiences sexual harassment ? _eos_<br>who is victim sexual sexual ? _eos_</em></pre><pre name="f69f" id="f69f" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">quelles sont les personnes qui visitent les communautés autochtones ? _eos_<br>who visits indigenous communities ? _eos_<br>who is people people aboriginal people ? _eos_</em></pre><pre name="1609" id="1609" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">pourquoi ces trois points en particulier ? _eos_<br>why these specific three ? _eos_<br>why are these three three ? ? _eos_</em></pre><pre name="f840" id="f840" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">pourquoi ou pourquoi pas ? _eos_<br>why or why not ? _eos_<br>why or why not ? _eos_</em></pre><p name="5188" id="5188" class="graf graf--p graf-after--pre">Not bad. It’s still not perfect but quite a few of them are correct and again considering that we are asking it to learn about the very idea of language for two different languages and how to translate them between the two, and grammar, and vocabulary, and we only have 50,000 sentences and a lot of the words only appear once, I would say this is actually pretty amazing.</p><p name="9e37" id="9e37" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question: </strong>Why do we use tanh instead of ReLU for the attention mini net? [<a href="https://youtu.be/tY0n9OT5_nA?t=1h46m23s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h46m23s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:46:23</a>] I don’t quite remember — it’s been a while since I looked at it. You should totally try using value and see how it goes. Obviously tanh the key difference is that it can go in each direction and it’s limited both at the top and the bottom. I know very often for the gates inside RNNs, LSTMs, and GRUs, tanh often works out better but it’s been about a year since I actually looked at that specific question so I’ll look at it during the week. The short answer is you should try a different activation function and see if you can get a better result.</p><blockquote name="ed74" id="ed74" class="graf graf--blockquote graf-after--p">From Lesson 7 [<a href="https://youtu.be/H3g26EVADgY?t=44m6s" data-href="https://youtu.be/H3g26EVADgY?t=44m6s" class="markup--anchor markup--blockquote-anchor" rel="nofollow noopener" target="_blank">44:06</a>]: As we have seen last week, tanh is forcing the value to be between -1 and 1. Since we are multiplying by this weight matrix again and again, we would worry that relu (since it is unbounded) might have more gradient explosion problem. Having said that, you can specify RNNCell to use different nonlineality whose default is tanh and ask it to use relu if you wanted to.</blockquote><h4 name="d511" id="d511" class="graf graf--h4 graf-after--blockquote">Visualization [<a href="https://youtu.be/tY0n9OT5_nA?t=1h47m12s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h47m12s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:47:12</a>]</h4><p name="a9cc" id="a9cc" class="graf graf--p graf-after--h4">What we can do also is we can grab the attentions out of the model by adding return attention parameter to <code class="markup--code markup--p-code">forward</code> function. You can put anything you’d like in <code class="markup--code markup--p-code">forward</code> function argument. So we added a return attention parameter, false by default because obviously the training loop it doesn’t know anything about it but then we just had something here says if return attention, then stick the attentions on as well (<code class="markup--code markup--p-code">if ret_attn: res = res,torch.stack(attns)</code>). The attentions is simply the value <code class="markup--code markup--p-code">a</code> just chuck it on a list (<code class="markup--code markup--p-code">attns.append(a)</code>). We can now call the model with return attention equals true and get back the probabilities and the attentions [<a href="https://youtu.be/tY0n9OT5_nA?t=1h47m53s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h47m53s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:47:53</a>]:</p><pre name="7b56" id="7b56" class="graf graf--pre graf-after--p">probs,attns = learn.model(V(x),ret_attn=<strong class="markup--strong markup--pre-strong">True</strong>)</pre><p name="245e" id="245e" class="graf graf--p graf-after--pre">We can now draw pictures, at each time step, of the attention.</p><pre name="3a90" id="3a90" class="graf graf--pre graf-after--p">attn = to_np(attns[...,180])</pre><pre name="800e" id="800e" class="graf graf--pre graf-after--pre">fig, axes = plt.subplots(3, 3, figsize=(15, 10))<br><strong class="markup--strong markup--pre-strong">for</strong> i,ax <strong class="markup--strong markup--pre-strong">in</strong> enumerate(axes.flat):<br>    ax.plot(attn[i])</pre><figure name="ebb1" id="ebb1" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_CSG2P8oBICyPDBnoT9z7Wg.png"></figure><p name="c0b4" id="c0b4" class="graf graf--p graf-after--figure">When you are Chris Olah and Shan Carter, you make things that looks like ☟when you are Jeremy Howard, the exact same information looks like ☝︎[<a href="https://youtu.be/tY0n9OT5_nA?t=1h48m24s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h48m24s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:48:24</a>]. You can see at each different time step, we have a different attention.</p><figure name="aa8d" id="aa8d" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_zOxcT0Nib1_VtVkyQN4FQQ.png"></figure><p name="9c69" id="9c69" class="graf graf--p graf-after--figure">It’s very important when you try to build something like this, you don’t really know if it’s not working right because if it’s not working (as per usual Jeremy’s first 12 attempts of this were broken) and they were broken in a sense that it wasn’t really learning anything useful. Therefore, it was giving equal attention to everything and it wasn’t worse — it just wasn’t much better. Until you actually find ways to visualize the thing in a way that you know what it ought to look like ahead of time, you don’t really know if it’s working [<a href="https://youtu.be/tY0n9OT5_nA?t=1h49m16s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h49m16s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:49:16</a>]. So it’s really important that you try to find ways to check your intermediate steps in your outputs.</p><p name="6032" id="6032" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: What is the loss function of the attentional neural network? [<a href="https://youtu.be/tY0n9OT5_nA?t=1h49m31s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h49m31s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:49:31</a>] No, there is no loss function for the attentional neural network. It is trained end-to-end. It is just sitting inside our decoder loop. The loss function for the decoder loop is the same loss function because the result contains exactly same thing as before — the probabilities of the words. How come the mini neural net learning something? Because in order to make the outputs better and better, it would be great if it made the weights of weighted-average better and better. So part of creating our output is to please do a good job of finding a good set of weights and if it doesn’t do a good job of finding good set of weights, then the loss function won’t improve from that bit. So end-to-end learning means you throw in everything you can into one loss function and the gradients of all the different parameters point in a direction that says “hey, you know if you had put more weight over there, it would have been better.” And thanks to the magic of the chain rule, it knows to put more weight over there, change the parameter in the matrix multiply a little, etc. That is the magic of end-to-end learning. It is a very understandable question but you have to realize there is nothing particular about this code that says this particular bits are separate mini neural network anymore than the GRU is a separate little neural network, or a linear layer is a separate little function. It’s all ends up pushed into one output which is a bunch of probabilities which ends up in one loss function that returns a single number that says this either was or wasn’t a good translation. So thanks to the magic of the chain rule, we then back propagate little updates to all the parameters to make them a little bit better. This is a big, weird, counterintuitive idea and it’s totally okay if it’s a bit mind-bending. It is the bit where even back to lesson 1 “how did we make it find dogs vs. cats?” — we didn’t. All we did was we said “this is our data, this is our architecture, this is our loss function. Please back propagate into the weights to make them better and after you’ve made them better a while, it will start finding cats from dogs.” In this case (i.e. translation), we haven’t used somebody else’s convolutional network architecture. We said “here is a custom architecture which we hope is going to be particularly good at this problem.” Even without this custom architecture, it was still okay. But we made it in a way that made more sense or we think it ought to do worked even better. But at no point, did we do anything different other than say “here is a data, here is an architecture, here is a loss function — go and find the parameters please” And it did it because that’s what neural nets do.</p><p name="d5d9" id="d5d9" class="graf graf--p graf-after--p">So that is sequence-to-sequence learning [<a href="https://youtu.be/tY0n9OT5_nA?t=1h53m19s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h53m19s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:53:19</a>].</p><ul class="postList"><li name="5de6" id="5de6" class="graf graf--li graf-after--p">If you want to encode an image into a CNN backbone of some kind, and then pass that into a decoder which is like RNN with attention, and you make your y-values the actual correct caption of each of those image, you will end up with an image caption generator.</li><li name="2fb5" id="2fb5" class="graf graf--li graf-after--li">If you do the same thing with videos and captions, you will end up with a video caption generator.</li><li name="b552" id="b552" class="graf graf--li graf-after--li">If you do the same thing with 3D CT scan and radiology reports, you will end up with a radiology report generator.</li><li name="f410" id="f410" class="graf graf--li graf-after--li">If you do the same thing with Github issues and people’s chosen summaries of them, you’ll get a Github issue summary generator.</li></ul><blockquote name="fc6c" id="fc6c" class="graf graf--blockquote graf-after--li">Seq-to-seq is magical but they work [<a href="https://youtu.be/tY0n9OT5_nA?t=1h54m7s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h54m7s" class="markup--anchor markup--blockquote-anchor" rel="nofollow noopener" target="_blank">1:54:07</a>]. And I don’t feel like people have begun to scratch the surface of how to use seq-to-seq models in their own domains. Not being a Github person, it would never have occurred to me that “it would be kind of cool to start with some issue and automatically create a summary”. But now, of course, next time I go into Github, I want to see a summary written there for me. I don’t want to write my own commit message. Why should I write my own summary of the code review when I finished adding comments to lots of lines — it should do that for me as well. Now I’m thinking Github so behind, it could be doing this stuff. So what are the thing in your industry? You could start with a sequence and generate something from it. I can’t begin to imagine. Again, it is a fairly new area and the tools for it are not easy to use — they are not even built into fastai yet. Hopefully there will be soon. I don’t think anybody knows what the opportunities are.</blockquote><h3 name="6e04" id="6e04" class="graf graf--h3 graf-after--blockquote">Devise [<a href="https://youtu.be/tY0n9OT5_nA?t=1h55m23s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h55m23s" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">1:55:23</a>]</h3><p name="1fb9" id="1fb9" class="graf graf--p graf-after--h3"><a href="https://github.com/fastai/fastai/blob/master/courses/dl2/devise.ipynb" data-href="https://github.com/fastai/fastai/blob/master/courses/dl2/devise.ipynb" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Notebook</a> / <a href="http://papers.nips.cc/paper/5204-devise-a-deep-visual-semantic-embedding-model.pdf" data-href="http://papers.nips.cc/paper/5204-devise-a-deep-visual-semantic-embedding-model.pdf" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Paper</a></p><p name="5d8b" id="5d8b" class="graf graf--p graf-after--p">We are going to do something bringing together for the first time our two little worlds we focused on — text and images [<a href="https://youtu.be/tY0n9OT5_nA?t=1h55m49s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h55m49s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:55:49</a>]. This idea came up in a paper by an extraordinary deep learning practitioner and researcher named Andrea Frome. Andrea was at Google at the time and her crazy idea was words can have a distributed representation, a space, which particularly at that time was just word vectors. And images can be represented in a space. In the end, if we have a fully connected layer, they ended up as a vector representation. Could we merge the two? Could we somehow encourage the vector space that the images end up with be the same vector space that the words are in? And if we could do that, what would that mean? What could we do with that? So what could we do with that covers things like well, what if I’m wrong what if I’m predicting that this image is a beagle and I predict jumbo jet and Yannet’s model predicts corgi. The normal loss function says that Yannet’s and Jeremy’s models are equally good (i.e. they are both wrong). But what if we could somehow say though you know what corgi is closer to beagle than it is to jumbo jets. So Yannet’s model is better than Jeremy’s. We should be able to do that because in word vector space, beagle and corgi are pretty close together but jumbo jet not so much. So it would give us a nice situation where hopefully our inferences would be wrong in saner ways if they are wrong. It would also allow us to search for things that are not in ImageNet Synset ID (i.e. a category in ImageNet). Why did we have to train a whole new model to find dog vs. cats when we already have something that found corgis and tabbies. Why can’t we just say find me dogs? If we had trained it in word vector space, we totally could because they are word vector, we can find things with the right image vector and so forth. We will look at some cool things we can do with it in a moment but first of all let’s train a model where this model is not learning a category (one hot encoded ID) where every category is equally far from every other category, let’s instead train a model where we’re finding a dependent variable which is a word vector. so What word vector? Obviously the word vector for the word you want. So if it’s corgi, let’s train it to create a word vector that’s the corgi word vector, and if it’s a jumbo jet, let’s train it with a dependent variable that says this is the word vector for a jumbo jet.</p><pre name="0500" id="0500" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">fastai.conv_learner</strong> <strong class="markup--strong markup--pre-strong">import</strong> *<br>torch.backends.cudnn.benchmark=<strong class="markup--strong markup--pre-strong">True</strong><br><br><strong class="markup--strong markup--pre-strong">import</strong> <strong class="markup--strong markup--pre-strong">fastText</strong> <strong class="markup--strong markup--pre-strong">as</strong> <strong class="markup--strong markup--pre-strong">ft</strong></pre><pre name="a6f9" id="a6f9" class="graf graf--pre graf-after--pre">PATH = Path('data/imagenet/')<br>TMP_PATH = PATH/'tmp'<br>TRANS_PATH = Path('data/translate/')<br>PATH_TRN = PATH/'train'</pre><p name="bbe2" id="bbe2" class="graf graf--p graf-after--pre">It is shockingly easy [<a href="https://youtu.be/tY0n9OT5_nA?t=1h59m17s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h59m17s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:59:17</a>]. Let’s grab the fast text word vectors again, load them in (we only need English this time).</p><pre name="30b6" id="30b6" class="graf graf--pre graf-after--p">ft_vecs = ft.load_model(str((TRANS_PATH/'wiki.en.bin')))</pre><pre name="c911" id="c911" class="graf graf--pre graf-after--pre">np.corrcoef(ft_vecs.get_word_vector('jeremy'), <br>            ft_vecs.get_word_vector('Jeremy'))</pre><pre name="7aa7" id="7aa7" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">array([[1.     , 0.60866],<br>       [0.60866, 1.     ]])</em></pre><p name="8dd9" id="8dd9" class="graf graf--p graf-after--pre">So for example, “jeremy” and “Jeremy” have a correlation of&nbsp;.6.</p><pre name="e8a1" id="e8a1" class="graf graf--pre graf-after--p">np.corrcoef(ft_vecs.get_word_vector('banana'), <br>            ft_vecs.get_word_vector('Jeremy'))</pre><pre name="ea1f" id="ea1f" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">array([[1.     , 0.14482],<br>       [0.14482, 1.     ]])</em></pre><p name="68c1" id="68c1" class="graf graf--p graf-after--pre">Jeremy doesn’t like bananas at all, and “banana” and “Jeremy”&nbsp;.14. So words that you would expect to be correlated are correlated and words that should be as far away from each other as possible, unfortunately, they are still slightly correlated but not so much [<a href="https://youtu.be/tY0n9OT5_nA?t=1h59m41s" data-href="https://youtu.be/tY0n9OT5_nA?t=1h59m41s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:59:41</a>].</p><h4 name="c597" id="c597" class="graf graf--h4 graf-after--p">Map ImageNet classes to word&nbsp;vectors</h4><p name="755f" id="755f" class="graf graf--p graf-after--h4">Let’s now grab all of the ImageNet classes because we actually want to know which one is corgi and which one is jumbo jet.</p><pre name="4ad4" id="4ad4" class="graf graf--pre graf-after--p">ft_words = ft_vecs.get_words(include_freq=<strong class="markup--strong markup--pre-strong">True</strong>)<br>ft_word_dict = {k:v <strong class="markup--strong markup--pre-strong">for</strong> k,v <strong class="markup--strong markup--pre-strong">in</strong> zip(*ft_words)}<br>ft_words = sorted(ft_word_dict.keys(), key=<strong class="markup--strong markup--pre-strong">lambda</strong> x: ft_word_dict[x])</pre><pre name="3550" id="3550" class="graf graf--pre graf-after--pre">len(ft_words)</pre><pre name="841a" id="841a" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">2519370</em></pre><pre name="6863" id="6863" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">fastai.io</strong> <strong class="markup--strong markup--pre-strong">import</strong> get_data</pre><p name="b4bd" id="b4bd" class="graf graf--p graf-after--pre">We have a list of all of those up on files.fast.ai that we can grab them.</p><pre name="28c7" id="28c7" class="graf graf--pre graf-after--p">CLASSES_FN = 'imagenet_class_index.json'<br>get_data(f'http://files.fast.ai/models/{CLASSES_FN}', <br>         TMP_PATH/CLASSES_FN)</pre><p name="f2ef" id="f2ef" class="graf graf--p graf-after--pre">Let’s also grab a list of all of the nouns in English which Jeremy made available here:</p><pre name="5ef8" id="5ef8" class="graf graf--pre graf-after--p">WORDS_FN = 'classids.txt'<br>get_data(f'http://files.fast.ai/data/{WORDS_FN}', PATH/WORDS_FN)</pre><p name="08d7" id="08d7" class="graf graf--p graf-after--pre">So we have the names of each of the thousand ImageNet classes and all of the nouns in English according to WordNet which is a popular thing for representing what words are and are not. We can now load that list of ImageNet classes, turn that into a dictionary, so <code class="markup--code markup--p-code">classids_1k</code> contains the class IDs for the 1000 images that are in the competition dataset.</p><pre name="9af1" id="9af1" class="graf graf--pre graf-after--p">class_dict = json.load((TMP_PATH/CLASSES_FN).open())<br>classids_1k = dict(class_dict.values())<br>nclass = len(class_dict); nclass</pre><pre name="8b3c" id="8b3c" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">1000</em></pre><p name="4b83" id="4b83" class="graf graf--p graf-after--pre">Here is an example. A “tench” apparently is a kind of fish.</p><pre name="6242" id="6242" class="graf graf--pre graf-after--p">class_dict['0']</pre><pre name="de10" id="de10" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">['n01440764', 'tench']</em></pre><p name="6928" id="6928" class="graf graf--p graf-after--pre">Let’s do the same thing for all those WordNet nouns [<a href="https://youtu.be/tY0n9OT5_nA?t=2h1m11s" data-href="https://youtu.be/tY0n9OT5_nA?t=2h1m11s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:01:11</a>]. It turns out that ImageNet is using WordNet class names so that makes it nice and easy to map between the two.</p><pre name="5bff" id="5bff" class="graf graf--pre graf-after--p">classid_lines = (PATH/WORDS_FN).open().readlines()<br>classid_lines[:5]</pre><pre name="bbf7" id="bbf7" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">['n00001740 entity\n',<br> 'n00001930 physical_entity\n',<br> 'n00002137 abstraction\n',<br> 'n00002452 thing\n',<br> 'n00002684 object\n']</em></pre><pre name="5fc9" id="5fc9" class="graf graf--pre graf-after--pre">classids = dict(l.strip().split() <strong class="markup--strong markup--pre-strong">for</strong> l <strong class="markup--strong markup--pre-strong">in</strong> classid_lines)<br>len(classids),len(classids_1k)</pre><pre name="1917" id="1917" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(82115, 1000)</em></pre><p name="a58c" id="a58c" class="graf graf--p graf-after--pre">So these are our two worlds — we have the ImageNet thousand and we have the 82,000 which are in WordNet.</p><pre name="7e7d" id="7e7d" class="graf graf--pre graf-after--p">lc_vec_d = {w.lower(): ft_vecs.get_word_vector(w) <strong class="markup--strong markup--pre-strong">for</strong> w <br>                           <strong class="markup--strong markup--pre-strong">in</strong> ft_words[-1000000:]}</pre><p name="e883" id="e883" class="graf graf--p graf-after--pre">So we want to map the two together which is as simple as creating a couple of dictionaries to map them based on the Synset ID or the WordNet ID.</p><pre name="a745" id="a745" class="graf graf--pre graf-after--p">syn_wv = [(k, lc_vec_d[v.lower()]) <strong class="markup--strong markup--pre-strong">for</strong> k,v <strong class="markup--strong markup--pre-strong">in</strong> classids.items()<br>          <strong class="markup--strong markup--pre-strong">if</strong> v.lower() <strong class="markup--strong markup--pre-strong">in</strong> lc_vec_d]<br>syn_wv_1k = [(k, lc_vec_d[v.lower()]) <strong class="markup--strong markup--pre-strong">for</strong> k,v <strong class="markup--strong markup--pre-strong">in</strong> classids_1k.items()<br>          <strong class="markup--strong markup--pre-strong">if</strong> v.lower() <strong class="markup--strong markup--pre-strong">in</strong> lc_vec_d]<br>syn2wv = dict(syn_wv)<br>len(syn2wv)</pre><pre name="ede6" id="ede6" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">49469</em></pre><p name="95ed" id="95ed" class="graf graf--p graf-after--pre">What we need to do now is grab the 82,000 nouns in WordNet and try and look them up in fast text. We’ve managed to look up 49,469 of them in fast text. We now have a dictionary that goes from synset ID which is what WordNet calls them to word vectors. We also have the same thing specifically for the 1k ImageNet classes.</p><pre name="fc98" id="fc98" class="graf graf--pre graf-after--p">pickle.dump(syn2wv, (TMP_PATH/'syn2wv.pkl').open('wb'))<br>pickle.dump(syn_wv_1k, (TMP_PATH/'syn_wv_1k.pkl').open('wb'))</pre><pre name="b392" id="b392" class="graf graf--pre graf-after--pre">syn2wv = pickle.load((TMP_PATH/'syn2wv.pkl').open('rb'))<br>syn_wv_1k = pickle.load((TMP_PATH/'syn_wv_1k.pkl').open('rb'))</pre><p name="c6f0" id="c6f0" class="graf graf--p graf-after--pre">Now we grab all of the ImageNet which you can download from Kaggle now [<a href="https://youtu.be/tY0n9OT5_nA?t=2h2m54s" data-href="https://youtu.be/tY0n9OT5_nA?t=2h2m54s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:02:54</a>]. If you look at the Kaggle ImageNet localization competition, that contains the entirety of the ImageNet classifications as well.</p><pre name="4e72" id="4e72" class="graf graf--pre graf-after--p">images = []<br>img_vecs = []</pre><pre name="3daf" id="3daf" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">for</strong> d <strong class="markup--strong markup--pre-strong">in</strong> (PATH/'train').iterdir():<br>    <strong class="markup--strong markup--pre-strong">if</strong> d.name <strong class="markup--strong markup--pre-strong">not</strong> <strong class="markup--strong markup--pre-strong">in</strong> syn2wv: <strong class="markup--strong markup--pre-strong">continue</strong><br>    vec = syn2wv[d.name]<br>    <strong class="markup--strong markup--pre-strong">for</strong> f <strong class="markup--strong markup--pre-strong">in</strong> d.iterdir():<br>        images.append(str(f.relative_to(PATH)))<br>        img_vecs.append(vec)</pre><pre name="d187" id="d187" class="graf graf--pre graf-after--pre">n_val=0<br><strong class="markup--strong markup--pre-strong">for</strong> d <strong class="markup--strong markup--pre-strong">in</strong> (PATH/'valid').iterdir():<br>    <strong class="markup--strong markup--pre-strong">if</strong> d.name <strong class="markup--strong markup--pre-strong">not</strong> <strong class="markup--strong markup--pre-strong">in</strong> syn2wv: <strong class="markup--strong markup--pre-strong">continue</strong><br>    vec = syn2wv[d.name]<br>    <strong class="markup--strong markup--pre-strong">for</strong> f <strong class="markup--strong markup--pre-strong">in</strong> d.iterdir():<br>        images.append(str(f.relative_to(PATH)))<br>        img_vecs.append(vec)<br>        n_val += 1</pre><pre name="e84e" id="e84e" class="graf graf--pre graf-after--pre">n_val</pre><pre name="12a9" id="12a9" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">28650</em></pre><p name="ee3d" id="ee3d" class="graf graf--p graf-after--pre">It has a validation set of 28,650 items in it. For every image in ImageNet, we can grab its fast text word vector using the synset to word vector (<code class="markup--code markup--p-code">syn2wv</code>) and we can stick that into the image vectors array (<code class="markup--code markup--p-code">img_vecs</code>), stack that all up into a single matrix and save that away.</p><pre name="a70c" id="a70c" class="graf graf--pre graf-after--p">img_vecs = np.stack(img_vecs)<br>img_vecs.shape</pre><p name="1bd7" id="1bd7" class="graf graf--p graf-after--pre">Now what we have is something for every ImageNet image, we also have the fast text word vector that it is associated with [<a href="https://youtu.be/tY0n9OT5_nA?t=2h3m43s" data-href="https://youtu.be/tY0n9OT5_nA?t=2h3m43s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:03:43</a>] by looking up the synset ID → WordNet → Fast text → word vector.</p><pre name="5f9d" id="5f9d" class="graf graf--pre graf-after--p">pickle.dump(images, (TMP_PATH/'images.pkl').open('wb'))<br>pickle.dump(img_vecs, (TMP_PATH/'img_vecs.pkl').open('wb'))</pre><pre name="6c87" id="6c87" class="graf graf--pre graf-after--pre">images = pickle.load((TMP_PATH/'images.pkl').open('rb'))<br>img_vecs = pickle.load((TMP_PATH/'img_vecs.pkl').open('rb'))</pre><pre name="53db" id="53db" class="graf graf--pre graf-after--pre">arch = resnet50</pre><pre name="532e" id="532e" class="graf graf--pre graf-after--pre">n = len(images); n</pre><pre name="8968" id="8968" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">766876</em></pre><pre name="146e" id="146e" class="graf graf--pre graf-after--pre">val_idxs = list(range(n-28650, n))</pre><p name="4b9e" id="4b9e" class="graf graf--p graf-after--pre">Here is a cool trick [<a href="https://youtu.be/tY0n9OT5_nA?t=2h4m6s" data-href="https://youtu.be/tY0n9OT5_nA?t=2h4m6s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:04:06</a>]. We can now create a model data object which specifically is an image classifier data object and we have this thing called <code class="markup--code markup--p-code">from_names_and_array</code> I’m not sure if we’ve used it before but we can pass it a list of file names (all of the file names in ImageNet) and an array of our dependent variables (all of the fast text word vectors). We can then pass in the validation indexes which in this case is just all of the last IDs — we need to make sure that they are the same as ImageNet uses otherwise we will be cheating. Then we pass in <code class="markup--code markup--p-code">continuous=True</code> which means this puts a lie again to this image classifier data is now an image regressive data so continuous equals True means don’t one hot encode my outputs but treat them just as continuous values. So now we have a model data object that contains all of our file names and for every file name a continuous array representing the word vector for that. So we have data, now we need an architecture and the loss function.</p><pre name="ac43" id="ac43" class="graf graf--pre graf-after--p">tfms = tfms_from_model(arch, 224, transforms_side_on, max_zoom=1.1)<br>md = ImageClassifierData.<strong class="markup--strong markup--pre-strong">from_names_and_array</strong>(PATH, images,  <br>        img_vecs, val_idxs=val_idxs, classes=<strong class="markup--strong markup--pre-strong">None</strong>, tfms=tfms,<br>        continuous=<strong class="markup--strong markup--pre-strong">True</strong>, bs=256)</pre><pre name="c264" id="c264" class="graf graf--pre graf-after--pre">x,y = next(iter(md.val_dl))</pre><p name="f8d2" id="f8d2" class="graf graf--p graf-after--pre">Let’s create an architecture [<a href="https://youtu.be/tY0n9OT5_nA?t=2h5m26s" data-href="https://youtu.be/tY0n9OT5_nA?t=2h5m26s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:05:26</a>]. We’ll revise this next week, but we can use the tricks we’ve learnt so far and it’s actually incredibly simple. Fastai has a <code class="markup--code markup--p-code">ConvnetBuilder</code> which is what gets called when you say <code class="markup--code markup--p-code">ConvLerner.pretrained</code> and you specify:</p><ul class="postList"><li name="13a8" id="13a8" class="graf graf--li graf-after--p"><code class="markup--code markup--li-code">f</code>: the architecture (we are going to use ResNet50)</li><li name="ce29" id="ce29" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">c</code>: how many classes you want (in this case, it’s not really classes — it’s how many outputs you want which is the length of the fast text word vector i.e. 300).</li><li name="7b85" id="7b85" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">is_multi</code>: It is not a multi classification as it is not classification at all.</li><li name="9922" id="9922" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">is_reg</code>: Yes, it is a regression.</li><li name="8c1d" id="8c1d" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">xtra_fc</code>&nbsp;: What fully connected layers you want. We are just going to add one fully connected hidden layer of a length of 1024. Why 1024? The last layer of ResNet50 I think is 1024 long, the final output we need is 300 long. We obviously need our penultimate (second to the last) layer to be longer than 300. Otherwise it’s not enough information, so we just picked something a bit bigger. Maybe different numbers would be better but this worked for Jeremy.</li><li name="e1f6" id="e1f6" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">ps</code>&nbsp;: how much dropout you want. Jeremy found that the default dropout, he was consistently under fitting so he just decreased the dropout from 0.5 to 0.2.</li></ul><p name="f179" id="f179" class="graf graf--p graf-after--li">So this is now a convolutional neural network that does not have any softmax or anything like that because it’s regression it’s just a linear layer at the end and that’s our model [<a href="https://youtu.be/tY0n9OT5_nA?t=2h6m55s" data-href="https://youtu.be/tY0n9OT5_nA?t=2h6m55s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:06:55</a>]. We can create a ConvLearner from that model and give it an optimization function. So now all we need is a loss function.</p><pre name="9662" id="9662" class="graf graf--pre graf-after--p">models = ConvnetBuilder(arch, md.c, is_multi=<strong class="markup--strong markup--pre-strong">False</strong>, is_reg=<strong class="markup--strong markup--pre-strong">True</strong>, <br>             xtra_fc=[1024], ps=[0.2,0.2])</pre><pre name="df16" id="df16" class="graf graf--pre graf-after--pre">learn = ConvLearner(md, models, precompute=<strong class="markup--strong markup--pre-strong">True</strong>)<br>learn.opt_fn = partial(optim.Adam, betas=(0.9,0.99))</pre><p name="12d1" id="12d1" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">Loss Function</strong> [<a href="https://youtu.be/tY0n9OT5_nA?t=2h7m38s" data-href="https://youtu.be/tY0n9OT5_nA?t=2h7m38s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:07:38</a>]: Default loss function for regression is L1 loss (the absolute differences) — that is not bad. But unfortunately in really high dimensional spaces (anybody who has studied a bit of machine learning probably knows this) everything is on the outside (in this case, it’s 300 dimensional). When everything is on the outside, distance is not meaningless but a little bit awkward. Things tend to be close together or far away, it doesn’t really mean much in these really high dimensional spaces where everything is on the edge. What does mean something, though, is that if one thing is on the edge over here, and one thing is on the edge over there, we can form an angle between those vectors and the angle is meaningful. That is why we use cosine similarity when we are looking for how close or far apart things are in high dimensional spaces. If you haven’t seen cosine similarity before, it is basically the same as Euclidean distance but it’s normalized to be a unit norm (i.e. divided by the length). So we don’t care about the length of the vector, we only care about its angle. There is a bunch of stuff that you could easily learn in a couple of hours but if you haven’t seen it before, it’s a bit mysterious. For now, just know that loss functions and high dimensional spaces where you are trying to find similarity, you care about angle and you don’t care about distance [<a href="https://youtu.be/tY0n9OT5_nA?t=2h9m13s" data-href="https://youtu.be/tY0n9OT5_nA?t=2h9m13s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:09:13</a>]. If you didn’t use the following custom loss function, it would still work but it’s a little bit less good. Now we have data, architecture, and loss function, therefore, we are done. We can go ahead and fit.</p><pre name="f274" id="f274" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> cos_loss(inp,targ):<br>    <strong class="markup--strong markup--pre-strong">return</strong> 1 - F.cosine_similarity(inp,targ).mean()<br>learn.crit = cos_loss</pre><pre name="8604" id="8604" class="graf graf--pre graf-after--pre">learn.lr_find(start_lr=1e-4, end_lr=1e15)</pre><pre name="ef44" id="ef44" class="graf graf--pre graf-after--pre">learn.sched.plot()</pre><pre name="3d61" id="3d61" class="graf graf--pre graf-after--pre">lr = 1e-2<br>wd = 1e-7</pre><p name="54cd" id="54cd" class="graf graf--p graf-after--pre">We are training on all of ImageNet that is going to take a long time. So <code class="markup--code markup--p-code">precompute=True</code> is your friend. Remember <code class="markup--code markup--p-code">precompute=True</code>? That is the thing we’ve learnt ages ago that caches the output of the final convolutional layer and just trains the fully connected bit. Even with <code class="markup--code markup--p-code">precompute=True</code>, it takes about 3 minutes to train an epoch on all of ImageNet. So this is about an hour worth of training, but it’s pretty cool that with fastai, we can train a new custom head on all of ImageNet for 40 epochs in an hour or so.</p><pre name="9aae" id="9aae" class="graf graf--pre graf-after--p">learn.precompute=<strong class="markup--strong markup--pre-strong">True</strong></pre><pre name="0370" id="0370" class="graf graf--pre graf-after--pre">learn.fit(lr, 1, cycle_len=20, wds=wd, use_clr=(20,10))</pre><pre name="a105" id="a105" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss                                  <br>    0      0.104692   0.125685  <br>    1      0.112455   0.129307                                 <br>    2      0.110631   0.126568                                 <br>    3      0.108629   0.127338                                 <br>    4      0.110791   0.125033                                 <br>    5      0.108859   0.125186                                 <br>    6      0.106582   0.123875                                 <br>    7      0.103227   0.123945                                 <br>    8      0.10396    0.12304                                  <br>    9      0.105898   0.124894                                 <br>    10     0.10498    0.122582                                 <br>    11     0.104983   0.122906                                 <br>    12     0.102317   0.121171                                  <br>    13     0.10017    0.121816                                  <br>    14     0.099454   0.119647                                  <br>    15     0.100425   0.120914                                  <br>    16     0.097226   0.119724                                  <br>    17     0.094666   0.118746                                  <br>    18     0.094137   0.118744                                  <br>    19     0.090076   0.117908</em></pre><pre name="c43b" id="c43b" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[0.11790786389489033]</em></pre><pre name="4894" id="4894" class="graf graf--pre graf-after--pre">learn.bn_freeze(<strong class="markup--strong markup--pre-strong">True</strong>)</pre><pre name="fc1e" id="fc1e" class="graf graf--pre graf-after--pre">learn.fit(lr, 1, cycle_len=20, wds=wd, use_clr=(20,10))</pre><pre name="bac5" id="bac5" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss                                  <br>    0      0.104692   0.125685  <br>    1      0.112455   0.129307                                 <br>    2      0.110631   0.126568                                 <br>    3      0.108629   0.127338                                 <br>    4      0.110791   0.125033                                 <br>    5      0.108859   0.125186                                 <br>    6      0.106582   0.123875                                 <br>    7      0.103227   0.123945                                 <br>    8      0.10396    0.12304                                  <br>    9      0.105898   0.124894                                 <br>    10     0.10498    0.122582                                 <br>    11     0.104983   0.122906                                 <br>    12     0.102317   0.121171                                  <br>    13     0.10017    0.121816                                  <br>    14     0.099454   0.119647                                  <br>    15     0.100425   0.120914                                  <br>    16     0.097226   0.119724                                  <br>    17     0.094666   0.118746                                  <br>    18     0.094137   0.118744                                  <br>    19     0.090076   0.117908</em></pre><pre name="ddc2" id="ddc2" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[0.11790786389489033]</em></pre><pre name="70e1" id="70e1" class="graf graf--pre graf-after--pre">lrs = np.array([lr/1000,lr/100,lr])</pre><pre name="6cfb" id="6cfb" class="graf graf--pre graf-after--pre">learn.precompute=<strong class="markup--strong markup--pre-strong">False</strong><br>learn.freeze_to(1)</pre><pre name="404b" id="404b" class="graf graf--pre graf-after--pre">learn.save('pre0')</pre><pre name="2c47" id="2c47" class="graf graf--pre graf-after--pre">learn.load('pre0')</pre><h3 name="1ae7" id="1ae7" class="graf graf--h3 graf-after--pre">Image search</h3><h4 name="5a9e" id="5a9e" class="graf graf--h4 graf-after--h3">Search imagenet&nbsp;classes</h4><p name="7eea" id="7eea" class="graf graf--p graf-after--h4">At the end of all that, we can now say let’s grab the 1000 ImageNet classes, let’s predict on our whole validation set, and take a look at a few pictures [<a href="https://youtu.be/tY0n9OT5_nA?t=2h10m26s" data-href="https://youtu.be/tY0n9OT5_nA?t=2h10m26s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:10:26</a>].</p><pre name="5e0f" id="5e0f" class="graf graf--pre graf-after--p">syns, wvs = list(zip(*syn_wv_1k))<br>wvs = np.array(wvs)</pre><pre name="da52" id="da52" class="graf graf--pre graf-after--pre">%time pred_wv = learn.predict()</pre><pre name="9568" id="9568" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">CPU times: user 18.4 s, sys: 7.91 s, total: 26.3 s<br>Wall time: 7.17 s</em></pre><pre name="6f55" id="6f55" class="graf graf--pre graf-after--pre">start=300</pre><pre name="03a5" id="03a5" class="graf graf--pre graf-after--pre">denorm = md.val_ds.denorm</pre><pre name="7083" id="7083" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> show_img(im, figsize=<strong class="markup--strong markup--pre-strong">None</strong>, ax=<strong class="markup--strong markup--pre-strong">None</strong>):<br>    <strong class="markup--strong markup--pre-strong">if</strong> <strong class="markup--strong markup--pre-strong">not</strong> ax: fig,ax = plt.subplots(figsize=figsize)<br>    ax.imshow(im)<br>    ax.axis('off')<br>    <strong class="markup--strong markup--pre-strong">return</strong> ax</pre><pre name="e2c3" id="e2c3" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> show_imgs(ims, cols, figsize=<strong class="markup--strong markup--pre-strong">None</strong>):<br>    fig,axes = plt.subplots(len(ims)//cols, cols, figsize=figsize)<br>    <strong class="markup--strong markup--pre-strong">for</strong> i,ax <strong class="markup--strong markup--pre-strong">in</strong> enumerate(axes.flat): show_img(ims[i], ax=ax)<br>    plt.tight_layout()</pre><p name="bf46" id="bf46" class="graf graf--p graf-after--pre">Because validation set is ordered, tall the stuff of the same type are in the same place.</p><pre name="da5f" id="da5f" class="graf graf--pre graf-after--p">show_imgs(denorm(md.val_ds[start:start+25][0]), 5, (10,10))</pre><figure name="c31f" id="c31f" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_exiD0uDeL6xx5EOLdPS3BA.png"></figure><p name="d8a5" id="d8a5" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Nearest neighbor search </strong>[<a href="https://youtu.be/tY0n9OT5_nA?t=2h10m56s" data-href="https://youtu.be/tY0n9OT5_nA?t=2h10m56s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:10:56</a>]: What we can now do is we can now use nearest neighbors search. So nearest neighbors search means here is one300 dimensional vector and here is a whole a lot of other 300 dimensional vectors, which things is it closest to? Normally that takes a very long time because you have to look through every 300 dimensional vector, calculate its distance, and find out how far away it is. But there is an amazing almost unknown library called <strong class="markup--strong markup--p-strong">NMSLib</strong> that does that incredibly fast. Some of you may have tried other nearest neighbor’s libraries, I guarantee this is faster than what you are using — I can tell you that because it’s been bench marked by people who do this stuff for a living. This is by far the fastest on every possible dimension. We want to create an index on angular distance, and we need to do it on all of our ImageNet word vectors. Adding a whole batch, create the index, and now we can query a bunch of vectors all at once, get the 10 nearest neighbors. The library uses multi-threading and is absolutely fantastic. You can install from pip (<code class="markup--code markup--p-code">pip install nmslib</code>) and it just works.</p><pre name="ef34" id="ef34" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">import</strong> <strong class="markup--strong markup--pre-strong">nmslib</strong></pre><pre name="9f2b" id="9f2b" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> create_index(a):<br>    index = nmslib.init(space='angulardist')<br>    index.addDataPointBatch(a)<br>    index.createIndex()<br>    <strong class="markup--strong markup--pre-strong">return</strong> index</pre><pre name="64fd" id="64fd" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> get_knns(index, vecs):<br>     <strong class="markup--strong markup--pre-strong">return</strong> zip(*index.knnQueryBatch(vecs, k=10, num_threads=4))</pre><pre name="07a0" id="07a0" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> get_knn(index, vec): <strong class="markup--strong markup--pre-strong">return</strong> index.knnQuery(vec, k=10)</pre><pre name="a7f1" id="a7f1" class="graf graf--pre graf-after--pre">nn_wvs = create_index(wvs)</pre><p name="8aa9" id="8aa9" class="graf graf--p graf-after--pre">It tells you how far away they are and their indexes [<a href="https://youtu.be/tY0n9OT5_nA?t=2h12m13s" data-href="https://youtu.be/tY0n9OT5_nA?t=2h12m13s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:12:13</a>].</p><pre name="2b14" id="2b14" class="graf graf--pre graf-after--p">idxs,dists = get_knns(nn_wvs, pred_wv)</pre><p name="9794" id="9794" class="graf graf--p graf-after--pre">So now we can go through and print out the top 3 so it turns out that bird actually is a limpkin. Interestingly the fourth one does not say it’s a limpkin and Jeremy looked it up. He doesn’t know much about birds but everything else is brown with white spots, but the 4th one isn’t. So we don’t know if that is actually a limpkin or if it is mislabeled but sure as heck it doesn’t look like the other birds.</p><pre name="7b8d" id="7b8d" class="graf graf--pre graf-after--p">[[classids[syns[id]] <strong class="markup--strong markup--pre-strong">for</strong> id <strong class="markup--strong markup--pre-strong">in</strong> ids[:3]] <br>                         <strong class="markup--strong markup--pre-strong">for</strong> ids <strong class="markup--strong markup--pre-strong">in</strong> idxs[start:start+10]]</pre><pre name="8e93" id="8e93" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[['limpkin', 'oystercatcher', 'spoonbill'],<br> ['limpkin', 'oystercatcher', 'spoonbill'],<br> ['limpkin', 'oystercatcher', 'spoonbill'],<br> ['spoonbill', 'bustard', 'oystercatcher'],<br> ['limpkin', 'oystercatcher', 'spoonbill'],<br> ['limpkin', 'oystercatcher', 'spoonbill'],<br> ['limpkin', 'oystercatcher', 'spoonbill'],<br> ['limpkin', 'oystercatcher', 'spoonbill'],<br> ['limpkin', 'oystercatcher', 'spoonbill'],<br> ['limpkin', 'oystercatcher', 'spoonbill']]</em></pre><p name="9dce" id="9dce" class="graf graf--p graf-after--pre">This is not a particularly hard thing to do because there is only a thousand ImageNet classes and it is not doing anything new. But what if we now bring in the entirety of WordNet and we now say which of those 45 thousand things is it closest to?</p><h4 name="d4bc" id="d4bc" class="graf graf--h4 graf-after--p">Search all WordMet noun&nbsp;classes</h4><pre name="4ed5" id="4ed5" class="graf graf--pre graf-after--h4">all_syns, all_wvs = list(zip(*syn2wv.items()))<br>all_wvs = np.array(all_wvs)</pre><pre name="cb4d" id="cb4d" class="graf graf--pre graf-after--pre">nn_allwvs = create_index(all_wvs)</pre><pre name="2b27" id="2b27" class="graf graf--pre graf-after--pre">idxs,dists = get_knns(nn_allwvs, pred_wv)</pre><pre name="d157" id="d157" class="graf graf--pre graf-after--pre">[[classids[all_syns[id]] <strong class="markup--strong markup--pre-strong">for</strong> id <strong class="markup--strong markup--pre-strong">in</strong> ids[:3]] <br>                             <strong class="markup--strong markup--pre-strong">for</strong> ids <strong class="markup--strong markup--pre-strong">in</strong> idxs[start:start+10]]</pre><pre name="80aa" id="80aa" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[['limpkin', 'oystercatcher', 'spoonbill'],<br> ['limpkin', 'oystercatcher', 'spoonbill'],<br> ['limpkin', 'oystercatcher', 'spoonbill'],<br> ['spoonbill', 'bustard', 'oystercatcher'],<br> ['limpkin', 'oystercatcher', 'spoonbill'],<br> ['limpkin', 'oystercatcher', 'spoonbill'],<br> ['limpkin', 'oystercatcher', 'spoonbill'],<br> ['limpkin', 'oystercatcher', 'spoonbill'],<br> ['limpkin', 'oystercatcher', 'spoonbill'],<br> ['limpkin', 'oystercatcher', 'spoonbill']]</em></pre><p name="ba81" id="ba81" class="graf graf--p graf-after--pre">Exactly the same result. It is now searching all of the WordNet.</p><h4 name="f233" id="f233" class="graf graf--h4 graf-after--p">Text -&gt; image search [<a href="https://youtu.be/tY0n9OT5_nA?t=2h13m16s" data-href="https://youtu.be/tY0n9OT5_nA?t=2h13m16s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">2:13:16</a>]</h4><p name="a2b4" id="a2b4" class="graf graf--p graf-after--h4">Now let’s do something a bit different — which is to take all of our predictions (<code class="markup--code markup--p-code">pred_wv</code>) so basically take our whole validation set of images and create a KNN index of the image representations because remember, it is predicting things that are meant to be word vectors. Now let’s grab the fast text vector for “boat” and boat is not an ImageNet concept — yet we can now find all of the images in our predicted word vectors (i.e. our validation set) that are closest to the word boat and it works even though it is not something that was ever trained on.</p><pre name="72e3" id="72e3" class="graf graf--pre graf-after--p">nn_predwv = create_index(pred_wv)<br>en_vecd = pickle.load(open(TRANS_PATH/'wiki.en.pkl','rb'))<br>vec = en_vecd['boat']</pre><pre name="bb76" id="bb76" class="graf graf--pre graf-after--pre">idxs,dists = get_knn(nn_predwv, vec)<br>show_imgs([open_image(PATH/md.val_ds.fnames[i]) <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> idxs[:3]],<br>                      3, figsize=(9,3));</pre><figure name="5791" id="5791" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_BLLI7IWFO84BPwB-Y1uCKg.png"></figure><p name="f197" id="f197" class="graf graf--p graf-after--figure">What if we now take engine’s vector and boat’s vector and take their average and what if we now look in our nearest neighbors for that [<a href="https://youtu.be/tY0n9OT5_nA?t=2h14m4s" data-href="https://youtu.be/tY0n9OT5_nA?t=2h14m4s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">2:14:04</a>]?</p><pre name="f02f" id="f02f" class="graf graf--pre graf-after--p">vec = (en_vecd['engine'] + en_vecd['boat'])/2 </pre><pre name="0551" id="0551" class="graf graf--pre graf-after--pre">idxs,dists = get_knn(nn_predwv, vec)<br>show_imgs([open_image(PATH/md.val_ds.fnames[i]) <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> idxs[:3]],<br>                      3, figsize=(9,3));</pre><figure name="4ace" id="4ace" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_EkwjxE8m8xeX2lDRLIJTCw.png"></figure><p name="d7be" id="d7be" class="graf graf--p graf-after--figure">These are boats with engines. I mean, yes, the middle one is actually a boat with an engine — it just happens to have wings on as well. By the way, sail is not an ImageNet thing&nbsp;, neither is boat. Here is the average of two things that are not ImageNet things and yet with one exception, it’s found us two sailboats.</p><pre name="1f6a" id="1f6a" class="graf graf--pre graf-after--p">vec = (en_vecd['sail'] + en_vecd['boat'])/2</pre><pre name="deb4" id="deb4" class="graf graf--pre graf-after--pre">idxs,dists = get_knn(nn_predwv, vec)<br>show_imgs([open_image(PATH/md.val_ds.fnames[i]) <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> idxs[:3]],<br>                      3, figsize=(9,3));</pre><figure name="32ff" id="32ff" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_-7X7LGxPWi1qv8fF1hkCog.png"></figure><h4 name="a464" id="a464" class="graf graf--h4 graf-after--figure">Image-&gt;image [<a href="https://youtu.be/tY0n9OT5_nA?t=2h14m35s" data-href="https://youtu.be/tY0n9OT5_nA?t=2h14m35s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">2:14:35</a>]</h4><p name="419f" id="419f" class="graf graf--p graf-after--h4">Okay, let’s do something else crazy. Let’s open up an image in the validation set. Let’s call <code class="markup--code markup--p-code">predict_array</code> on that image to get its word vector like thing, and let’s do a nearest neighbor search on all the other images.</p><pre name="78d8" id="78d8" class="graf graf--pre graf-after--p">fname = 'valid/n01440764/ILSVRC2012_val_00007197.JPEG'<br>img = open_image(PATH/fname)<br>show_img(img);</pre><figure name="caf8" id="caf8" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_FVCX6O367r2oJXVhR1r-Sg.png"></figure><pre name="5582" id="5582" class="graf graf--pre graf-after--figure">t_img = md.val_ds.transform(img)<br>pred = learn.predict_array(t_img[<strong class="markup--strong markup--pre-strong">None</strong>])</pre><pre name="bbcb" id="bbcb" class="graf graf--pre graf-after--pre">idxs,dists = get_knn(nn_predwv, pred)<br>show_imgs([open_image(PATH/md.val_ds.fnames[i]) <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> idxs[1:4]],<br>                      3, figsize=(9,3));</pre><figure name="a044" id="a044" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_hQ8r1gcwNfh7KlZGjS-lcQ.png"></figure><p name="8e97" id="8e97" class="graf graf--p graf-after--figure">And here are all the other images of whatever that is. So you can see, this is crazy — we’ve trained a thing on all of ImageNet in an hour, using a custom head that required basically like two lines fo code, and these things run in 300 milliseconds to do these searches.</p><p name="787d" id="787d" class="graf graf--p graf-after--p">Jeremy taught this basic idea last year as well, but it was in Keras, and it was pages and pages of code, and everything took a long time and complicated. And back then, Jeremy said he can’t begin to think all of the stuff you could do with this. He doesn’t think anybody has really thought deeply about this yet, but he thinks it’s fascinating. So go back and read the DeVICE paper because Andrea had a whole bunch of other thoughts and now that it is so easy to do, hopefully people will dig into this now. Jeremy thinks it’s crazy and amazing.</p><p name="ffb1" id="ffb1" class="graf graf--p graf-after--p graf--trailing">Alright, see you next week!</p><hr class="section-divider"><p name="1c54" id="1c54" class="graf graf--p graf--leading graf--trailing">Lessons: <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" class="markup--anchor markup--p-anchor" target="_blank">1</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" class="markup--anchor markup--p-anchor" target="_blank">2</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" class="markup--anchor markup--p-anchor" target="_blank">3</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" class="markup--anchor markup--p-anchor" target="_blank">4</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" class="markup--anchor markup--p-anchor" target="_blank">5</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" class="markup--anchor markup--p-anchor" target="_blank">6</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" class="markup--anchor markup--p-anchor" target="_blank">7</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" class="markup--anchor markup--p-anchor" target="_blank">8</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" class="markup--anchor markup--p-anchor" target="_blank">9</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" class="markup--anchor markup--p-anchor" target="_blank">10</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" class="markup--anchor markup--p-anchor" target="_blank"><strong class="markup--strong markup--p-strong">11</strong></a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" class="markup--anchor markup--p-anchor" target="_blank">12</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" class="markup--anchor markup--p-anchor" target="_blank">13</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" class="markup--anchor markup--p-anchor" target="_blank">14</a></p></body></html>