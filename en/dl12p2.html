<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></head><body><p name="ca9d" id="ca9d" class="graf graf--p graf-after--pre">This will look pretty familiar [<a href="https://youtu.be/ondivPiwQho?t=57m10s" data-href="https://youtu.be/ondivPiwQho?t=57m10s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">57:10</a>]. This is before Jeremy realized that sequential models are much better. So if you compare this to the previous conv block with a sequential model, there is a lot more lines of code here — but it does the same thing of conv, ReLU, batch norm.</p><pre name="1bee" id="1bee" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">ConvBlock</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, ni, no, ks, stride, bn=<strong class="markup--strong markup--pre-strong">True</strong>, pad=<strong class="markup--strong markup--pre-strong">None</strong>):<br>        super().__init__()<br>        <strong class="markup--strong markup--pre-strong">if</strong> pad <strong class="markup--strong markup--pre-strong">is</strong> <strong class="markup--strong markup--pre-strong">None</strong>: pad = ks//2//stride<br>        self.conv = nn.Conv2d(ni, no, ks, stride, padding=pad, <br>                              bias=<strong class="markup--strong markup--pre-strong">False</strong>)<br>        self.bn = nn.BatchNorm2d(no) <strong class="markup--strong markup--pre-strong">if</strong> bn <strong class="markup--strong markup--pre-strong">else</strong> <strong class="markup--strong markup--pre-strong">None</strong><br>        self.relu = nn.LeakyReLU(0.2, inplace=<strong class="markup--strong markup--pre-strong">True</strong>)<br>    <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x):<br>        x = self.relu(self.conv(x))<br>        <strong class="markup--strong markup--pre-strong">return</strong> self.bn(x) <strong class="markup--strong markup--pre-strong">if</strong> self.bn <strong class="markup--strong markup--pre-strong">else</strong> x</pre><p name="3471" id="3471" class="graf graf--p graf-after--pre">The first thing we are going to do is to build a discriminator [<a href="https://youtu.be/ondivPiwQho?t=57m47s" data-href="https://youtu.be/ondivPiwQho?t=57m47s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">57:47</a>]. A discriminator is going to receive an image as an input, and it’s going to spit out a number. The number is meant to be lower if it thinks this image is real. Of course “what does it do for a lower number” thing does not appear in the architecture, that will be in the loss function. So all we have to do is to create something that takes an image and spits out a number. A lot of this code is borrowed from the original authors of this paper, so some of the naming scheme is different to what we are used to. But it looks similar to what we had before. We start out with a convolution (conv, ReLU, batch norm). Then we have a bunch of extra conv layers — this is not going to use a residual so it looks very similar to before a bunch of extra layers but these are going to be conv layers rather than res layers. At the end, we need to append enough stride 2 conv layers that we decrease the grid size down to no bigger than 4x4. So it’s going to keep using stride 2, divide the size by 2, and repeat till our grid size is no bigger than 4. This is quite a nice way of creating as many layers as you need in a network to handle arbitrary sized images and turn them into a fixed known grid size.</p><p name="b0b3" id="b0b3" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: Does GAN need a lot more data than say dogs vs. cats or NLP? Or is it comparable [<a href="https://youtu.be/ondivPiwQho?t=59m48s" data-href="https://youtu.be/ondivPiwQho?t=59m48s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">59:48</a>]? Honestly, I am kind of embarrassed to say I am not an expert practitioner in GANs. The stuff I teach in part one is things I am happy to say I know the best way to do these things and so I can show you state-of-the-art results like we just did with CIFAR10 with the help of some of the students. I am not there at all with GANs so I am not quite sure how much you need. In general, it seems it needs quite a lot but remember the only reason we didn’t need too much in dogs and cats is because we had a pre-trained model and could we leverage pre-trained GAN models and fine tune them, probably. I don’t think anybody has done it as far as I know. That could be really interesting thing for people to think about and experiment with. Maybe people have done it and there is some literature there we haven’t come across. I’m somewhat familiar with the main pieces of literature in GANs but I don’t know all of it, so maybe I’ve missed something about transfer learning in GANs. But that would be the trick to not needing too much data.</p><p name="19a7" id="19a7" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: So the huge speed-up a combination of one cycle learning rate and momentum annealing plus the eight GPU parallel training in the half precision? Is that only possible to do the half precision calculation with consumer GPU? Another question, why is the calculation 8 times faster from single to half precision, while from double the single is only 2 times faster [<a href="https://youtu.be/ondivPiwQho?t=1h1m9s" data-href="https://youtu.be/ondivPiwQho?t=1h1m9s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:01:09</a>]? Okay, so the CIFAR10 result, it’s not 8 times faster from single to half. It’s about 2 or 3 times as fast from single to half. NVIDIA claims about the flops performance of the tensor cores, academically correct, but in practice meaningless because it really depends on what calls you need for what piece — so about 2 or 3x improvement for half. So the half precision helps a bit, the extra GPUs helps a bit, the one cycle helps an enormous amount, then another key piece was the playing around with the parameters that I told you about. So reading the wide ResNet paper carefully, identifying the kinds of things that they found there, and then writing a version of the architecture you just saw that made it really easy for us to fiddle around with parameters, staying up all night trying every possible combination of different kernel sizes, numbers of kernels, number of layer groups, size of layer groups. And remember, we did a bottleneck but actually we tended to focus instead on widening so we increase the size and then decrease it because it takes better advantage of the GPU. So all those things combined together, I’d say the one cycle was perhaps the most critical but every one of those resulted in a big speed-up. That’s why we were able to get this 30x improvement over the state-of-the-art CIFAR10. We have some ideas for other things — after this DAWN bench finishes, maybe we’ll try and go even further to see if we can beat one minute one day. That’ll be fun.</p><pre name="0f03" id="0f03" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">DCGAN_D</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, isize, nc, ndf, n_extra_layers=0):<br>        super().__init__()<br>        <strong class="markup--strong markup--pre-strong">assert</strong> isize % 16 == 0, "isize has to be a multiple of 16"<br><br>        self.initial = ConvBlock(nc, ndf, 4, 2, bn=<strong class="markup--strong markup--pre-strong">False</strong>)<br>        csize,cndf = isize/2,ndf<br>        self.extra = nn.Sequential(*[ConvBlock(cndf, cndf, 3, 1)<br>                                    <strong class="markup--strong markup--pre-strong">for</strong> t <strong class="markup--strong markup--pre-strong">in</strong> range(n_extra_layers)])<br><br>        pyr_layers = []<br>        <strong class="markup--strong markup--pre-strong">while</strong> csize &gt; 4:<br>            pyr_layers.append(ConvBlock(cndf, cndf*2, 4, 2))<br>            cndf *= 2; csize /= 2<br>        self.pyramid = nn.Sequential(*pyr_layers)<br>        <br>        self.final = nn.Conv2d(cndf, 1, 4, padding=0, bias=<strong class="markup--strong markup--pre-strong">False</strong>)<br><br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, input):<br>        x = self.initial(input)<br>        x = self.extra(x)<br>        x = self.pyramid(x)<br>        <strong class="markup--strong markup--pre-strong">return</strong> self.final(x).mean(0).view(1)</pre><p name="1cc5" id="1cc5" class="graf graf--p graf-after--pre">So here is our discriminator [<a href="https://youtu.be/ondivPiwQho?t=1h3m37s" data-href="https://youtu.be/ondivPiwQho?t=1h3m37s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:03:37</a>].The important thing to remember about an architecture is it doesn’t do anything rather than have some input tensor size and rank, and some output tensor size and rank. As you see the last conv has one channel. This is different from what we are used to because normally our last thing is a linear block. But our last layer here is a conv block. It only has one channel but it has a grid size of something around 4x4 (no more than 4x4). So we are going to spit out (let’s say it’s 4x4), 4 by 4 by 1 tensor. What we then do is we then take the mean of that. So it goes from 4x4x1 to a scalar. This is kind of like the ultimate adaptive average pooling because we have something with just one channel and we take the mean. So this is a bit different — normally we first do average pooling and then we put it through a fully connected layer to get our one thing out. But this is getting one channel out and then taking the mean of that. Jeremy suspects that it would work better if we did the normal way, but he hasn’t tried it yet and he doesn’t really have a good enough intuition to know whether he is missing something — but it will be an interesting experiment to try if somebody wants to stick an adaptive average pooling layer and a fully connected layer afterwards with a single output.</p><p name="e32b" id="e32b" class="graf graf--p graf-after--p">So that’s a discriminator. Let’s assume we already have a generator — somebody says “okay, here is a generator which generates bedrooms. I want you to build a model that can figure out which ones are real and which ones aren’t”. We are going to take the dataset and label bunch of images which are fake bedrooms from the generator, and a bunch of images of real bedrooms from LSUN dataset to stick a 1 or a 0 on each one. Then we’ll try to get the discriminator to tell the difference. So that is going to be simple enough. But we haven’t been given a generator. We need to build one. We haven’t talked about the loss function yet — we are going to assume that there’s some loss function that does this thing.</p><h4 name="e226" id="e226" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Generator</strong> [<a href="https://youtu.be/ondivPiwQho?t=1h6m15s" data-href="https://youtu.be/ondivPiwQho?t=1h6m15s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:06:15</a>]</h4><p name="5ef8" id="5ef8" class="graf graf--p graf-after--h4">A generator is also an architecture which doesn’t do anything by itself until we have a loss function and data. But what are the ranks and sizes of the tensors? The input to the generator is going to be a vector of random numbers. In the paper, they call that the “prior.” How big? We don’t know. The idea is that a different bunch of random numbers will generate a different bedroom. So our generator has to take as input a vector, stick it through sequential models, and turn it into a rank 4 tensor (rank 3 without the batch dimension) — height by width by 3. So in the final step, <code class="markup--code markup--p-code">nc</code> (number of channel) is going to have to end up being 3 because it’s going to create a 3 channel image of some size.</p><pre name="f01e" id="f01e" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">DeconvBlock</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, ni, no, ks, stride, pad, bn=<strong class="markup--strong markup--pre-strong">True</strong>):<br>        super().__init__()<br>        self.conv = nn.ConvTranspose2d(ni, no, ks, stride, <br>                         padding=pad, bias=<strong class="markup--strong markup--pre-strong">False</strong>)<br>        self.bn = nn.BatchNorm2d(no)<br>        self.relu = nn.ReLU(inplace=<strong class="markup--strong markup--pre-strong">True</strong>)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x):<br>        x = self.relu(self.conv(x))<br>        <strong class="markup--strong markup--pre-strong">return</strong> self.bn(x) <strong class="markup--strong markup--pre-strong">if</strong> self.bn <strong class="markup--strong markup--pre-strong">else</strong> x</pre><pre name="a0d5" id="a0d5" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">DCGAN_G</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, isize, nz, nc, ngf, n_extra_layers=0):<br>        super().__init__()<br>        <strong class="markup--strong markup--pre-strong">assert</strong> isize % 16 == 0, "isize has to be a multiple of 16"<br><br>        cngf, tisize = ngf//2, 4<br>        <strong class="markup--strong markup--pre-strong">while</strong> tisize!=isize: cngf*=2; tisize*=2<br>        layers = [DeconvBlock(nz, cngf, 4, 1, 0)]<br><br>        csize, cndf = 4, cngf<br>        <strong class="markup--strong markup--pre-strong">while</strong> csize &lt; isize//2:<br>            layers.append(DeconvBlock(cngf, cngf//2, 4, 2, 1))<br>            cngf //= 2; csize *= 2<br><br>        layers += [DeconvBlock(cngf, cngf, 3, 1, 1) <br>                       <strong class="markup--strong markup--pre-strong">for</strong> t <strong class="markup--strong markup--pre-strong">in</strong> range(n_extra_layers)]<br>        layers.append(nn.ConvTranspose2d(cngf, nc, 4, 2, 1,<br>                                            bias=<strong class="markup--strong markup--pre-strong">False</strong>))<br>        self.features = nn.Sequential(*layers)<br><br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, input): <strong class="markup--strong markup--pre-strong">return</strong> F.tanh(self.features(input))</pre><p name="ddfc" id="ddfc" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">Question</strong>: In ConvBlock, is there a reason why batch norm comes after ReLU (i.e. <code class="markup--code markup--p-code">self.bn(self.relu(…))</code>) [<a href="https://youtu.be/ondivPiwQho?t=1h7m50s" data-href="https://youtu.be/ondivPiwQho?t=1h7m50s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:07:50</a>]? I would normally expect to go ReLU then batch norm [<a href="https://youtu.be/ondivPiwQho?t=1h8m23s" data-href="https://youtu.be/ondivPiwQho?t=1h8m23s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:08:23</a>] that this is actually the order that makes sense to Jeremy. The order we had in the darknet was what they used in the darknet paper, so everybody seems to have a different order of these things. In fact, most people for CIFAR10 have a different order again which is batch norm → ReLU → conv which is a quirky way of thinking about it, but it turns out that often for residual blocks that works better. That is called a “pre-activation ResNet.” There is a few blog posts out there where people have experimented with different order of those things and it seems to depend a lot on what specific dataset it is and what you are doing with — although the difference in performance is small enough that you won’t care unless it’s for a competition.</p><h4 name="cf8f" id="cf8f" class="graf graf--h4 graf-after--p">Deconvolution [<a href="https://youtu.be/ondivPiwQho?t=1h9m36s" data-href="https://youtu.be/ondivPiwQho?t=1h9m36s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:09:36</a>]</h4><p name="dd33" id="dd33" class="graf graf--p graf-after--h4">So the generator needs to start with a vector and end up with a rank 3 tensor. We don’t really know how to do that yet. We need to use something called a “deconvolution” and PyTorch calls it transposed convolution — same thing, different name. Deconvolution is something which rather than decreasing the grid size, it increases the grid size. So as with all things, it’s easiest to see in an Excel spreadsheet.</p><p name="6fd4" id="6fd4" class="graf graf--p graf-after--p">Here is a convolution. We start, let’s say, with a 4 by 4 grid cell with a single channel. Let’s put it through a 3 by 3 kernel with a single output filter. So we have a single channel in, a single filter kernel, so if we don’t add any padding, we are going to end up with 2 by 2. Remember, the convolution is just the sum of the product of the kernel and the appropriate grid cell [<a href="https://youtu.be/ondivPiwQho?t=1h11m9s" data-href="https://youtu.be/ondivPiwQho?t=1h11m9s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:11:09</a>]. So there is our standard 3 by 3 conv one channel one filter.</p><figure name="0bb4" id="0bb4" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_FqkDO90rEDwa_CgxTAlyIQ.png"></figure><p name="7d0c" id="7d0c" class="graf graf--p graf-after--figure">So the idea now is we want to go the opposite direction [<a href="https://youtu.be/ondivPiwQho?t=1h11m25s" data-href="https://youtu.be/ondivPiwQho?t=1h11m25s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:11:25</a>]. We want to start with our 2 by 2 and we want to create a 4 by 4. Specifically we want to create the same 4 by 4 that we started with. And we want to do that by using a convolution. How would we do that?</p><p name="02d3" id="02d3" class="graf graf--p graf-after--p">If we have a 3 by 3 convolution, then if we want to create a 4 by 4 output, we are going to need to create this much padding:</p><figure name="8bc7" id="8bc7" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_flOxFmF21kUyLpPDJ6kr-w.png"></figure><p name="e028" id="e028" class="graf graf--p graf-after--figure">Because with this much padding, we are going to end up with 4 by 4. So let’s say our convolutional filter was just a bunch of zeros then we can calculate our error for each cell just by taking this subtraction:</p><figure name="2438" id="2438" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_HKcU-wgdLPgxd5kJfEkmlg.png"></figure><p name="41b9" id="41b9" class="graf graf--p graf-after--figure">Then we can get the sum of absolute values (L1 loss) by summing up the absolute values of those errors:</p><figure name="a468" id="a468" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_mjLTOFUXneXGeER4hKj4Kw.png"></figure><p name="9ace" id="9ace" class="graf graf--p graf-after--figure">So now we could use optimization, in Excel it’s called “solver” to do a gradient descent. So we will set the Total cell equal to minimum and we’ll try and reduce our loss by changing our filter. You can see it’s come up with a filter such that Result is almost like Data. It’s not perfect, and in general, you can’t assume that a deconvolution can exactly create the same exact thing you want because there is just not enough. Because there is 9 things in the filter and 16 things in the result. But it’s made a pretty good attempt. So this is what a deconvolution looks like — a stride 1, 3x3 deconvolution on a 2x2 grid cell input.</p><figure name="e1fa" id="e1fa" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_QzJe8qhpZl6hfKAB0Zw-vQ.png"></figure><p name="802f" id="802f" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Question</strong>: How difficult is it to create a discriminator to identify fake news vs. real news [<a href="https://youtu.be/ondivPiwQho?t=1h13m43s" data-href="https://youtu.be/ondivPiwQho?t=1h13m43s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:13:43</a>]? You don’t need anything special — that’s just a classifier. So you would just use the NLP classifier from previous class and lesson 4. In that case, there is no generative piece, so you just need a dataset that says these are the things that we believe are fake news and these are the things we consider to be real news and it should actually work very well. To the best of our knowledge, if you try it you should get as good a result as anybody else has got — whether it’s good enough to be useful in practice, Jeremy doesn’t know. The best thing you could do at this stage would be to generate a kind of a triage that says these things look pretty sketchy based on how they are written and then some human could go in and fact check them. NLP classifier and RNN can’t fact-check things but it could recognize that these are written in that kind of highly popularized style which often fake news is written in so maybe these ones are worth paying attention to. That would probably be the best you could hope for without drawing on some kind of external data sources. But it’s important to remember the discriminator is basically just a classifier and you don’t need any special techniques beyond what we’ve already learned to do NLP classification.</p><h4 name="6b71" id="6b71" class="graf graf--h4 graf-after--p">ConvTranspose2d [<a href="https://youtu.be/ondivPiwQho?t=1h16m" data-href="https://youtu.be/ondivPiwQho?t=1h16m" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:16:00</a>]</h4><p name="5f8e" id="5f8e" class="graf graf--p graf-after--h4">To do deconvolution in PyTorch, just say:</p><p name="e693" id="e693" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">nn.ConvTranspose2d(ni, no, ks, stride, padding=pad, bias=False)</code></p><ul class="postList"><li name="1c41" id="1c41" class="graf graf--li graf-after--p"><code class="markup--code markup--li-code">ni</code>&nbsp;: number of input channels</li><li name="6445" id="6445" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">no</code>: number of ourput channels</li><li name="81f1" id="81f1" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">ks</code>: kernel size</li></ul><p name="6719" id="6719" class="graf graf--p graf-after--li">The reason it’s called a ConvTranspose is because it turns out that this is the same as the calculation of the gradient of convolution. That’s why they call it that.</p><p name="e42c" id="e42c" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Visualizing</strong> [<a href="https://youtu.be/ondivPiwQho?t=1h16m33s" data-href="https://youtu.be/ondivPiwQho?t=1h16m33s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:16:33</a>]</p><figure name="df2f" id="df2f" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_GZz25GtnzqaYy5MV5iQPmA.png"><figcaption class="imageCaption"><a href="http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html" data-href="http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html</a></figcaption></figure><p name="5e46" id="5e46" class="graf graf--p graf-after--figure">One on the left is what we just saw of doing a 2x2 deconvolution. If there is a stride 2, then you don’t just have padding around the outside, but you actually have to put padding in the middle as well. They are not actually quite implemented this way because this is slow to do. In practice, you’ll implement them in a different way but it all happens behind the scene, so you don’t have to worry about it. We’ve talked about this <a href="http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html" data-href="http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">convolution arithmetic tutorial</a> before and if you are still not comfortable with convolutions and in order to get comfortable with deconvolutions, this is a great site to go to. If you want to see the paper, it is <a href="https://arxiv.org/abs/1603.07285" data-href="https://arxiv.org/abs/1603.07285" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">A guide to convolution arithmetic for deep learning</a>.</p><p name="c5dd" id="c5dd" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code">DeconvBlock</code> looks identical to a <code class="markup--code markup--p-code">ConvBlock</code> except it has the word <code class="markup--code markup--p-code">Transpose</code> [<a href="https://youtu.be/ondivPiwQho?t=1h17m49s" data-href="https://youtu.be/ondivPiwQho?t=1h17m49s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:17:49</a>]. We just go conv → relu → batch norm as before, and it has input filters and output filters. The only difference is taht stride 2 means that the grid size will double rather than half.</p><figure name="9c6c" id="9c6c" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_vUpDoEX5vPs6y3auKiCFsQ.png"></figure><p name="80e0" id="80e0" class="graf graf--p graf-after--figure">Question: Both <code class="markup--code markup--p-code">nn.ConvTranspose2d</code> and <code class="markup--code markup--p-code">nn.Upsample</code> seem to do the same thing, i.e. expand grid-size (height and width) from previous layer. Can we say <code class="markup--code markup--p-code">nn.ConvTranspose2d</code> is always better than <code class="markup--code markup--p-code">nn.Upsample</code>, since <code class="markup--code markup--p-code">nn.Upsample</code> is merely resize and fill unknowns by zero’s or interpolation [<a href="https://youtu.be/ondivPiwQho?t=1h18m10s" data-href="https://youtu.be/ondivPiwQho?t=1h18m10s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:18:10</a>]? No, you can’t. There is a fantastic interactive paper on distill.pub called <a href="https://distill.pub/2016/deconv-checkerboard/" data-href="https://distill.pub/2016/deconv-checkerboard/" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">Deconvolution and Checkerboard Artifacts</a> which points out that what we are doing right now is extremely suboptimal but the good news is everybody else does it.</p><figure name="c74c" id="c74c" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_-EmXZ1cNtZEO-2SwEYG6bA.png"></figure><p name="64b4" id="64b4" class="graf graf--p graf-after--figure">Have a look here, could you see these checkerboard artifacts? These are all from actual papers and basically they noticed every one of these papers with generative models have these checkerboard artifacts and what they realized is it’s because when you have a stride 2 convolution of size three kernel, they overlap. So some grid cells gets twice as much activation,</p><figure name="09a6" id="09a6" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_rafmdyh7EfqCsptcOppq1w.png"></figure><p name="d420" id="d420" class="graf graf--p graf-after--figure">So even if you start with random weights, you end up with a checkerboard artifacts. So deeper you get, the worse it gets. Their advice is less direct than it ought to be, Jeremy found that for most generative models, upsampling is better. If you <code class="markup--code markup--p-code">nn.Upsample</code>, it’s basically doing the opposite of pooling — it says let’s replace this one grid cell with four (2x2). There is a number of ways to upsample — one is just to copy it all across to those four, and other is to use bilinear or bicubic interpolation. There are various techniques to try and create a smooth upsampled version and you can choose any of them in PyTorch. If you do a 2 x 2 upsample and then regular stride one 3 x 3 convolution, that is another way of doing the same kind of thing as a ConvTranspose — it’s doubling the grid size and doing some convolutional arithmetic on it. For generative models, it pretty much always works better. In that distil.pub publication, they indicate that maybe that’s a good approach but they don’t just come out and say just do this whereas Jeremy would just say just do this. Having said that, for GANS, he hasn’t had that much success with it yet and he thinks it probably requires some tweaking to get it to work, The issue is that in the early stages, it doesn’t create enough noise. He had a version where he tried to do it with an upsample and you could kind of see that the noise didn’t look very noisy. Next week when we look at style transfer and super-resolution, you will see <code class="markup--code markup--p-code">nn.Upsample</code> really comes into its own.</p><p name="d93d" id="d93d" class="graf graf--p graf-after--p">The generator, we can now start with the vector [<a href="https://youtu.be/ondivPiwQho?t=1h22m04s" data-href="https://youtu.be/ondivPiwQho?t=1h22m04s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:22:04</a>]. We can decide and say okay let’s not think of it as a vector but actually it’s 1x1 grid cell, and then we can turn it into a 4x4 then 8x8 and so forth. That is why we have to make sure it’s a suitable multiple so that we can create something of the right size. As you can see, it’s doing the exact opposite as before. It’s making the cell size bigger and bigger by 2 at a time as long as it can until it gets to half the size that we want, and then finally we add <code class="markup--code markup--p-code">n</code> more on at the end with stride 1. Then we add one more ConvTranspose to finally get to the size that we wanted and we are done. Finally we put that through a <code class="markup--code markup--p-code">tanh</code> and that will force us to be in the zero to one range because of course we don’t want to spit out arbitrary size pixel values. So we have a generator architecture which spits out an image of some given size with the correct number of channels with values between zero and one.</p><figure name="b092" id="b092" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_sNvYsoGpBl6vzCdcjWkH1Q.png"></figure><p name="c744" id="c744" class="graf graf--p graf-after--figure">At this point, we can now create our model data object [<a href="https://youtu.be/ondivPiwQho?t=1h23m38s" data-href="https://youtu.be/ondivPiwQho?t=1h23m38s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:23:38</a>]. These things take a while to train, so we made it 128 by 128 (just a convenient way to make it a little bit faster). So that is going to be the size of the input, but then we are going to use transformation to turn it into 64 by 64.</p><p name="bcb7" id="bcb7" class="graf graf--p graf-after--p">There’s been more recent advances which have attempted to really increase this up to high resolution sizes but they still tend to require either a batch size of 1 or lots and lots of GPUs [<a href="https://youtu.be/ondivPiwQho?t=1h24m5s" data-href="https://youtu.be/ondivPiwQho?t=1h24m5s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:24:05</a>]. So we are trying to do things that we can do with a single consumer GPU. Here is an example of one of the 64 by 64 bedrooms.</p><pre name="41d7" id="41d7" class="graf graf--pre graf-after--p">bs,sz,nz = 64,64,100</pre><pre name="c418" id="c418" class="graf graf--pre graf-after--pre">tfms = tfms_from_stats(inception_stats, sz)<br>md = ImageClassifierData.from_csv(PATH, 'bedroom', CSV_PATH, <br>         tfms=tfms, bs=128, skip_header=<strong class="markup--strong markup--pre-strong">False</strong>, continuous=<strong class="markup--strong markup--pre-strong">True</strong>)</pre><pre name="933f" id="933f" class="graf graf--pre graf-after--pre">md = md.resize(128)</pre><pre name="e838" id="e838" class="graf graf--pre graf-after--pre">x,_ = next(iter(md.val_dl))</pre><pre name="2cfc" id="2cfc" class="graf graf--pre graf-after--pre">plt.imshow(md.trn_ds.denorm(x)[0]);</pre><figure name="981e" id="981e" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_FIBPb5I8EloAjg7mvtRXaQ.png"></figure><h4 name="153c" id="153c" class="graf graf--h4 graf-after--figure">Putting them all together [<a href="https://youtu.be/ondivPiwQho?t=1h24m30s" data-href="https://youtu.be/ondivPiwQho?t=1h24m30s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:24:30</a>]</h4><p name="c565" id="c565" class="graf graf--p graf-after--h4">We are going to do pretty much everything manually so let’s go ahead and create our two models — our generator and discriminator and as you can see they are DCGAN, so in other words, they are the same modules that appeared in <a href="https://arxiv.org/abs/1511.06434" data-href="https://arxiv.org/abs/1511.06434" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">this paper</a>. It is well worth going back and looking at the DCGAN paper to see what these architectures are because it’s assumed that when you read the Wasserstein GAN paper that you already know that.</p><pre name="11df" id="11df" class="graf graf--pre graf-after--p">netG = DCGAN_G(sz, nz, 3, 64, 1).cuda()<br>netD = DCGAN_D(sz, 3, 64, 1).cuda()</pre><p name="6890" id="6890" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">Question</strong>: Shouldn’t we use a sigmoid if we want values between 0 and 1 [<a href="https://youtu.be/ondivPiwQho?t=1h25m6s" data-href="https://youtu.be/ondivPiwQho?t=1h25m6s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:25:06</a>]? As usual, our images have been normalized to have a range from -1 to 1, so their pixel values don’t go between 0 and 1 anymore. This is why we want values going from -1 to 1 otherwise we wouldn’t give a correct input for the discriminator (via <a href="http://forums.fast.ai/t/part-2-lesson-12-wiki/15023/140" data-href="http://forums.fast.ai/t/part-2-lesson-12-wiki/15023/140" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">this post</a>).</p><p name="2a92" id="2a92" class="graf graf--p graf-after--p">So we have a generator and a discriminator, and we need a function that returns a “prior” vector (i.e. a bunch of noise)[<a href="https://youtu.be/ondivPiwQho?t=1h25m49s" data-href="https://youtu.be/ondivPiwQho?t=1h25m49s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:25:49</a>]. We do that by creating a bunch of zeros. <code class="markup--code markup--p-code">nz</code> is the size of <code class="markup--code markup--p-code">z</code> — very often in our code, if you see a mysterious letter, it’s because that’s the letter they used in the paper. Here, <code class="markup--code markup--p-code">z</code> is the size of our noise vector. We then use normal distribution to generate random numbers between 0 and 1. And that needs to be a variable because it’s going to be participating in the gradient updates.</p><pre name="ffc4" id="ffc4" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> create_noise(b): <br>   <strong class="markup--strong markup--pre-strong">return</strong> V(torch.zeros(b, nz, 1, 1).normal_(0, 1))</pre><pre name="9c2d" id="9c2d" class="graf graf--pre graf-after--pre">preds = netG(create_noise(4))<br>pred_ims = md.trn_ds.denorm(preds)<br><br>fig, axes = plt.subplots(2, 2, figsize=(6, 6))<br><strong class="markup--strong markup--pre-strong">for</strong> i,ax <strong class="markup--strong markup--pre-strong">in</strong> enumerate(axes.flat): ax.imshow(pred_ims[i])</pre><figure name="bd78" id="bd78" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_4nHm3LLiShNb0pSS3dCuCw.png"></figure><p name="f958" id="f958" class="graf graf--p graf-after--figure">So here is an example of creating some noise and resulting four different pieces of noise.</p><pre name="d7b4" id="d7b4" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> gallery(x, nc=3):<br>    n,h,w,c = x.shape<br>    nr = n//nc<br>    <strong class="markup--strong markup--pre-strong">assert</strong> n == nr*nc<br>    <strong class="markup--strong markup--pre-strong">return</strong> (x.reshape(nr, nc, h, w, c)<br>              .swapaxes(1,2)<br>              .reshape(h*nr, w*nc, c))</pre><p name="880f" id="880f" class="graf graf--p graf-after--pre">We need an optimizer in order to update our gradients [<a href="https://youtu.be/ondivPiwQho?t=1h26m41s" data-href="https://youtu.be/ondivPiwQho?t=1h26m41s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:26:41</a>]. In the Wasserstein GAN paper, they told us to use RMSProp:</p><figure name="5e2b" id="5e2b" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_5o4cwLlNjQfgrNVgLrsVlg.png"></figure><p name="d1a0" id="d1a0" class="graf graf--p graf-after--figure">We can easily do that in PyTorch:</p><pre name="8d3d" id="8d3d" class="graf graf--pre graf-after--p">optimizerD = optim.RMSprop(netD.parameters(), lr = 1e-4)<br>optimizerG = optim.RMSprop(netG.parameters(), lr = 1e-4)</pre><p name="e8a8" id="e8a8" class="graf graf--p graf-after--pre">In the paper, they suggested a learning rate of 0.00005 (<code class="markup--code markup--p-code">5e-5</code>), we found <code class="markup--code markup--p-code">1e-4</code> seem to work, so we made it a little bit bigger.</p><p name="76b3" id="76b3" class="graf graf--p graf-after--p">Now we need a training loop [<a href="https://youtu.be/ondivPiwQho?t=1h27m14s" data-href="https://youtu.be/ondivPiwQho?t=1h27m14s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:27:14</a>]:</p><figure name="214c" id="214c" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_VROXSgyt6HWaJiMMY6ogFQ.png"><figcaption class="imageCaption">For easier&nbsp;reading</figcaption></figure><p name="23ec" id="23ec" class="graf graf--p graf-after--figure">A training loop will go through some number of epochs that we get to pick (so that’s going to be a parameter). Remember, when you do everything manually, you’ve got to remember all the manual steps to do:</p><ol class="postList"><li name="de06" id="de06" class="graf graf--li graf-after--p">You have to set your modules into training mode when you are training them and into evaluation mode when you are evaluating because in training mode batch norm updates happen and dropout happens, in evaluation mode, those two things gets turned off.</li><li name="9f6d" id="9f6d" class="graf graf--li graf-after--li">We are going to grab an iterator from our training data loader</li><li name="db40" id="db40" class="graf graf--li graf-after--li">We are going to see how many steps we have to go through and then we will use <code class="markup--code markup--li-code">tqdm</code> to give us a progress bar, and we are going to go through that many steps.</li></ol><p name="8bb6" id="8bb6" class="graf graf--p graf-after--li">The first step of the algorithm in the paper is to update the discriminator (in the paper, they call discriminator a “critic” and <code class="markup--code markup--p-code">w</code> is the weights of the critic). So the first step is to train our critic a little bit, and then we are going to train our generator a little bit, and we will go back to the top of the loop. The inner <code class="markup--code markup--p-code">for</code> loop in the paper correspond to the second <code class="markup--code markup--p-code">while</code> loop in our code.</p><p name="a506" id="a506" class="graf graf--p graf-after--p">What we are going to do now is we have a generator that is random at the moment [<a href="https://youtu.be/ondivPiwQho?t=1h29m6s" data-href="https://youtu.be/ondivPiwQho?t=1h29m6s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:29:06</a>]. So our generator will generate something that looks like the noise. First of all, we need to teach our discriminator to tell the difference between the noise and a bedroom — which shouldn’t be too hard you would hope. So we just do it in the usual way but there is a few little tweaks:</p><ol class="postList"><li name="e712" id="e712" class="graf graf--li graf-after--p">We are going to grab a mini batch of real bedroom photos so we can just grab the next batch from our iterator, turn it into a variable.</li><li name="496d" id="496d" class="graf graf--li graf-after--li">Then we are going to calculate the loss for that — so this is going to be how much the discriminator thinks this looks fake (“does the real one look fake?”).</li><li name="7d54" id="7d54" class="graf graf--li graf-after--li">Then we are going to create some fake images and to do that we will create some random noise, and we will stick it through our generator which at this stage is just a bunch of random weights. That will create a mini batch of fake images.</li><li name="2218" id="2218" class="graf graf--li graf-after--li">Then we will put that through the same discriminator module as before to get the loss for that (“how fake does the fake one look?”). Remember, when you do everything manually, you have to zero the gradients (<code class="markup--code markup--li-code">netD.zero_grad()</code>) in your loop. If you have forgotten about that, go back to the part 1 lesson where we do everything from scratch.</li><li name="1238" id="1238" class="graf graf--li graf-after--li">Finally, the total discriminator loss is equal to the real loss minus the fake loss.</li></ol><p name="d220" id="d220" class="graf graf--p graf-after--li">So you can see that here [<a href="https://youtu.be/ondivPiwQho?t=1h30m58s" data-href="https://youtu.be/ondivPiwQho?t=1h30m58s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:30:58</a>]:</p><figure name="00e3" id="00e3" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*atls5DInIbp5wHZz8szQ1A.png" data-width="528" data-height="32" src="../img/1_atls5DInIbp5wHZz8szQ1A.png"></figure><p name="a7c2" id="a7c2" class="graf graf--p graf-after--figure">They don’t talk about the loss, they actually just talk about one of the gradient updates.</p><figure name="83a0" id="83a0" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_9nGWityXFzNdgOxN15flRA.png"></figure><p name="4ab6" id="4ab6" class="graf graf--p graf-after--figure">In PyTorch, we don’t have to worry about getting the gradients, we can just specify the loss and call <code class="markup--code markup--p-code">loss.backward()</code> then discriminator’s <code class="markup--code markup--p-code">optimizer.step()</code>[<a href="https://youtu.be/ondivPiwQho?t=1h34m27s" data-href="https://youtu.be/ondivPiwQho?t=1h34m27s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:34:27</a>]. There is one key step which is that we have to keep all of our weights which are the parameters in PyTorch module in the small range of -0.01 and 0.01. Why? Because the mathematical assumptions that make this algorithm work only apply in a small ball. It is interesting to understand the math of why that is the case, but it’s very specific to this one paper and understanding it won’t help you understand any other paper, so only study it if you are interested. It is nicely explained and Jeremy thinks it’s fun but it won’t be information that you will reuse elsewhere unless you get super into GANs. He also mentioned that after the came out and improved Wasserstein GAN came out that said there are better ways to ensure that your weight space is in this tight ball which was to penalize gradients that are too high, so nowadays there are slightly different ways to do this. But this line of code is the key contribution and it is what makes it Wasserstein GAN:</p><pre name="4628" id="4628" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">for</strong> p <strong class="markup--strong markup--pre-strong">in</strong> netD.parameters(): p.data.clamp_(-0.01, 0.01)</pre><p name="63d0" id="63d0" class="graf graf--p graf-after--pre">At the end of this, we have a discriminator that can recognize real bedrooms and our totally random crappy generated images [<a href="https://youtu.be/ondivPiwQho?t=1h36m20s" data-href="https://youtu.be/ondivPiwQho?t=1h36m20s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:36:20</a>]. Let’s now try and create some better images. So now set trainable discriminator to false, set trainable generator to true, zero out the gradients of the generator. Our loss again is <code class="markup--code markup--p-code">fw</code> (discriminator) of the generator applied to some more random noise. So it’s exactly the same as before where we did generator on the noise and then pass that to a discriminator, but this time, the thing that’s trainable is the generator, not the discriminator. In other words, in the pseudo code, the thing they update is Ɵ which is the generator’s parameters. So it takes noise, generate some images, try and figure out if they are fake or real, and use that to get gradients with respect to the generator, as opposed to earlier we got them with respect to the discriminator, and use that to update our weights with RMSProp with an alpha learning rate [<a href="https://youtu.be/ondivPiwQho?t=1h38m21s" data-href="https://youtu.be/ondivPiwQho?t=1h38m21s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:38:21</a>].</p><pre name="fb18" id="fb18" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> train(niter, first=<strong class="markup--strong markup--pre-strong">True</strong>):<br>    gen_iterations = 0<br>    <strong class="markup--strong markup--pre-strong">for</strong> epoch <strong class="markup--strong markup--pre-strong">in</strong> trange(niter):<br>        netD.train(); netG.train()<br>        data_iter = iter(md.trn_dl)<br>        i,n = 0,len(md.trn_dl)<br>        <strong class="markup--strong markup--pre-strong">with</strong> tqdm(total=n) <strong class="markup--strong markup--pre-strong">as</strong> pbar:<br>            <strong class="markup--strong markup--pre-strong">while</strong> i &lt; n:<br>                set_trainable(netD, <strong class="markup--strong markup--pre-strong">True</strong>)<br>                set_trainable(netG, <strong class="markup--strong markup--pre-strong">False</strong>)<br>                d_iters = 100 <strong class="markup--strong markup--pre-strong">if</strong> (first <strong class="markup--strong markup--pre-strong">and</strong> (gen_iterations &lt; 25) <br>                              <strong class="markup--strong markup--pre-strong">or</strong> (gen_iterations % 500 == 0)) <strong class="markup--strong markup--pre-strong">else</strong> 5<br>                j = 0<br>                <strong class="markup--strong markup--pre-strong">while</strong> (j &lt; d_iters) <strong class="markup--strong markup--pre-strong">and</strong> (i &lt; n):<br>                    j += 1; i += 1<br>                    <strong class="markup--strong markup--pre-strong">for</strong> p <strong class="markup--strong markup--pre-strong">in</strong> netD.parameters(): <br>                        p.data.clamp_(-0.01, 0.01)<br>                    real = V(next(data_iter)[0])<br>                    real_loss = netD(real)<br>                    fake = netG(create_noise(real.size(0)))<br>                    fake_loss = netD(V(fake.data))<br>                    netD.zero_grad()<br>                    lossD = real_loss-fake_loss<br>                    lossD.backward()<br>                    optimizerD.step()<br>                    pbar.update()<br><br>                set_trainable(netD, <strong class="markup--strong markup--pre-strong">False</strong>)<br>                set_trainable(netG, <strong class="markup--strong markup--pre-strong">True</strong>)<br>                netG.zero_grad()<br>                lossG = netD(netG(create_noise(bs))).mean(0).view(1)<br>                lossG.backward()<br>                optimizerG.step()<br>                gen_iterations += 1<br>            <br>        print(f'Loss_D {to_np(lossD)}; Loss_G {to_np(lossG)}; '<br>              f'D_real {to_np(real_loss)}; Loss_D_fake<br>              {to_np(fake_loss)}')</pre><p name="d689" id="d689" class="graf graf--p graf-after--pre">You’ll see that it’s unfair that the discriminator is getting trained <em class="markup--em markup--p-em">ncritic</em> times (<code class="markup--code markup--p-code">d_iters</code> in above code) which they set to 5 for every time we train the generator once. And the paper talks a bit about this but the basic idea is there is no point making the generator better if the discriminator doesn’t know how to discriminate yet. So that’s why we have the second while loop. And here is that 5:</p><pre name="e3d8" id="e3d8" class="graf graf--pre graf-after--p">d_iters = 100 <strong class="markup--strong markup--pre-strong">if</strong> (first <strong class="markup--strong markup--pre-strong">and</strong> (gen_iterations &lt; 25) <br>                              <strong class="markup--strong markup--pre-strong">or</strong> (gen_iterations % 500 == 0)) <strong class="markup--strong markup--pre-strong">else</strong> 5</pre><p name="586b" id="586b" class="graf graf--p graf-after--pre">Actually something which was added in the later paper or maybe supplementary material is the idea that from time to time and a bunch of times at the start, you should do more steps at the discriminator to make sure that the discriminator is capable.</p><pre name="c2f1" id="c2f1" class="graf graf--pre graf-after--p">torch.backends.cudnn.benchmark=<strong class="markup--strong markup--pre-strong">True</strong></pre><p name="56e9" id="56e9" class="graf graf--p graf-after--pre">Let’s train that for one epoch:</p><pre name="6c49" id="6c49" class="graf graf--pre graf-after--p">train(1, <strong class="markup--strong markup--pre-strong">False</strong>)</pre><pre name="72c2" id="72c2" class="graf graf--pre graf-after--pre">0%|          | 0/1 [00:00&lt;?, ?it/s]<br>100%|██████████| 18957/18957 [19:48&lt;00:00, 10.74it/s]<br>Loss_D [-0.67574]; Loss_G [0.08612]; D_real [-0.1782]; Loss_D_fake [0.49754]<br>100%|██████████| 1/1 [19:49&lt;00:00, 1189.02s/it]</pre><p name="40b8" id="40b8" class="graf graf--p graf-after--pre">Then let’s create some noise so we can generate some examples.</p><pre name="4975" id="4975" class="graf graf--pre graf-after--p">fixed_noise = create_noise(bs)</pre><p name="fbed" id="fbed" class="graf graf--p graf-after--pre">But before that, reduce the learning rate by 10 and do one more pass:</p><pre name="6b40" id="6b40" class="graf graf--pre graf-after--p">set_trainable(netD, <strong class="markup--strong markup--pre-strong">True</strong>)<br>set_trainable(netG, <strong class="markup--strong markup--pre-strong">True</strong>)<br>optimizerD = optim.RMSprop(netD.parameters(), lr = 1e-5)<br>optimizerG = optim.RMSprop(netG.parameters(), lr = 1e-5)</pre><pre name="9992" id="9992" class="graf graf--pre graf-after--pre">train(1, <strong class="markup--strong markup--pre-strong">False</strong>)</pre><pre name="00f0" id="00f0" class="graf graf--pre graf-after--pre">0%|          | 0/1 [00:00&lt;?, ?it/s]<br>100%|██████████| 18957/18957 [23:31&lt;00:00, 13.43it/s]<br>Loss_D [-1.01657]; Loss_G [0.51333]; D_real [-0.50913]; Loss_D_fake [0.50744]<br>100%|██████████| 1/1 [23:31&lt;00:00, 1411.84s/it]</pre><p name="fd3f" id="fd3f" class="graf graf--p graf-after--pre">Then let’s use the noise to pass it to our generator, then put it through our denormalization to turn it back into something we can see, and then plot it:</p><pre name="3ece" id="3ece" class="graf graf--pre graf-after--p">netD.eval(); netG.eval();<br>fake = netG(fixed_noise).data.cpu()<br>faked = np.clip(md.trn_ds.denorm(fake),0,1)<br><br>plt.figure(figsize=(9,9))<br>plt.imshow(gallery(faked, 8));</pre><figure name="1f9a" id="1f9a" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_b8XHbkL7E3tREt_T2mXFqQ.png"></figure><p name="e6e1" id="e6e1" class="graf graf--p graf-after--figure">And we have some bedrooms. These are not real bedrooms, and some of them don’t look particularly like bedrooms, but some of them look a lot like bedrooms, so that’s the idea. That’s GAN. The best way to think about GAN is it is like an underlying technology that you will probably never use like this, but you will use in lots of interesting ways. For example, we are going to use it to create a cycle GAN.</p><p name="a340" id="a340" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Question</strong>: Is there any reason for using RMSProp specifically as the optimizer as opposed to Adam etc. [<a href="https://youtu.be/ondivPiwQho?t=1h41m38s" data-href="https://youtu.be/ondivPiwQho?t=1h41m38s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:41:38</a>]? I don’t remember it being explicitly discussed in the paper. I don’t know if it’s just experimental or the theoretical reason. Have a look in the paper and see what it says.</p><p name="5b72" id="5b72" class="graf graf--p graf-after--p"><a href="http://forums.fast.ai/t/part-2-lesson-12-wiki/15023/211" data-href="http://forums.fast.ai/t/part-2-lesson-12-wiki/15023/211" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">From the forum</a></p><blockquote name="5f5d" id="5f5d" class="graf graf--blockquote graf-after--p">From experimenting I figured that Adam and WGANs not just work worse — it causes to completely fail to train meaningful generator.</blockquote><blockquote name="80d2" id="80d2" class="graf graf--blockquote graf-after--blockquote">from WGAN paper:</blockquote><blockquote name="3df9" id="3df9" class="graf graf--blockquote graf-after--blockquote"><em class="markup--em markup--blockquote-em">Finally, as a negative result, we report that WGAN training becomes unstable at times when one uses a momentum based optimizer such as Adam [8] (with β1&gt;0) on the critic, or when one uses high learning rates. Since the loss for the critic is nonstationary, momentum based methods seemed to perform worse. We identified momentum as a potential cause because, as the loss blew up and samples got worse, the cosine between the Adam step and the gradient usually turned negative. The only places where this cosine was negative was in these situations of instability. We therefore switched to RMSProp [21] which is known to perform well even on very nonstationary problems</em></blockquote><p name="3111" id="3111" class="graf graf--p graf-after--blockquote"><strong class="markup--strong markup--p-strong">Question</strong>: Which could be a reasonable way of detecting overfitting while training? Or of evaluating the performance of one of these GAN models once we are done training? In other words, how does the notion of train/val/test sets translate to GANs [<a href="https://youtu.be/ondivPiwQho?t=1h41m57s" data-href="https://youtu.be/ondivPiwQho?t=1h41m57s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:41:57</a>]? That is an awesome question, and there’s a lot of people who make jokes about how GANs is the one field where you don’t need a test set and people take advantage of that by making stuff up and saying it looks great. There are some famous problems with GANs, one of them is called Mode Collapse. Mode collapse happens where you look at your bedrooms and it turns out that there’s only three kinds of bedrooms that every possible noise vector maps to. You look at your gallery and it turns out they are all just the same thing or just three different things. Mode collapse is easy to see if you collapse down to a small number of modes, like 3 or 4. But what if you have a mode collapse down to 10,000 modes? So there are only 10,000 possible bedrooms that all of your noise vectors collapse to. You wouldn’t be able to see in the gallery view we just saw because it’s unlikely you would have two identical bedrooms out of 10,000. Or what if every one of these bedrooms is basically a direct copy of one of the input — it basically memorized some input. Could that be happening? And the truth is, most papers don’t do a good job or sometimes any job of checking those things. So the question of how do we evaluate GANS and even the point of maybe we should actually evaluate GANs properly is something that is not widely enough understood even now. Some people are trying to really push. Ian Goodfellow was the first author on the most famous deep learning book and is the inventor of GANs and he’s been sending continuous stream of tweets reminding people about the importance of testing GANs properly. If you see a paper that claims exceptional GAN results, then this is definitely something to look at. Have they talked about mode collapse? Have they talked about memorization? And so forth.</p></body></html>