<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></head><body><ul class="postList"><li name="0b51" id="0b51" class="graf graf--li graf-after--pre">Calculate the mean of each channel or each filter and standard deviation of each channel or each filter. Then subtract the means and divide by the standard deviations.</li><li name="a619" id="a619" class="graf graf--li graf-after--li">We no longer need to normalize our input because it is normalizing it per channel or for later layers it is normalizing per filter.</li><li name="98bc" id="98bc" class="graf graf--li graf-after--li">Turns out this is not enough since SGD is bloody-minded [<a href="https://youtu.be/H3g26EVADgY?t=1h29m20s" data-href="https://youtu.be/H3g26EVADgY?t=1h29m20s" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">01:29:20</a>]. If SGD decided that it wants matrix to be bigger/smaller overall, doing <code class="markup--code markup--li-code">(x=self.means) / self.stds</code> is not enough because SGD will undo it and try to do it again in the next mini-batch. So we will add two parameters: <code class="markup--code markup--li-code">a</code> — adder (initial value zeros) and <code class="markup--code markup--li-code">m</code> — multiplier (initial value ones) for each channel.</li><li name="4345" id="4345" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">Parameter</code> tells PyTorch that it is allowed to learn these as weights.</li><li name="b2ad" id="b2ad" class="graf graf--li graf-after--li">Why does this work? If it wants to scale the layer up, it does not have to scale up every single value in the matrix. It can just scale up this single trio of numbers <code class="markup--code markup--li-code">self.m</code>&nbsp;, if it wants to shift it all up or down a bit, it does not have to shift the entire weight matrix, they can just shift this trio of numbers <code class="markup--code markup--li-code">self.a</code>. Intuition: We are normalizing the data and then we are saying you can then shift it and scale it using far fewer parameters than would have been necessary if it were to actually shift and scale the entire set of convolutional filters. In practice, it allows us to increase our learning rates, it increase the resilience of training, and it allows us to add more layers and still train effectively.</li><li name="5106" id="5106" class="graf graf--li graf-after--li">The other thing batch norm does is that it regularizes, in other words, you can often decrease or remove dropout or weight decay. The reason why is each mini-batch is going to have a different mean and a different standard deviation to the previous mini-batch. So they keep changing and it is changing the meaning of the filters in a subtle way acting as a noise (i.e. regularization).</li><li name="2794" id="2794" class="graf graf--li graf-after--li">In real version, it does not use this batch’s mean and standard deviation but takes an exponentially weighted moving average standard deviation and mean.</li><li name="a040" id="a040" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code u-paddingRight0 u-marginRight0"><strong class="markup--strong markup--li-strong">if</strong> self.training</code> — this is important because when you are going through the validation set, you do not want to be changing the meaning of the model. There are some types of layer that are actually sensitive to what the mode of the network is whether it is in training mode or evaluation/test mode. There was a bug when we implemented mini net for MovieLens that dropout was applied during the validation — which was fixed. In PyTorch, there are two such layer: dropout and batch norm. <code class="markup--code markup--li-code">nn.Dropout</code> already does the check.</li><li name="1474" id="1474" class="graf graf--li graf-after--li">[<a href="https://youtu.be/H3g26EVADgY?t=1h37m1s" data-href="https://youtu.be/H3g26EVADgY?t=1h37m1s" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">01:37:01</a>] The key difference in fast.ai which no other library does is that these means and standard deviations get updated in training mode in every other library as soon as you basically say I am training, regardless of whether that layer is set to trainable or not. With a pre-trained network, that is a terrible idea. If you have a pre-trained network for specific values of those means and standard deviations in batch norm, if you change them, it changes the meaning of those pre-trained layers. In fast.ai, always by default, it will not touch those means and standard deviations if your layer is frozen. As soon as you un-freeze it, it will start updating them unless you set <code class="markup--code markup--li-code">learn.bn_freeze=True</code>. In practice, this often seems to work a lot better for pre-trained models particularly if you are working with data that is quite similar to what the pre-trained model was trained with.</li><li name="89ad" id="89ad" class="graf graf--li graf-after--li">Where do you put batch-norm layer? We will talk more in a moment, but for now, after <code class="markup--code markup--li-code">relu</code></li></ul><h4 name="e607" id="e607" class="graf graf--h4 graf-after--li">Ablation Study [<a href="https://youtu.be/H3g26EVADgY?t=1h39m41s" data-href="https://youtu.be/H3g26EVADgY?t=1h39m41s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">01:39:41</a>]</h4><p name="9816" id="9816" class="graf graf--p graf-after--h4">It is something where you try turning on and off different pieces of your model to see which bits make which impacts, and one of the things that wasn’t done in the original batch norm paper was any kind of effective ablation. And one of the things therefore that was missing was this question which was just asked — where to put the batch norm. That oversight caused a lot of problems because it turned out the original paper did not actually put it in the best spot. Other people since then have now figured that out and when Jeremy show people code where it is actually in the spot that is better, people say his batch norm is in the wrong spot.</p><ul class="postList"><li name="9076" id="9076" class="graf graf--li graf-after--p">Try and always use batch norm on every layer if you can</li><li name="e256" id="e256" class="graf graf--li graf-after--li">Don’t stop normalizing your data so that people using your data will know how you normalized your data. Other libraries might not deal with batch norm for pre-trained models correctly, so when people start re-training, it might cause problems.</li></ul><pre name="97ea" id="97ea" class="graf graf--pre graf-after--li"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">ConvBnNet</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, layers, c):<br>        super().__init__()<br>        <strong class="markup--strong markup--pre-strong">self.conv1 = nn.Conv2d(3, 10, kernel_size=5, stride=1, padding=2)</strong><br>        self.layers = nn.ModuleList([<strong class="markup--strong markup--pre-strong">BnLayer</strong>(layers[i], layers[i + 1])<br>            <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(len(layers) - 1)])<br>        self.out = nn.Linear(layers[-1], c)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x):<br>        x = self.conv1(x)<br>        <strong class="markup--strong markup--pre-strong">for</strong> l <strong class="markup--strong markup--pre-strong">in</strong> self.layers: x = l(x)<br>        x = F.adaptive_max_pool2d(x, 1)<br>        x = x.view(x.size(0), -1)<br>        <strong class="markup--strong markup--pre-strong">return</strong> F.log_softmax(self.out(x), dim=-1)</pre><ul class="postList"><li name="cc26" id="cc26" class="graf graf--li graf-after--pre">Rest of the code is similar — Using <code class="markup--code markup--li-code">BnLayer</code> instead of <code class="markup--code markup--li-code">ConvLayer</code></li><li name="bae3" id="bae3" class="graf graf--li graf-after--li">A single convolutional layer was added at the start trying to get closer to the modern approaches. It has a bigger kernel size and a stride of 1. The basic idea is that we want the first layer to have a richer input. It does convolution using the 5 by 5 area which allows it to try and find more interesting richer features in that 5 by 5 area, then spit out bigger output (in this case, it’s 10 by 5 by 5 filters). Typically it is 5 by 5 or 7 by 7, or even 11 by 11 convolution with quite a few filters coming out (e.g. 32 filters).</li><li name="1226" id="1226" class="graf graf--li graf-after--li">Since <code class="markup--code markup--li-code">padding = kernel_size — 1 / 2</code> and <code class="markup--code markup--li-code">stride=1</code>&nbsp;, the input size is the same as the output size — just more filters.</li><li name="77f8" id="77f8" class="graf graf--li graf-after--li">It is a good way of trying to create a richer starting point.</li></ul><h4 name="5991" id="5991" class="graf graf--h4 graf-after--li">Deep BatchNorm [<a href="https://youtu.be/H3g26EVADgY?t=1h50m52s" data-href="https://youtu.be/H3g26EVADgY?t=1h50m52s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">01:50:52</a>]</h4><p name="3230" id="3230" class="graf graf--p graf-after--h4">Let’s increase the depth of the model. We cannot just add more of stride 2 layers since it halves the size of the image each time. Instead, after each stride 2 layer, we insert a stride 1 layer.</p><pre name="c821" id="c821" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">ConvBnNet2</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, layers, c):<br>        super().__init__()<br>        self.conv1 = nn.Conv2d(3, 10, kernel_size=5, stride=1, padding=2)<br>        self.layers = nn.ModuleList([BnLayer(layers[i], layers[i+1])<br>            <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(len(layers) - 1)])<br>        self.layers2 = nn.ModuleList([BnLayer(layers[i+1], layers[i + 1], 1)<br>            <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(len(layers) - 1)])<br>        self.out = nn.Linear(layers[-1], c)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x):<br>        x = self.conv1(x)<br>        <strong class="markup--strong markup--pre-strong">for</strong> l,l2 <strong class="markup--strong markup--pre-strong">in</strong> zip(self.layers, self.layers2):<br>            x = l(x)<br>            x = l2(x)<br>        x = F.adaptive_max_pool2d(x, 1)<br>        x = x.view(x.size(0), -1)<br>        <strong class="markup--strong markup--pre-strong">return</strong> F.log_softmax(self.out(x), dim=-1)</pre><pre name="0298" id="0298" class="graf graf--pre graf-after--pre">learn = ConvLearner.from_model_data((ConvBnNet2([10, 20, 40, 80, 160], 10), data)</pre><pre name="fe51" id="fe51" class="graf graf--pre graf-after--pre">%time learn.fit(1e-2, 2)</pre><pre name="e029" id="e029" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">A Jupyter Widget</em></pre><pre name="58f1" id="58f1" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[ 0.       1.53499  1.43782  0.47588]                       <br>[ 1.       1.28867  1.22616  0.55537]                       <br><br>CPU times: user 1min 22s, sys: 34.5 s, total: 1min 56s<br>Wall time: 58.2 s</em></pre><pre name="b483" id="b483" class="graf graf--pre graf-after--pre">%time learn.fit(1e-2, 2, cycle_len=1)</pre><pre name="16ed" id="16ed" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">A Jupyter Widget</em></pre><pre name="7b64" id="7b64" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[ 0.       1.10933  1.06439  0.61582]                       <br>[ 1.       1.04663  0.98608  0.64609]                       <br><br>CPU times: user 1min 21s, sys: 32.9 s, total: 1min 54s<br>Wall time: 57.6 s</em></pre><p name="bc77" id="bc77" class="graf graf--p graf-after--pre">The accuracy remained the same as before. This is now 12 layers deep, and it is too deep even for batch norm to handle. It is possible to train 12 layer deep conv net but it starts to get difficult. And it does not seem to be helping much if at all.</p><h4 name="9bfa" id="9bfa" class="graf graf--h4 graf-after--p">ResNet [<a href="https://youtu.be/H3g26EVADgY?t=1h52m43s" data-href="https://youtu.be/H3g26EVADgY?t=1h52m43s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">01:52:43</a>]</h4><pre name="6c2d" id="6c2d" class="graf graf--pre graf-after--h4"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">ResnetLayer</strong>(BnLayer):<br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x): <strong class="markup--strong markup--pre-strong">return</strong> <strong class="markup--strong markup--pre-strong">x + super().forward(x)</strong></pre><pre name="f565" id="f565" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">Resnet</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, layers, c):<br>        super().__init__()<br>        self.conv1 = nn.Conv2d(3, 10, kernel_size=5, stride=1, padding=2)<br>        self.layers = nn.ModuleList([BnLayer(layers[i], layers[i+1])<br>            <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(len(layers) - 1)])<br>        self.layers2 = nn.ModuleList([ResnetLayer(layers[i+1], layers[i + 1], 1)<br>            <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(len(layers) - 1)])<br>        self.layers3 = nn.ModuleList([ResnetLayer(layers[i+1], layers[i + 1], 1)<br>            <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(len(layers) - 1)])<br>        self.out = nn.Linear(layers[-1], c)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x):<br>        x = self.conv1(x)<br>        <strong class="markup--strong markup--pre-strong">for</strong> l,l2,l3 <strong class="markup--strong markup--pre-strong">in</strong> zip(self.layers, self.layers2, self.layers3):<br>            x = l3(l2(l(x)))<br>        x = F.adaptive_max_pool2d(x, 1)<br>        x = x.view(x.size(0), -1)<br>        <strong class="markup--strong markup--pre-strong">return</strong> F.log_softmax(self.out(x), dim=-1)</pre><ul class="postList"><li name="2181" id="2181" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">ResnetLayer</code> inherit from <code class="markup--code markup--li-code">BnLayer</code> and override <code class="markup--code markup--li-code">forward</code>.</li><li name="6706" id="6706" class="graf graf--li graf-after--li">Then add bunch of layers and make it 3 times deeper, ad it still trains beautifully just because of <code class="markup--code markup--li-code">x + super().forward(x)</code>&nbsp;.</li></ul><pre name="9932" id="9932" class="graf graf--pre graf-after--li">learn = ConvLearner.from_model_data(Resnet([10, 20, 40, 80, 160], 10), data)</pre><pre name="0db7" id="0db7" class="graf graf--pre graf-after--pre">wd=1e-5</pre><pre name="721c" id="721c" class="graf graf--pre graf-after--pre">%time learn.fit(1e-2, 2, wds=wd)</pre><pre name="fa4d" id="fa4d" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">A Jupyter Widget</em></pre><pre name="7ff9" id="7ff9" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[ 0.       1.58191  1.40258  0.49131]                       <br>[ 1.       1.33134  1.21739  0.55625]                       <br><br>CPU times: user 1min 27s, sys: 34.3 s, total: 2min 1s<br>Wall time: 1min 3s</em></pre><pre name="7057" id="7057" class="graf graf--pre graf-after--pre">%time learn.fit(1e-2, 3, cycle_len=1, cycle_mult=2, wds=wd)</pre><pre name="8b6a" id="8b6a" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">A Jupyter Widget</em></pre><pre name="42cc" id="42cc" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[ 0.       1.11534  1.05117  0.62549]                       <br>[ 1.       1.06272  0.97874  0.65185]                       <br>[ 2.       0.92913  0.90472  0.68154]                        <br>[ 3.       0.97932  0.94404  0.67227]                        <br>[ 4.       0.88057  0.84372  0.70654]                        <br>[ 5.       0.77817  0.77815  0.73018]                        <br>[ 6.       0.73235  0.76302  0.73633]                        <br><br>CPU times: user 5min 2s, sys: 1min 59s, total: 7min 1s<br>Wall time: 3min 39s</em></pre><pre name="3216" id="3216" class="graf graf--pre graf-after--pre">%time learn.fit(1e-2, 8, cycle_len=4, wds=wd)</pre><pre name="f593" id="f593" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">A Jupyter Widget</em></pre><pre name="41f5" id="41f5" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[ 0.       0.8307   0.83635  0.7126 ]                        <br>[ 1.       0.74295  0.73682  0.74189]                        <br>[ 2.       0.66492  0.69554  0.75996]                        <br>[ 3.       0.62392  0.67166  0.7625 ]                        <br>[ 4.       0.73479  0.80425  0.72861]                        <br>[ 5.       0.65423  0.68876  0.76318]                        <br>[ 6.       0.58608  0.64105  0.77783]                        <br>[ 7.       0.55738  0.62641  0.78721]                        <br>[ 8.       0.66163  0.74154  0.7501 ]                        <br>[ 9.       0.59444  0.64253  0.78106]                        <br>[ 10.        0.53      0.61772   0.79385]                    <br>[ 11.        0.49747   0.65968   0.77832]                    <br>[ 12.        0.59463   0.67915   0.77422]                    <br>[ 13.        0.55023   0.65815   0.78106]                    <br>[ 14.        0.48959   0.59035   0.80273]                    <br>[ 15.        0.4459    0.61823   0.79336]                    <br>[ 16.        0.55848   0.64115   0.78018]                    <br>[ 17.        0.50268   0.61795   0.79541]                    <br>[ 18.        0.45084   0.57577   0.80654]                    <br>[ 19.        0.40726   0.5708    0.80947]                    <br>[ 20.        0.51177   0.66771   0.78232]                    <br>[ 21.        0.46516   0.6116    0.79932]                    <br>[ 22.        0.40966   0.56865   0.81172]                    <br>[ 23.        0.3852    0.58161   0.80967]                    <br>[ 24.        0.48268   0.59944   0.79551]                    <br>[ 25.        0.43282   0.56429   0.81182]                    <br>[ 26.        0.37634   0.54724   0.81797]                    <br>[ 27.        0.34953   0.54169   0.82129]                    <br>[ 28.        0.46053   0.58128   0.80342]                    <br>[ 29.        0.4041    0.55185   0.82295]                    <br>[ 30.        0.3599    0.53953   0.82861]                    <br>[ 31.        0.32937   0.55605   0.82227]                    <br><br>CPU times: user 22min 52s, sys: 8min 58s, total: 31min 51s<br>Wall time: 16min 38s</em></pre><p name="0ed8" id="0ed8" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">ResNet block </strong>[<a href="https://youtu.be/H3g26EVADgY?t=1h53m18s" data-href="https://youtu.be/H3g26EVADgY?t=1h53m18s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">01:53:18</a>]</p><p name="a633" id="a633" class="graf graf--p graf-after--p"><code class="markup--code markup--p-code u-paddingRight0 u-marginRight0"><strong class="markup--strong markup--p-strong">return</strong> <strong class="markup--strong markup--p-strong">x + super().forward(x)</strong></code></p><p name="d1d3" id="d1d3" class="graf graf--p graf-after--p"><em class="markup--em markup--p-em">y = x + f(x)</em></p><p name="a354" id="a354" class="graf graf--p graf-after--p">Where <em class="markup--em markup--p-em">x</em> is prediction from the previous layer, <em class="markup--em markup--p-em">y</em> is prediction from the current layer.Shuffle around the formula and we get:formula shuffle</p><p name="a4eb" id="a4eb" class="graf graf--p graf-after--p"><em class="markup--em markup--p-em">f(x) = y − x</em></p><p name="36d4" id="36d4" class="graf graf--p graf-after--p">The difference <em class="markup--em markup--p-em">y − x </em>is <strong class="markup--strong markup--p-strong">residual</strong>. The residual is the error in terms of what we have calculated so far. What this is saying is that try to find a set of convolutional weights that attempts to fill in the amount we were off by. So in other words, we have an input, and we have a function which tries to predict the error (i.e. how much we are off by). Then we add a prediction of how much we were wrong by to the input, then add another prediction of how much we were wrong by that time, and repeat that layer after layer — zooming into the correct answer. This is based on a theory called <strong class="markup--strong markup--p-strong">boosting</strong>.</p><ul class="postList"><li name="0b95" id="0b95" class="graf graf--li graf-after--p">The full ResNet does two convolutions before it gets added back to the original input (we did just one here).</li><li name="5177" id="5177" class="graf graf--li graf-after--li">In every block <code class="markup--code markup--li-code">x = l3(l2(l(x)))</code>&nbsp;, one of the layers is not a <span class="markup--quote markup--li-quote is-other" name="anon_c28637c34a43" data-creator-ids="anon"><code class="markup--code markup--li-code">ResnetLayer</code></span> but a standard convolution with <code class="markup--code markup--li-code">stride=2</code> — this is called a “bottleneck layer”. ResNet does not convolutional layer but a different form of bottleneck block which we will cover in Part 2.</li></ul><figure name="20af" id="20af" class="graf graf--figure graf-after--li"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_0_0J8BFYOTK4Mupk94Izrw.png"></figure><h4 name="efe6" id="efe6" class="graf graf--h4 graf-after--figure">ResNet 2 [<a href="https://youtu.be/H3g26EVADgY?t=1h59m33s" data-href="https://youtu.be/H3g26EVADgY?t=1h59m33s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">01:59:33</a>]</h4><p name="3b17" id="3b17" class="graf graf--p graf-after--h4">Here, we increased the size of features and added dropout.</p><pre name="57e5" id="57e5" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">Resnet2</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, layers, c, p=0.5):<br>        super().__init__()<br>        self.conv1 = BnLayer(3, 16, stride=1, kernel_size=7)<br>        self.layers = nn.ModuleList([BnLayer(layers[i], layers[i+1])<br>            <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(len(layers) - 1)])<br>        self.layers2 = nn.ModuleList([ResnetLayer(layers[i+1], layers[i + 1], 1)<br>            <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(len(layers) - 1)])<br>        self.layers3 = nn.ModuleList([ResnetLayer(layers[i+1], layers[i + 1], 1)<br>            <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(len(layers) - 1)])<br>        self.out = nn.Linear(layers[-1], c)<br>        self.drop = nn.Dropout(p)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x):<br>        x = self.conv1(x)<br>        <strong class="markup--strong markup--pre-strong">for</strong> l,l2,l3 <strong class="markup--strong markup--pre-strong">in</strong> zip(self.layers, self.layers2, self.layers3):<br>            x = l3(l2(l(x)))<br>        x = F.adaptive_max_pool2d(x, 1)<br>        x = x.view(x.size(0), -1)<br>        x = self.drop(x)<br>        <strong class="markup--strong markup--pre-strong">return</strong> F.log_softmax(self.out(x), dim=-1)</pre><pre name="2c9e" id="2c9e" class="graf graf--pre graf-after--pre">learn = ConvLearner.from_model_data(Resnet2([<strong class="markup--strong markup--pre-strong">16, 32, 64, 128, 256</strong>], 10, 0.2), data)</pre><pre name="19ca" id="19ca" class="graf graf--pre graf-after--pre">wd=1e-6</pre><pre name="0392" id="0392" class="graf graf--pre graf-after--pre">%time learn.fit(1e-2, 2, wds=wd)<br>%time learn.fit(1e-2, 3, cycle_len=1, cycle_mult=2, wds=wd)<br>%time learn.fit(1e-2, 8, cycle_len=4, wds=wd)</pre><pre name="b438" id="b438" class="graf graf--pre graf-after--pre">log_preds,y = learn.TTA()<br>preds = np.mean(np.exp(log_preds),0)</pre><pre name="ccc8" id="ccc8" class="graf graf--pre graf-after--pre">metrics.log_loss(y,preds), accuracy(preds,y)<br><em class="markup--em markup--pre-em">(0.44507397166057938, 0.84909999999999997)</em></pre><p name="43f2" id="43f2" class="graf graf--p graf-after--pre">85% was a state-of-the-art back in 2012 or 2013 for CIFAR 10. Nowadays, it is up to 97% so there is a room for improvement but all based on these tecniques:</p><ul class="postList"><li name="1c58" id="1c58" class="graf graf--li graf-after--p">Better approaches to data augmentation</li><li name="3a64" id="3a64" class="graf graf--li graf-after--li">Better approaches to regularization</li><li name="6632" id="6632" class="graf graf--li graf-after--li">Some tweaks on ResNet</li></ul><p name="9561" id="9561" class="graf graf--p graf-after--li">Question [<a href="https://youtu.be/H3g26EVADgY?t=2h1m7s" data-href="https://youtu.be/H3g26EVADgY?t=2h1m7s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">02:01:07</a>]: Can we apply “training on the residual” approach for non-image problem? Yes! But it has been ignored everywhere else. In NLP, “transformer architecture” recently appeared and was shown to be the state of the art for translation, and it has a simple ResNet structure in it. This general approach is called “skip connection” (i.e. the idea of skipping over a layer) and appears a lot in computer vision, but nobody else much seems to be using it even through there is nothing computer vision specific about it. Good opportunity!</p><h3 name="6bed" id="6bed" class="graf graf--h3 graf-after--p"><a href="https://github.com/fastai/fastai/blob/master/courses/dl1/lesson7-CAM.ipynb" data-href="https://github.com/fastai/fastai/blob/master/courses/dl1/lesson7-CAM.ipynb" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">Dogs vs. Cats</a> [<a href="https://youtu.be/H3g26EVADgY?t=2h2m3s" data-href="https://youtu.be/H3g26EVADgY?t=2h2m3s" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">02:02:03</a>]</h3><p name="27ad" id="27ad" class="graf graf--p graf-after--h3">Going back dogs and cats. We will create resnet34 (if you are interested in what the trailing number means, <a href="https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py" data-href="https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">see here</a> — just different parameters).</p><pre name="89bb" id="89bb" class="graf graf--pre graf-after--p">PATH = "data/dogscats/"<br>sz = 224<br>arch = resnet34  # &lt;-- Name of the function <br>bs = 64</pre><pre name="27df" id="27df" class="graf graf--pre graf-after--pre">m = arch(pretrained=True) # Get a model w/ pre-trained weight loaded<br>m</pre><pre name="885d" id="885d" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">ResNet(<br>  (conv1): Conv2d (3, 64, </em><strong class="markup--strong markup--pre-strong"><em class="markup--em markup--pre-em">kernel_size=(7, 7)</em></strong><em class="markup--em markup--pre-em">, stride=(2, 2), padding=(3, 3), bias=False)<br>  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)<br>  (relu): ReLU(inplace)<br>  (maxpool): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=(1, 1))<br>  (</em><strong class="markup--strong markup--pre-strong"><em class="markup--em markup--pre-em">layer1</em></strong><em class="markup--em markup--pre-em">): Sequential(<br>    (0): BasicBlock(<br>      (conv1): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br>      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)<br>      (relu): ReLU(inplace)<br>      (conv2): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br>      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)<br>    )<br>    (1): BasicBlock(<br>      (conv1): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br>      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)<br>      (relu): ReLU(inplace)<br>      (conv2): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br>      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)<br>    )<br>    (2): BasicBlock(<br>      (conv1): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br>      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)<br>      (relu): ReLU(inplace)<br>      (conv2): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br>      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)<br>    )<br>  )<br>  (</em><strong class="markup--strong markup--pre-strong"><em class="markup--em markup--pre-em">layer2</em></strong><em class="markup--em markup--pre-em">): Sequential(<br>    (0): BasicBlock(<br>      (conv1): Conv2d (64, 128, kernel_size=(3, 3), </em><strong class="markup--strong markup--pre-strong"><em class="markup--em markup--pre-em">stride=(2, 2)</em></strong><em class="markup--em markup--pre-em">, padding=(1, 1), bias=False)<br>      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)<br>      (relu): ReLU(inplace)<br>      (conv2): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br>      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)<br>      (downsample): Sequential(<br>        (0): Conv2d (64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)<br>        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)<br>      )<br>    )<br>    (1): BasicBlock(<br>      (conv1): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br>      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)<br>      (relu): ReLU(inplace)<br>      (conv2): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br>      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)<br>    )<br>    (2): BasicBlock(<br>      (conv1): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br>      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)<br>      (relu): ReLU(inplace)<br>      (conv2): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br>      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)<br>    )<br>    (3): BasicBlock(<br>      (conv1): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br>      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)<br>      (relu): ReLU(inplace)<br>      (conv2): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br>      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)<br>    )<br>  )</em></pre><pre name="96b4" id="96b4" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">  ...</em></pre><pre name="c627" id="c627" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">  (avgpool): AvgPool2d(kernel_size=7, stride=7, padding=0, ceil_mode=False, count_include_pad=True)<br>  (fc): Linear(in_features=512, out_features=1000)<br>)</em></pre><p name="b9af" id="b9af" class="graf graf--p graf-after--pre">Our ResNet model had Relu → BatchNorm. TorchVision does BatchNorm →Relu. There are three different versions of ResNet floating around, and the best one is PreAct (<a href="https://arxiv.org/pdf/1603.05027.pdf" data-href="https://arxiv.org/pdf/1603.05027.pdf" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">https://arxiv.org/pdf/1603.05027.pdf</a>).</p><ul class="postList"><li name="3c9a" id="3c9a" class="graf graf--li graf-after--p">Currently, the final layer has a thousands features because ImageNet has 1000 features, so we need to get rid of it.</li><li name="011b" id="011b" class="graf graf--li graf-after--li">When you use fast.ai’s <code class="markup--code markup--li-code">ConvLearner</code>&nbsp;, it deletes the last two layers for you. fast.ai replaces <code class="markup--code markup--li-code">AvgPool2d</code> with Adaptive Average Pooling and Adaptive Max Pooling and concatenate the two together.</li><li name="4ccc" id="4ccc" class="graf graf--li graf-after--li">For this exercise, we will do a simple version.</li></ul><pre name="7af8" id="7af8" class="graf graf--pre graf-after--li">m = nn.Sequential(*children(m)[:-2], <br>                  nn.Conv2d(512, 2, 3, padding=1), <br>                  nn.AdaptiveAvgPool2d(1), Flatten(), <br>                  nn.LogSoftmax())</pre><ul class="postList"><li name="9f05" id="9f05" class="graf graf--li graf-after--pre">Remove the last two layers</li><li name="810e" id="810e" class="graf graf--li graf-after--li">Add a convolution which just has 2 outputs.</li><li name="af87" id="af87" class="graf graf--li graf-after--li">Do average pooling then softmax</li><li name="c02d" id="c02d" class="graf graf--li graf-after--li">There is no linear layer at the end. This is a different way of producing just two numbers — which allows us to do CAM!</li></ul><pre name="a6db" id="a6db" class="graf graf--pre graf-after--li">tfms = tfms_from_model(arch, sz, aug_tfms=transforms_side_on, max_zoom=1.1)<br>data = ImageClassifierData.from_paths(PATH, tfms=tfms, bs=bs)</pre><pre name="b24a" id="b24a" class="graf graf--pre graf-after--pre">learn = <strong class="markup--strong markup--pre-strong">ConvLearner.from_model_data</strong>(m, data)</pre><pre name="a856" id="a856" class="graf graf--pre graf-after--pre">learn.freeze_to(-4)</pre><pre name="a331" id="a331" class="graf graf--pre graf-after--pre">learn.fit(0.01, 1)<br>learn.fit(0.01, 1, cycle_len=1)</pre><ul class="postList"><li name="0e57" id="0e57" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">ConvLearner.from_model</code> is what we learned about earlier — allows us to create a Learner object with custom model.</li><li name="05e1" id="05e1" class="graf graf--li graf-after--li">Then freeze the layer except the ones we just added.</li></ul><h4 name="dc54" id="dc54" class="graf graf--h4 graf-after--li">Class Activation Maps (CAM) [<a href="https://youtu.be/H3g26EVADgY?t=2h8m55s" data-href="https://youtu.be/H3g26EVADgY?t=2h8m55s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">02:08:55</a>]</h4><p name="4f34" id="4f34" class="graf graf--p graf-after--h4">We pick a specific image, and use a technique called CAM where we take a model and we ask it which parts of the image turned out to be important.</p><figure name="281d" id="281d" class="graf graf--figure graf--layoutOutsetRow is-partialWidth graf-after--p" style="width: 50.866%;" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_BrMBBupbny4CFsqBVjgcfA.png"></figure><figure name="d92f" id="d92f" class="graf graf--figure graf--layoutOutsetRowContinue is-partialWidth graf-after--figure" style="width: 49.134%;" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_zayLvr0jvnUXe-G27odldQ.png"></figure><p name="9782" id="9782" class="graf graf--p graf-after--figure">How did it do this? Let’s work backwards. The way it did it was by producing this matrix:</p><figure name="182a" id="182a" class="graf graf--figure graf--layoutOutsetCenter graf-after--p" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_DPIlEiNjJOeAbiIQUubNLg.png"></figure><p name="8f4f" id="8f4f" class="graf graf--p graf-after--figure">Big numbers correspond to the cat. So what is this matrix? This matrix simply equals to the value of feature matrix <code class="markup--code markup--p-code">feat</code> times <code class="markup--code markup--p-code">py</code> vector:</p><pre name="0f8a" id="0f8a" class="graf graf--pre graf-after--p">f2=np.dot(np.rollaxis(<strong class="markup--strong markup--pre-strong">feat</strong>,0,3), <strong class="markup--strong markup--pre-strong">py</strong>)<br>f2-=f2.min()<br>f2/=f2.max()<br>f2</pre><p name="8d04" id="8d04" class="graf graf--p graf-after--pre"><code class="markup--code markup--p-code">py</code> vector is the predictions that says “I am 100% confident it’s a cat.” <code class="markup--code markup--p-code">feat</code> is the values (2×7×7) coming out of the final convolutional layer (the <code class="markup--code markup--p-code">Conv2d</code> layer we added). If we multiply <code class="markup--code markup--p-code">feat</code> by <code class="markup--code markup--p-code">py</code>&nbsp;, we get all of the first channel and none of the second channel. Therefore, it is going to return the value of the last convolutional layers for the section which lines up with being a cat. In other words, if we multiply <code class="markup--code markup--p-code">feat</code> by <code class="markup--code markup--p-code">[0, 1]</code>&nbsp;, it will line up with being a dog.</p><pre name="388c" id="388c" class="graf graf--pre graf-after--p">sf = SaveFeatures(m[-4])<br>py = m(Variable(x.cuda()))<br>sf.remove()<br><br>py = np.exp(to_np(py)[0]); py</pre><pre name="0669" id="0669" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">array([ 1.,  0.], dtype=float32)</em></pre><pre name="2e1e" id="2e1e" class="graf graf--pre graf-after--pre">feat = np.maximum(0, sf.features[0])<br>feat.shape</pre><p name="89c7" id="89c7" class="graf graf--p graf-after--pre">Put it in another way, in the model, the only thing that happened after the convolutional layer was an average pooling layer. The average pooling layer took took the 7 by 7 grid and averaged out how much each part is “cat-like”. We then took the “cattyness” matrix, resized it to be the same size as the original cat image, and overlaid it on top, then you get the heat map.</p><p name="41b3" id="41b3" class="graf graf--p graf-after--p">The way you can use this technique at home is</p><ol class="postList"><li name="c79e" id="c79e" class="graf graf--li graf-after--p">when you have a large image, you can calculate this matrix on a quick small little convolutional net</li><li name="f177" id="f177" class="graf graf--li graf-after--li">zoom into the area that has the highest value</li><li name="deef" id="deef" class="graf graf--li graf-after--li">re-run it just on that part</li></ol><p name="7737" id="7737" class="graf graf--p graf-after--li">We skipped this over quickly as we ran out of time, but we will learn more about these kind of approaches in Part 2.</p><p name="3ce1" id="3ce1" class="graf graf--p graf--startsWithDoubleQuote graf-after--p">“Hook” is the mechanism that lets us ask the model to return the matrix. <code class="markup--code markup--p-code">register_forward_hook</code> asks PyTorch that every time it calculates a layer it runs the function given — sort of like a callback that happens every time it calculates a layer. In the following case, it saves the value of the particular layer we were interested in:</p><pre name="75b8" id="75b8" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">SaveFeatures</strong>():<br>    features=<strong class="markup--strong markup--pre-strong">None</strong><br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, m): <br>        self.hook = m.register_forward_hook(self.hook_fn)<br>    <strong class="markup--strong markup--pre-strong">def</strong> hook_fn(self, module, input, output): <br>        self.features = to_np(output)<br>    <strong class="markup--strong markup--pre-strong">def</strong> remove(self): self.hook.remove()</pre><h4 name="b432" id="b432" class="graf graf--h4 graf-after--pre">Questions to Jeremy [<a href="https://youtu.be/H3g26EVADgY?t=2h14m27s" data-href="https://youtu.be/H3g26EVADgY?t=2h14m27s" class="markup--anchor markup--h4-anchor" rel="noopener nofollow" target="_blank">02:14:27</a>]: “Your journey into Deep Learning” and “How to keep up with important research for practitioners”</h4><p name="cb8a" id="cb8a" class="graf graf--p graf--startsWithDoubleQuote graf-after--h4">“If you intend to come to Part 2, you are expected to master all the techniques er have learned in Part 1”. Here are something you can do:</p><ol class="postList"><li name="90cf" id="90cf" class="graf graf--li graf-after--p">Watch each of the video at least 3 times.</li><li name="171b" id="171b" class="graf graf--li graf-after--li">Make sure you can re-create the notebooks without watching the videos — maybe do so with different datasets to make it more interesting.</li><li name="0834" id="0834" class="graf graf--li graf-after--li">Keep an eye on the forum for recent papers, recent advances.</li><li name="961d" id="961d" class="graf graf--li graf-after--li graf--trailing">Be tenacious and keep working at it!</li></ol><hr class="section-divider"><p name="3c17" id="3c17" class="graf graf--p graf--leading graf--trailing">Lessons: <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" class="markup--anchor markup--p-anchor" target="_blank">1</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" class="markup--anchor markup--p-anchor" target="_blank">2</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" class="markup--anchor markup--p-anchor" target="_blank">3</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" class="markup--anchor markup--p-anchor" target="_blank">4</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" class="markup--anchor markup--p-anchor" target="_blank">5</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" class="markup--anchor markup--p-anchor" target="_blank">6</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" class="markup--anchor markup--p-anchor" target="_blank"><strong class="markup--strong markup--p-strong">7</strong></a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" class="markup--anchor markup--p-anchor" target="_blank">8</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" class="markup--anchor markup--p-anchor" target="_blank">9</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" class="markup--anchor markup--p-anchor" target="_blank">10</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" class="markup--anchor markup--p-anchor" target="_blank">11</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" class="markup--anchor markup--p-anchor" target="_blank">12</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" class="markup--anchor markup--p-anchor" target="_blank">13</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" class="markup--anchor markup--p-anchor" target="_blank">14</a></p></body></html>