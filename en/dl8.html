
<!-- saved from url=(0063)file:///C:/Users/asus/Desktop/fastai-ml-dl-notes-zh/en/dl8.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></head><body><hr class="section-divider"><h1 name="36c5" id="36c5" class="graf graf--h3 graf--leading graf--title">Deep Learning 2: Part 2 Lesson&nbsp;8</h1><p name="2560" id="2560" class="graf graf--p graf-after--h3"><em class="markup--em markup--p-em">My personal notes from </em><a href="http://www.fast.ai/" data-href="http://www.fast.ai/" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener noopener noopener nofollow noopener" target="_blank"><em class="markup--em markup--p-em">fast.ai course</em></a><em class="markup--em markup--p-em">. These notes will continue to be updated and improved as I continue to review the course to “really” understand it. Much appreciation to </em><a href="https://twitter.com/jeremyphoward" data-href="https://twitter.com/jeremyphoward" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener noopener noopener nofollow noopener" target="_blank"><em class="markup--em markup--p-em">Jeremy</em></a><em class="markup--em markup--p-em"> and </em><a href="https://twitter.com/math_rachel" data-href="https://twitter.com/math_rachel" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener noopener noopener nofollow noopener" target="_blank"><em class="markup--em markup--p-em">Rachel</em></a><em class="markup--em markup--p-em"> who gave me this opportunity to learn.</em></p><p name="fa73" id="fa73" class="graf graf--p graf-after--p graf--trailing">Lessons: <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" class="markup--anchor markup--p-anchor" target="_blank">1</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" class="markup--anchor markup--p-anchor" target="_blank">2</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" class="markup--anchor markup--p-anchor" target="_blank">3</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" class="markup--anchor markup--p-anchor" target="_blank">4</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" class="markup--anchor markup--p-anchor" target="_blank">5</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" class="markup--anchor markup--p-anchor" target="_blank">6</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" class="markup--anchor markup--p-anchor" target="_blank">7</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" class="markup--anchor markup--p-anchor" target="_blank"><strong class="markup--strong markup--p-strong">8</strong></a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" class="markup--anchor markup--p-anchor" target="_blank">9</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" class="markup--anchor markup--p-anchor" target="_blank">10</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" class="markup--anchor markup--p-anchor" target="_blank">11</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" class="markup--anchor markup--p-anchor" target="_blank">12</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" class="markup--anchor markup--p-anchor" target="_blank">13</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" class="markup--anchor markup--p-anchor" target="_blank">14</a></p><hr class="section-divider"><h3 name="32b3" id="32b3" class="graf graf--h3 graf--leading">Object Detection</h3><p name="bea2" id="bea2" class="graf graf--p graf-after--h3"><a href="http://forums.fast.ai/t/part-2-lesson-8-in-class/13556/1" data-href="http://forums.fast.ai/t/part-2-lesson-8-in-class/13556/1" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank"><strong class="markup--strong markup--p-strong">Forum</strong></a><strong class="markup--strong markup--p-strong"> / </strong><a href="https://youtu.be/Z0ssNAbe81M" data-href="https://youtu.be/Z0ssNAbe81M" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong">Video</strong></a><strong class="markup--strong markup--p-strong"> / </strong><a href="https://github.com/fastai/fastai/blob/master/courses/dl2/pascal.ipynb" data-href="https://github.com/fastai/fastai/blob/master/courses/dl2/pascal.ipynb" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank"><strong class="markup--strong markup--p-strong">Notebook</strong></a><strong class="markup--strong markup--p-strong"> / </strong><a href="https://github.com/fastai/fastai/blob/master/courses/dl2/ppt/lesson8.pptx" data-href="https://github.com/fastai/fastai/blob/master/courses/dl2/ppt/lesson8.pptx" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank"><strong class="markup--strong markup--p-strong">Slides</strong></a></p><h4 name="4a90" id="4a90" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">What we covered in Part 1&nbsp;[</strong><a href="https://youtu.be/Z0ssNAbe81M?t=2m" data-href="https://youtu.be/Z0ssNAbe81M?t=2m" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--h4-strong">02:00</strong></a><strong class="markup--strong markup--h4-strong">]</strong></h4><figure name="4a4e" id="4a4e" class="graf graf--figure graf-after--h4"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_EDzZucEfAL2aeRCZKkdkag.png"></figure><p name="fa3b" id="fa3b" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Differentiable layer [</strong><a href="https://youtu.be/Z0ssNAbe81M?t=2m11s" data-href="https://youtu.be/Z0ssNAbe81M?t=2m11s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong">02:11</strong></a><strong class="markup--strong markup--p-strong">]</strong></p><figure name="7a09" id="7a09" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_SFx5Gwk9KRuOZRgiZTuvbg.png"></figure><p name="d0f8" id="d0f8" class="graf graf--p graf-after--figure">Yann LeCun has been promoting the idea that we do not call this “deep learning” but “differentiable programming”. All we did in part 1 was really about setting up a differentiable function and a loss function that describes how good the parameters are and then pressing go and it makes it work. If you can configure a loss function that scores how good something is doing your task sand you have a reasonably flexible neural network architecture, you are done.</p><blockquote name="3ef7" id="3ef7" class="graf graf--blockquote graf-after--p">Yeah, Differentiable Programming is little more than a rebranding of the modern collection Deep Learning techniques, the same way Deep Learning was a rebranding of the modern incarnations of neural nets with more than two layers.</blockquote><blockquote name="51f1" id="51f1" class="graf graf--blockquote graf-after--blockquote">The important point is that people are now building a new kind of software by assembling networks of parameterized functional blocks and by training them from examples using some form of gradient-based optimization….It’s really very much like a regular program, except it’s parameterized, automatically differentiated, and trainable/optimizable.</blockquote><blockquote name="7096" id="7096" class="graf graf--blockquote graf-after--blockquote">- <a href="https://www.facebook.com/yann.lecun/posts/10155003011462143" data-href="https://www.facebook.com/yann.lecun/posts/10155003011462143" class="markup--anchor markup--blockquote-anchor" rel="noopener nofollow" target="_blank">Yann LeCun, Director of FAIR</a></blockquote><p name="fe87" id="fe87" class="graf graf--p graf-after--blockquote"><strong class="markup--strong markup--p-strong">2. Transfer Learning [</strong><a href="https://youtu.be/Z0ssNAbe81M?t=3m23s" data-href="https://youtu.be/Z0ssNAbe81M?t=3m23s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong">03:23</strong></a><strong class="markup--strong markup--p-strong">]</strong></p><figure name="03cb" id="03cb" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_sDycvgDmfivum0HQhHpOuA.png"></figure><p name="e77f" id="e77f" class="graf graf--p graf-after--figure">Transfer learning is the most important single thing to be able to do to use deep learning effectively. You almost never would want to or need to start with random weights unless nobody had ever trained a model on a vaguely similar set of data with an even remotely connected kind of problem to solve as what you are doing — which almost never happens. Fastai library focuses on transfer learning which makes it different from other libraries. The basic idea of transfer learning is:</p><ul class="postList"><li name="b481" id="b481" class="graf graf--li graf-after--p">Given a network that does thing A, remove the last layer.</li><li name="31ad" id="31ad" class="graf graf--li graf-after--li">Replace it with a few random layers at the end</li><li name="2003" id="2003" class="graf graf--li graf-after--li">Fine-tune those layers to do thing B while taking advantage of the features that the original network learned</li><li name="7d0f" id="7d0f" class="graf graf--li graf-after--li">Then optionally fine tune the whole thing end-to-end and you now have something which probably uses orders of magnitude less data, is more accurate, and trains a lot faster.</li></ul><p name="0620" id="0620" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">3. Architecture design [</strong><a href="https://youtu.be/Z0ssNAbe81M?t=5m17s" data-href="https://youtu.be/Z0ssNAbe81M?t=5m17s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong">05:17</strong></a><strong class="markup--strong markup--p-strong">]</strong></p><figure name="f8e8" id="f8e8" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_Dn8YbBY47oaDWG9KwEQkvw.png"></figure><p name="3720" id="3720" class="graf graf--p graf-after--figure">There is a pretty small range of architectures that generally works pretty well quite a lot of the time. We have been focusing on using CNN’s for generally fixed size ordered data, RNN’s for sequences that have some kind of state. We also fiddled around a tiny bit with activation functions — softmax if you have a single categorical outcome, or sigmoid if you have multiple outcomes. Some of the architecture design we will be studying in part 2 gets more interesting. Particularly this first session about object detection. But on the whole, we probably spend less time talking about architecture design than most courses or papers because it is generally not the hard bit.</p><p name="00aa" id="00aa" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">4. Handling over-fitting [</strong><a href="https://youtu.be/Z0ssNAbe81M?t=6m26s" data-href="https://youtu.be/Z0ssNAbe81M?t=6m26s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong">06:26</strong></a><strong class="markup--strong markup--p-strong">]</strong></p><figure name="f84d" id="f84d" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_Fg2M4xw2F2f1jZNOCg5Cpg.png"></figure><p name="3bca" id="3bca" class="graf graf--p graf-after--figure">The way Jeremy likes to build a model:</p><ul class="postList"><li name="320b" id="320b" class="graf graf--li graf-after--p">Create something that is definitely terribly over-parameterized which will massively overfit for sure, train it and make sure it does overfit. At that point, you’ve got a model that is capable of reflecting the training set. Then it is as simple as doing these things to reduce that overfitting.</li></ul><p name="36e2" id="36e2" class="graf graf--p graf-after--li">If you don’t start with something that is overfitting, you are lost. So you start with something overfitting and to make it overfit less you can:</p><ul class="postList"><li name="bc22" id="bc22" class="graf graf--li graf-after--p">add more data</li><li name="e82f" id="e82f" class="graf graf--li graf-after--li">add more data augmentation</li><li name="78e5" id="78e5" class="graf graf--li graf-after--li">do things like more batch norm layers, dense nets, or various things that can handle less data.</li><li name="c01b" id="c01b" class="graf graf--li graf-after--li">add regularization like weight decay and dropout</li><li name="2de0" id="2de0" class="graf graf--li graf-after--li">finally (this is often the thing people do first but this should be the thing you do last) reduce the complexity of your architecture. have less layers or less activations.</li></ul><p name="5244" id="5244" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">5. Embeddings [</strong><a href="https://youtu.be/Z0ssNAbe81M?t=7m46s" data-href="https://youtu.be/Z0ssNAbe81M?t=7m46s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong">07:46</strong></a><strong class="markup--strong markup--p-strong">]</strong></p><figure name="5e3e" id="5e3e" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_TGNCaF5RGYO8iylV43oSSg.png"></figure><p name="c28f" id="c28f" class="graf graf--p graf-after--figure">We have talked quite a bit about embeddings — both for NLP and the general idea of any kind of categorical data as being something you can now model with neural nets. Just earlier this year, there were almost no examples about using tabular data in deep learning, but it is becoming more and more popular approach to use neural nets for time series and tabular data analysis.</p><h4 name="b0fb" id="b0fb" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Part 1 to Part 2&nbsp;[</strong><a href="https://youtu.be/Z0ssNAbe81M?t=8m54s" data-href="https://youtu.be/Z0ssNAbe81M?t=8m54s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--h4-strong">08:54</strong></a><strong class="markup--strong markup--h4-strong">]</strong></h4><figure name="f5f6" id="f5f6" class="graf graf--figure graf-after--h4"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_ZlspbXQEsEpBIgNqaq3KUg.png"></figure><p name="2fbe" id="2fbe" class="graf graf--p graf-after--figure">Part 1 really was all about introducing best practices in deep learning. We saw techniques which were mature enough that they definitely work reasonably reliably for practical real-world problems. Jeremy had researched and tuned enough over quite a long period of time, came up with a sequences of steps, architectures, etc, and put them into the fastai library in a way we could do that quickly and easily.</p><p name="a24e" id="a24e" class="graf graf--p graf-after--p">Part 2 is cutting edge deep learning for coders, and what that means is Jeremy often does not know the exact best parameters, architecture details, and so forth to solve a particular problem. We do not necessarily know if it’s going to solve a problem well enough to be practically useful. It almost certainly won’t be integrated well enough into fastai or any other library that you can just press a few buttons and it will start working. Jeremy will not going to teach it unless he is very confident that it either is now or will be soon very practically useful technique. But it will require a lot of tweaking often and experimenting to get it to work on your particular problem because we don’t know the details well enough to know how to make it work for every data set or every example.</p><p name="2ad6" id="2ad6" class="graf graf--p graf-after--p">This means rather than Fastai and PyTorch being obscure black boxes which you just know these recipes for, you are going to learn the details of them well enough that you can customize them exactly the way you want, you can debug them, you can read the source code of them to see what’s happening. If you are not confident of object-oriented Python, then that is something you want to focus on studying during this course as we will not cover it in the class. But Jeremy will introduce some tools that he thinks are particularly helpful like the Python debugger, how to use your editor to jump through the code. In general, there will be a lot more detailed and specific code walkthroughs, coding technique discussions, as well as more detailed walkthroughs of papers.</p><p name="437a" id="437a" class="graf graf--p graf-after--p">Be aware of sample codes [<a href="https://youtu.be/Z0ssNAbe81M?t=13m20s" data-href="https://youtu.be/Z0ssNAbe81M?t=13m20s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">13:20</a>]! The code academics have put up to go along with papers or example code somebody else has written on github, Jeremy nearly always find there is some massive critical flaw, so be careful of taking code from online resources and be ready to do some debugging.</p><p name="99e6" id="99e6" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">How to use notebooks [</strong><a href="https://youtu.be/Z0ssNAbe81M?t=14m17s" data-href="https://youtu.be/Z0ssNAbe81M?t=14m17s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong">14:17</strong></a><strong class="markup--strong markup--p-strong">]</strong></p><figure name="f3fb" id="f3fb" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_Ank7Dub7DwvSlozpLr1Lqg.png"></figure><figure name="2a16" id="2a16" class="graf graf--figure graf-after--figure"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_GyRVknri5gUktxgDmnxo4A.png"><figcaption class="imageCaption">Building your own box&nbsp;[<a href="https://youtu.be/Z0ssNAbe81M?t=16m50s" data-href="https://youtu.be/Z0ssNAbe81M?t=16m50s" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">16:50</a>]</figcaption></figure><figure name="1d9f" id="1d9f" class="graf graf--figure graf-after--figure"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1__r-uV41M5zUGTdV26N9bsg.png"><figcaption class="imageCaption">Reading papers&nbsp;[<a href="https://youtu.be/Z0ssNAbe81M?t=21m37s" data-href="https://youtu.be/Z0ssNAbe81M?t=21m37s" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">21:37</a>]</figcaption></figure><p name="15bd" id="15bd" class="graf graf--p graf-after--figure">Each week, we will be implementing a paper or two. On the left is an extract from the paper that implements adam (you have also seen adam as a single excel formula on a spreadsheet). In academic papers, people love to use Greek letters. They also hate to refactor, so you will often see a page long formula where when you look at it carefully you’ll realize the same sub equation appears 8 times. Academic papers are a bit weird, but in the end, it’s the way that the research community communicates their findings so we need to learn to read them. A great thing to do is to take a paper, put in the effort to understand it, then write a blog where you explain it in code and normal English. Lots of people who do that end up getting quite a following, end up getting some pretty great job offers and so forth because it is such a useful skill to be able to show that you can understand these papers, implement them in code, and explain them in English. It is very hard to read or understand something you cannot vocalize. So learn Greek letters!</p><figure name="15e3" id="15e3" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_LBOcbbeBFypTgQ2AOit1EQ.png"><figcaption class="imageCaption">More opportunities [<a href="https://youtu.be/Z0ssNAbe81M?t=25m29s" data-href="https://youtu.be/Z0ssNAbe81M?t=25m29s" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">25:29</a>]</figcaption></figure><figure name="a1a1" id="a1a1" class="graf graf--figure graf-after--figure"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_cNNnbJwImpFbqSKdA5_RIQ.png"><figcaption class="imageCaption">Part 2’s Topics&nbsp;[<a href="https://youtu.be/Z0ssNAbe81M?t=30m12s" data-href="https://youtu.be/Z0ssNAbe81M?t=30m12s" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">30:12</a>]</figcaption></figure><p name="a337" id="a337" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Generative Models</strong></p><p name="3e35" id="3e35" class="graf graf--p graf-after--p">In part 1, the output of our neural network was generally a number or a category, where else, the outputs of a lot of the things in part 2 are going to be a whole a lot of things like:</p><ul class="postList"><li name="f99e" id="f99e" class="graf graf--li graf-after--p">the top left and bottom right location of every object in an image along with what that object is</li><li name="6c6e" id="6c6e" class="graf graf--li graf-after--li">a complete picture with the class of every single pixel in that picture</li><li name="0a8e" id="0a8e" class="graf graf--li graf-after--li">an enhanced super resolution version of the input image</li><li name="e3df" id="e3df" class="graf graf--li graf-after--li">the entire original input paragraph translated into French</li></ul><p name="d983" id="d983" class="graf graf--p graf-after--li">Vast majority of the data we will be looking at will be either text or image data.</p><p name="8b4b" id="8b4b" class="graf graf--p graf-after--p graf--trailing">We will be looking at some larger datasets both in terms of the number of objects in the dataset and the size of each of those objects. For those of you that are working with limited computational resources, please don’t let that put you off. Feel free to replace it with something smaller and simpler. Jeremy actually wrote large amount of the course with no internet (in Point Leo) on a surface book 15 inch. Pretty much all of this course works well on Windows on a laptop. You can always use smaller batch sizes, cut-down version of the dataset. But if you have the resources, you will get better results with bigger datasets when they are available.</p><hr class="section-divider"><h4 name="2a6a" id="2a6a" class="graf graf--h4 graf--leading">Object Detection [<a href="https://youtu.be/Z0ssNAbe81M?t=35m32s" data-href="https://youtu.be/Z0ssNAbe81M?t=35m32s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">35:32</a>]</h4><figure name="0690" id="0690" class="graf graf--figure graf-after--h4"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_EGZrr-rVX29Ot1j20i116w.png"></figure><p name="6ecc" id="6ecc" class="graf graf--p graf-after--figure">Two main differences from what we are used to:</p><p name="98e0" id="98e0" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">1.We have multiple things that we are classifying.</strong></p><p name="92b0" id="92b0" class="graf graf--p graf-after--p">This if not unheard of — we did that in the planet satellite data in part 1.</p><p name="cc9f" id="cc9f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">2. Bounding boxes around what we are classifying.</strong></p><p name="85dc" id="85dc" class="graf graf--p graf-after--p">A bounding box has a very specific definition which is it’s a rectangle and the rectangle has the object entirely fitting within it but it is no bigger than it has to be.</p><p name="6aef" id="6aef" class="graf graf--p graf-after--p">Our job will be to take data that has been labeled in this way and on data that is unlabeled to generate classes of the objects and each one of those their bounding boxes. One thing to note is that labeling this kind of data is generally more expensive [<a href="https://youtu.be/Z0ssNAbe81M?t=37m09s" data-href="https://youtu.be/Z0ssNAbe81M?t=37m09s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">37:09</a>]. For object detection datasets, annotators are given a list of object classes and asked to find every single one of them of any type in a picture along with where they are. In this case why isn’t there a tree or jump labeled? That is because for this particular dataset, they were not one of the classes that annotators were asked to find and therefore not part of this particular problem.</p><h4 name="f8a4" id="f8a4" class="graf graf--h4 graf-after--p">Stages [<a href="https://youtu.be/Z0ssNAbe81M?t=38m33s" data-href="https://youtu.be/Z0ssNAbe81M?t=38m33s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">38:33</a>]:</h4><ol class="postList"><li name="9327" id="9327" class="graf graf--li graf-after--h4">Classify the largest object in each image.</li><li name="2f48" id="2f48" class="graf graf--li graf-after--li">Find the location of the largest object at each image.</li><li name="7764" id="7764" class="graf graf--li graf-after--li">Finally we will try and do both at the same time (i.e. label what it is and where it is for the largest object in the picture).</li></ol><figure name="0bf0" id="0bf0" class="graf graf--figure graf-after--li"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_RAxYMkvF3zHFe_cst52STA.png"></figure><h4 name="0c93" id="0c93" class="graf graf--h4 graf-after--figure"><a href="https://github.com/fastai/fastai/blob/master/courses/dl2/pascal.ipynb" data-href="https://github.com/fastai/fastai/blob/master/courses/dl2/pascal.ipynb" class="markup--anchor markup--h4-anchor" rel="noopener nofollow" target="_blank">Pascal Notebook</a>&nbsp;[<a href="https://youtu.be/Z0ssNAbe81M?t=40m06s" data-href="https://youtu.be/Z0ssNAbe81M?t=40m06s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">40:06</a>]</h4><pre name="4f10" id="4f10" class="graf graf--pre graf-after--h4">%matplotlib inline<br>%reload_ext autoreload<br>%autoreload 2</pre><pre name="592e" id="592e" class="graf graf--pre graf-after--pre">from fastai.conv_learner import *<br>from fastai.dataset import *</pre><pre name="986a" id="986a" class="graf graf--pre graf-after--pre">from pathlib import Path<br>import json<br>from PIL import ImageDraw, ImageFont<br>from matplotlib import patches, patheffects<br># torch.cuda.set_device(1)   </pre><p name="943d" id="943d" class="graf graf--p graf-after--pre">You may find a line <code class="markup--code markup--p-code">torch.cuda.set_device(1)</code> left behind which will give you an error if you only have one GPU. This is how you select a GPU when you have multiple, so just set it to zero or take out the line entirely.</p><p name="9d4f" id="9d4f" class="graf graf--p graf-after--p">There is a number of standard object detection datasets just like ImageNet being a standard object classification dataset [<a href="https://youtu.be/Z0ssNAbe81M?t=41m12s" data-href="https://youtu.be/Z0ssNAbe81M?t=41m12s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">41:12</a>]. The classic ImageNet equivalent is Pascal VOC.</p><h4 name="04a3" id="04a3" class="graf graf--h4 graf-after--p">Pascal VOC</h4><p name="d1a3" id="d1a3" class="graf graf--p graf-after--h4">We will be looking at the <a href="http://host.robots.ox.ac.uk/pascal/VOC/" data-href="http://host.robots.ox.ac.uk/pascal/VOC/" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">Pascal VOC</a> dataset. It’s quite slow, so you may prefer to download from <a href="https://pjreddie.com/projects/pascal-voc-dataset-mirror/" data-href="https://pjreddie.com/projects/pascal-voc-dataset-mirror/" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">this mirror</a>. There are two different competition/research datasets, from 2007 and 2012. We’ll be using the 2007 version. You can use the larger 2012 for better results, or even combine them [<a href="https://youtu.be/Z0ssNAbe81M?t=42m25s" data-href="https://youtu.be/Z0ssNAbe81M?t=42m25s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">42:25</a>](but be careful to avoid data leakage between the validation sets if you do this).</p><p name="8ce7" id="8ce7" class="graf graf--p graf-after--p">Unlike previous lessons, we are using the python 3 standard library <code class="markup--code markup--p-code">pathlib</code> for our paths and file access. Note that it returns an OS-specific class (on Linux, <code class="markup--code markup--p-code">PosixPath</code>) so your output may look a little different [<a href="https://youtu.be/Z0ssNAbe81M?t=44m50s" data-href="https://youtu.be/Z0ssNAbe81M?t=44m50s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">44:50</a>]. Most libraries that take paths as input can take a pathlib object - although some (like <code class="markup--code markup--p-code">cv2</code>) can't, in which case you can use <code class="markup--code markup--p-code">str()</code> to convert it to a string.</p><p name="7a75" id="7a75" class="graf graf--p graf-after--p"><a href="http://pbpython.com/pathlib-intro.html" data-href="http://pbpython.com/pathlib-intro.html" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Pathlib Cheat Sheet</a></p><pre name="2814" id="2814" class="graf graf--pre graf-after--p">PATH = Path('data/pascal')<br>list(PATH.iterdir())</pre><pre name="c5fc" id="c5fc" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[PosixPath('data/pascal/PASCAL_VOC.zip'),<br> PosixPath('data/pascal/VOCdevkit'),<br> PosixPath('data/pascal/VOCtrainval_06-Nov-2007.tar'),<br> PosixPath('data/pascal/pascal_train2012.json'),<br> PosixPath('data/pascal/pascal_val2012.json'),<br> PosixPath('data/pascal/pascal_val2007.json'),<br> PosixPath('data/pascal/pascal_train2007.json'),<br> PosixPath('data/pascal/pascal_test2007.json')]</em></pre><p name="dfa0" id="dfa0" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">A little bit about generator [</strong><a href="https://youtu.be/Z0ssNAbe81M?t=43m23s" data-href="https://youtu.be/Z0ssNAbe81M?t=43m23s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong">43:23</strong></a><strong class="markup--strong markup--p-strong">]:</strong></p><p name="4130" id="4130" class="graf graf--p graf-after--p">Generator is something in Python 3 which you can iterate over.</p><ul class="postList"><li name="8723" id="8723" class="graf graf--li graf-after--p"><code class="markup--code markup--li-code">for i in PATH.iterdir(): print(i)</code></li><li name="6282" id="6282" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">[i for i in PATH.iterdir()]</code> (list comprehension)</li><li name="e431" id="e431" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">list(PATH.iterdir())</code> (turn a generator into a list)</li></ul><p name="65b6" id="65b6" class="graf graf--p graf-after--li">The reason that things generally return generators is that if the directory had 10 million items in, you don’t necessarily want 10 million long list. Generator lets you do things “lazily”.</p><h4 name="a9bf" id="a9bf" class="graf graf--h4 graf-after--p">Loading annotations</h4><p name="8ffc" id="8ffc" class="graf graf--p graf-after--h4">As well as the images, there are also <em class="markup--em markup--p-em">annotations</em> — <em class="markup--em markup--p-em">bounding boxes</em> showing where each object is. These were hand labeled. The original version were in XML [<a href="https://youtu.be/Z0ssNAbe81M?t=47m59s" data-href="https://youtu.be/Z0ssNAbe81M?t=47m59s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">47:59</a>], which is a little hard to work with nowadays, so we uses the more recent JSON version which you can download from <a href="https://storage.googleapis.com/coco-dataset/external/PASCAL_VOC.zip" data-href="https://storage.googleapis.com/coco-dataset/external/PASCAL_VOC.zip" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">this link</a>.</p><p name="853c" id="853c" class="graf graf--p graf-after--p">You can see here how <code class="markup--code markup--p-code">pathlib</code> includes the ability to open files (amongst many other capabilities).</p><pre name="4a9b" id="4a9b" class="graf graf--pre graf-after--p">trn_j = json.load((PATH/'pascal_train2007.json').open())<br>trn_j.keys()</pre><pre name="76f2" id="76f2" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">dict_keys(['images', 'type', 'annotations', 'categories'])</em></pre><p name="cdca" id="cdca" class="graf graf--p graf-after--pre">Here <code class="markup--code markup--p-code">/</code> is not divided by but it is path slash [<a href="https://youtu.be/Z0ssNAbe81M?t=45m55s" data-href="https://youtu.be/Z0ssNAbe81M?t=45m55s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">45:55</a>]. <code class="markup--code markup--p-code">PATH/</code> gets you children in that path. <code class="markup--code markup--p-code">PATH/’pascal_train2007.json’</code> returns a <code class="markup--code markup--p-code">pathlib</code> object which has an <code class="markup--code markup--p-code">open</code> method. This JSON file contains not the images but the bounding boxes and the classes of the objects.</p><pre name="b4d5" id="b4d5" class="graf graf--pre graf-after--p">IMAGES,ANNOTATIONS,CATEGORIES = ['images', 'annotations', <br>                                 'categories'] </pre><pre name="07ff" id="07ff" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">trn_j[IMAGES]</strong>[:5]</pre><pre name="db30" id="db30" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[{'file_name': '000012.jpg', 'height': 333, 'id': 12, 'width': 500},  {'file_name': '000017.jpg', 'height': 364, 'id': 17, 'width': 480},  {'file_name': '000023.jpg', 'height': 500, 'id': 23, 'width': 334},  {'file_name': '000026.jpg', 'height': 333, 'id': 26, 'width': 500},  {'file_name': '000032.jpg', 'height': 281, 'id': 32, 'width': 500}]</em></pre><h4 name="3361" id="3361" class="graf graf--h4 graf-after--pre">Annotations [<a href="https://youtu.be/Z0ssNAbe81M?t=49m16s" data-href="https://youtu.be/Z0ssNAbe81M?t=49m16s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">49:16</a>]</h4><ul class="postList"><li name="f163" id="f163" class="graf graf--li graf-after--h4"><code class="markup--code markup--li-code">bbox</code>&nbsp;: column, row (of top left), height, width</li><li name="ca9a" id="ca9a" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">image_id</code>&nbsp;: you’d have join this up with <code class="markup--code markup--li-code">trn_j[IMAGES]</code> (above) to find <code class="markup--code markup--li-code">file_name</code> etc.</li><li name="b418" id="b418" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">category_id</code>&nbsp;: see <code class="markup--code markup--li-code">trn_j[CATEGORIES]</code> (below)</li><li name="b79a" id="b79a" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">segmentation</code>&nbsp;: polygon segmentation (we will be using them)</li><li name="46e4" id="46e4" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">ignore</code>&nbsp;: we will ignore the ignore flags</li><li name="46a6" id="46a6" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">iscrowd</code>&nbsp;: specifies that it is a crowd of that object, not just one of them</li></ul><pre name="202b" id="202b" class="graf graf--pre graf-after--li"><strong class="markup--strong markup--pre-strong">trn_j[ANNOTATIONS]</strong>[:2]</pre><pre name="487d" id="487d" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[{'area': 34104,<br>  'bbox': [155, 96, 196, 174],<br>  'category_id': 7,<br>  'id': 1,<br>  'ignore': 0,<br>  'image_id': 12,<br>  'iscrowd': 0,<br>  'segmentation': [[155, 96, 155, 270, 351, 270, 351, 96]]},<br> {'area': 13110,<br>  'bbox': [184, 61, 95, 138],<br>  'category_id': 15,<br>  'id': 2,<br>  'ignore': 0,<br>  'image_id': 17,<br>  'iscrowd': 0,<br>  'segmentation': [[184, 61, 184, 199, 279, 199, 279, 61]]}]</em></pre><h4 name="5e31" id="5e31" class="graf graf--h4 graf-after--pre">Categories [<a href="https://youtu.be/Z0ssNAbe81M?t=50m15s" data-href="https://youtu.be/Z0ssNAbe81M?t=50m15s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">50:15</a>]</h4><pre name="ae53" id="ae53" class="graf graf--pre graf-after--h4"><strong class="markup--strong markup--pre-strong">trn_j[CATEGORIES]</strong>[:4]</pre><pre name="a655" id="a655" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[{'id': 1, 'name': 'aeroplane', 'supercategory': 'none'},<br> {'id': 2, 'name': 'bicycle', 'supercategory': 'none'},<br> {'id': 3, 'name': 'bird', 'supercategory': 'none'},<br> {'id': 4, 'name': 'boat', 'supercategory': 'none'}]</em></pre><p name="6327" id="6327" class="graf graf--p graf-after--pre">It’s helpful to use constants instead of strings, since we get tab-completion and don’t mistype.</p><pre name="6e94" id="6e94" class="graf graf--pre graf-after--p">FILE_NAME,ID,IMG_ID,CAT_ID,BBOX = <br>                   'file_name','id','image_id','category_id','bbox'</pre><pre name="fb4e" id="fb4e" class="graf graf--pre graf-after--pre">cats = dict((o[ID], o['name']) for o in trn_j[CATEGORIES])<br>trn_fns = dict((o[ID], o[FILE_NAME]) for o in trn_j[IMAGES])<br>trn_ids = [o[ID] for o in trn_j[IMAGES]]</pre><p name="dcf6" id="dcf6" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">Side Note: What people most comment on when they see Jeremy working in real time having seen his classes [</strong><a href="https://youtu.be/Z0ssNAbe81M?t=51m21s" data-href="https://youtu.be/Z0ssNAbe81M?t=51m21s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong">51:21</strong></a><strong class="markup--strong markup--p-strong">]:</strong></p><p name="6a91" id="6a91" class="graf graf--p graf--startsWithDoubleQuote graf-after--p">“Wow, you actually don’t know what you are doing, do you”. 99% of the things he does don’t work and small percentage of things that do work end up here. He mentioned this because machine learning, particularly deep learning is incredibly frustrating [<a href="https://youtu.be/Z0ssNAbe81M?t=51m45s" data-href="https://youtu.be/Z0ssNAbe81M?t=51m45s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">51:45</a>]. In theory, you just define the correct loss function and the flexible enough architecture, and you press train and you are done. But if that was actually all that took, then nothing would take any time. The problem is that all the steps along the way until it works, it doesn’t work. Like it goes straight to infinity, crashes with an incorrect tensor size, etc. He will endeavor to show you some kind of debugging techniques as we go, but it is one of the hardest things to teach. The main thing it requires is tenacity. The difference between the people who are super effective and the ones who do not seem to go very far has never been about intellect. It’s always been about sticking with it — basically never giving up. It’s particularly important with this kind of deep learning because you don’t get that continuous reward cycle [<a href="https://youtu.be/Z0ssNAbe81M?t=53m04s" data-href="https://youtu.be/Z0ssNAbe81M?t=53m04s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">53:04</a>]. It’s a constant stream of doesn’t work, doesn’t work, doesn’t work, until eventually it does so it’s kind of annoying.</p><h4 name="9f35" id="9f35" class="graf graf--h4 graf-after--p">Let’s take a look at the images&nbsp;[<a href="https://youtu.be/Z0ssNAbe81M?t=53m45s" data-href="https://youtu.be/Z0ssNAbe81M?t=53m45s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">53:45</a>]</h4><pre name="78d7" id="78d7" class="graf graf--pre graf-after--h4">list((PATH/'VOCdevkit'/'VOC2007').iterdir())</pre><pre name="2025" id="2025" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[PosixPath('data/pascal/VOCdevkit/VOC2007/JPEGImages'),<br> PosixPath('data/pascal/VOCdevkit/VOC2007/SegmentationObject'),<br> PosixPath('data/pascal/VOCdevkit/VOC2007/ImageSets'),<br> PosixPath('data/pascal/VOCdevkit/VOC2007/SegmentationClass'),<br> PosixPath('data/pascal/VOCdevkit/VOC2007/Annotations')]</em></pre><pre name="e14e" id="e14e" class="graf graf--pre graf-after--pre">JPEGS = 'VOCdevkit/VOC2007/JPEGImages'</pre><pre name="ad64" id="ad64" class="graf graf--pre graf-after--pre">IMG_PATH = PATH/JPEGS<br>list(IMG_PATH.iterdir())[:5]</pre><pre name="2baa" id="2baa" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[PosixPath('data/pascal/VOCdevkit/VOC2007/JPEGImages/007594.jpg'),<br> PosixPath('data/pascal/VOCdevkit/VOC2007/JPEGImages/005682.jpg'),<br> PosixPath('data/pascal/VOCdevkit/VOC2007/JPEGImages/005016.jpg'),<br> PosixPath('data/pascal/VOCdevkit/VOC2007/JPEGImages/001930.jpg'),<br> PosixPath('data/pascal/VOCdevkit/VOC2007/JPEGImages/007666.jpg')]</em></pre><h4 name="bd22" id="bd22" class="graf graf--h4 graf-after--pre">Creating a dictionary (key: image ID, value: annotations) [<a href="https://youtu.be/Z0ssNAbe81M?t=54m16s" data-href="https://youtu.be/Z0ssNAbe81M?t=54m16s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">54:16</a>]</h4><p name="5895" id="5895" class="graf graf--p graf-after--h4">Each image has a unique ID.</p><pre name="25dc" id="25dc" class="graf graf--pre graf-after--p">im0_d = trn_j[IMAGES][0]<br>im0_d[FILE_NAME],im0_d[ID]</pre><pre name="d75a" id="d75a" class="graf graf--pre graf-after--pre">('000012.jpg', 12)</pre><p name="e8e3" id="e8e3" class="graf graf--p graf-after--pre">A <code class="markup--code markup--p-code">defaultdict</code> is useful any time you want to have a default dictionary entry for new keys [<a href="https://youtu.be/Z0ssNAbe81M?t=55m05s" data-href="https://youtu.be/Z0ssNAbe81M?t=55m05s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">55:05</a>]. If you try and access a key that doesn’t exist, it magically makes itself exist and it sets itself equal to the return value of the function you specify (in this case <code class="markup--code markup--p-code">lambda:[]</code>).</p><p name="224a" id="224a" class="graf graf--p graf-after--p">Here we create a dict from image IDs to a list of annotations (tuple of bounding box and class id).</p><p name="5bb1" id="5bb1" class="graf graf--p graf-after--p">We convert VOC’s height/width into top-left/bottom-right, and switch x/y coords to be consistent with numpy. If given datasets are in crappy formats, take a couple of moments to make things consistent and make them the way you want them to be [<a href="https://youtu.be/Z0ssNAbe81M?t=1h1m24s" data-href="https://youtu.be/Z0ssNAbe81M?t=1h1m24s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:01:24</a>]</p><pre name="46d3" id="46d3" class="graf graf--pre graf-after--p">trn_anno = <strong class="markup--strong markup--pre-strong">collections.defaultdict</strong>(lambda:[])<br>for o in trn_j[ANNOTATIONS]:<br>    if not o['ignore']:<br>        bb = o[BBOX]<br>        bb = np.array([bb[1], bb[0], bb[3]+bb[1]-1, bb[2]+bb[0]-1])<br>        trn_anno[o[IMG_ID]]<strong class="markup--strong markup--pre-strong">.append</strong>((bb,o[CAT_ID]))<br>        <br>len(trn_anno)</pre><pre name="c3c5" id="c3c5" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">2501</em></pre><p name="e4d9" id="e4d9" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">Variable naming, coding style philosophy, etc [</strong><a href="https://youtu.be/Z0ssNAbe81M?t=56m15s" data-href="https://youtu.be/Z0ssNAbe81M?t=56m15s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong">56:15</strong></a><strong class="markup--strong markup--p-strong">−</strong><a href="https://youtu.be/Z0ssNAbe81M?t=59m33s" data-href="https://youtu.be/Z0ssNAbe81M?t=59m33s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong">59:33</strong></a><strong class="markup--strong markup--p-strong">]</strong></p><p name="3f08" id="3f08" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">example 1</strong></p><ul class="postList"><li name="874f" id="874f" class="graf graf--li graf-after--p"><code class="markup--code markup--li-code">[ 96, 155, 269, 350]</code>&nbsp;: a bounding box [<a href="https://youtu.be/Z0ssNAbe81M?t=59m53s" data-href="https://youtu.be/Z0ssNAbe81M?t=59m53s" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">59:53</a>]. As you see above, when we created the bounding box, we did a couple of things. The first is we switched the x and y coordinates. The reason for this that in computer vision world, when you say “my screen is 640 by 480” that is width by height. Or else, the math world, when you say “my array is 640 by 480” it is rows by columns. So pillow image library tends to do things in width by height or columns by rows, and numpy is the opposite way around. The second is that we are going to do things by describing the top-left xy coordinate and the bottom right xy coordinate — rather than x, y, height, width.</li><li name="0b6e" id="0b6e" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">7</code>&nbsp;: class label / category</li></ul><pre name="0314" id="0314" class="graf graf--pre graf-after--li"><code class="markup--code markup--pre-code">im0_a = im_a[0]; im0_a</code></pre><pre name="1d7e" id="1d7e" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[(array(</em><strong class="markup--strong markup--pre-strong"><em class="markup--em markup--pre-em">[ 96, 155, 269, 350]</em></strong><em class="markup--em markup--pre-em">), </em><strong class="markup--strong markup--pre-strong"><em class="markup--em markup--pre-em">7</em></strong><em class="markup--em markup--pre-em">)]</em></pre><pre name="146a" id="146a" class="graf graf--pre graf-after--pre">im0_a = im_a[0]; im0_a</pre><pre name="74c7" id="74c7" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(array([ 96, 155, 269, 350]), 7)</em></pre><pre name="6a57" id="6a57" class="graf graf--pre graf-after--pre">cats[7]</pre><pre name="cf0e" id="cf0e" class="graf graf--pre graf-after--pre"><code class="markup--code markup--pre-code"><em class="markup--em markup--pre-em">'car'</em></code></pre><p name="e207" id="e207" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">example 2</strong></p><pre name="2960" id="2960" class="graf graf--pre graf-after--p">trn_anno[17]</pre><pre name="24d0" id="24d0" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[(array([61, 184, 198, 278]), 15), (array([77, 89, 335, 402]), 13)]</em></pre><pre name="8a2e" id="8a2e" class="graf graf--pre graf-after--pre">cats[15],cats[13]</pre><pre name="b5f1" id="b5f1" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">('person', 'horse')</em></pre><p name="c94e" id="c94e" class="graf graf--p graf-after--pre">Some libs take VOC format bounding boxes, so this let’s us convert back when required [<a href="https://youtu.be/Z0ssNAbe81M?t=1h2m23s" data-href="https://youtu.be/Z0ssNAbe81M?t=1h2m23s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:02:23</a>]:</p><pre name="43c3" id="43c3" class="graf graf--pre graf-after--p">def bb_hw(a): return np.array([a[1],a[0],a[3]-a[1],a[2]-a[0]])</pre><p name="0823" id="0823" class="graf graf--p graf-after--pre">We will use fast.ai’s <code class="markup--code markup--p-code">open_image</code> in order to display it:</p><pre name="726f" id="726f" class="graf graf--pre graf-after--p">im = open_image(IMG_PATH/im0_d[FILE_NAME])</pre><h4 name="8dbf" id="8dbf" class="graf graf--h4 graf-after--pre"><strong class="markup--strong markup--h4-strong">Introduction to Integrated Development Environment (IDE) [</strong><a href="https://youtu.be/Z0ssNAbe81M?t=1h3m13s" data-href="https://youtu.be/Z0ssNAbe81M?t=1h3m13s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--h4-strong">1:03:13</strong></a><strong class="markup--strong markup--h4-strong">]</strong></h4><p name="8bba" id="8bba" class="graf graf--p graf-after--h4">You can use <a href="https://code.visualstudio.com/" data-href="https://code.visualstudio.com/" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Visual Studio Code</a> (vscode — open source editor that comes with recent versions of Anaconda, or can be installed separately), or most editors and IDEs, to find out all about the <code class="markup--code markup--p-code">open_image</code> function. vscode things to know:</p><ul class="postList"><li name="2028" id="2028" class="graf graf--li graf-after--p">Command palette (<code class="markup--code markup--li-code">Ctrl-shift-p</code>)</li><li name="8575" id="8575" class="graf graf--li graf-after--li">Select interpreter (for fastai env)</li><li name="267c" id="267c" class="graf graf--li graf-after--li">Select terminal shell</li><li name="a1ad" id="a1ad" class="graf graf--li graf-after--li">Go to symbol (<code class="markup--code markup--li-code">Ctrl-t</code>)</li><li name="bc32" id="bc32" class="graf graf--li graf-after--li">Find references (<code class="markup--code markup--li-code">Shift-F12</code>)</li><li name="febc" id="febc" class="graf graf--li graf-after--li">Go to definition (<code class="markup--code markup--li-code">F12</code>)</li><li name="9537" id="9537" class="graf graf--li graf-after--li">Go back (<code class="markup--code markup--li-code">alt-left</code>)</li><li name="ad24" id="ad24" class="graf graf--li graf-after--li">View documentation</li><li name="ce1e" id="ce1e" class="graf graf--li graf-after--li">Hide sidebar (<code class="markup--code markup--li-code">Ctrl-b</code>)</li><li name="80e5" id="80e5" class="graf graf--li graf-after--li">Zen mode (<code class="markup--code markup--li-code">Ctrl-k,z</code>)</li></ul><p name="c371" id="c371" class="graf graf--p graf-after--li">If you are using PyCharm Professional Edition on Mac like I am:</p><ul class="postList"><li name="5eed" id="5eed" class="graf graf--li graf-after--p">Command palette (<code class="markup--code markup--li-code">Shift-command-a</code>)</li><li name="99d8" id="99d8" class="graf graf--li graf-after--li">Select interpreter (for fastai env) (<code class="markup--code markup--li-code">Shift-command-a</code> and then look for “interpreter”)</li><li name="5b9f" id="5b9f" class="graf graf--li graf-after--li">Select terminal shell (<code class="markup--code markup--li-code">Option-F12</code> )</li><li name="ac3b" id="ac3b" class="graf graf--li graf-after--li">Go to symbol (<code class="markup--code markup--li-code">Option-command-shift-n</code> and type name of the class, function, etc. If it’s in camelcase or underscore separated, you can type in first few letters of each bit)</li><li name="73c3" id="73c3" class="graf graf--li graf-after--li">Find references (<code class="markup--code markup--li-code">Option-F7</code>), next occurrence (<code class="markup--code markup--li-code">Option-command-⬇︎</code>), previous occurrence (<code class="markup--code markup--li-code">Option-command-⬆︎</code>)</li><li name="3ddb" id="3ddb" class="graf graf--li graf-after--li">Go to definition (<code class="markup--code markup--li-code">Command-b</code>)</li><li name="fe26" id="fe26" class="graf graf--li graf-after--li">Go back (<code class="markup--code markup--li-code">Option-command-⬅︎</code>)</li><li name="80f5" id="80f5" class="graf graf--li graf-after--li">View documentation</li><li name="e757" id="e757" class="graf graf--li graf-after--li">Zen mode (<code class="markup--code markup--li-code">Control-`-4–2</code> or search for “distraction free mode”)</li></ul><h4 name="d0dd" id="d0dd" class="graf graf--h4 graf-after--li">Let’s talk about open_image [<a href="https://youtu.be/Z0ssNAbe81M?t=1h10m52s" data-href="https://youtu.be/Z0ssNAbe81M?t=1h10m52s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:10:52</a>]</h4><p name="1e29" id="1e29" class="graf graf--p graf-after--h4">Fastai uses OpenCV. TorchVision uses PyTorch tensors for data augmentations etc. A lot of people use Pillow <code class="markup--code markup--p-code">PIL</code>. Jeremy did a lot of testing of all of these and he found OpenCV was about 5 to 10 times faster than TorchVision. For the <a href="https://www.kaggle.com/c/planet-understanding-the-amazon-from-space" data-href="https://www.kaggle.com/c/planet-understanding-the-amazon-from-space" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">planet satellite image competition</a> [<a href="https://youtu.be/Z0ssNAbe81M?t=1h11m55s" data-href="https://youtu.be/Z0ssNAbe81M?t=1h11m55s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:11:55</a>], TorchVision was so slow that they could only get 25% GPU utilization because they were doing a lot of data augmentation. Profiler showed that it was all in TorchVision.</p><p name="fc0b" id="fc0b" class="graf graf--p graf-after--p">Pillow is quite a bit faster but it is not as fast as OpenCV and also is not nearly as thread-safe [<a href="https://youtu.be/Z0ssNAbe81M?t=1h12m19s" data-href="https://youtu.be/Z0ssNAbe81M?t=1h12m19s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:12:19</a>]. Python has this thing called the global interpreter lock (GIL) which means that two thread can’t do pythonic things at the same time — which makes Python a crappy language for modern programming but we are stuck with it. OpenCV releases the GIL. One of the reasons fast.ai library is so fast is because it does not use multiple processors like every other library does for data augmentations — it actually does multiple threads. The reason it could do multiple thread is because it uses OpenCV. Unfortunately OpenCV has an inscrutable API and documentations are somewhat obtuse. That is why Jeremy tried to make it so that no one using fast.ai needs to know that it’s using OpenCV. You don’t need to know what flags to pass to open an image. You don’t need to know that if the reading fails, it doesn’t show an exception — it silently returns <code class="markup--code markup--p-code">None</code>.</p><figure name="2130" id="2130" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_afXUCCfpzM6E1anLo8bKxA.png"></figure><p name="0b71" id="0b71" class="graf graf--p graf-after--figure">Don’t start using PyTorch for your data augmentation or start bringing in Pillow — you will find suddenly things slow down horribly or the multi-threading won’t work anymore. You should stick to using OpenCV for your processing [<a href="https://youtu.be/Z0ssNAbe81M?t=1h14m10s" data-href="https://youtu.be/Z0ssNAbe81M?t=1h14m10s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:14:10</a>]</p><h4 name="74ec" id="74ec" class="graf graf--h4 graf-after--p">Using Matplotlib better [<a href="https://youtu.be/Z0ssNAbe81M?t=1h14m45s" data-href="https://youtu.be/Z0ssNAbe81M?t=1h14m45s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:14:45</a>]</h4><p name="e72e" id="e72e" class="graf graf--p graf-after--h4">Matplotlib is so named because it was originally a clone of Matlab’s plotting library. Unfortunately, Matlab’s plotting library is not great, but at that time, it was what everybody knew. At some point, the matplotlib folks realized that and added a second API which was an object-oriented API. Unfortunately, because nobody who originally learnt matplotlib learnt the OO API, they then taught the next generation of people the old Matlab style API. Now there are not many examples or tutorials that use the much better, easier to understand, and simpler OO API. Because plotting is so important in deep learning, one of the things we are going to learn in this class is how to use this API.</p><p name="d717" id="d717" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Trick 1: plt.subplots [</strong><a href="https://youtu.be/Z0ssNAbe81M?t=1h16m" data-href="https://youtu.be/Z0ssNAbe81M?t=1h16m" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong">1:16:00</strong></a><strong class="markup--strong markup--p-strong">]</strong></p><p name="70bd" id="70bd" class="graf graf--p graf-after--p">Matplotlib’s <code class="markup--code markup--p-code">plt.subplots</code> is a really useful wrapper for creating plots, regardless of whether you have more than one subplot. Note that Matplotlib has an optional object-oriented API which I think is much easier to understand and use (although few examples online use it!)</p><pre name="9451" id="9451" class="graf graf--pre graf-after--p">def show_img(im, figsize=None, ax=None):<br>    if not ax: fig,ax = plt.subplots(figsize=figsize)<br>    ax.imshow(im)<br>    ax.get_xaxis().set_visible(False)<br>    ax.get_yaxis().set_visible(False)<br>    return ax</pre><p name="0717" id="0717" class="graf graf--p graf-after--pre">It returns two things — you probably won’t care about the first one (Figure object), the second one is Axes object (or an array of them). Basically anywhere you used to say <code class="markup--code markup--p-code">plt.</code> something, you now say <code class="markup--code markup--p-code">ax.</code> something, and it will now do the plotting to that particular subplot. This is helpful when you want to plot multiple plots so you can compare next to each other.</p><p name="d7f3" id="d7f3" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Trick 2: Visible text regardless of background color [</strong><a href="https://youtu.be/Z0ssNAbe81M?t=1h17m59s" data-href="https://youtu.be/Z0ssNAbe81M?t=1h17m59s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong">1:17:59</strong></a><strong class="markup--strong markup--p-strong">]</strong></p><p name="c136" id="c136" class="graf graf--p graf-after--p">A simple but rarely used trick to making text visible regardless of background is to use white text with black outline, or visa versa. Here’s how to do it in matplotlib.</p><pre name="e65f" id="e65f" class="graf graf--pre graf-after--p">def draw_outline(o, lw):<br>    o.set_path_effects([patheffects.Stroke(<br>        linewidth=lw, foreground='black'), patheffects.Normal()])</pre><p name="eb82" id="eb82" class="graf graf--p graf-after--pre">Note that <code class="markup--code markup--p-code">*</code> in argument lists is the <a href="https://stackoverflow.com/questions/5239856/foggy-on-asterisk-in-python" data-href="https://stackoverflow.com/questions/5239856/foggy-on-asterisk-in-python" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">splat operator</a>. In this case it's a little shortcut compared to writing out <code class="markup--code markup--p-code">b[-2],b[-1]</code>.</p><pre name="29b6" id="29b6" class="graf graf--pre graf-after--p">def draw_rect(ax, b):<br>    patch = ax.add_patch(patches.Rectangle(b[:2], *b[-2:], <br>                         fill=False, edgecolor='white', lw=2))<br>    draw_outline(patch, 4)</pre><pre name="d355" id="d355" class="graf graf--pre graf-after--pre">def draw_text(ax, xy, txt, sz=14):<br>    text = ax.text(*xy, txt, verticalalignment='top', color='white',<br>                   fontsize=sz, weight='bold')<br>    draw_outline(text, 1)</pre><pre name="8004" id="8004" class="graf graf--pre graf-after--pre">ax = show_img(im)<br>b = bb_hw(im0_a[0])<br>draw_rect(ax, b)<br>draw_text(ax, b[:2], cats[im0_a[1]])</pre><figure name="95da" id="95da" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_qAHYi8J1TjnAtuQ9IGGIYg.png"></figure><p name="f8a0" id="f8a0" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Packaging it all up [</strong><a href="https://youtu.be/Z0ssNAbe81M?t=1h21m20s" data-href="https://youtu.be/Z0ssNAbe81M?t=1h21m20s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong">1:21:20</strong></a><strong class="markup--strong markup--p-strong">]</strong></p><pre name="9471" id="9471" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> draw_im(im, ann):<br>    ax = show_img(im, figsize=(16,8))<br>    <strong class="markup--strong markup--pre-strong">for</strong> b,c <strong class="markup--strong markup--pre-strong">in</strong> ann:<br>        b = bb_hw(b)<br>        draw_rect(ax, b)<br>        draw_text(ax, b[:2], cats[c], sz=16)</pre><pre name="7165" id="7165" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">def</strong> draw_idx(i):<br>    im_a = trn_anno[i]<br>    im = open_image(IMG_PATH/trn_fns[i])<br>    print(im.shape)<br>    draw_im(im, im_a)</pre><pre name="3837" id="3837" class="graf graf--pre graf-after--pre">draw_idx(17)</pre><figure name="f06f" id="f06f" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_QrfruESADB4Fsbl1cvD8vA.png"></figure><p name="e784" id="e784" class="graf graf--p graf-after--figure">When you are working with a new dataset, getting to the point that you can rapidly explore it pays off.</p><h3 name="488b" id="488b" class="graf graf--h3 graf-after--p">Largest item classifier [<a href="https://youtu.be/Z0ssNAbe81M?t=1h22m57s" data-href="https://youtu.be/Z0ssNAbe81M?t=1h22m57s" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">1:22:57</a>]</h3><p name="3f17" id="3f17" class="graf graf--p graf-after--h3">Rather than trying to solve everything at once, let’s make continual progress. We know how to find the biggest object in each image and classify it, so let’s start from there. Jeremy’s approach to Kaggle competition is half an hour every day [<a href="https://youtu.be/Z0ssNAbe81M?t=1h24m" data-href="https://youtu.be/Z0ssNAbe81M?t=1h24m" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:24:00</a>]. At the end of that half hour, submit something and try to make it a little bit better than yesterday’s.</p><p name="aefd" id="aefd" class="graf graf--p graf-after--p">The first thing we need to do is to go through each of the bounding boxes in an image and get the largest one. A <em class="markup--em markup--p-em">lambda function</em> is simply a way to define an anonymous function inline. Here we use it to describe how to sort the annotation for each image — by bounding box size (descending).</p><p name="537a" id="537a" class="graf graf--p graf-after--p">We subtract the upper left from the bottom right and multiply (<code class="markup--code markup--p-code">np.product</code>) the values to get an area <code class="markup--code markup--p-code">lambda x: np.product(x[0][-2:]-x[0][:2])</code>.</p><pre name="60d2" id="60d2" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> get_lrg(b):<br>    if not b: raise Exception()<br>    b = sorted(b, key=lambda x: np.product(x[0][-2:]-x[0][:2]), <br>               reverse=True)<br>    <strong class="markup--strong markup--pre-strong">return</strong> b[0]</pre><p name="2d54" id="2d54" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">Dictionary comprehension [</strong><a href="https://youtu.be/Z0ssNAbe81M?t=1h27m04s" data-href="https://youtu.be/Z0ssNAbe81M?t=1h27m04s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong">1:27:04</strong></a><strong class="markup--strong markup--p-strong">]</strong></p><pre name="c017" id="c017" class="graf graf--pre graf-after--p">trn_lrg_anno = {a: get_lrg(b) for a,b in trn_anno.items()}</pre><p name="15a3" id="15a3" class="graf graf--p graf-after--pre">Now we have a dictionary from image id to a single bounding box — the largest for that image.</p><pre name="a334" id="a334" class="graf graf--pre graf-after--p">b,c = trn_lrg_anno[23]<br>b = bb_hw(b)<br>ax = show_img(open_image(IMG_PATH/trn_fns[23]), figsize=(5,10))<br>draw_rect(ax, b)<br>draw_text(ax, b[:2], cats[c], sz=16)</pre><figure name="45db" id="45db" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_ncFID5QGdWrtpeIgVVhLeA.png"></figure><p name="c063" id="c063" class="graf graf--p graf-after--figure">You need to look at every stage when you have any kind of processing pipeline [<a href="https://youtu.be/Z0ssNAbe81M?t=1h28m1s" data-href="https://youtu.be/Z0ssNAbe81M?t=1h28m1s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:28:01</a>]. Assume that everything you do will be wrong the first time you do it.</p><pre name="b300" id="b300" class="graf graf--pre graf-after--p">(PATH/'tmp').mkdir(exist_ok=True)<br>CSV = PATH/'tmp/lrg.csv'</pre><p name="eb26" id="eb26" class="graf graf--p graf-after--pre">Often it’s easiest to simply create a CSV of the data you want to model, rather than trying to create a custom dataset [<a href="https://youtu.be/Z0ssNAbe81M?t=1h29m06s" data-href="https://youtu.be/Z0ssNAbe81M?t=1h29m06s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:29:06</a>]. Here we use Pandas to help us create a CSV of the image filename and class. <code class="markup--code markup--p-code">columns=[‘fn’,’cat’]</code> is there because dictionary does not have an order and the order of columns matters.</p><pre name="75e0" id="75e0" class="graf graf--pre graf-after--p">df = pd.DataFrame({'fn': [trn_fns[o] for o in trn_ids],<br>    'cat': [cats[trn_lrg_anno[o][1]] for o in trn_ids]}, <br>    columns=['fn','cat'])<br>df.to_csv(CSV, index=False)</pre><pre name="c43d" id="c43d" class="graf graf--pre graf-after--pre">f_model = resnet34<br>sz=224<br>bs=64</pre><p name="bce4" id="bce4" class="graf graf--p graf-after--pre">From here it’s just like Dogs vs Cats!</p><pre name="d0bb" id="d0bb" class="graf graf--pre graf-after--p">tfms = tfms_from_model(f_model, sz, aug_tfms=transforms_side_on, <br>                       crop_type=<strong class="markup--strong markup--pre-strong">CropType.NO</strong>)<br>md = ImageClassifierData.from_csv(PATH, JPEGS, CSV, tfms=tfms)</pre><h4 name="f169" id="f169" class="graf graf--h4 graf-after--pre"><strong class="markup--strong markup--h4-strong">Let’s take a look at this [</strong><a href="https://youtu.be/Z0ssNAbe81M?t=1h30m48s" data-href="https://youtu.be/Z0ssNAbe81M?t=1h30m48s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--h4-strong">1:30:48</strong></a><strong class="markup--strong markup--h4-strong">]</strong></h4><p name="69ee" id="69ee" class="graf graf--p graf-after--h4">One thing that is different is <code class="markup--code markup--p-code">crop_type</code>. The default strategy for creating 224 by 224 image in fast.ai is to first resize it so that the smallest side is 224. Then to take a random squared crop during the training. During validation, we take the center crop unless we use data augmentation.</p><p name="277f" id="277f" class="graf graf--p graf-after--p">For bounding boxes, we do not want to do that because unlike an image net where the thing we care about is pretty much in the middle and pretty big, a lot of the things in object detection is quite small and close to the edge. By setting <code class="markup--code markup--p-code">crop_type</code> to <code class="markup--code markup--p-code">CropType.NO</code>, it will not crop and therefore, to make it square, it squishes it [<a href="https://youtu.be/Z0ssNAbe81M?t=1h32m9s" data-href="https://youtu.be/Z0ssNAbe81M?t=1h32m9s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:32:09</a>]. Generally speaking, a lot of computer vision models work a little bit better if you crop rather than squish, but they still work pretty well if you squish. In this case, we definitely do not want to crop, so this is perfectly fine.</p><pre name="9d89" id="9d89" class="graf graf--pre graf-after--p">x,y=next(iter(md.val_dl))<br>show_img(md.val_ds.denorm(to_np(x))[0]);</pre><figure name="ac55" id="ac55" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_bTBWgFXrJYD7sKtiPnCt5g.png"></figure><h4 name="c44e" id="c44e" class="graf graf--h4 graf-after--figure">Data loaders [<a href="https://youtu.be/Z0ssNAbe81M?t=1h33m4s" data-href="https://youtu.be/Z0ssNAbe81M?t=1h33m4s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:33:04</a>]</h4><p name="fc85" id="fc85" class="graf graf--p graf-after--h4">You already know that inside of a model data object, we have bunch of things which include training data loader and training data set. The main thing to know about data loader is that it is an iterator that each time you grab the next iteration of stuff from it, you get a mini batch. The mini batch you get is of whatever size you asked for and by default the batch size is 64. In Python, the way you grab the next thing from an iterator is with next <code class="markup--code markup--p-code">next(md.trn_dl)</code> but you can’t just do that. The reason you can’t say that is because you need to say “start a new epoch now”. In general, not just in PyTorch but for any Python iterator, you need to say “start at the beginning of the sequence please”. The say you do that is to use<code class="markup--code markup--p-code">iter(md.trn_dl)</code> which will grab an iterator out of <code class="markup--code markup--p-code">md.trn_dl</code> — specifically as we will learn later, it means that this class has to have defined an <code class="markup--code markup--p-code">__iter__</code> method which returns some different object which then has an <code class="markup--code markup--p-code">__next__</code> method.</p><p name="2bc3" id="2bc3" class="graf graf--p graf-after--p">If you want to grab just a single batch, this is how you do it (<code class="markup--code markup--p-code">x</code>: independent variable, <code class="markup--code markup--p-code">y</code>: dependent variable):</p><pre name="3bf5" id="3bf5" class="graf graf--pre graf-after--p">x,y=next(iter(md.val_dl))</pre><p name="db8b" id="db8b" class="graf graf--p graf-after--pre">We cannot send this straight to <code class="markup--code markup--p-code">show_image</code>[<a href="https://youtu.be/Z0ssNAbe81M?t=1h35m30s" data-href="https://youtu.be/Z0ssNAbe81M?t=1h35m30s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:35:30</a>]. For example, <code class="markup--code markup--p-code">x</code> is not a numpy array, not on CPU, and the shape is all wrong (<code class="markup--code markup--p-code">3x224x224</code>). Further more, they are not numbers between 0 and 1 because all of the standard ImageNet pre-trained models expect our data to have been normalized to have a zero mean and 1 standard deviation.</p><figure name="a5a7" id="a5a7" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_CbjuSpn8ZnX6SMLNiBzoag.png"></figure><p name="620c" id="620c" class="graf graf--p graf-after--figure">As you see, there is a whole bunch of things that has been done to the input to get it ready to be passed to a pre-trained model. So we have a function called <code class="markup--code markup--p-code">denorm</code> for denormalize and also fixes up dimension order etc. Since denormalization depends on the transform [<a href="https://youtu.be/Z0ssNAbe81M?t=1h37m52s" data-href="https://youtu.be/Z0ssNAbe81M?t=1h37m52s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:37:52</a>], and dataset knows what transform was used to create it, so that is why you have to do <code class="markup--code markup--p-code">md.val_ds.denorm</code> and pass the mini-batch after turning it into numpy array:</p><pre name="7dc0" id="7dc0" class="graf graf--pre graf-after--p">show_img(md.val_ds.denorm(to_np(x))[0]);</pre><h4 name="05fc" id="05fc" class="graf graf--h4 graf-after--pre">Training with ResNet34 [<a href="https://youtu.be/Z0ssNAbe81M?t=1h38m36s" data-href="https://youtu.be/Z0ssNAbe81M?t=1h38m36s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:38:36</a>]</h4><pre name="dd95" id="dd95" class="graf graf--pre graf-after--h4">learn = ConvLearner.pretrained(f_model, md, metrics=[accuracy])<br>learn.opt_fn = optim.Adam</pre><pre name="6cff" id="6cff" class="graf graf--pre graf-after--pre">lrf=learn.lr_find(1e-5,100)<br>learn.sched.plot()</pre><figure name="3f45" id="3f45" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_oZe5esLqorSwfDyN9ld3Rw.png"></figure><p name="f68d" id="f68d" class="graf graf--p graf-after--figure">We intentionally remove the first few points and the last few points [<a href="https://youtu.be/Z0ssNAbe81M?t=1h38m54s" data-href="https://youtu.be/Z0ssNAbe81M?t=1h38m54s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:38:54</a>], because often the last few points shoots so high up towards infinity that you can’t see anything so it is generally a good idea. But when you have very few mini-batches, it is not a good idea. When your LR finder graph looks like above, you can ask for more points on each end (you can also make your batch size really small):</p><pre name="6844" id="6844" class="graf graf--pre graf-after--p">learn.sched.plot(n_skip=5, n_skip_end=1)</pre><figure name="fe1e" id="fe1e" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_KhVaT1KpcVj6JsXzjMlxdw.png"></figure><pre name="5713" id="5713" class="graf graf--pre graf-after--figure">lr = 2e-2<br>learn.fit(lr, 1, cycle_len=1)</pre><pre name="4eb8" id="4eb8" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss   accuracy                      <br>    0      1.280753   0.604127   0.806941</em></pre><p name="ccf0" id="ccf0" class="graf graf--p graf-after--pre">Unfreeze a couple of layers:</p><pre name="45f9" id="45f9" class="graf graf--pre graf-after--p">lrs = np.array([lr/1000,lr/100,lr])<br>learn.freeze_to(-2)<br>learn.fit(lrs/5, 1, cycle_len=1)</pre><pre name="9389" id="9389" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss   accuracy                      <br>    0      0.780925   0.575539   0.821064</em></pre><p name="7aec" id="7aec" class="graf graf--p graf-after--pre">Unfreeze the whole thing:</p><pre name="dba3" id="dba3" class="graf graf--pre graf-after--p">learn.unfreeze()<br>learn.fit(lrs/5, 1, cycle_len=2)</pre><pre name="2648" id="2648" class="graf graf--pre graf-after--pre">epoch      trn_loss   val_loss   accuracy                       <br>    0      0.676254   0.546998   0.834285       <br>    1      0.460609   0.533741   0.833233</pre><p name="4f03" id="4f03" class="graf graf--p graf-after--pre">Accuracy isn’t improving much — since many images have multiple different objects, it’s going to be impossible to be that accurate.</p><h4 name="6ef9" id="6ef9" class="graf graf--h4 graf-after--p">Let’s look at the result [<a href="https://youtu.be/Z0ssNAbe81M?t=1h40m48s" data-href="https://youtu.be/Z0ssNAbe81M?t=1h40m48s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:40:48</a>]</h4><pre name="5b00" id="5b00" class="graf graf--pre graf-after--h4">fig, axes = plt.subplots(3, 4, figsize=(12, 8))<br>for i,ax in enumerate(axes.flat):<br>    ima=md.val_ds.denorm(x)[i]<br>    b = md.classes[preds[i]]<br>    ax = show_img(ima, ax=ax)<br>    draw_text(ax, (0,0), b)<br>plt.tight_layout()</pre><figure name="2715" id="2715" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_0Tq4_OSCmZnT_TFyZ5JScg.png"></figure><p name="eed9" id="eed9" class="graf graf--p graf-after--figure">How to understand the unfamiliar code:</p><ul class="postList"><li name="eafd" id="eafd" class="graf graf--li graf-after--p">Run each line of code step by step, print out the inputs and outputs.</li></ul><p name="d643" id="d643" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">Method 1</strong> [<a href="https://youtu.be/Z0ssNAbe81M?t=1h42m28s" data-href="https://youtu.be/Z0ssNAbe81M?t=1h42m28s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:42:28</a>]: You can take the contents of the loop, copy it, create a cell above it, paste it, un-indent it, set <code class="markup--code markup--p-code">i=0</code> and put them all in separate cells.</p><figure name="4a81" id="4a81" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_mOfiv9blUSSx5iFEArlZNw.png"></figure><p name="30a7" id="30a7" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Method 2 </strong>[<a href="https://youtu.be/Z0ssNAbe81M?t=1h43m4s" data-href="https://youtu.be/Z0ssNAbe81M?t=1h43m4s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">1:43:04</a>]: Use Python debugger</p><p name="c9fd" id="c9fd" class="graf graf--p graf-after--p">You can use the python debugger <code class="markup--code markup--p-code">pdb</code> to step through code.</p><ul class="postList"><li name="dd73" id="dd73" class="graf graf--li graf-after--p"><code class="markup--code markup--li-code">pdb.set_trace()</code> to set a breakpoint</li><li name="a102" id="a102" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">%debug</code> magic to trace an error (after the exception happened)</li></ul><p name="c1b4" id="c1b4" class="graf graf--p graf-after--li">Commands you need to know:</p><ul class="postList"><li name="4b86" id="4b86" class="graf graf--li graf-after--p"><code class="markup--code markup--li-code">h</code> (help)</li><li name="17a6" id="17a6" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">s</code> (step into)</li><li name="ab39" id="ab39" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">n</code> (next line / step over — you can also hit enter)</li><li name="0a4e" id="0a4e" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">c</code> (continue to the next breakpoint)</li><li name="eb89" id="eb89" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">u</code> (up the call stack)</li><li name="57f5" id="57f5" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">d</code> (down the call stack)</li><li name="96b5" id="96b5" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">p</code> (print) — force print when there is a single letter variable that’s also a command.</li><li name="0e59" id="0e59" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">l</code> (list) — show the line above and below it</li><li name="9e11" id="9e11" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">q</code> (quit) — very important</li></ul><p name="58b8" id="58b8" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">Comment [</strong><a href="https://youtu.be/Z0ssNAbe81M?t=1h49m10s" data-href="https://youtu.be/Z0ssNAbe81M?t=1h49m10s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong">1:49:10</strong></a><strong class="markup--strong markup--p-strong">]:</strong><code class="markup--code markup--p-code"><a href="http://ipython.readthedocs.io/en/stable/api/generated/IPython.core.debugger.html" data-href="http://ipython.readthedocs.io/en/stable/api/generated/IPython.core.debugger.html" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">IPython.core.debugger</a></code> (on the right below) makes it all pretty:</p><figure name="c1c6" id="c1c6" class="graf graf--figure graf--layoutOutsetRow is-partialWidth graf-after--p" style="width: 49.645%;"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_4WryeZDtKFciD7qchVA6WQ.png"></figure><figure name="fe61" id="fe61" class="graf graf--figure graf--layoutOutsetRowContinue is-partialWidth graf-after--figure" style="width: 50.355%;"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_aztHN3af_MxEhHS71_SUDQ.png"></figure><h4 name="0291" id="0291" class="graf graf--h4 graf-after--figure">Creating the bounding box [<a href="https://youtu.be/Z0ssNAbe81M?t=1h52m51s" data-href="https://youtu.be/Z0ssNAbe81M?t=1h52m51s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:52:51</a>]</h4><p name="956e" id="956e" class="graf graf--p graf-after--h4">Creating a bounding box around the largest object may seem like something you haven’t done before, but actually it is totally something you have done before. We can create a regression rather than a classification neural net. Classification neural net is the one that has a sigmoid or softmax output, and we use a cross entropy, binary cross entropy, or negative log likelihood loss function. That is basically what makes it classifier. If we don’t have the softmax or sigmoid at the end and we use mean squared error as a loss function, it is now a regression model which predict continuous number rather than a category. We also know that we can have multiple outputs like in the planet competition (multiple classification). What if we combine the two ideas and do a multiple column regression?</p><p name="56ff" id="56ff" class="graf graf--p graf-after--p">This is where you are thinking about it like differentiable programming. It is not like “how do I create a bounding box model?” but it is more like:</p><ul class="postList"><li name="86f1" id="86f1" class="graf graf--li graf-after--p">We need four numbers, therefore, we need a neural network with 4 activations</li><li name="f7ee" id="f7ee" class="graf graf--li graf-after--li">For loss function, what is a function that when it is lower means that the four numbers are better? Mean squared loss function!</li></ul><p name="38f8" id="38f8" class="graf graf--p graf-after--li">That’s it. Let’s try it.</p><h4 name="ffc0" id="ffc0" class="graf graf--h4 graf-after--p">Bbox only [<a href="https://youtu.be/Z0ssNAbe81M?t=1h55m27s" data-href="https://youtu.be/Z0ssNAbe81M?t=1h55m27s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:55:27</a>]</h4><p name="0d96" id="0d96" class="graf graf--p graf-after--h4">Now we’ll try to find the bounding box of the largest object. This is simply a regression with 4 outputs. So we can use a CSV with multiple ‘labels’. If you remember from part 1 to do a multiple label classification, your multiple labels have to be space separated, and the file name is comma separated.</p><pre name="f300" id="f300" class="graf graf--pre graf-after--p">BB_CSV = PATH/'tmp/bb.csv'<br>bb = np.array([trn_lrg_anno[o][0] for o in trn_ids])<br>bbs = [' '.join(str(p) for p in o) for o in bb]</pre><pre name="e249" id="e249" class="graf graf--pre graf-after--pre">df = pd.DataFrame({'fn': [trn_fns[o] for o in trn_ids], <br>                   'bbox': bbs}, columns=['fn','bbox'])<br>df.to_csv(BB_CSV, index=False)</pre><pre name="0816" id="0816" class="graf graf--pre graf-after--pre">BB_CSV.open().readlines()[:5]</pre><pre name="2a2e" id="2a2e" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">['fn,bbox\n',<br> '000012.jpg,96 155 269 350\n',<br> '000017.jpg,77 89 335 402\n',<br> '000023.jpg,1 2 461 242\n',<br> '000026.jpg,124 89 211 336\n']</em></pre><h4 name="932d" id="932d" class="graf graf--h4 graf-after--pre">Training [<a href="https://youtu.be/Z0ssNAbe81M?t=1h56m11s" data-href="https://youtu.be/Z0ssNAbe81M?t=1h56m11s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:56:11</a>]</h4><pre name="f19e" id="f19e" class="graf graf--pre graf-after--h4">f_model=resnet34<br>sz=224<br>bs=64</pre><p name="8238" id="8238" class="graf graf--p graf-after--pre">Set <code class="markup--code markup--p-code">continuous=True</code> to tell fastai this is a regression problem, which means it won't one-hot encode the labels, and will use MSE as the default crit.</p><p name="01ce" id="01ce" class="graf graf--p graf-after--p">Note that we have to tell the transforms constructor that our labels are coordinates, so that it can handle the transforms correctly.</p><p name="1465" id="1465" class="graf graf--p graf-after--p">Also, we use CropType.NO because we want to ‘squish’ the rectangular images into squares, rather than center cropping, so that we don’t accidentally crop out some of the objects. (This is less of an issue in something like imagenet, where there is a single object to classify, and it’s generally large and centrally located).</p><pre name="a65c" id="a65c" class="graf graf--pre graf-after--p">tfms = tfms_from_model(f_model, sz, crop_type=CropType.NO, <br>                       <strong class="markup--strong markup--pre-strong">tfm_y=TfmType.COORD</strong>)<br>md = ImageClassifierData.from_csv(PATH, JPEGS, BB_CSV, tfms=tfms, <br>                                  <strong class="markup--strong markup--pre-strong">continuous=True</strong>)</pre><p name="8ccd" id="8ccd" class="graf graf--p graf-after--pre">We will look at <code class="markup--code markup--p-code">TfmType.COORD</code> next week, but for now, just realize that when we are doing scaling and data augmentation, that needs to happen to the bounding boxes, not just images.</p><pre name="8302" id="8302" class="graf graf--pre graf-after--p">x,y=next(iter(md.val_dl))</pre><pre name="1d01" id="1d01" class="graf graf--pre graf-after--pre">ima=md.val_ds.denorm(to_np(x))[0]<br>b = bb_hw(to_np(y[0])); b</pre><pre name="e365" id="e365" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">array([  49.,    0.,  131.,  205.], dtype=float32)</em></pre><pre name="19ee" id="19ee" class="graf graf--pre graf-after--pre">ax = show_img(ima)<br>draw_rect(ax, b)<br>draw_text(ax, b[:2], 'label')</pre><figure name="c325" id="c325" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_nahTyZS46y9PuRseHjNg4g.png"></figure><h4 name="fb51" id="fb51" class="graf graf--h4 graf-after--figure">Let’s create a convolutional net based on ResNet34 [<a href="https://youtu.be/Z0ssNAbe81M?t=1h56m57s" data-href="https://youtu.be/Z0ssNAbe81M?t=1h56m57s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:56:57</a>]:</h4><p name="a4d3" id="a4d3" class="graf graf--p graf-after--h4">fastai lets you use a <code class="markup--code markup--p-code">custom_head</code> to add your own module on top of a convnet, instead of the adaptive pooling and fully connected net which is added by default. In this case, we don't want to do any pooling, since we need to know the activations of each grid cell.</p><p name="3946" id="3946" class="graf graf--p graf-after--p">The final layer has 4 activations, one per bounding box coordinate. Our target is continuous, not categorical, so the MSE loss function used does not do any sigmoid or softmax to the module outputs.</p><pre name="d1d3" id="d1d3" class="graf graf--pre graf-after--p">head_reg4 = nn.Sequential(Flatten(), nn.Linear(25088,4))<br>learn = ConvLearner.pretrained(f_model, md, <strong class="markup--strong markup--pre-strong">custom_head</strong>=head_reg4)<br>learn.opt_fn = optim.Adam<br>learn.crit = nn.L1Loss()</pre><ul class="postList"><li name="1f62" id="1f62" class="graf graf--li graf-after--pre"><code class="markup--code markup--li-code">Flatten()</code>&nbsp;: Normally the previous layer has <code class="markup--code markup--li-code">7x7x512</code> in ResNet34, so flatten that out into a single vector of length 2508</li><li name="1bcc" id="1bcc" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">L1Loss</code> [<a href="https://youtu.be/Z0ssNAbe81M?t=1h58m22s" data-href="https://youtu.be/Z0ssNAbe81M?t=1h58m22s" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">1:58:22</a>]: Rather than adding up the squared errors, add up the absolute values of the errors. It is normally what you want because adding up the squared errors really penalizes bad misses by too much. So L1Loss is generally better to work with.</li></ul><pre name="fd7d" id="fd7d" class="graf graf--pre graf-after--li">learn.lr_find(1e-5,100)<br>learn.sched.plot(5)</pre><pre name="3a37" id="3a37" class="graf graf--pre graf-after--pre">78%|███████▊  | 25/32 [00:04&lt;00:01,  6.16it/s, loss=395]</pre><figure name="021d" id="021d" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_aPhLm7eoGKPjlE1syDyQ-g.png"></figure><pre name="9aa1" id="9aa1" class="graf graf--pre graf-after--figure">lr = 2e-3<br>learn.fit(lr, 2, cycle_len=1, cycle_mult=2)</pre><pre name="adf7" id="adf7" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss                            <br>    0      49.523444  34.764141 <br>    1      36.864003  28.007317                           <br>    2      30.925234  27.230705</em></pre><pre name="5768" id="5768" class="graf graf--pre graf-after--pre">lrs = np.array([lr/100,lr/10,lr])<br>learn.freeze_to(-2)<br>lrf=learn.lr_find(lrs/1000)<br>learn.sched.plot(1)</pre><figure name="bc0f" id="bc0f" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_SBUJiX2JsdtzHrXRyEuknw.png"></figure><pre name="f182" id="f182" class="graf graf--pre graf-after--figure">learn.fit(lrs, 2, cycle_len=1, cycle_mult=2)</pre><pre name="e9d6" id="e9d6" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss                            <br>    0      25.616161  22.83597  <br>    1      21.812624  21.387115                           <br>    2      17.867176  20.335539</em></pre><pre name="a4d4" id="a4d4" class="graf graf--pre graf-after--pre">learn.freeze_to(-3)<br>learn.fit(lrs, 1, cycle_len=2)</pre><pre name="f477" id="f477" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">epoch      trn_loss   val_loss                            <br>    0      16.571885  20.948696 <br>    1      15.072718  19.925312</em></pre><p name="f82e" id="f82e" class="graf graf--p graf-after--pre">Validation loss is the mean of the absolute value with pixels were off by.</p><pre name="4acc" id="4acc" class="graf graf--pre graf-after--p">learn.save('reg4')</pre><h4 name="3a8b" id="3a8b" class="graf graf--h4 graf-after--pre">Take a look at the result [<a href="https://youtu.be/Z0ssNAbe81M?t=1h59m18s" data-href="https://youtu.be/Z0ssNAbe81M?t=1h59m18s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">1:59:18</a>]</h4><pre name="1169" id="1169" class="graf graf--pre graf-after--h4">x,y = next(iter(md.val_dl))<br>learn.model.eval()<br>preds = to_np(learn.model(VV(x)))</pre><pre name="c729" id="c729" class="graf graf--pre graf-after--pre">fig, axes = plt.subplots(3, 4, figsize=(12, 8))<br><strong class="markup--strong markup--pre-strong">for</strong> i,ax <strong class="markup--strong markup--pre-strong">in</strong> enumerate(axes.flat):<br>    ima=md.val_ds.denorm(to_np(x))[i]<br>    b = bb_hw(preds[i])<br>    ax = show_img(ima, ax=ax)<br>    draw_rect(ax, b)<br>plt.tight_layout()</pre><figure name="325c" id="325c" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_xM98QR8U9kz3MJZDHfz7BA.png"></figure><p name="a388" id="a388" class="graf graf--p graf-after--figure">We will revise this more next week. Before this class, if you were asked “do you know how to create a bounding box model?”, you might have said “no, nobody’s taught me that”. But the question actually is:</p><ul class="postList"><li name="3df5" id="3df5" class="graf graf--li graf-after--p">Can you create a model with 4 continuous outputs? Yes.</li><li name="8ea3" id="8ea3" class="graf graf--li graf-after--li">Can you create a loss function that is lower if those 4 outputs are near to 4 other numbers? Yes</li></ul><p name="68e1" id="68e1" class="graf graf--p graf-after--li">Then you are done.</p><p name="90f7" id="90f7" class="graf graf--p graf-after--p graf--trailing">As you look further down, it starts looking a bit crappy — anytime we have more than one object. This is not surprising. Overall, it did a pretty good job.</p><hr class="section-divider"><p name="e8e9" id="e8e9" class="graf graf--p graf--leading graf--trailing">Lessons: <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" class="markup--anchor markup--p-anchor" target="_blank">1</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" class="markup--anchor markup--p-anchor" target="_blank">2</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" class="markup--anchor markup--p-anchor" target="_blank">3</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" class="markup--anchor markup--p-anchor" target="_blank">4</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" class="markup--anchor markup--p-anchor" target="_blank">5</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" class="markup--anchor markup--p-anchor" target="_blank">6</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" class="markup--anchor markup--p-anchor" target="_blank">7</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" class="markup--anchor markup--p-anchor" target="_blank"><strong class="markup--strong markup--p-strong">8</strong></a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" class="markup--anchor markup--p-anchor" target="_blank">9</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" class="markup--anchor markup--p-anchor" target="_blank">10</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" class="markup--anchor markup--p-anchor" target="_blank">11</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" class="markup--anchor markup--p-anchor" target="_blank">12</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" class="markup--anchor markup--p-anchor" target="_blank">13</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" class="markup--anchor markup--p-anchor" target="_blank">14</a></p></body></html>