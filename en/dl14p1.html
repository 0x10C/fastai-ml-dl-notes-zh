
<!-- saved from url=(0064)file:///C:/Users/asus/Desktop/fastai-ml-dl-notes-zh/en/dl14.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></head><body><hr class="section-divider"><h1 name="36cf" id="36cf" class="graf graf--h3 graf--leading graf--title">Deep Learning 2: Part 2 Lesson&nbsp;14</h1><p name="5787" id="5787" class="graf graf--p graf-after--h3"><em class="markup--em markup--p-em">My personal notes from </em><a href="http://www.fast.ai/" data-href="http://www.fast.ai/" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener noopener noopener nofollow noopener nofollow noopener noopener nofollow noopener noopener nofollow noopener nofollow noopener" target="_blank"><em class="markup--em markup--p-em">fast.ai course</em></a><em class="markup--em markup--p-em">. These notes will continue to be updated and improved as I continue to review the course to “really” understand it. Much appreciation to </em><a href="https://twitter.com/jeremyphoward" data-href="https://twitter.com/jeremyphoward" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener noopener noopener nofollow noopener nofollow noopener noopener nofollow noopener noopener nofollow noopener nofollow noopener" target="_blank"><em class="markup--em markup--p-em">Jeremy</em></a><em class="markup--em markup--p-em"> and </em><a href="https://twitter.com/math_rachel" data-href="https://twitter.com/math_rachel" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener noopener noopener nofollow noopener nofollow noopener noopener nofollow noopener noopener nofollow noopener nofollow noopener" target="_blank"><em class="markup--em markup--p-em">Rachel</em></a><em class="markup--em markup--p-em"> who gave me this opportunity to learn.</em></p><p name="bd83" id="bd83" class="graf graf--p graf-after--p graf--trailing">Lessons: <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197" class="markup--anchor markup--p-anchor" target="_blank">1</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4" class="markup--anchor markup--p-anchor" target="_blank">2</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56" class="markup--anchor markup--p-anchor" target="_blank">3</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa" class="markup--anchor markup--p-anchor" target="_blank">4</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8" class="markup--anchor markup--p-anchor" target="_blank">5</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c" class="markup--anchor markup--p-anchor" target="_blank">6</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c" class="markup--anchor markup--p-anchor" target="_blank">7</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493" class="markup--anchor markup--p-anchor" target="_blank">8</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b" class="markup--anchor markup--p-anchor" target="_blank">9</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-10-422d87c3340c" class="markup--anchor markup--p-anchor" target="_blank">10</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34" class="markup--anchor markup--p-anchor" target="_blank">11</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94" class="markup--anchor markup--p-anchor" target="_blank">12</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-13-43454b21a5d0" class="markup--anchor markup--p-anchor" target="_blank">13</a> ・ <a href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" data-href="https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add" class="markup--anchor markup--p-anchor" target="_blank"><strong class="markup--strong markup--p-strong">14</strong></a></p><hr class="section-divider"><p name="88ab" id="88ab" class="graf graf--p graf--leading"><a href="http://forums.fast.ai/t/part-2-lesson-14-wiki/15650/1" data-href="http://forums.fast.ai/t/part-2-lesson-14-wiki/15650/1" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Forum</a> / <a href="https://youtu.be/nG3tT31nPmQ" data-href="https://youtu.be/nG3tT31nPmQ" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Video</a></p><figure name="ed31" id="ed31" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_X98pzSCWnxb5gbQxDyZ92Q.png"></figure><h4 name="56df" id="56df" class="graf graf--h4 graf-after--figure">Show and tell from last&nbsp;week</h4><figure name="f17e" id="f17e" class="graf graf--figure graf-after--h4"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_iZP-sgkKoKU2dlGj5CUBaw.jpeg"></figure><p name="563a" id="563a" class="graf graf--p graf-after--figure">Alena Harley did something really interesting which was she tried finding out what would happen if you did cycle GAN on just three or four hundred images and I really like these projects where people just go to Google Image Search using the API or one of the libraries out there. Some of our students have created some very good libraries for interacting with Google images API to download a bunch of stuff they are interested in, in this case some photos and some stained glass windows. With 300~400 photos of that, she trained a few different model — this is what I particularly liked. As you can see, with quite a small number of images, she gets very nice stained-glass effects. So I thought that was an interesting example of using pretty small amounts of data that was readily available that she was able to download pretty quickly. There is more information about that on the forum if you are interested.&nbsp;<br>It’s interesting to wonder about what kinds of things people will come up with with this kind of generative model. It’s clearly a great artistic medium. It’s clearly a great medium for forgeries and fakeries. I wonder what other kinds of things people will realize they can do with these kind of generative models. I think audio is going to be the next big area. Also very interactive type stuff. Nvidia just released a paper showing a interactive kind of photo repair tool where you just brush over an object and it replaces it with a deep learning generated replacement very nicely. Those kinds of interactive tools, I think would be very interesting too.</p><h3 name="0c8a" id="0c8a" class="graf graf--h3 graf-after--p">Super-Resolution [<a href="https://youtu.be/nG3tT31nPmQ?t=2m6s" data-href="https://youtu.be/nG3tT31nPmQ?t=2m6s" class="markup--anchor markup--h3-anchor" rel="noopener nofollow" target="_blank">2:06</a>]</h3><p name="8d3c" id="8d3c" class="graf graf--p graf-after--h3"><a href="https://arxiv.org/abs/1603.08155" data-href="https://arxiv.org/abs/1603.08155" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Perceptual Losses for Real-Time Style Transfer and Super-Resolution</a></p><p name="110a" id="110a" class="graf graf--p graf-after--p">Last time, we looked at doing style transfer by actually directly optimizing pixels. Like with most of the things in part two, it’s not so much that I’m wanting you to understand style transfer per se, but the kind of idea of optimizing your input directly and using activations as part of a loss function is really the key takeaway here.</p><p name="3153" id="3153" class="graf graf--p graf-after--p">So it’s interesting then to see effectively the follow-up paper, not from the same people but the paper that came next in the sequence of these vision generative models with this one from Justin Johnson and folks at Stanford. It actually does the same thing — style transfer, but does it in a different way. Rather than optimizing the pixels, we are going to go back to something much more familiar and optimize some weights. So specifically, we are going to train a model which learns to take a photo and translate it into a photo in the style of a particular artwork. So each conv net will learn to produce one kind of style.</p><p name="a777" id="a777" class="graf graf--p graf-after--p">Now it turns out that getting to that point, there is an intermediate point which (I actually think more useful and takes us half way there) is something called super resolution. So we are actually going to start with super resolution [<a href="https://youtu.be/nG3tT31nPmQ?t=3m55s" data-href="https://youtu.be/nG3tT31nPmQ?t=3m55s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">3:55</a>]. Because then we’ll build on top of super resolution to finish off the conv net based style transfer.</p><p name="ad6b" id="ad6b" class="graf graf--p graf-after--p">Super resolution is where we take a low resolution image (we are going to take 72 by 72) and upscale it to a larger image (288 by 288 in our case) trying to create a higher res image that looks as real as possible. This is a challenging thing to do because at 72 by 72, there’s not that much information about a lot of the details. The cool thing is that we are going to do it in a way as we tend to do with vision models which is not tied to the input size so you could totally then take this model and apply it to a 288 by 288 image and get something that’s four times bigger on each side so 16 times bigger than the original. Often it even works better at that level because you’re really introducing a lot of detail into the finer details and you could really print out a high resolution print of something which earlier on was pretty pixelated.</p><h4 name="6b95" id="6b95" class="graf graf--h4 graf-after--p"><a href="https://github.com/fastai/fastai/blob/master/courses/dl2/enhance.ipynb" data-href="https://github.com/fastai/fastai/blob/master/courses/dl2/enhance.ipynb" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">Notebook</a> [<a href="https://youtu.be/nG3tT31nPmQ?t=5m6s" data-href="https://youtu.be/nG3tT31nPmQ?t=5m6s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">5:06</a>]</h4><p name="2a13" id="2a13" class="graf graf--p graf-after--h4">It is a lot like that kind of CSI style enhancement where we’re going to take something that appears the information is just not there and we kind of invent it — but the conv net is going to learn to invent it in a way that’s consistent with the information that is there, so hopefully it’s inventing the right information. One of the really nice things about this kind of problem is that we can create our own dataset as big as we like without any labeling requirements because we can easily create a low res image from a high res image just by down sampling our images. So something I would love some of you to try this week would be to do other types of image-to-image translation where you can invent “labels” (your dependent variables). For example:</p><ul class="postList"><li name="db7f" id="db7f" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Deskewing</strong>: Either recognize things that have been rotated by 90 degrees or better still that have been rotated by 5 degrees and straighten them.</li><li name="6b3f" id="6b3f" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Colorization</strong>: Make a bunch of images into black-and-white and learn to put the color back again.</li><li name="184a" id="184a" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Noise-reduction</strong>: Maybe do a really low quality JPEG save, and learn to put it back to how it should have been.</li><li name="4610" id="4610" class="graf graf--li graf-after--li">Maybe taking something that’s in a 16 color palette and put it back to a higher color palette.</li></ul><p name="4147" id="4147" class="graf graf--p graf-after--li">I think these things are all interesting because they can be used to take pictures that you may have taken back on crappy old digital cameras before there are high resolution or you may have scanned in some old photos that are now faded, etc. I think it’s really useful thing to be able to do and it’s a good project because it’s really similar to what we are doing here but different enough that you come across some interesting challenges on the way, I’m sure.</p><p name="daf5" id="daf5" class="graf graf--p graf-after--p">I’m going to use ImageNet again [<a href="https://youtu.be/nG3tT31nPmQ?t=7m19s" data-href="https://youtu.be/nG3tT31nPmQ?t=7m19s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">7:19</a>]. You don’t need to use all of the ImageNet at all, I just happen to have it lying around. You can download the one percent sample of ImageNet from files.fast.ai. You can use any set of pictures you have lying around honestly.</p><pre name="60e7" id="60e7" class="graf graf--pre graf-after--p">matplotlib inline<br>%reload_ext autoreload<br>%autoreload 2</pre><h3 name="1757" id="1757" class="graf graf--h3 graf-after--pre">Super resolution data</h3><pre name="5e20" id="5e20" class="graf graf--pre graf-after--h3"><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">fastai.conv_learner</strong> <strong class="markup--strong markup--pre-strong">import</strong> *<br><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">pathlib</strong> <strong class="markup--strong markup--pre-strong">import</strong> Path<br><br>torch.backends.cudnn.benchmark=<strong class="markup--strong markup--pre-strong">True</strong></pre><pre name="8467" id="8467" class="graf graf--pre graf-after--pre">PATH = Path('data/imagenet')<br>PATH_TRN = PATH/'train'</pre><p name="706d" id="706d" class="graf graf--p graf-after--pre">In this case, as I say we don’t really have labels per se, so I’m just going to give everything a label of zero just so we can use it with our existing infrastructure more easily.</p><pre name="0bcc" id="0bcc" class="graf graf--pre graf-after--p">fnames_full,label_arr_full,all_labels = folder_source(PATH, 'train')<br>fnames_full = ['/'.join(Path(fn).parts[-2:]) <strong class="markup--strong markup--pre-strong">for</strong> fn <strong class="markup--strong markup--pre-strong">in</strong> fnames_full]<br>list(zip(fnames_full[:5],label_arr_full[:5]))</pre><pre name="e032" id="e032" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">[('n01440764/n01440764_9627.JPEG', 0),<br> ('n01440764/n01440764_9609.JPEG', 0),<br> ('n01440764/n01440764_5176.JPEG', 0),<br> ('n01440764/n01440764_6936.JPEG', 0),<br> ('n01440764/n01440764_4005.JPEG', 0)]</em></pre><pre name="56be" id="56be" class="graf graf--pre graf-after--pre">all_labels[:5]</pre><pre name="ae3e" id="ae3e" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">['n01440764', 'n01443537', 'n01484850', 'n01491361', 'n01494475']</em></pre><p name="8c9c" id="8c9c" class="graf graf--p graf-after--pre">Now, because I’m pointing at a folder that contains all of ImageNet, I certainly don’t want to wait for all of ImageNet to finish to run an epoch. So here, I’m just, most of the time, I would set “keep percent” ( <code class="markup--code markup--p-code">keep_pct</code> ) to 1 or 2%. And then I just generate a bunch of random numbers and then I just keep those which are less than 0.02 and so that lets me quickly subsample my rows.</p><pre name="5e38" id="5e38" class="graf graf--pre graf-after--p">np.random.seed(42)<br># keep_pct = 1.<br><em class="markup--em markup--pre-em">keep_pct = 0.02</em><br>keeps = np.random.rand(len(fnames_full)) &lt; keep_pct<br>fnames = np.array(fnames_full, copy=<strong class="markup--strong markup--pre-strong">False</strong>)[keeps]<br>label_arr = np.array(label_arr_full, copy=<strong class="markup--strong markup--pre-strong">False</strong>)[keeps]</pre><p name="499f" id="499f" class="graf graf--p graf-after--pre">So we are going to use VGG16 [<a href="https://youtu.be/nG3tT31nPmQ?t=8m21s" data-href="https://youtu.be/nG3tT31nPmQ?t=8m21s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">8:21</a>] and VGG16 is something that we haven’t really looked at in this class but it’s a very simple model where we take our normal presumably 3 channel input, and we basically run it through a number of 3x3 convolutions, and then from time to time, we put it through a 2x2 maxpool and then we do a few more 3x3 convolutions, maxpool, so on so forth. And this is our backbone.</p><figure name="c65d" id="c65d" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_kj2sH_5R5tNvT7ajbHqXKw.png"></figure><p name="165f" id="165f" class="graf graf--p graf-after--figure">Then we don’t do an adaptive average pooling layer. After a few of these, we end up with this 7x7x512 grid as usual (or something similar). So rather than average pooling, we do something different which is we flatten the whole thing — so that spits out a very long vector of activations of size 7x7x512 if memory serves correctly. Then that gets fed into two fully connected layers each one of which has 4096 activations, and one more fully connected layer which has however many classes. So if you think about it, the weight matrix here, it’s HUGE 7x7x512x4096. It’s because of that weight matrix really that VGG went out of favor pretty quickly — because it takes a lot of memory and takes a lot of computation and it’s really slow. And there’s a lot of redundant stuff going on here because really those 512 activations are not that specific to which of those 7x7 grid cells they are in. But when you have this entire weight matrix here of every possible combination, it treats all of them uniquely. So that can also lead to generalization problems because there’s just a lot of weights and so forth.</p><figure name="7e65" id="7e65" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1__UB-iwca2SW15UhI8VLZ6g.png"></figure><p name="5ddf" id="5ddf" class="graf graf--p graf-after--figure">My view is that the approach that is used in every modern network which is here we do an adaptive average pooling (in Keras it’s known as a global average pooling, in fast.ai, we do an AdaptiveConcatPool) which spits it straight down to a 512 long activation [<a href="https://youtu.be/nG3tT31nPmQ?t=11m6s" data-href="https://youtu.be/nG3tT31nPmQ?t=11m6s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">11:06</a>]. I think that’s throwing away too much geometry. So to me, probably the correct answer is somewhere in between and will involve some kind of factored convolution or some kind tensor decomposition which maybe some of us can think about in the coming months. So for now, anyway, we’ve gone from one extreme which is the adaptive average pooling to the other extreme which is this huge flattened fully connected layer.</p><p name="e437" id="e437" class="graf graf--p graf-after--p">A couple of things which are interesting about VGG that make it still useful today [<a href="https://youtu.be/nG3tT31nPmQ?t=11m59s" data-href="https://youtu.be/nG3tT31nPmQ?t=11m59s" class="markup--anchor markup--p-anchor" rel="nofollow noopener noopener" target="_blank">11:59</a>]. The first one is that there’s more interesting layers going on here with most modern networks including the ResNet family, the very first layer generally is a 7x7 conv with stride 2 or something similar. Which means we throw away half the grid size straight away and so there is little opportunity to use the fine detail because we never do any computation with it. So that’s a bit of a problem for things like segmentation or super resolution models because the fine details matters. We actually want to restore it. Then the second problem is that the adaptive pooling layer entirely throws away the geometry in the last few sections which means that the rest of the model doesn’t really have as much interesting kind of learning that geometry as it otherwise might. Therefore for things which are dependent on position, any kind of localization based approach to anything that requires generative model is going to be less effective. So one of the things I’m hoping you are hearing as I describe this is that probably <span class="markup--quote markup--p-quote is-other" name="9887f0844031" data-creator-ids="eab3a535185e">none of the existing architectures are actually ideal. We can invent a new one</span>. Actually, I just tried inventing a new one over the week which was to take the VGG head and attach it to a ResNet backbone. Interestingly, I found I actually got a slightly better classifier than a normal ResNet but it also was something with a little bit more useful information in it. It took 5 or 10% longer to train but nothing worth worrying about. Maybe we could, in ResNet, replace this (7x7 conv stride 2) as we’ve talked about briefly before. This very early convolution with something more like an Inception stem which has a bit more computation. I think there’s definitely room for some nice little tweaks to these architectures so that we can build some models which are maybe more versatile. <span class="markup--quote markup--p-quote is-other" name="21ea327e4a82 anon_715b4d288832" data-creator-ids="eab3a535185e anon">At the moment, people tend to build architectures that just do one thing. They don’t really think what am I throwing away in terms of opportunity</span> because that’s how publishing works. You published “I’ve got state of the art of this one thing rather than you have created something that’s good at a lots of things.</p><p name="d054" id="d054" class="graf graf--p graf-after--p">For these reasons, we are going to use VGG today even though it’s ancient and it’s missing lots of great stuff [<a href="https://youtu.be/nG3tT31nPmQ?t=14m42s" data-href="https://youtu.be/nG3tT31nPmQ?t=14m42s" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">14:42</a>]. One thing we are going to do though is use a slightly more modern version which is a version of VGG where batch norm has been added after all the convolutions. In fast.ai when you ask for a VGG network, you always get the batch norm one because that’s basically always what you want. So this is VGG with batch norm. There is 16 and 19, the 19 is way bigger and heavier, and doesn’t really do any better, so no one really uses it.</p><pre name="2acc" id="2acc" class="graf graf--pre graf-after--p">arch = vgg16<br>sz_lr = 72</pre><p name="cd55" id="cd55" class="graf graf--p graf-after--pre">We are going to go from 72 by 72 LR (<code class="markup--code markup--p-code">sz_lr</code>: size low resolution) input. We are going to initially scale it up by times 2 with the batch size of 64 to get 2 * 72 so 144 by 144 output. That is going to be our stage one.</p><pre name="a195" id="a195" class="graf graf--pre graf-after--p">scale,bs = 2,64<br><em class="markup--em markup--pre-em"># scale,bs = 4,32</em><br>sz_hr = sz_lr*scale</pre><p name="f6fd" id="f6fd" class="graf graf--p graf-after--pre">We’ll create our own dataset for this and it’s very worthwhile looking inside the fastai.dataset module and seeing what’s there [<a href="https://youtu.be/nG3tT31nPmQ?t=15m45s" data-href="https://youtu.be/nG3tT31nPmQ?t=15m45s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">15:45</a>]. Because just about anything you’d want, we probably have something that’s almost what you want. So in this case, I want a dataset where my <em class="markup--em markup--p-em">x</em>’s are images and my <em class="markup--em markup--p-em">y</em>’s are also images. There’s already a files dataset we can inherit from where the <em class="markup--em markup--p-em">x</em>’s are images and then I just inherit from that and I just copied and pasted the <code class="markup--code markup--p-code">get_x</code> and turn that into <code class="markup--code markup--p-code">get_y</code> so it just opens an image. Now I’ve got something where the <em class="markup--em markup--p-em">x</em> is an image and the <em class="markup--em markup--p-em">y</em> is an image, and in both cases, what we’re passing in is an array of files names.</p><pre name="4140" id="4140" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">MatchedFilesDataset</strong>(FilesDataset):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, fnames, y, transform, path):<br>        self.y=y<br>        <strong class="markup--strong markup--pre-strong">assert</strong>(len(fnames)==len(y))<br>        super().__init__(fnames, transform, path)<br>    <strong class="markup--strong markup--pre-strong">def</strong> get_y(self, i): <br>        <strong class="markup--strong markup--pre-strong">return</strong> open_image(os.path.join(self.path, self.y[i]))<br>    <strong class="markup--strong markup--pre-strong">def</strong> get_c(self): <strong class="markup--strong markup--pre-strong">return</strong> 0</pre><p name="21b4" id="21b4" class="graf graf--p graf-after--pre">I’m going to do some data augmentation [<a href="https://youtu.be/nG3tT31nPmQ?t=16m32s" data-href="https://youtu.be/nG3tT31nPmQ?t=16m32s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">16:32</a>]. Obviously with all of ImageNet, we don’t really need it but this is mainly here for anybody who is using smaller datasets to make the most of it. <code class="markup--code markup--p-code">RandomDihedral</code> is referring to every possible 90 degree rotation plus optional left/right flipping so they are dihedral group of eight symmetries. Normally we don’t use this transformation for ImageNet pictures because you don’t normally flip dogs upside down but in this case, we are not trying to classify whether it’s a dog or a cat, we are just trying to keep the general structure of it. So actually every possible flip is a reasonably sensible thing to do for this problem.</p><pre name="64a6" id="64a6" class="graf graf--pre graf-after--p">aug_tfms = [RandomDihedral(tfm_y=TfmType.PIXEL)]</pre><p name="3cb1" id="3cb1" class="graf graf--p graf-after--pre">Create a validation set in the usual way [<a href="https://youtu.be/nG3tT31nPmQ?t=17m19s" data-href="https://youtu.be/nG3tT31nPmQ?t=17m19s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">17:19</a>]. You can see I’m using a few more slightly lower level functions — generally speaking, I just copy and paste them out of the fastai source code to find the bits I want. So here is the bit which takes an array of validation set indexes and one or more arrays of variables, and simply splits. In this case, this (<code class="markup--code markup--p-code">np.array(fnames)</code>) into a training and validation set, and this (the second <code class="markup--code markup--p-code">np.array(fnames)</code>) into a training and validation set to give us our <em class="markup--em markup--p-em">x</em>’s and our <em class="markup--em markup--p-em">y</em>’s. In this case, the <em class="markup--em markup--p-em">x</em> and the <em class="markup--em markup--p-em">y</em> are the same. Our input image and our output image are the same. We are going to use transformations to make one of them lower resolution. That’s why these are the same thing&nbsp;.</p><pre name="3283" id="3283" class="graf graf--pre graf-after--p">val_idxs = get_cv_idxs(len(fnames), val_pct=min(0.01/keep_pct, 0.1))<br>((val_x,trn_x),(val_y,trn_y)) = split_by_idx(val_idxs, <br>                                np.array(fnames), np.array(fnames))<br>len(val_x),len(trn_x)</pre><pre name="d882" id="d882" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(12811, 1268356)</em></pre><pre name="f19a" id="f19a" class="graf graf--pre graf-after--pre">img_fn = PATH/'train'/'n01558993'/'n01558993_9684.JPEG'</pre><p name="5c5e" id="5c5e" class="graf graf--p graf-after--pre">The next thing that we need to do is to create our transformations as per usual [<a href="https://youtu.be/nG3tT31nPmQ?t=18m13s" data-href="https://youtu.be/nG3tT31nPmQ?t=18m13s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">18:13</a>]. We are going to use <code class="markup--code markup--p-code">tfm_y</code> parameter like we did for bounding boxes but rather than use <code class="markup--code markup--p-code">TfmType.COORD</code> we are going to use <code class="markup--code markup--p-code">TfmType.PIXEL</code>. That tells our transformations framework that your <em class="markup--em markup--p-em">y</em> values are images with normal pixels in them, so anything you do to the <em class="markup--em markup--p-em">x</em>, you also need to do the same thing to the <em class="markup--em markup--p-em">y</em>. You need to make sure any data augmentation transformations you use have the same parameter as well.</p><pre name="e163" id="e163" class="graf graf--pre graf-after--p">tfms = tfms_from_model(arch, sz_lr, tfm_y=TfmType.PIXEL, <br>          aug_tfms=aug_tfms, sz_y=sz_hr)<br>datasets = ImageData.get_ds(MatchedFilesDataset, (trn_x,trn_y), <br>               (val_x,val_y), tfms, path=PATH_TRN)<br>md = ImageData(PATH, datasets, bs, num_workers=16, classes=<strong class="markup--strong markup--pre-strong">None</strong>)</pre><p name="bd93" id="bd93" class="graf graf--p graf-after--pre">You can see the possible transform types you got:</p><ul class="postList"><li name="a58c" id="a58c" class="graf graf--li graf-after--p">CLASS: classification which we are about to use the segmentation in the second half of today</li><li name="f819" id="f819" class="graf graf--li graf-after--li">COORD: coordinates — no transformation at all</li><li name="6560" id="6560" class="graf graf--li graf-after--li">PIXEL</li></ul><p name="e31c" id="e31c" class="graf graf--p graf-after--li">Once we have <code class="markup--code markup--p-code">Dataset</code> class and some <em class="markup--em markup--p-em">x</em> and <em class="markup--em markup--p-em">y</em> training and validation sets. There is a handy little method called get datasets (<code class="markup--code markup--p-code">get_ds</code>) which basically runs that constructor over all the different things that you have to return all the datasets you need in exactly the right format to pass to a ModelData constructor (in this case the <code class="markup--code markup--p-code">ImageData</code> constructor). So we are kind of going back under the covers of fastai a little bit and building it up from scratch. In the next few weeks, this will all be wrapped up and refactored into something that you can do in a single step in fastai. But the point of this class is to learn a bit about going under the covers.</p><p name="c3c1" id="c3c1" class="graf graf--p graf-after--p">Something we’ve briefly seen before is that when we take images in, we transform them not just with data augmentation but we also move the channel dimension up to the start, we subtract the mean divided by the standard deviation etc [<a href="https://youtu.be/nG3tT31nPmQ?t=20m8s" data-href="https://youtu.be/nG3tT31nPmQ?t=20m8s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">20:08</a>]. So if we want to be able to display those pictures that have come out of our datasets or data loaders, we need to de-normalize them. So the model data object’s (<code class="markup--code markup--p-code">md</code>) dataset (<code class="markup--code markup--p-code">val_ds</code>) has denorm function that knows how to do that. I’m just going to give that a short name for convenience:</p><pre name="2013" id="2013" class="graf graf--pre graf-after--p">denorm = md.val_ds.denorm</pre><p name="2cc4" id="2cc4" class="graf graf--p graf-after--pre">So now I’m going to create a function that can show an image from a dataset and if you pass in something saying this is a normalized image, then we’ll denorm it.</p><pre name="c3e6" id="c3e6" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> show_img(ims, idx, figsize=(5,5), normed=<strong class="markup--strong markup--pre-strong">True</strong>, ax=<strong class="markup--strong markup--pre-strong">None</strong>):<br>    <strong class="markup--strong markup--pre-strong">if</strong> ax <strong class="markup--strong markup--pre-strong">is</strong> <strong class="markup--strong markup--pre-strong">None</strong>: fig,ax = plt.subplots(figsize=figsize)<br>    <strong class="markup--strong markup--pre-strong">if</strong> normed: ims = denorm(ims)<br>    <strong class="markup--strong markup--pre-strong">else</strong>:      ims = np.rollaxis(to_np(ims),1,4)<br>    ax.imshow(np.clip(ims,0,1)[idx])<br>    ax.axis('off')</pre><pre name="4c65" id="4c65" class="graf graf--pre graf-after--pre">x,y = next(iter(md.val_dl))<br>x.size(),y.size()</pre><pre name="1382" id="1382" class="graf graf--pre graf-after--pre"><em class="markup--em markup--pre-em">(torch.Size([32, 3, 72, 72]), torch.Size([32, 3, 288, 288]))</em></pre><p name="4c2c" id="4c2c" class="graf graf--p graf-after--pre">You’ll see here we’ve passed in size low res (<code class="markup--code markup--p-code">sz_lr</code>) as our size for the transforms and size high res (<code class="markup--code markup--p-code">sz_hr</code>) as, this is something new, the size y parameter (<code class="markup--code markup--p-code">sz_y</code>) [<a href="https://youtu.be/nG3tT31nPmQ?t=20m58s" data-href="https://youtu.be/nG3tT31nPmQ?t=20m58s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">20:58</a>]. So the two bits are going to get different sizes.</p><figure name="7cd3" id="7cd3" class="graf graf--figure graf--layoutOutsetCenter graf-after--p" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_7-vIZ1E_I3mvI59kTj_sVQ.png"></figure><p name="7584" id="7584" class="graf graf--p graf-after--figure">Here you can see the two different resolutions of our <em class="markup--em markup--p-em">x</em> and our <em class="markup--em markup--p-em">y</em> for a whole bunch of fish.</p><pre name="fd45" id="fd45" class="graf graf--pre graf-after--p">idx=1<br>fig,axes = plt.subplots(1, 2, figsize=(9,5))<br>show_img(x,idx, ax=axes[0])<br>show_img(y,idx, ax=axes[1])</pre><figure name="7aa7" id="7aa7" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_vPyOJ9-D-s2gzRhraY21YA.png"></figure><p name="2b89" id="2b89" class="graf graf--p graf-after--figure">As per usual, <code class="markup--code markup--p-code">plt.subplots</code> to create our two plots and then we can just use the different axes that came back to put stuff next to each other.</p><pre name="6a1b" id="6a1b" class="graf graf--pre graf-after--p">batches = [next(iter(md.aug_dl)) <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(9)]</pre><p name="f6ce" id="f6ce" class="graf graf--p graf-after--pre">We can then have a look at a few different versions of the data transformation [<a href="https://youtu.be/nG3tT31nPmQ?t=21m37s" data-href="https://youtu.be/nG3tT31nPmQ?t=21m37s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">21:37</a>]. There you can see them being flipped in all different directions.</p><pre name="8960" id="8960" class="graf graf--pre graf-after--p">fig, axes = plt.subplots(3, 6, figsize=(18, 9))<br><strong class="markup--strong markup--pre-strong">for</strong> i,(x,y) <strong class="markup--strong markup--pre-strong">in</strong> enumerate(batches):<br>    show_img(x,idx, ax=axes.flat[i*2])<br>    show_img(y,idx, ax=axes.flat[i*2+1])</pre><figure name="d73b" id="d73b" class="graf graf--figure graf-after--pre"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_9OOex0WAIoQPqzT6SwvW8g.png"></figure><h4 name="1415" id="1415" class="graf graf--h4 graf-after--figure">Model [<a href="https://youtu.be/nG3tT31nPmQ?t=21m48s" data-href="https://youtu.be/nG3tT31nPmQ?t=21m48s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">21:48</a>]</h4><p name="ef39" id="ef39" class="graf graf--p graf-after--h4">Let’s create our model. We are going to have a small image coming in, and we want to have a big image coming out. So we need to do some computation between those two to calculate what the big image would look like. Essentially there’re two ways of doing that computation:</p><ul class="postList"><li name="c25a" id="c25a" class="graf graf--li graf-after--p">We could first of all do some upsampling and then do a few stride one layers to do lots of computation.</li><li name="af3a" id="af3a" class="graf graf--li graf-after--li">We could first do lots of stride one layers to do all the computation and then at the end do some upsampling.</li></ul><p name="9b8f" id="9b8f" class="graf graf--p graf-after--li">We are going to pick the second approach because we want to do lots of computation on something smaller because it’s much faster to do it that way. Also, all that computation we get to leverage during the upsampling process. Upsampling, we know a couple of possible ways to do that. We can use:</p><ul class="postList"><li name="55d6" id="55d6" class="graf graf--li graf-after--p">Transposed or fractionally strided convolutions</li><li name="2aa1" id="2aa1" class="graf graf--li graf-after--li">Nearest neighbor upsampling followed by a 1x1 conv</li></ul><p name="ff45" id="ff45" class="graf graf--p graf-after--li">And in “do lots of computation” section, we could just have a whole bunch of 3x3 convs. But in this case particular, it seems likely that ResNet blocks are going to be better because really the output and the input are very very similar. So we really want a flow through path that allows as little fussing around as possible except a minimal amount necessary to do our super resolution. If we use ResNet blocks, then they have an identity path already. So you can imagine those simple version where it does a bilinear sampling approach or something it could just go through identity block all the way through and then in the upsampling blocks, just learn to take the averages of the inputs and get something that’s not too terrible.</p><p name="fc59" id="fc59" class="graf graf--p graf-after--p">So that’s what we are going to do. We are going to create something with five ResNet blocks and then for each 2x scale up we have to do, we’ll have one upsampling block.</p><figure name="6523" id="6523" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_d6GkM4JtsJb3WHmTA5NLUg.png"></figure><p name="824c" id="824c" class="graf graf--p graf-after--figure">They are all going to consist of, as per usual, convolution layers possibly with activation functions after many of them [<a href="https://youtu.be/nG3tT31nPmQ?t=24m37s" data-href="https://youtu.be/nG3tT31nPmQ?t=24m37s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">24:37</a>]. I like to put my standard convolution block into a function so I can refactor it more easily. I won’t worry about passing in padding and just calculate it directly as kernel size over two.</p><pre name="e738" id="e738" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> conv(ni, nf, kernel_size=3, actn=<strong class="markup--strong markup--pre-strong">False</strong>):<br>    layers = [nn.Conv2d(ni, nf, kernel_size, <br>              padding=kernel_size//2)]<br>    <strong class="markup--strong markup--pre-strong">if</strong> actn: layers.append(nn.ReLU(<strong class="markup--strong markup--pre-strong">True</strong>))<br>    <strong class="markup--strong markup--pre-strong">return</strong> nn.Sequential(*layers)</pre><p name="621b" id="621b" class="graf graf--p graf-after--pre">One interesting thing about our little conv block is that there is no batch norm which is pretty unusual for ResNet type models.</p><figure name="aeae" id="aeae" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_rMC3ob6YdywFeTHcAruD_A.png"><figcaption class="imageCaption"><a href="https://arxiv.org/abs/1707.02921" data-href="https://arxiv.org/abs/1707.02921" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">https://arxiv.org/abs/1707.02921</a></figcaption></figure><p name="f7e1" id="f7e1" class="graf graf--p graf-after--figure">The reason there is no batch norm is because I’m stealing ideas from this fantastic recent paper which actually won a recent competition in super resolution performance. To see how good this paper is, SRResNet is the previous state of the art and what they’ve done here is they’ve zoomed way in to an upsampled mesh/fence. HR is the original. You can see in the previous best approach, there’s a whole lot of distortion and blurring going on. Or else, in their approach, it’s nearly perfect. So this paper was a really big step-up. They call their model EDSR ( Enhanced Deep Super-Resolution network) and they did two things differently to the previous standard approaches:</p><ol class="postList"><li name="6bee" id="6bee" class="graf graf--li graf-after--p">Take the ResNet blocks and throw away the batch norms. Why would they throw away the batch norm? The reason is because batch norm changes stuff and we want a nice straight through path that doesn’t change stuff. So the idea here is if you don’t want to fiddle with the input more than you have to, then don’t force it to have to calculate things like batch norm parameters — so throw away the batch norm.</li><li name="6c67" id="6c67" class="graf graf--li graf-after--li">Scaling factor (we will see shortly).</li></ol><pre name="99ed" id="99ed" class="graf graf--pre graf-after--li"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">ResSequential</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, layers, res_scale=1.0):<br>        super().__init__()<br>        self.res_scale = res_scale<br>        self.m = nn.Sequential(*layers)<br><br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x): <strong class="markup--strong markup--pre-strong">return</strong> x + self.m(x) * self.res_scale</pre><p name="e41d" id="e41d" class="graf graf--p graf-after--pre">So we are going to create a residual block containing two convolutions. As you see in their approach, they don’t even have a ReLU after their second conv. So that’s why I’ve only got activation on the first one.</p><pre name="064a" id="064a" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> res_block(nf):<br>    <strong class="markup--strong markup--pre-strong">return</strong> ResSequential(<br>        [conv(nf, nf, actn=<strong class="markup--strong markup--pre-strong">True</strong>), conv(nf, nf)],<br>        0.1)</pre><p name="df6b" id="df6b" class="graf graf--p graf-after--pre">A couple of interesting things here [<a href="https://youtu.be/nG3tT31nPmQ?t=27m10s" data-href="https://youtu.be/nG3tT31nPmQ?t=27m10s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">27:10</a>]. One is that this idea of having some kind of a main ResNet path (conv, ReLU, conv) and then turning that into a ReLU block by adding it back to the identity — it’s something we do so often that I factored it out into a tiny little module called ResSequential. It simply takes a bunch of layers that you want to put into your residual path, turns that into a sequential model, runs it, and then adds it back to the input. With this little module, we can now turn anything, like conv activation conv, into a ResNet block just by wrapping in ResSequential.</p><p name="79d6" id="79d6" class="graf graf--p graf-after--p">But that’s not quite all I’m doing because normally a Res block just has <code class="markup--code markup--p-code">x + self.m(x)</code> in its <code class="markup--code markup--p-code">forward</code>. But I’ve also got <code class="markup--code markup--p-code">* self.res_scale</code>. What’s <code class="markup--code markup--p-code">res_scale</code>? <code class="markup--code markup--p-code">res_scale</code> is the number 0.1. Why is it there? I’m not sure anybody quite knows. But the short answer is that the guy who invented batch norm also somewhat more recently did a paper in which he showed for (I think) the first time the ability to train ImageNet in under an hour. The way he did it was fire up lots and lots of machines and have them work in parallel to create really large batch sizes. Now generally when you increase the batch size by order <em class="markup--em markup--p-em">N</em>, you also increase the learning rate by order <em class="markup--em markup--p-em">N</em> to go with it. So generally a very large batch size training means very high learning rate training as well. He found that with these very large batch sizes of 8,000+ or even up to 32,000, at the start of training, <span class="markup--quote markup--p-quote is-other" name="a289def0e1ff" data-creator-ids="eab3a535185e">his activations would basicall y go straight to infinity</span>. And a lot of other people have found that. We actually found that when we were competing in DAWN bench both on the CIFAR and ImageNet competitions that we really struggled to make the most of even the eight GPUs that we were trying to take advantage of because of these challenges with these larger batch sizes and taking advantage of them. Something Christian found was that in the ResNet blocks, if he multiplied them by some number smaller than 1, something like&nbsp;.1 or&nbsp;.2, it really helped stabilize training at the start. That’s kind of weird because mathematically, it’s identical. Because obviously whatever I’m multiplying it by here, I could just scale the weights by the opposite amount and have the same number. But we are not dealing with abstract math — we are dealing with real optimization problems, different initializations, learning rates, and whatever else. So the problem of weights disappearing off into infinity, I guess generally is really about the discrete and finite nature of computers in practice partly. So often these kind of little tricks can make the difference.</p><p name="782c" id="782c" class="graf graf--p graf-after--p">In this case, we are just toning things down based on our initial initialization. So there are probably other ways to do this. For example, one approach from some folks at Nvidia called LARS which I briefly mentioned last week is an approach which uses discriminative learning rates calculated in real time. Basically looking at the ration between the gradients and the activations to scale learning rates by layer. So they found that they didn’t need this trick to scale up the batch sizes a lot. Maybe a different initialization would be all that’s necessary. The reason I mentioned this is not so much because I think a lot of you are likely to want to train on massive clusters of computers but rather that I think a lot of you want to train models quickly and that means using high learning rates and ideally getting super convergence. I think these kinds of tricks are the tricks that we’ll need to be able to get super convergence across more different architectures and so forth. Other than Leslie Smith, no one else is really working on super convergence other than some fastai students nowadays. So these kind of things about how do we train at very very high learning rates, we’re going to have to be the ones who figure it out because as far as I can tell, nobody else cares yet. So looking at the literature around training ImageNet in one hour, or more recently there’s now train ImageNet in 15 minutes, these papers actually, I think, have some of the tricks to allow us to train things at high learning rates. So here is one of them.</p><p name="e80e" id="e80e" class="graf graf--p graf-after--p">Interestingly, other than the train ImageNet in one hour paper, the only other place I’ve seen this mentioned was in this EDSR paper. It’s really cool because people who win competitions, I find them to be very pragmatic and well-read. They actually have to get things to work. So this paper describes an approach which actually worked better than anybody else’s approach and they did these pragmatic things like throw away batch norm and use this little scaling factor which almost nobody seems to know about. So that’s where&nbsp;.1 comes from.</p><pre name="9c34" id="9c34" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">def</strong> upsample(ni, nf, scale):<br>    layers = []<br>    <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(int(math.log(scale,2))):<br>        layers += [conv(ni, nf*4), nn.PixelShuffle(2)]<br>    <strong class="markup--strong markup--pre-strong">return</strong> nn.Sequential(*layers)</pre><p name="a5d2" id="a5d2" class="graf graf--p graf-after--pre">So basically our super-resolution ResNet (<code class="markup--code markup--p-code">SrResnet</code>) is going to do a convolution to go from our three channels to 64 channels just to richen up the space a little bit [<a href="https://youtu.be/nG3tT31nPmQ?t=33m25s" data-href="https://youtu.be/nG3tT31nPmQ?t=33m25s" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">33:25</a>]. Then also we’ve got actually 8 not 5 Res blocks. Remember, every one of these Res block is stride 1 so the grid size doesn’t change, the number of filters doesn’t change. It’s just 64 all the way through. We’ll do one more convolution, and then we’ll do our upsampling by however much scale we asked for. Then something I’ve added which is one batch norm here because it felt like it might be helpful just to scale the last layer. Then finally conv to go back to the three channels we want. So you can see that here’s lots and lots of computation and then a little bit of upsampling just like we described.</p><pre name="0e18" id="0e18" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">class</strong> <strong class="markup--strong markup--pre-strong">SrResnet</strong>(nn.Module):<br>    <strong class="markup--strong markup--pre-strong">def</strong> __init__(self, nf, scale):<br>        super().__init__()<br>        features = [conv(3, 64)]<br>        <strong class="markup--strong markup--pre-strong">for</strong> i <strong class="markup--strong markup--pre-strong">in</strong> range(8): features.append(res_block(64))<br>        features += [conv(64,64), upsample(64, 64, scale),<br>                     nn.BatchNorm2d(64),<br>                     conv(64, 3)]<br>        self.features = nn.Sequential(*features)<br>        <br>    <strong class="markup--strong markup--pre-strong">def</strong> forward(self, x): <strong class="markup--strong markup--pre-strong">return</strong> self.features(x)</pre><p name="f28f" id="f28f" class="graf graf--p graf-after--pre">Just to mention, as I’m tending to do now, this whole thing is done by creating a list with layers and then at the end, turning into a sequential model so my forward function is as simple as can be.</p><p name="0412" id="0412" class="graf graf--p graf-after--p">Here is our upsampling and upsampling is a bit interesting because it is not doing either of two things (transposed or fractionally strided convolutions or nearest neighbor upsampling followed by a 1x1 conv). So let’s talk a bit about upsampling.</p><figure name="8a1e" id="8a1e" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_he7T9_w-2Q2wo0jgno5Qfg.png"></figure><p name="8c42" id="8c42" class="graf graf--p graf-after--figure">Here is the picture from the paper (Perceptual Losses for Real-Time Style Transfer and Super Resolution). So they are saying “hey, our approach is so much better” but look at their approach. It’s got artifacts in it. These just pop up everywhere, don’t they. One of the reason for this is that they use transposed convolutions and we all know don’t use transposed convolutions.</p><figure name="d819" id="d819" class="graf graf--figure graf--layoutOutsetCenter graf-after--p" data-scroll="native"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_s9IHmwTn9La0u0M8omsd2w.png"></figure><p name="53f0" id="53f0" class="graf graf--p graf-after--figure">Here are transposed convolutions [<a href="https://youtu.be/nG3tT31nPmQ?t=35m39s" data-href="https://youtu.be/nG3tT31nPmQ?t=35m39s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">35:39</a>]. This is from this fantastic convolutional arithmetic paper that was shown also in the Theano docs. If we are going from (blue is the original image) 3x3 image up to a 5x5 image (6x6 if we added a layer of padding), then all a transpose convolution does is it uses a regular 3x3 conv but it sticks white zero pixels between every pair of pixels. That makes the input image bigger and when we run this convolution over it, therefore gives us a larger output. But that’s obviously stupid because when we get here, for example, of the nine pixels coming in, eight of them are zero. So we are just wasting a whole a lot of computation. On the other hand, if we are slightly off then four of our nine are non-zero. But yet, we only have one filter/kernel to use so it can’t change depending on how many zeros are coming in. So it has to be suitable for both and it’s just not possible so we end up with these artifacts.</p><figure name="e093" id="e093" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_afBXEvE8aOwzNjRNb5bt6Q.png"><figcaption class="imageCaption"><a href="http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html" data-href="http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html</a></figcaption></figure><p name="baa4" id="baa4" class="graf graf--p graf-after--figure">One approach we’ve learnt to make it a bit better is to not put white things here but instead to copy the pixel’s value to each of these three locations [<a href="https://youtu.be/nG3tT31nPmQ?t=36m53s" data-href="https://youtu.be/nG3tT31nPmQ?t=36m53s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">36:53</a>]. So that’s a nearest neighbor upsampling. That’s certainly a bit better, but it’s still pretty crappy because now when we get to these nine (as shown above), 4 of them are exactly the same number. And when we move across one, then now we’ve got a different situation entirely. So depending on where we are, in particular, if we are here, there’s going to be a lot less repetition:</p><figure name="b592" id="b592" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_uOVWwijHzuyf8IY4g6lKhQ.png"></figure><p name="c4a0" id="c4a0" class="graf graf--p graf-after--figure">So again, we have this problem where there’s wasted computation and too much structure in the data, and it’s going to lead to artifacts again. So upsampling is better than transposed convolutions — it’s better to copy them rather than replace them with zero. But it’s still not quite good enough.</p><p name="ed11" id="ed11" class="graf graf--p graf-after--p">So instead, we are going to do the pixel shuffle [<a href="https://youtu.be/nG3tT31nPmQ?t=37m56s" data-href="https://youtu.be/nG3tT31nPmQ?t=37m56s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">37:56</a>]. Pixel shuffle is an operation in this sub-pixel convolutional neural network and it’s a little bit mind-bending but it’s kind of fascinating.</p><figure name="b087" id="b087" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_Yj-niImdJg30IKlk0aBJFQ.png"><figcaption class="imageCaption"><a href="https://arxiv.org/abs/1609.05158" data-href="https://arxiv.org/abs/1609.05158" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--figure-strong">Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural&nbsp;Network</strong></a></figcaption></figure><p name="6ffc" id="6ffc" class="graf graf--p graf-after--figure">We start with our input, we go through some convolutions to create some feature maps for a while until eventually we get to layer <em class="markup--em markup--p-em">n[i-1]</em> which has n[i-1] feature maps. We are going to do another 3x3 conv and our goal here is to go from a 7x7 grid cell (we’re going to do a 3x3 upscaling) so we are going to go up to a 21x21 grid cell. So what’s another way we could do that? To make it simpler, let’s just pick one face/layer- so let’s take the top most filter and just do a convolution over that just to see what happens. What we are going to do is we are going to use a convolution where the kernel size (the number of filters) is nine times bigger than we need (strictly speaking). So if we needed 64 filters, we are actually going to do 64 times 9 filters. Why? Here, r is the the scale factor so 3² is 9, so here are the nine filters to cover one of these input layers/slices. But what we can do is we started with 7x7, and we turned it into 7x7x9. The output that we want is equal to 7 times 3 by 7 times 3. In other words, there is an equal number of pixels/activations here as there are activations in the previous step. So we can literally re-shuffle these 7x7x9 activations to create this 7x3 by 7x3 map [<a href="https://youtu.be/nG3tT31nPmQ?t=40m16s" data-href="https://youtu.be/nG3tT31nPmQ?t=40m16s" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">40:16</a>]. So what we are going to do is we’re going to take one little tube here (all the top left hand of each grid) and we are going to put the purple one up in the top left, then the blue one one to the right, and light blue one on to the right of that, then the slightly darker one in the middle of the far left, the green one in the middle, and so forth. So each of these nine cells in the top left, they are going to end up in the little 3x3 section of our grid. Then we are going to take (2, 1) and take all of those 9 and more them to these 3x3 part of the grid and so on. So we are going to end up having every one of these 7x7x9 activations inside the 7x3 by 7x3 image.</p><p name="4149" id="4149" class="graf graf--p graf-after--p">So the first thing to realize is yes of course this works under some definition of works because we have a learnable convolution here and it’s going to get some gradients which is going to do the best job it can of filling in the correct activation such that this output is the thing we want. So the first step is to realize there’s nothing particularly magical here. We can create any architecture we like. We can move things around anyhow we want to and our weights in the convolution will do their best to do all we asked. The real question is — is it good idea? Is this an easier thing for it to do and a more flexible thing for it to do than the transposed convolution or the upsampling followed by one by one conv? The short answer is yes it is, and the reason it’s better in short is that the convolution here is happening in the low resolution 7x7 space which is quite efficient. Or else, if we first of all upsampled and then did our conv then our conv would be happening in the 21 by 21 space which is a lot of computation. Furthermore, as we discussed, there’s a lot of replication and redundancy in the nearest neighbor upsample version. They actually show in this paper, in fact, I think they have a follow-up technical note where they provide some more mathematical details as to exactly what work is being done and show that the work really is more efficient this way. So that’s what we are going to do. For our upsampling, we have two steps:</p><ol class="postList"><li name="dd15" id="dd15" class="graf graf--li graf-after--p">3x3 conv with <em class="markup--em markup--li-em">r</em>² times more channels than we originally wanted</li><li name="c78c" id="c78c" class="graf graf--li graf-after--li">Then a pixel shuffle operation which moves everything in each grid cell into the little <em class="markup--em markup--li-em">r</em> by <em class="markup--em markup--li-em">r</em> grids that are located through out here.</li></ol><p name="a91b" id="a91b" class="graf graf--p graf-after--li">So here it is:</p><figure name="3341" id="3341" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_he7T9_w-2Q2wo0jgno5Qfg.png"></figure><p name="d039" id="d039" class="graf graf--p graf-after--figure">It’s one line of code. Here is a conv with number of in to number of filters out times four because we are doing a scale two upsample (2²=4). That’s our convolution and then here is our pixel shuffle it’s built into PyTorch. Pixel shuffle is the thing that moves each thing into its right spot. So that will upsample by a scale factor of 2. So we need to do that log base 2 scale times. If scale is four, then we’ll do two times to go two times two. So that’s what this upsample here does.</p><h4 name="1f51" id="1f51" class="graf graf--h4 graf-after--p">Checkerboard pattern&nbsp;[<a href="https://youtu.be/nG3tT31nPmQ?t=44m19s" data-href="https://youtu.be/nG3tT31nPmQ?t=44m19s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">44:19</a>]</h4><p name="a78d" id="a78d" class="graf graf--p graf-after--h4">Great. Guess what. That does not get rid of the checkerboard patterns. We still have checkerboard patterns. So I’m sure in great fury and frustration, the same team from Twitter I think this is back when they used to be a startup called magic pony that Twitter bought came back again with another paper saying okay, this time we’ve got rid of the checkerboard.</p><figure name="f7b0" id="f7b0" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="../img/1_GHf4mB-n_o6owwX6MoY_NQ.png"><figcaption class="imageCaption"><a href="https://arxiv.org/abs/1707.02937" data-href="https://arxiv.org/abs/1707.02937" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">https://arxiv.org/abs/1707.02937</a></figcaption></figure><p name="03b1" id="03b1" class="graf graf--p graf-after--figure">Why do we still have a checkerboard? The reason we still have a checkerboard even after doing this is that when we randomly initialize this convolutional kernel at the start, it means that each of these 9 pixels in this little 3x3 grid over here are going to be totally randomly different. But then the next set of 3 pixels will be randomly different to each other but will be very similar to their corresponding pixel in the previous 3x3 section. So we are going to have repeating 3x3 things all the way across. Then as we try to learn something better, it’s starting from this repeating 3x3 starting point which is not what we want. What we actually would want is for these 3x3 pixels to be the same to start with. To make these 3x3 pixels the same, we would need to make these 9 channels the same here for each filter. So the solution in this paper is very simple. It’s that when we initialize this convolution at start when we randomly initialize it, we don’t totally randomly initialize it. We randomly initialize one of the <em class="markup--em markup--p-em">r</em>² sets of channels then we copy that to the other <em class="markup--em markup--p-em">r</em>² so they are all the same. That way, initially, each of these 3x3 will be the same. So that is called ICNR and that’s what we are going to use in a moment.</p><h4 name="686e" id="686e" class="graf graf--h4 graf-after--p">Pixel loss&nbsp;[<a href="https://youtu.be/nG3tT31nPmQ?t=46m41s" data-href="https://youtu.be/nG3tT31nPmQ?t=46m41s" class="markup--anchor markup--h4-anchor" rel="nofollow noopener" target="_blank">46:41</a>]</h4></body></html>